[ {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\main\\java\\org\\apache\\storm\\loadgen\\LoadMetricsServer.java",
  "methodName" : "convert",
  "sourceCode" : "@VisibleForTesting\r\nstatic double convert(double value, TimeUnit from, TimeUnit target) {\r\n    if (target.compareTo(from) > 0) {\r\n        return value / from.convert(1, target);\r\n    }\r\n    return value * target.convert(1, from);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\test\\java\\org\\apache\\storm\\loadgen\\LoadCompConfTest.java",
  "methodName" : "scaleParallel",
  "sourceCode" : "@Test\r\npublic void scaleParallel() {\r\n    LoadCompConf orig = new LoadCompConf.Builder().withId(\"SOME_SPOUT\").withParallelism(1).withStream(new OutputStream(\"default\", new NormalDistStats(500.0, 100.0, 300.0, 600.0), false)).build();\r\n    assertEquals(500.0, orig.getAllEmittedAggregate(), 0.001);\r\n    LoadCompConf scaled = orig.scaleParallel(2);\r\n    //Parallelism is double\r\n    assertEquals(2, scaled.parallelism);\r\n    assertEquals(\"SOME_SPOUT\", scaled.id);\r\n    //But throughput is the same\r\n    assertEquals(500.0, scaled.getAllEmittedAggregate(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\test\\java\\org\\apache\\storm\\loadgen\\LoadCompConfTest.java",
  "methodName" : "scaleThroughput",
  "sourceCode" : "@Test\r\npublic void scaleThroughput() {\r\n    LoadCompConf orig = new LoadCompConf.Builder().withId(\"SOME_SPOUT\").withParallelism(1).withStream(new OutputStream(\"default\", new NormalDistStats(500.0, 100.0, 300.0, 600.0), false)).build();\r\n    assertEquals(500.0, orig.getAllEmittedAggregate(), 0.001);\r\n    LoadCompConf scaled = orig.scaleThroughput(2.0);\r\n    //Parallelism is same\r\n    assertEquals(1, scaled.parallelism);\r\n    assertEquals(\"SOME_SPOUT\", scaled.id);\r\n    //But throughput is the same\r\n    assertEquals(1000.0, scaled.getAllEmittedAggregate(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\test\\java\\org\\apache\\storm\\loadgen\\LoadMetricsServerTest.java",
  "methodName" : "convertTest",
  "sourceCode" : "@Test\r\npublic void convertTest() {\r\n    for (TimeUnit from : TimeUnit.values()) {\r\n        for (TimeUnit to : TimeUnit.values()) {\r\n            assertEquals(1.0, convert(convert(1.0, from, to), to, from), 0.0001, from + \" to \" + to + \" and back\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\test\\java\\org\\apache\\storm\\loadgen\\NormalDistStatsTest.java",
  "methodName" : "scaleBy",
  "sourceCode" : "@Test\r\npublic void scaleBy() {\r\n    NormalDistStats orig = new NormalDistStats(1.0, 0.5, 0.0, 2.0);\r\n    assertNDSEquals(orig, orig.scaleBy(1.0));\r\n    NormalDistStats expectedDouble = new NormalDistStats(2.0, 0.5, 1.0, 3.0);\r\n    assertNDSEquals(expectedDouble, orig.scaleBy(2.0));\r\n    NormalDistStats expectedHalf = new NormalDistStats(0.5, 0.5, 0.0, 1.5);\r\n    assertNDSEquals(expectedHalf, orig.scaleBy(0.5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-loadgen\\src\\test\\java\\org\\apache\\storm\\loadgen\\OutputStreamTest.java",
  "methodName" : "scaleThroughput",
  "sourceCode" : "@Test\r\npublic void scaleThroughput() {\r\n    OutputStream orig = new OutputStream(\"ID\", new NormalDistStats(100.0, 1.0, 99.0, 101.0), false);\r\n    OutputStream scaled = orig.scaleThroughput(2.0);\r\n    assertEquals(orig.id, scaled.id);\r\n    assertEquals(orig.areKeysSkewed, scaled.areKeysSkewed);\r\n    assertEquals(scaled.rate.mean, 200.0, 0.0001);\r\n    assertEquals(scaled.rate.stddev, 1.0, 0.0001);\r\n    assertEquals(scaled.rate.min, 199.0, 0.0001);\r\n    assertEquals(scaled.rate.max, 201.0, 0.0001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "negativeOrZeroTopNShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalTopN\")\r\npublic void negativeOrZeroTopNShouldThrowIAE(int topN) {\r\n    new IntermediateRankingsBolt(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "negativeOrZeroEmitFrequencyShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalEmitFrequency\")\r\npublic void negativeOrZeroEmitFrequencyShouldThrowIAE(int emitFrequencyInSeconds) {\r\n    new IntermediateRankingsBolt(ANY_TOPN, emitFrequencyInSeconds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "positiveTopNShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalTopN\")\r\npublic void positiveTopNShouldBeOk(int topN) {\r\n    new IntermediateRankingsBolt(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "positiveEmitFrequencyShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalEmitFrequency\")\r\npublic void positiveEmitFrequencyShouldBeOk(int emitFrequencyInSeconds) {\r\n    new IntermediateRankingsBolt(ANY_TOPN, emitFrequencyInSeconds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "shouldEmitSomethingIfTickTupleIsReceived",
  "sourceCode" : "@Test\r\npublic void shouldEmitSomethingIfTickTupleIsReceived() {\r\n    // given\r\n    Tuple tickTuple = MockTupleHelpers.mockTickTuple();\r\n    BasicOutputCollector collector = mock(BasicOutputCollector.class);\r\n    IntermediateRankingsBolt bolt = new IntermediateRankingsBolt();\r\n    // when\r\n    bolt.execute(tickTuple, collector);\r\n    // then\r\n    // verifyNoInteractions(collector);\r\n    verify(collector).emit(any(Values.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "shouldEmitNothingIfNormalTupleIsReceived",
  "sourceCode" : "@Test\r\npublic void shouldEmitNothingIfNormalTupleIsReceived() {\r\n    // given\r\n    Tuple normalTuple = mockRankableTuple(ANY_OBJECT, ANY_COUNT);\r\n    BasicOutputCollector collector = mock(BasicOutputCollector.class);\r\n    IntermediateRankingsBolt bolt = new IntermediateRankingsBolt();\r\n    // when\r\n    bolt.execute(normalTuple, collector);\r\n    // then\r\n    verifyNoInteractions(collector);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "shouldDeclareOutputFields",
  "sourceCode" : "@Test\r\npublic void shouldDeclareOutputFields() {\r\n    // given\r\n    OutputFieldsDeclarer declarer = mock(OutputFieldsDeclarer.class);\r\n    IntermediateRankingsBolt bolt = new IntermediateRankingsBolt();\r\n    // when\r\n    bolt.declareOutputFields(declarer);\r\n    // then\r\n    verify(declarer, times(1)).declare(any(Fields.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\IntermediateRankingsBoltTest.java",
  "methodName" : "shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue",
  "sourceCode" : "@Test\r\npublic void shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue() {\r\n    // given\r\n    IntermediateRankingsBolt bolt = new IntermediateRankingsBolt();\r\n    // when\r\n    Map<String, Object> componentConfig = bolt.getComponentConfiguration();\r\n    // then\r\n    assertThat(componentConfig).containsKey(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    Integer emitFrequencyInSeconds = (Integer) componentConfig.get(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    assertThat(emitFrequencyInSeconds).isGreaterThan(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\RollingCountBoltTest.java",
  "methodName" : "shouldEmitNothingIfNoObjectHasBeenCountedYetAndTickTupleIsReceived",
  "sourceCode" : "@SuppressWarnings(\"rawtypes\")\r\n@Test\r\npublic void shouldEmitNothingIfNoObjectHasBeenCountedYetAndTickTupleIsReceived() {\r\n    // given\r\n    Tuple tickTuple = MockTupleHelpers.mockTickTuple();\r\n    RollingCountBolt bolt = new RollingCountBolt();\r\n    Map<String, Object> conf = mock(Map.class);\r\n    TopologyContext context = mock(TopologyContext.class);\r\n    OutputCollector collector = mock(OutputCollector.class);\r\n    bolt.prepare(conf, context, collector);\r\n    // when\r\n    bolt.execute(tickTuple);\r\n    // then\r\n    verifyNoInteractions(collector);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\RollingCountBoltTest.java",
  "methodName" : "shouldEmitSomethingIfAtLeastOneObjectWasCountedAndTickTupleIsReceived",
  "sourceCode" : "@SuppressWarnings(\"rawtypes\")\r\n@Test\r\npublic void shouldEmitSomethingIfAtLeastOneObjectWasCountedAndTickTupleIsReceived() {\r\n    // given\r\n    Tuple normalTuple = mockNormalTuple(new Object());\r\n    Tuple tickTuple = MockTupleHelpers.mockTickTuple();\r\n    RollingCountBolt bolt = new RollingCountBolt();\r\n    Map<String, Object> conf = mock(Map.class);\r\n    TopologyContext context = mock(TopologyContext.class);\r\n    OutputCollector collector = mock(OutputCollector.class);\r\n    bolt.prepare(conf, context, collector);\r\n    // when\r\n    bolt.execute(normalTuple);\r\n    bolt.execute(tickTuple);\r\n    // then\r\n    verify(collector).emit(any(Values.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\RollingCountBoltTest.java",
  "methodName" : "shouldDeclareOutputFields",
  "sourceCode" : "@Test\r\npublic void shouldDeclareOutputFields() {\r\n    // given\r\n    OutputFieldsDeclarer declarer = mock(OutputFieldsDeclarer.class);\r\n    RollingCountBolt bolt = new RollingCountBolt();\r\n    // when\r\n    bolt.declareOutputFields(declarer);\r\n    // then\r\n    verify(declarer, times(1)).declare(any(Fields.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\RollingCountBoltTest.java",
  "methodName" : "shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue",
  "sourceCode" : "@Test\r\npublic void shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue() {\r\n    // given\r\n    RollingCountBolt bolt = new RollingCountBolt();\r\n    // when\r\n    Map<String, Object> componentConfig = bolt.getComponentConfiguration();\r\n    // then\r\n    assertThat(componentConfig).containsKey(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    Integer emitFrequencyInSeconds = (Integer) componentConfig.get(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    assertThat(emitFrequencyInSeconds).isGreaterThan(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "negativeOrZeroTopNShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalTopN\")\r\npublic void negativeOrZeroTopNShouldThrowIAE(int topN) {\r\n    new TotalRankingsBolt(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "negativeOrZeroEmitFrequencyShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalEmitFrequency\")\r\npublic void negativeOrZeroEmitFrequencyShouldThrowIAE(int emitFrequencyInSeconds) {\r\n    new TotalRankingsBolt(ANY_TOPN, emitFrequencyInSeconds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "positiveTopNShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalTopN\")\r\npublic void positiveTopNShouldBeOk(int topN) {\r\n    new TotalRankingsBolt(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "positiveEmitFrequencyShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalEmitFrequency\")\r\npublic void positiveEmitFrequencyShouldBeOk(int emitFrequencyInSeconds) {\r\n    new TotalRankingsBolt(ANY_TOPN, emitFrequencyInSeconds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "shouldEmitSomethingIfTickTupleIsReceived",
  "sourceCode" : "@Test\r\npublic void shouldEmitSomethingIfTickTupleIsReceived() {\r\n    // given\r\n    Tuple tickTuple = MockTupleHelpers.mockTickTuple();\r\n    BasicOutputCollector collector = mock(BasicOutputCollector.class);\r\n    TotalRankingsBolt bolt = new TotalRankingsBolt();\r\n    // when\r\n    bolt.execute(tickTuple, collector);\r\n    // then\r\n    // verifyNoInteractions(collector);\r\n    verify(collector).emit(any(Values.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "shouldEmitNothingIfNormalTupleIsReceived",
  "sourceCode" : "@Test\r\npublic void shouldEmitNothingIfNormalTupleIsReceived() {\r\n    // given\r\n    Tuple normalTuple = mockRankingsTuple(ANY_OBJECT, ANY_COUNT);\r\n    BasicOutputCollector collector = mock(BasicOutputCollector.class);\r\n    TotalRankingsBolt bolt = new TotalRankingsBolt();\r\n    // when\r\n    bolt.execute(normalTuple, collector);\r\n    // then\r\n    verifyNoInteractions(collector);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "shouldDeclareOutputFields",
  "sourceCode" : "@Test\r\npublic void shouldDeclareOutputFields() {\r\n    // given\r\n    OutputFieldsDeclarer declarer = mock(OutputFieldsDeclarer.class);\r\n    TotalRankingsBolt bolt = new TotalRankingsBolt();\r\n    // when\r\n    bolt.declareOutputFields(declarer);\r\n    // then\r\n    verify(declarer, times(1)).declare(any(Fields.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\bolt\\TotalRankingsBoltTest.java",
  "methodName" : "shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue",
  "sourceCode" : "@Test\r\npublic void shouldSetTickTupleFrequencyInComponentConfigurationToNonZeroValue() {\r\n    // given\r\n    TotalRankingsBolt bolt = new TotalRankingsBolt();\r\n    // when\r\n    Map<String, Object> componentConfig = bolt.getComponentConfiguration();\r\n    // then\r\n    assertThat(componentConfig).containsKey(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    Integer emitFrequencyInSeconds = (Integer) componentConfig.get(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS);\r\n    assertThat(emitFrequencyInSeconds).isGreaterThan(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\NthLastModifiedTimeTrackerTest.java",
  "methodName" : "negativeOrZeroNumTimesToTrackShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalNumTimesData\")\r\npublic void negativeOrZeroNumTimesToTrackShouldThrowIAE(int numTimesToTrack) {\r\n    new NthLastModifiedTimeTracker(numTimesToTrack);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\NthLastModifiedTimeTrackerTest.java",
  "methodName" : "positiveNumTimesToTrackShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalNumTimesData\")\r\npublic void positiveNumTimesToTrackShouldBeOk(int numTimesToTrack) {\r\n    new NthLastModifiedTimeTracker(numTimesToTrack);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\NthLastModifiedTimeTrackerTest.java",
  "methodName" : "shouldReturnCorrectModifiedTimeEvenWhenNotYetMarkedAsModified",
  "sourceCode" : "@Test(dataProvider = \"whenNotYetMarkedAsModifiedData\")\r\npublic void shouldReturnCorrectModifiedTimeEvenWhenNotYetMarkedAsModified(int secondsToAdvance) {\r\n    // given\r\n    try (SimulatedTime t = new SimulatedTime()) {\r\n        NthLastModifiedTimeTracker tracker = new NthLastModifiedTimeTracker(ANY_NUM_TIMES_TO_TRACK);\r\n        // when\r\n        Time.advanceTimeSecs(secondsToAdvance);\r\n        int seconds = tracker.secondsSinceOldestModification();\r\n        // then\r\n        assertThat(seconds).isEqualTo(secondsToAdvance);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\NthLastModifiedTimeTrackerTest.java",
  "methodName" : "shouldReturnCorrectModifiedTimeWhenMarkedAsModified",
  "sourceCode" : "@Test(dataProvider = \"simulatedTrackerIterations\")\r\npublic void shouldReturnCorrectModifiedTimeWhenMarkedAsModified(int numTimesToTrack, int[] secondsToAdvancePerIteration, int[] expLastModifiedTimes) {\r\n    // given\r\n    try (SimulatedTime t = new SimulatedTime()) {\r\n        NthLastModifiedTimeTracker tracker = new NthLastModifiedTimeTracker(numTimesToTrack);\r\n        int[] modifiedTimes = new int[expLastModifiedTimes.length];\r\n        // when\r\n        int i = 0;\r\n        for (int secondsToAdvance : secondsToAdvancePerIteration) {\r\n            Time.advanceTimeSecs(secondsToAdvance);\r\n            tracker.markAsModified();\r\n            modifiedTimes[i] = tracker.secondsSinceOldestModification();\r\n            i++;\r\n        }\r\n        // then\r\n        assertThat(modifiedTimes).isEqualTo(expLastModifiedTimes);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "constructorWithNullObjectAndNoFieldsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class)\r\npublic void constructorWithNullObjectAndNoFieldsShouldThrowIAE() {\r\n    new RankableObjectWithFields(null, ANY_COUNT);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "constructorWithNullObjectAndFieldsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class)\r\npublic void constructorWithNullObjectAndFieldsShouldThrowIAE() {\r\n    Object someAdditionalField = new Object();\r\n    new RankableObjectWithFields(null, ANY_COUNT, someAdditionalField);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "constructorWithNegativeCountAndNoFieldsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class)\r\npublic void constructorWithNegativeCountAndNoFieldsShouldThrowIAE() {\r\n    new RankableObjectWithFields(ANY_OBJECT, -1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "constructorWithNegativeCountAndFieldsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class)\r\npublic void constructorWithNegativeCountAndFieldsShouldThrowIAE() {\r\n    Object someAdditionalField = new Object();\r\n    new RankableObjectWithFields(ANY_OBJECT, -1, someAdditionalField);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldBeEqualToItself",
  "sourceCode" : "@Test\r\npublic void shouldBeEqualToItself() {\r\n    RankableObjectWithFields r = new RankableObjectWithFields(ANY_OBJECT, ANY_COUNT);\r\n    assertThat(r).isEqualTo(r);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldNotBeEqualToInstancesOfOtherClasses",
  "sourceCode" : "@Test(dataProvider = \"otherClassesData\")\r\npublic void shouldNotBeEqualToInstancesOfOtherClasses(Object notARankable) {\r\n    RankableObjectWithFields r = new RankableObjectWithFields(ANY_OBJECT, ANY_COUNT);\r\n    assertFalse(r.equals(notARankable), r + \" is equal to \" + notARankable + \" but it should not be\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldNotBeEqualToFalseDuplicates",
  "sourceCode" : "@Test(dataProvider = \"falseDuplicatesData\")\r\npublic void shouldNotBeEqualToFalseDuplicates(RankableObjectWithFields r, RankableObjectWithFields falseDuplicate) {\r\n    assertFalse(r.equals(falseDuplicate), r + \" is equal to \" + falseDuplicate + \" but it should not be\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldHaveDifferentHashCodeThanFalseDuplicates",
  "sourceCode" : "@Test(dataProvider = \"falseDuplicatesData\")\r\npublic void shouldHaveDifferentHashCodeThanFalseDuplicates(RankableObjectWithFields r, RankableObjectWithFields falseDuplicate) {\r\n    assertThat(r.hashCode()).isNotEqualTo(falseDuplicate.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldBeEqualToTrueDuplicates",
  "sourceCode" : "@Test(dataProvider = \"trueDuplicatesData\")\r\npublic void shouldBeEqualToTrueDuplicates(RankableObjectWithFields r, RankableObjectWithFields trueDuplicate) {\r\n    assertTrue(r.equals(trueDuplicate), r + \" is not equal to \" + trueDuplicate + \" but it should be\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldHaveSameHashCodeAsTrueDuplicates",
  "sourceCode" : "@Test(dataProvider = \"trueDuplicatesData\")\r\npublic void shouldHaveSameHashCodeAsTrueDuplicates(RankableObjectWithFields r, RankableObjectWithFields trueDuplicate) {\r\n    assertThat(r.hashCode()).isEqualTo(trueDuplicate.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "verifyCompareTo",
  "sourceCode" : "@Test(dataProvider = \"compareToData\")\r\npublic void verifyCompareTo(RankableObjectWithFields first, RankableObjectWithFields second, int expCompareToValue) {\r\n    assertThat(first.compareTo(second)).isEqualTo(expCompareToValue);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "toStringShouldContainStringRepresentationsOfObjectAndCount",
  "sourceCode" : "@Test(dataProvider = \"toStringData\")\r\npublic void toStringShouldContainStringRepresentationsOfObjectAndCount(Object obj, long count) {\r\n    // given\r\n    RankableObjectWithFields r = new RankableObjectWithFields(obj, count);\r\n    // when\r\n    String strRepresentation = r.toString();\r\n    // then\r\n    assertThat(strRepresentation).contains(obj.toString()).contains(\"\" + count);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldReturnTheObject",
  "sourceCode" : "@Test\r\npublic void shouldReturnTheObject() {\r\n    // given\r\n    RankableObjectWithFields r = new RankableObjectWithFields(ANY_OBJECT, ANY_COUNT, ANY_FIELD);\r\n    // when\r\n    Object obj = r.getObject();\r\n    // then\r\n    assertThat(obj).isEqualTo(ANY_OBJECT);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldReturnTheCount",
  "sourceCode" : "@Test\r\npublic void shouldReturnTheCount() {\r\n    // given\r\n    RankableObjectWithFields r = new RankableObjectWithFields(ANY_OBJECT, ANY_COUNT, ANY_FIELD);\r\n    // when\r\n    long count = r.getCount();\r\n    // then\r\n    assertThat(count).isEqualTo(ANY_COUNT);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldReturnTheFields",
  "sourceCode" : "@Test(dataProvider = \"fieldsData\")\r\npublic void shouldReturnTheFields(Object obj, long count, Object[] fields) {\r\n    // given\r\n    RankableObjectWithFields r = new RankableObjectWithFields(obj, count, fields);\r\n    // when\r\n    List<Object> actualFields = r.getFields();\r\n    // then\r\n    assertThat(actualFields).isEqualTo(Lists.newArrayList(fields));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "fieldsShouldBeImmutable",
  "sourceCode" : "@Test(expectedExceptions = UnsupportedOperationException.class)\r\npublic void fieldsShouldBeImmutable() {\r\n    // given\r\n    RankableObjectWithFields r = new RankableObjectWithFields(ANY_OBJECT, ANY_COUNT, ANY_FIELD);\r\n    // when\r\n    List<Object> fields = r.getFields();\r\n    // try to modify the list, which should fail\r\n    fields.remove(0);\r\n    // then (exception)\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "shouldCreateRankableObjectFromTuple",
  "sourceCode" : "@Test\r\npublic void shouldCreateRankableObjectFromTuple() {\r\n    // given\r\n    Tuple tuple = mock(Tuple.class);\r\n    List<Object> tupleValues = Lists.newArrayList(ANY_OBJECT, ANY_COUNT, ANY_FIELD);\r\n    when(tuple.getValues()).thenReturn(tupleValues);\r\n    // when\r\n    RankableObjectWithFields r = RankableObjectWithFields.from(tuple);\r\n    // then\r\n    assertThat(r.getObject()).isEqualTo(ANY_OBJECT);\r\n    assertThat(r.getCount()).isEqualTo(ANY_COUNT);\r\n    List<Object> fields = new ArrayList<>();\r\n    fields.add(ANY_FIELD);\r\n    assertThat(r.getFields()).isEqualTo(fields);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankableObjectWithFieldsTest.java",
  "methodName" : "copyShouldReturnCopy",
  "sourceCode" : "// TODO: What would be a good test to ensure that RankableObjectWithFields is at least somewhat defensively copied?\r\n//       The contract of Rankable#copy() returns a Rankable value, not a RankableObjectWithFields.\r\n@Test(dataProvider = \"copyData\")\r\npublic void copyShouldReturnCopy(RankableObjectWithFields original) {\r\n    // given\r\n    // when\r\n    Rankable copy = original.copy();\r\n    // then\r\n    assertThat(copy.getObject()).isEqualTo(original.getObject());\r\n    assertThat(copy.getCount()).isEqualTo(original.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "constructorWithNegativeOrZeroTopNShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalTopNData\")\r\npublic void constructorWithNegativeOrZeroTopNShouldThrowIAE(int topN) {\r\n    new Rankings(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "copyConstructorShouldReturnCopy",
  "sourceCode" : "@Test(dataProvider = \"copyRankingsData\")\r\npublic void copyConstructorShouldReturnCopy(int topN, List<Rankable> rankables) {\r\n    // given\r\n    Rankings rankings = new Rankings(topN);\r\n    for (Rankable r : rankables) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // when\r\n    Rankings copy = new Rankings(rankings);\r\n    // then\r\n    assertThat(copy.maxSize()).isEqualTo(rankings.maxSize());\r\n    assertThat(copy.getRankings()).isEqualTo(rankings.getRankings());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "copyConstructorShouldReturnDefensiveCopy",
  "sourceCode" : "@Test(dataProvider = \"defensiveCopyRankingsData\")\r\npublic void copyConstructorShouldReturnDefensiveCopy(int topN, List<Rankable> rankables, List<Rankable> changes) {\r\n    // given\r\n    Rankings original = new Rankings(topN);\r\n    for (Rankable r : rankables) {\r\n        original.updateWith(r);\r\n    }\r\n    int expSize = original.size();\r\n    List<Rankable> expRankings = original.getRankings();\r\n    // when\r\n    Rankings copy = new Rankings(original);\r\n    for (Rankable r : changes) {\r\n        copy.updateWith(r);\r\n    }\r\n    // then\r\n    assertThat(original.size()).isEqualTo(expSize);\r\n    assertThat(original.getRankings()).isEqualTo(expRankings);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "constructorWithPositiveTopNShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalTopNData\")\r\npublic void constructorWithPositiveTopNShouldBeOk(int topN) {\r\n    // given/when\r\n    Rankings rankings = new Rankings(topN);\r\n    // then\r\n    assertThat(rankings.maxSize()).isEqualTo(topN);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldHaveDefaultConstructor",
  "sourceCode" : "@Test\r\npublic void shouldHaveDefaultConstructor() {\r\n    new Rankings();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "defaultConstructorShouldSetPositiveTopN",
  "sourceCode" : "@Test\r\npublic void defaultConstructorShouldSetPositiveTopN() {\r\n    // given/when\r\n    Rankings rankings = new Rankings();\r\n    // then\r\n    assertThat(rankings.maxSize()).isGreaterThan(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "sizeOfRankingsShouldNotGrowBeyondTopN",
  "sourceCode" : "@Test(dataProvider = \"rankingsGrowData\")\r\npublic void sizeOfRankingsShouldNotGrowBeyondTopN(int topN, List<Rankable> rankables) {\r\n    // sanity check of the provided test data\r\n    assertThat(rankables.size()).overridingErrorMessage(\"The supplied test data is not correct: the number of rankables <%d> should be greater than <%d>\", rankables.size(), topN).isGreaterThan(topN);\r\n    // given\r\n    Rankings rankings = new Rankings(topN);\r\n    // when\r\n    for (Rankable r : rankables) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // then\r\n    assertThat(rankings.size()).isLessThanOrEqualTo(rankings.maxSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldCorrectlyRankWhenUpdatedWithRankables",
  "sourceCode" : "@Test(dataProvider = \"simulatedRankingsData\")\r\npublic void shouldCorrectlyRankWhenUpdatedWithRankables(List<Rankable> unsorted, List<Rankable> expSorted) {\r\n    // given\r\n    Rankings rankings = new Rankings(unsorted.size());\r\n    // when\r\n    for (Rankable r : unsorted) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // then\r\n    assertThat(rankings.getRankings()).isEqualTo(expSorted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldCorrectlyRankWhenEmptyAndUpdatedWithOtherRankings",
  "sourceCode" : "@Test(dataProvider = \"simulatedRankingsData\")\r\npublic void shouldCorrectlyRankWhenEmptyAndUpdatedWithOtherRankings(List<Rankable> unsorted, List<Rankable> expSorted) {\r\n    // given\r\n    Rankings rankings = new Rankings(unsorted.size());\r\n    Rankings otherRankings = new Rankings(rankings.maxSize());\r\n    for (Rankable r : unsorted) {\r\n        otherRankings.updateWith(r);\r\n    }\r\n    // when\r\n    rankings.updateWith(otherRankings);\r\n    // then\r\n    assertThat(rankings.getRankings()).isEqualTo(expSorted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldCorrectlyRankWhenUpdatedWithEmptyOtherRankings",
  "sourceCode" : "@Test(dataProvider = \"simulatedRankingsData\")\r\npublic void shouldCorrectlyRankWhenUpdatedWithEmptyOtherRankings(List<Rankable> unsorted, List<Rankable> expSorted) {\r\n    // given\r\n    Rankings rankings = new Rankings(unsorted.size());\r\n    for (Rankable r : unsorted) {\r\n        rankings.updateWith(r);\r\n    }\r\n    Rankings emptyRankings = new Rankings(ANY_TOPN);\r\n    // when\r\n    rankings.updateWith(emptyRankings);\r\n    // then\r\n    assertThat(rankings.getRankings()).isEqualTo(expSorted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldCorrectlyRankWhenNotEmptyAndUpdatedWithOtherRankings",
  "sourceCode" : "@Test(dataProvider = \"simulatedRankingsAndOtherRankingsData\")\r\npublic void shouldCorrectlyRankWhenNotEmptyAndUpdatedWithOtherRankings(List<Rankable> unsorted, List<Rankable> unsortedForOtherRankings, List<Rankable> expSorted) {\r\n    // given\r\n    Rankings rankings = new Rankings(expSorted.size());\r\n    for (Rankable r : unsorted) {\r\n        rankings.updateWith(r);\r\n    }\r\n    Rankings otherRankings = new Rankings(unsortedForOtherRankings.size());\r\n    for (Rankable r : unsortedForOtherRankings) {\r\n        otherRankings.updateWith(r);\r\n    }\r\n    // when\r\n    rankings.updateWith(otherRankings);\r\n    // then\r\n    assertThat(rankings.getRankings()).isEqualTo(expSorted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldNotRankDuplicateObjectsMoreThanOnce",
  "sourceCode" : "@Test(dataProvider = \"duplicatesData\")\r\npublic void shouldNotRankDuplicateObjectsMoreThanOnce(List<Rankable> duplicates) {\r\n    // given\r\n    Rankings rankings = new Rankings(duplicates.size());\r\n    // when\r\n    for (Rankable r : duplicates) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // then\r\n    assertThat(rankings.size()).isEqualTo(1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "shouldRemoveZeroCounts",
  "sourceCode" : "@Test(dataProvider = \"removeZeroRankingsData\")\r\npublic void shouldRemoveZeroCounts(List<Rankable> unsorted, List<Rankable> expSorted) {\r\n    // given\r\n    Rankings rankings = new Rankings(unsorted.size());\r\n    for (Rankable r : unsorted) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // when\r\n    rankings.pruneZeroCounts();\r\n    // then\r\n    assertThat(rankings.getRankings()).isEqualTo(expSorted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "updatingWithNewRankablesShouldBeThreadSafe",
  "sourceCode" : "@Test\r\npublic void updatingWithNewRankablesShouldBeThreadSafe() throws InterruptedException {\r\n    // given\r\n    final List<Rankable> entries = ImmutableList.of(A, B, C, D);\r\n    final Rankings rankings = new Rankings(entries.size());\r\n    // We are capturing exceptions thrown in Blitzer's child threads into this data structure so that we can properly\r\n    // pass/fail this test.  The reason is that Blitzer doesn't report exceptions, which is a known bug in Blitzer\r\n    // (JMOCK-263).  See https://github.com/jmock-developers/jmock-library/issues/22 for more information.\r\n    final List<Exception> exceptions = Lists.newArrayList();\r\n    Blitzer blitzer = new Blitzer(1000);\r\n    // when\r\n    blitzer.blitz(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            for (Rankable r : entries) {\r\n                try {\r\n                    rankings.updateWith(r);\r\n                } catch (RuntimeException e) {\r\n                    synchronized (exceptions) {\r\n                        exceptions.add(e);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    });\r\n    blitzer.shutdown();\r\n    // then\r\n    //\r\n    if (!exceptions.isEmpty()) {\r\n        for (Exception e : exceptions) {\r\n            System.err.println(Throwables.getStackTraceAsString(e));\r\n        }\r\n    }\r\n    assertThat(exceptions).isEmpty();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "copyShouldReturnCopy",
  "sourceCode" : "@Test(dataProvider = \"copyRankingsData\")\r\npublic void copyShouldReturnCopy(int topN, List<Rankable> rankables) {\r\n    // given\r\n    Rankings rankings = new Rankings(topN);\r\n    for (Rankable r : rankables) {\r\n        rankings.updateWith(r);\r\n    }\r\n    // when\r\n    Rankings copy = rankings.copy();\r\n    // then\r\n    assertThat(copy.maxSize()).isEqualTo(rankings.maxSize());\r\n    assertThat(copy.getRankings()).isEqualTo(rankings.getRankings());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\RankingsTest.java",
  "methodName" : "copyShouldReturnDefensiveCopy",
  "sourceCode" : "@Test(dataProvider = \"defensiveCopyRankingsData\")\r\npublic void copyShouldReturnDefensiveCopy(int topN, List<Rankable> rankables, List<Rankable> changes) {\r\n    // given\r\n    Rankings original = new Rankings(topN);\r\n    for (Rankable r : rankables) {\r\n        original.updateWith(r);\r\n    }\r\n    int expSize = original.size();\r\n    List<Rankable> expRankings = original.getRankings();\r\n    // when\r\n    Rankings copy = original.copy();\r\n    for (Rankable r : changes) {\r\n        copy.updateWith(r);\r\n    }\r\n    copy.pruneZeroCounts();\r\n    // then\r\n    assertThat(original.size()).isEqualTo(expSize);\r\n    assertThat(original.getRankings()).isEqualTo(expRankings);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlidingWindowCounterTest.java",
  "methodName" : "lessThanTwoSlotsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalWindowLengths\")\r\npublic void lessThanTwoSlotsShouldThrowIAE(int windowLengthInSlots) {\r\n    new SlidingWindowCounter<Object>(windowLengthInSlots);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlidingWindowCounterTest.java",
  "methodName" : "twoOrMoreSlotsShouldBeValid",
  "sourceCode" : "@Test(dataProvider = \"legalWindowLengths\")\r\npublic void twoOrMoreSlotsShouldBeValid(int windowLengthInSlots) {\r\n    new SlidingWindowCounter<Object>(windowLengthInSlots);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlidingWindowCounterTest.java",
  "methodName" : "newInstanceShouldHaveEmptyCounts",
  "sourceCode" : "@Test\r\npublic void newInstanceShouldHaveEmptyCounts() {\r\n    // given\r\n    SlidingWindowCounter<Object> counter = new SlidingWindowCounter<Object>(ANY_WINDOW_LENGTH_IN_SLOTS);\r\n    // when\r\n    Map<Object, Long> counts = counter.getCountsThenAdvanceWindow();\r\n    // then\r\n    assertThat(counts).isEmpty();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlidingWindowCounterTest.java",
  "methodName" : "testCounterWithSimulatedRuns",
  "sourceCode" : "@Test(dataProvider = \"simulatedCounterIterations\")\r\npublic void testCounterWithSimulatedRuns(int windowLengthInSlots, int[] incrementsPerIteration, long[] expCountsPerIteration) {\r\n    // given\r\n    SlidingWindowCounter<Object> counter = new SlidingWindowCounter<Object>(windowLengthInSlots);\r\n    int numIterations = incrementsPerIteration.length;\r\n    for (int i = 0; i < numIterations; i++) {\r\n        int numIncrements = incrementsPerIteration[i];\r\n        long expCounts = expCountsPerIteration[i];\r\n        // Objects are absent if they were zero both this iteration\r\n        // and the last -- if only this one, we need to report zero.\r\n        boolean expAbsent = ((expCounts == 0) && ((i == 0) || (expCountsPerIteration[i - 1] == 0)));\r\n        // given (for this iteration)\r\n        for (int j = 0; j < numIncrements; j++) {\r\n            counter.incrementCount(ANY_OBJECT);\r\n        }\r\n        // when (for this iteration)\r\n        Map<Object, Long> counts = counter.getCountsThenAdvanceWindow();\r\n        // then (for this iteration)\r\n        if (expAbsent) {\r\n            assertThat(counts).doesNotContainKey(ANY_OBJECT);\r\n        } else {\r\n            assertThat(counts.get(ANY_OBJECT)).isEqualTo(expCounts);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "negativeOrZeroNumSlotsShouldThrowIAE",
  "sourceCode" : "@Test(expectedExceptions = IllegalArgumentException.class, dataProvider = \"illegalNumSlotsData\")\r\npublic void negativeOrZeroNumSlotsShouldThrowIAE(int numSlots) {\r\n    new SlotBasedCounter<Object>(numSlots);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "positiveNumSlotsShouldBeOk",
  "sourceCode" : "@Test(dataProvider = \"legalNumSlotsData\")\r\npublic void positiveNumSlotsShouldBeOk(int numSlots) {\r\n    new SlotBasedCounter<Object>(numSlots);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "newInstanceShouldHaveEmptyCounts",
  "sourceCode" : "@Test\r\npublic void newInstanceShouldHaveEmptyCounts() {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(ANY_NUM_SLOTS);\r\n    // when\r\n    Map<Object, Long> counts = counter.getCounts();\r\n    // then\r\n    assertThat(counts).isEmpty();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "shouldReturnNonEmptyCountsWhenAtLeastOneObjectWasCounted",
  "sourceCode" : "@Test\r\npublic void shouldReturnNonEmptyCountsWhenAtLeastOneObjectWasCounted() {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(ANY_NUM_SLOTS);\r\n    counter.incrementCount(ANY_OBJECT, ANY_SLOT);\r\n    // when\r\n    Map<Object, Long> counts = counter.getCounts();\r\n    // then\r\n    assertThat(counts).isNotEmpty();\r\n    // additional tests that go beyond what this test is primarily about\r\n    assertThat(counts.size()).isEqualTo(1);\r\n    assertThat(counts.get(ANY_OBJECT)).isEqualTo(1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "shouldIncrementCount",
  "sourceCode" : "@Test(dataProvider = \"incrementCountData\")\r\npublic void shouldIncrementCount(Object[] objects, int[] expCounts) {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(ANY_NUM_SLOTS);\r\n    // when\r\n    for (int i = 0; i < objects.length; i++) {\r\n        Object obj = objects[i];\r\n        int numIncrements = expCounts[i];\r\n        for (int j = 0; j < numIncrements; j++) {\r\n            counter.incrementCount(obj, ANY_SLOT);\r\n        }\r\n    }\r\n    // then\r\n    for (int i = 0; i < objects.length; i++) {\r\n        assertThat(counter.getCount(objects[i], ANY_SLOT)).isEqualTo(expCounts[i]);\r\n    }\r\n    assertThat(counter.getCount(\"nonexistentObject\", ANY_SLOT)).isEqualTo(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "shouldReturnZeroForNonexistentObject",
  "sourceCode" : "@Test\r\npublic void shouldReturnZeroForNonexistentObject() {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(ANY_NUM_SLOTS);\r\n    // when\r\n    counter.incrementCount(\"somethingElse\", ANY_SLOT);\r\n    // then\r\n    assertThat(counter.getCount(\"nonexistentObject\", ANY_SLOT)).isEqualTo(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "shouldIncrementCountOnlyOneSlotAtATime",
  "sourceCode" : "@Test\r\npublic void shouldIncrementCountOnlyOneSlotAtATime() {\r\n    // given\r\n    int numSlots = 3;\r\n    Object obj = Long.valueOf(10);\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(numSlots);\r\n    // when (empty)\r\n    // then\r\n    assertThat(counter.getCount(obj, 0)).isEqualTo(0);\r\n    assertThat(counter.getCount(obj, 1)).isEqualTo(0);\r\n    assertThat(counter.getCount(obj, 2)).isEqualTo(0);\r\n    // when\r\n    counter.incrementCount(obj, 1);\r\n    // then\r\n    assertThat(counter.getCount(obj, 0)).isEqualTo(0);\r\n    assertThat(counter.getCount(obj, 1)).isEqualTo(1);\r\n    assertThat(counter.getCount(obj, 2)).isEqualTo(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "wipeSlotShouldSetAllCountsInSlotToZero",
  "sourceCode" : "@Test\r\npublic void wipeSlotShouldSetAllCountsInSlotToZero() {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(ANY_NUM_SLOTS);\r\n    Object countWasOne = \"countWasOne\";\r\n    Object countWasThree = \"countWasThree\";\r\n    counter.incrementCount(countWasOne, ANY_SLOT);\r\n    counter.incrementCount(countWasThree, ANY_SLOT);\r\n    counter.incrementCount(countWasThree, ANY_SLOT);\r\n    counter.incrementCount(countWasThree, ANY_SLOT);\r\n    // when\r\n    counter.wipeSlot(ANY_SLOT);\r\n    // then\r\n    assertThat(counter.getCount(countWasOne, ANY_SLOT)).isEqualTo(0);\r\n    assertThat(counter.getCount(countWasThree, ANY_SLOT)).isEqualTo(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\examples\\storm-starter\\test\\jvm\\org\\apache\\storm\\starter\\tools\\SlotBasedCounterTest.java",
  "methodName" : "wipeZerosShouldRemoveAnyObjectsWithZeroTotalCount",
  "sourceCode" : "@Test\r\npublic void wipeZerosShouldRemoveAnyObjectsWithZeroTotalCount() {\r\n    // given\r\n    SlotBasedCounter<Object> counter = new SlotBasedCounter<Object>(2);\r\n    int wipeSlot = 0;\r\n    int otherSlot = 1;\r\n    Object willBeRemoved = \"willBeRemoved\";\r\n    Object willContinueToBeTracked = \"willContinueToBeTracked\";\r\n    counter.incrementCount(willBeRemoved, wipeSlot);\r\n    counter.incrementCount(willContinueToBeTracked, wipeSlot);\r\n    counter.incrementCount(willContinueToBeTracked, otherSlot);\r\n    // when\r\n    counter.wipeSlot(wipeSlot);\r\n    counter.wipeZeros();\r\n    // then\r\n    assertThat(counter.getCounts()).doesNotContainKey(willBeRemoved);\r\n    assertThat(counter.getCounts()).containsKey(willContinueToBeTracked);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\avro\\TestFixedAvroSerializer.java",
  "methodName" : "testSchemas",
  "sourceCode" : "@Test\r\npublic void testSchemas() {\r\n    testTheSchema(schema1);\r\n    testTheSchema(schema2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\avro\\TestFixedAvroSerializer.java",
  "methodName" : "testDifferentFPs",
  "sourceCode" : "@Test\r\npublic void testDifferentFPs() {\r\n    String fp1 = reg.getFingerprint(schema1);\r\n    String fp2 = reg.getFingerprint(schema2);\r\n    assertNotEquals(fp1, fp2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\avro\\TestGenericAvroSerializer.java",
  "methodName" : "testSchemas",
  "sourceCode" : "@Test\r\npublic void testSchemas() {\r\n    testTheSchema(schema1);\r\n    testTheSchema(schema2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\avro\\TestGenericAvroSerializer.java",
  "methodName" : "testDifferentFPs",
  "sourceCode" : "@Test\r\npublic void testDifferentFPs() {\r\n    String fp1 = reg.getFingerprint(schema1);\r\n    String fp2 = reg.getFingerprint(schema2);\r\n    assertNotEquals(fp1, fp2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\AvroGenericRecordBoltTest.java",
  "methodName" : "multipleTuplesOneFile",
  "sourceCode" : "@Test\r\npublic void multipleTuplesOneFile() throws IOException {\r\n    AvroGenericRecordBolt bolt = makeAvroBolt(hdfsURI, 1, 1f, schemaV1);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot));\r\n    verifyAllAvroFiles(testRoot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\AvroGenericRecordBoltTest.java",
  "methodName" : "multipleTuplesMultiplesFiles",
  "sourceCode" : "@Test\r\npublic void multipleTuplesMultiplesFiles() throws IOException {\r\n    AvroGenericRecordBolt bolt = makeAvroBolt(hdfsURI, 1, .0001f, schemaV1);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple1);\r\n    assertEquals(4, countNonZeroLengthFiles(testRoot));\r\n    verifyAllAvroFiles(testRoot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\AvroGenericRecordBoltTest.java",
  "methodName" : "forwardSchemaChangeWorks",
  "sourceCode" : "@Test\r\npublic void forwardSchemaChangeWorks() throws IOException {\r\n    AvroGenericRecordBolt bolt = makeAvroBolt(hdfsURI, 1, 1000f, schemaV1);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    //Schema change should have forced a rotation\r\n    assertEquals(2, countNonZeroLengthFiles(testRoot));\r\n    verifyAllAvroFiles(testRoot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\AvroGenericRecordBoltTest.java",
  "methodName" : "backwardSchemaChangeWorks",
  "sourceCode" : "@Test\r\npublic void backwardSchemaChangeWorks() throws IOException {\r\n    AvroGenericRecordBolt bolt = makeAvroBolt(hdfsURI, 1, 1000f, schemaV2);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    //Schema changes should have forced file rotations\r\n    assertEquals(2, countNonZeroLengthFiles(testRoot));\r\n    verifyAllAvroFiles(testRoot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\AvroGenericRecordBoltTest.java",
  "methodName" : "schemaThrashing",
  "sourceCode" : "@Test\r\npublic void schemaThrashing() throws IOException {\r\n    AvroGenericRecordBolt bolt = makeAvroBolt(hdfsURI, 1, 1000f, schemaV2);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    //Two distinct schema should result in only two files\r\n    assertEquals(2, countNonZeroLengthFiles(testRoot));\r\n    verifyAllAvroFiles(testRoot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\format\\TestSimpleFileNameFormat.java",
  "methodName" : "testDefaults",
  "sourceCode" : "@Test\r\npublic void testDefaults() {\r\n    Map<String, Object> topoConf = new HashMap();\r\n    SimpleFileNameFormat format = new SimpleFileNameFormat();\r\n    format.prepare(null, createTopologyContext(topoConf));\r\n    long now = System.currentTimeMillis();\r\n    String path = format.getPath();\r\n    String name = format.getName(1, now);\r\n    assertEquals(\"/storm\", path);\r\n    String time = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(now);\r\n    assertEquals(time + \".1.txt\", name);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\format\\TestSimpleFileNameFormat.java",
  "methodName" : "testParameters",
  "sourceCode" : "@Test\r\npublic void testParameters() {\r\n    SimpleFileNameFormat format = new SimpleFileNameFormat().withName(\"$TIME.$HOST.$COMPONENT.$TASK.$NUM.txt\").withPath(\"/mypath\").withTimeFormat(\"yyyy-MM-dd HH:mm:ss\");\r\n    Map<String, Object> topoConf = new HashMap();\r\n    format.prepare(null, createTopologyContext(topoConf));\r\n    long now = System.currentTimeMillis();\r\n    String path = format.getPath();\r\n    String name = format.getName(1, now);\r\n    assertEquals(\"/mypath\", path);\r\n    String time = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(now);\r\n    String host = null;\r\n    try {\r\n        host = Utils.localHostname();\r\n    } catch (UnknownHostException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertEquals(time + \".\" + host + \".Xcom.7.1.txt\", name);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testTwoTuplesTwoFiles",
  "sourceCode" : "@Test\r\npublic void testTwoTuplesTwoFiles() throws IOException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 1, .00001f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    verify(collector).ack(tuple1);\r\n    verify(collector).ack(tuple2);\r\n    assertEquals(2, countNonZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testPartitionedOutput",
  "sourceCode" : "@Test\r\npublic void testPartitionedOutput() throws IOException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 1, 1000f);\r\n    Partitioner partitoner = new Partitioner() {\r\n\r\n        @Override\r\n        public String getPartitionPath(Tuple tuple) {\r\n            return Path.SEPARATOR + tuple.getStringByField(\"city\");\r\n        }\r\n    };\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.withPartitioner(partitoner);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    verify(collector).ack(tuple1);\r\n    verify(collector).ack(tuple2);\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot + \"/SFO\"));\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot + \"/SJO\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testTwoTuplesOneFile",
  "sourceCode" : "@Test\r\npublic void testTwoTuplesOneFile() throws IOException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 2, 10000f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    verifyNoInteractions(collector);\r\n    bolt.execute(tuple2);\r\n    verify(collector).ack(tuple1);\r\n    verify(collector).ack(tuple2);\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testFailedSync",
  "sourceCode" : "@Test\r\npublic void testFailedSync() throws IOException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 2, 10000f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\r\n    // All writes/syncs will fail so this should cause a RuntimeException\r\n    assertThrows(RuntimeException.class, () -> bolt.execute(tuple1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testFailureFilecount",
  "sourceCode" : "// One tuple and one rotation should yield one file with data\r\n// The failed executions should not cause rotations and any new files\r\n@Test\r\npublic void testFailureFilecount() throws IOException, InterruptedException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 1, .000001f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\r\n    try {\r\n        bolt.execute(tuple2);\r\n    } catch (RuntimeException e) {\r\n        //\r\n    }\r\n    try {\r\n        bolt.execute(tuple2);\r\n    } catch (RuntimeException e) {\r\n        //\r\n    }\r\n    try {\r\n        bolt.execute(tuple2);\r\n    } catch (RuntimeException e) {\r\n        //\r\n    }\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot));\r\n    assertEquals(0, countZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testTickTuples",
  "sourceCode" : "@Test\r\npublic void testTickTuples() throws IOException {\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 10, 10000f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    //Should not have flushed to file system yet\r\n    assertEquals(0, countNonZeroLengthFiles(testRoot));\r\n    bolt.execute(MockTupleHelpers.mockTickTuple());\r\n    //Tick should have flushed it\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestHdfsBolt.java",
  "methodName" : "testCleanupDoesNotThrowExceptionWhenRotationPolicyIsNotTimed",
  "sourceCode" : "@Test\r\npublic void testCleanupDoesNotThrowExceptionWhenRotationPolicyIsNotTimed() {\r\n    //STORM-3372: Rotation policy other than TimedRotationPolicy causes NPE on cleanup\r\n    FileRotationPolicy fieldsRotationPolicy = new FileSizeRotationPolicy(10_000, FileSizeRotationPolicy.Units.MB);\r\n    HdfsBolt bolt = makeHdfsBolt(hdfsURI, 10, 10000f).withRotationPolicy(fieldsRotationPolicy);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.cleanup();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestSequenceFileBolt.java",
  "methodName" : "testTwoTuplesTwoFiles",
  "sourceCode" : "@Test\r\npublic void testTwoTuplesTwoFiles() throws IOException {\r\n    SequenceFileBolt bolt = makeSeqBolt(hdfsURI, 1, .00001f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    bolt.execute(tuple2);\r\n    verify(collector).ack(tuple1);\r\n    verify(collector).ack(tuple2);\r\n    assertEquals(2, countNonZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestSequenceFileBolt.java",
  "methodName" : "testTwoTuplesOneFile",
  "sourceCode" : "@Test\r\npublic void testTwoTuplesOneFile() throws IOException {\r\n    SequenceFileBolt bolt = makeSeqBolt(hdfsURI, 2, 10000f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    verifyNoInteractions(collector);\r\n    bolt.execute(tuple2);\r\n    verify(collector).ack(tuple1);\r\n    verify(collector).ack(tuple2);\r\n    assertEquals(1, countNonZeroLengthFiles(testRoot));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestSequenceFileBolt.java",
  "methodName" : "testFailedSync",
  "sourceCode" : "@Test\r\npublic void testFailedSync() throws IOException {\r\n    SequenceFileBolt bolt = makeSeqBolt(hdfsURI, 2, 10000f);\r\n    bolt.prepare(new Config(), topologyContext, collector);\r\n    bolt.execute(tuple1);\r\n    fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\r\n    // All writes/syncs will fail so this should cause a RuntimeException\r\n    assertThrows(RuntimeException.class, () -> bolt.execute(tuple1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\bolt\\TestWritersMap.java",
  "methodName" : "testLRUBehavior",
  "sourceCode" : "@Test\r\npublic void testLRUBehavior() {\r\n    map.put(\"FOO\", foo);\r\n    map.put(\"BAR\", bar);\r\n    //Access foo to make it most recently used\r\n    map.get(\"FOO\");\r\n    //Add an element and bar should drop out\r\n    map.put(\"BAZ\", baz);\r\n    assertTrue(map.keySet().contains(\"FOO\"));\r\n    assertTrue(map.keySet().contains(\"BAZ\"));\r\n    assertFalse(map.keySet().contains(\"BAR\"));\r\n    // The removed writer should have been closed\r\n    assertTrue(bar.isClosed);\r\n    assertFalse(foo.isClosed);\r\n    assertFalse(baz.isClosed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\ConfigsTest.java",
  "methodName" : "testGood",
  "sourceCode" : "@SuppressWarnings(\"deprecation\")\r\n@Test\r\npublic void testGood() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Configs.READER_TYPE, Configs.TEXT);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.READER_TYPE, Configs.SEQ);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.READER_TYPE, TextFileReader.class.getName());\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.HDFS_URI, \"hdfs://namenode/\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.SOURCE_DIR, \"/input/source\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.ARCHIVE_DIR, \"/input/done\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.BAD_DIR, \"/input/bad\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.LOCK_DIR, \"/topology/lock\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.COMMIT_FREQ_COUNT, 0);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.COMMIT_FREQ_COUNT, 100);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.COMMIT_FREQ_SEC, 100);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.MAX_OUTSTANDING, 500);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.LOCK_TIMEOUT, 100);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.CLOCKS_INSYNC, true);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Configs.IGNORE_SUFFIX, \".writing\");\r\n    ConfigValidation.validateFields(conf);\r\n    Map<String, String> hdfsConf = new HashMap<>();\r\n    hdfsConf.put(\"A\", \"B\");\r\n    conf.put(Configs.DEFAULT_HDFS_CONFIG_KEY, hdfsConf);\r\n    ConfigValidation.validateFields(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\ConfigsTest.java",
  "methodName" : "testBad",
  "sourceCode" : "@SuppressWarnings(\"deprecation\")\r\n@Test\r\npublic void testBad() {\r\n    verifyBad(Configs.READER_TYPE, \"SomeString\");\r\n    verifyBad(Configs.HDFS_URI, 100);\r\n    verifyBad(Configs.COMMIT_FREQ_COUNT, -10);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestDirLock.java",
  "methodName" : "testBasicLocking",
  "sourceCode" : "@Test\r\npublic void testBasicLocking() throws Exception {\r\n    // 1 grab lock\r\n    DirLock lock = DirLock.tryLock(fs, locksDir);\r\n    assertTrue(fs.exists(lock.getLockFile()));\r\n    // 2 try to grab another lock while dir is locked\r\n    // should fail\r\n    DirLock lock2 = DirLock.tryLock(fs, locksDir);\r\n    assertNull(lock2);\r\n    // 3 let go first lock\r\n    lock.release();\r\n    assertFalse(fs.exists(lock.getLockFile()));\r\n    // 4 try locking again\r\n    lock2 = DirLock.tryLock(fs, locksDir);\r\n    assertTrue(fs.exists(lock2.getLockFile()));\r\n    lock2.release();\r\n    assertFalse(fs.exists(lock.getLockFile()));\r\n    // should be thrown\r\n    lock2.release();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestDirLock.java",
  "methodName" : "testConcurrentLocking",
  "sourceCode" : "@Test\r\npublic void testConcurrentLocking() throws Exception {\r\n    DirLockingThread[] threads = null;\r\n    try {\r\n        threads = startThreads(100, locksDir);\r\n        for (DirLockingThread thd : threads) {\r\n            thd.join(30_000);\r\n            assertTrue(thd.cleanExit, thd.getName() + \" did not exit cleanly\");\r\n        }\r\n        Path lockFile = new Path(locksDir + Path.SEPARATOR + DirLock.DIR_LOCK_FILE);\r\n        assertFalse(fs.exists(lockFile));\r\n    } finally {\r\n        if (threads != null) {\r\n            for (DirLockingThread thread : threads) {\r\n                thread.interrupt();\r\n                thread.join(30_000);\r\n                if (thread.isAlive()) {\r\n                    throw new RuntimeException(\"Failed to stop threads within 30 seconds, threads may leak into other tests\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestDirLock.java",
  "methodName" : "testLockRecovery",
  "sourceCode" : "@Test\r\npublic void testLockRecovery() throws Exception {\r\n    // should pass\r\n    DirLock lock1 = DirLock.tryLock(fs, locksDir);\r\n    assertNotNull(lock1);\r\n    // should fail\r\n    DirLock lock2 = DirLock.takeOwnershipIfStale(fs, locksDir, LOCK_EXPIRY_SEC);\r\n    assertNull(lock2);\r\n    // wait for lock to expire\r\n    Thread.sleep(LOCK_EXPIRY_SEC * 1000 + 500);\r\n    assertTrue(fs.exists(lock1.getLockFile()));\r\n    // should pass now\r\n    DirLock lock3 = DirLock.takeOwnershipIfStale(fs, locksDir, LOCK_EXPIRY_SEC);\r\n    assertNotNull(lock3);\r\n    assertTrue(fs.exists(lock3.getLockFile()));\r\n    lock3.release();\r\n    assertFalse(fs.exists(lock3.getLockFile()));\r\n    // should not throw\r\n    lock1.release();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testBasicLocking",
  "sourceCode" : "@Test\r\npublic void testBasicLocking() throws Exception {\r\n    // create empty files in filesDir\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    Path file2 = new Path(filesDir + Path.SEPARATOR + \"file2\");\r\n    fs.create(file1).close();\r\n    // create empty file\r\n    fs.create(file2).close();\r\n    // acquire lock on file1 and verify if worked\r\n    FileLock lock1a = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    assertNotNull(lock1a);\r\n    assertTrue(fs.exists(lock1a.getLockFile()));\r\n    // verify lock file location\r\n    assertEquals(lock1a.getLockFile().getParent(), locksDir);\r\n    // verify lock filename\r\n    assertEquals(lock1a.getLockFile().getName(), file1.getName());\r\n    // acquire another lock on file1 and verify it failed\r\n    FileLock lock1b = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    assertNull(lock1b);\r\n    // release lock on file1 and check\r\n    lock1a.release();\r\n    assertFalse(fs.exists(lock1a.getLockFile()));\r\n    // Retry locking and verify\r\n    FileLock lock1c = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    assertNotNull(lock1c);\r\n    assertTrue(fs.exists(lock1c.getLockFile()));\r\n    // verify lock file location\r\n    assertEquals(lock1c.getLockFile().getParent(), locksDir);\r\n    // verify lock filename\r\n    assertEquals(lock1c.getLockFile().getName(), file1.getName());\r\n    // try locking another file2 at the same time\r\n    FileLock lock2a = FileLock.tryLock(fs, file2, locksDir, \"spout1\");\r\n    assertNotNull(lock2a);\r\n    assertTrue(fs.exists(lock2a.getLockFile()));\r\n    // verify lock file location\r\n    assertEquals(lock2a.getLockFile().getParent(), locksDir);\r\n    // verify lock filename\r\n    assertEquals(lock2a.getLockFile().getName(), file2.getName());\r\n    // release both locks\r\n    lock2a.release();\r\n    assertFalse(fs.exists(lock2a.getLockFile()));\r\n    lock1c.release();\r\n    assertFalse(fs.exists(lock1c.getLockFile()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testHeartbeat",
  "sourceCode" : "@Test\r\npublic void testHeartbeat() throws Exception {\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    fs.create(file1).close();\r\n    // acquire lock on file1\r\n    FileLock lock1 = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    assertNotNull(lock1);\r\n    assertTrue(fs.exists(lock1.getLockFile()));\r\n    ArrayList<String> lines = readTextFile(lock1.getLockFile());\r\n    assertEquals(1, lines.size(), \"heartbeats appear to be missing\");\r\n    // heartbeat upon it\r\n    lock1.heartbeat(\"1\");\r\n    lock1.heartbeat(\"2\");\r\n    lock1.heartbeat(\"3\");\r\n    lines = readTextFile(lock1.getLockFile());\r\n    assertEquals(4, lines.size(), \"heartbeats appear to be missing\");\r\n    lock1.heartbeat(\"4\");\r\n    lock1.heartbeat(\"5\");\r\n    lock1.heartbeat(\"6\");\r\n    lines = readTextFile(lock1.getLockFile());\r\n    assertEquals(7, lines.size(), \"heartbeats appear to be missing\");\r\n    lock1.release();\r\n    lines = readTextFile(lock1.getLockFile());\r\n    assertNull(lines);\r\n    assertFalse(fs.exists(lock1.getLockFile()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testConcurrentLocking",
  "sourceCode" : "@Test\r\npublic void testConcurrentLocking() throws IOException, InterruptedException {\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    fs.create(file1).close();\r\n    FileLockingThread[] threads = null;\r\n    try {\r\n        threads = startThreads(100, file1, locksDir);\r\n        for (FileLockingThread thd : threads) {\r\n            thd.join(30_000);\r\n            assertTrue(thd.cleanExit, thd.getName() + \" did not exit cleanly\");\r\n        }\r\n        Path lockFile = new Path(locksDir + Path.SEPARATOR + file1.getName());\r\n        assertFalse(fs.exists(lockFile));\r\n    } finally {\r\n        if (threads != null) {\r\n            for (FileLockingThread thread : threads) {\r\n                thread.interrupt();\r\n                thread.join(30_000);\r\n                if (thread.isAlive()) {\r\n                    throw new RuntimeException(\"Failed to stop threads within 30 seconds, threads may leak into other tests\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testStaleLockDetection_SingleLock",
  "sourceCode" : "@Test\r\npublic void testStaleLockDetection_SingleLock() throws Exception {\r\n    final int LOCK_EXPIRY_SEC = 1;\r\n    final int WAIT_MSEC = 1500;\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    fs.create(file1).close();\r\n    FileLock lock1 = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    try {\r\n        // acquire lock on file1\r\n        assertNotNull(lock1);\r\n        assertTrue(fs.exists(lock1.getLockFile()));\r\n        // wait for lock to expire\r\n        Thread.sleep(WAIT_MSEC);\r\n        HdfsUtils.Pair<Path, FileLock.LogEntry> expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);\r\n        assertNotNull(expired);\r\n        // heartbeat, ensure its no longer stale and read back the heartbeat data\r\n        lock1.heartbeat(\"1\");\r\n        expired = FileLock.locateOldestExpiredLock(fs, locksDir, 1);\r\n        assertNull(expired);\r\n        FileLock.LogEntry lastEntry = lock1.getLastLogEntry();\r\n        assertNotNull(lastEntry);\r\n        assertEquals(\"1\", lastEntry.fileOffset);\r\n        // wait and check for expiry again\r\n        Thread.sleep(WAIT_MSEC);\r\n        expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);\r\n        assertNotNull(expired);\r\n    } finally {\r\n        lock1.release();\r\n        fs.delete(file1, false);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testStaleLockDetection_MultipleLocks",
  "sourceCode" : "@Test\r\npublic void testStaleLockDetection_MultipleLocks() throws Exception {\r\n    final int LOCK_EXPIRY_SEC = 1;\r\n    final int WAIT_MSEC = 1500;\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    Path file2 = new Path(filesDir + Path.SEPARATOR + \"file2\");\r\n    Path file3 = new Path(filesDir + Path.SEPARATOR + \"file3\");\r\n    fs.create(file1).close();\r\n    fs.create(file2).close();\r\n    fs.create(file3).close();\r\n    // 1) acquire locks on file1,file2,file3\r\n    FileLock lock1 = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    FileLock lock2 = FileLock.tryLock(fs, file2, locksDir, \"spout2\");\r\n    FileLock lock3 = FileLock.tryLock(fs, file3, locksDir, \"spout3\");\r\n    assertNotNull(lock1);\r\n    assertNotNull(lock2);\r\n    assertNotNull(lock3);\r\n    try {\r\n        HdfsUtils.Pair<Path, FileLock.LogEntry> expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);\r\n        assertNull(expired);\r\n        // 2) wait for all 3 locks to expire then heart beat on 2 locks and verify stale lock\r\n        Thread.sleep(WAIT_MSEC);\r\n        lock1.heartbeat(\"1\");\r\n        lock2.heartbeat(\"1\");\r\n        expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);\r\n        assertNotNull(expired);\r\n        assertEquals(\"spout3\", expired.getValue().componentId);\r\n    } finally {\r\n        lock1.release();\r\n        lock2.release();\r\n        lock3.release();\r\n        fs.delete(file1, false);\r\n        fs.delete(file2, false);\r\n        fs.delete(file3, false);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestFileLock.java",
  "methodName" : "testLockRecovery",
  "sourceCode" : "@Test\r\npublic void testLockRecovery() throws Exception {\r\n    final int LOCK_EXPIRY_SEC = 1;\r\n    final int WAIT_MSEC = LOCK_EXPIRY_SEC * 1000 + 500;\r\n    Path file1 = new Path(filesDir + Path.SEPARATOR + \"file1\");\r\n    Path file2 = new Path(filesDir + Path.SEPARATOR + \"file2\");\r\n    Path file3 = new Path(filesDir + Path.SEPARATOR + \"file3\");\r\n    fs.create(file1).close();\r\n    fs.create(file2).close();\r\n    fs.create(file3).close();\r\n    // 1) acquire locks on file1,file2,file3\r\n    FileLock lock1 = FileLock.tryLock(fs, file1, locksDir, \"spout1\");\r\n    FileLock lock2 = FileLock.tryLock(fs, file2, locksDir, \"spout2\");\r\n    FileLock lock3 = FileLock.tryLock(fs, file3, locksDir, \"spout3\");\r\n    assertNotNull(lock1);\r\n    assertNotNull(lock2);\r\n    assertNotNull(lock3);\r\n    try {\r\n        HdfsUtils.Pair<Path, FileLock.LogEntry> expired = FileLock.locateOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC);\r\n        assertNull(expired);\r\n        // 1) Simulate lock file lease expiring and getting closed by HDFS\r\n        closeUnderlyingLockFile(lock3);\r\n        // 2) wait for all 3 locks to expire then heart beat on 2 locks\r\n        // wait for locks to expire\r\n        Thread.sleep(WAIT_MSEC * 2);\r\n        lock1.heartbeat(\"1\");\r\n        lock2.heartbeat(\"1\");\r\n        // 3) Take ownership of stale lock\r\n        FileLock lock3b = FileLock.acquireOldestExpiredLock(fs, locksDir, LOCK_EXPIRY_SEC, \"spout1\");\r\n        assertNotNull(lock3b);\r\n        assertEquals(Path.getPathWithoutSchemeAndAuthority(lock3b.getLockFile()), lock3.getLockFile(), \"Expected lock3 file\");\r\n    } finally {\r\n        lock1.release();\r\n        lock2.release();\r\n        lock3.release();\r\n        fs.delete(file1, false);\r\n        fs.delete(file2, false);\r\n        try {\r\n            fs.delete(file3, false);\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSemantics.java",
  "methodName" : "testDeleteSemantics",
  "sourceCode" : "@Test\r\npublic void testDeleteSemantics() throws Exception {\r\n    Path file = new Path(dir.toString() + Path.SEPARATOR_CHAR + \"file1\");\r\n    //    try {\r\n    // 1) Delete absent file - should return false\r\n    assertFalse(fs.exists(file));\r\n    try {\r\n        assertFalse(fs.delete(file, false));\r\n    } catch (IOException e) {\r\n        e.printStackTrace();\r\n    }\r\n    // 2) deleting open file - should return true\r\n    fs.create(file, false);\r\n    assertTrue(fs.delete(file, false));\r\n    // 3) deleting closed file  - should return true\r\n    FSDataOutputStream os = fs.create(file, false);\r\n    os.close();\r\n    assertTrue(fs.exists(file));\r\n    assertTrue(fs.delete(file, false));\r\n    assertFalse(fs.exists(file));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSemantics.java",
  "methodName" : "testConcurrentDeletion",
  "sourceCode" : "@Test\r\npublic void testConcurrentDeletion() throws Exception {\r\n    Path file = new Path(dir.toString() + Path.SEPARATOR_CHAR + \"file1\");\r\n    fs.create(file).close();\r\n    // 1 concurrent deletion - only one thread should succeed\r\n    FileDeletionThread[] threads = null;\r\n    try {\r\n        threads = startThreads(10, file);\r\n        int successCount = 0;\r\n        for (FileDeletionThread thd : threads) {\r\n            thd.join(30_000);\r\n            if (thd.succeeded) {\r\n                successCount++;\r\n            }\r\n            if (thd.exception != null) {\r\n                assertNotNull(thd.exception);\r\n            }\r\n        }\r\n        System.err.println(successCount);\r\n        assertEquals(1, successCount);\r\n    } finally {\r\n        if (threads != null) {\r\n            for (FileDeletionThread thread : threads) {\r\n                thread.interrupt();\r\n                thread.join(30_000);\r\n                if (thread.isAlive()) {\r\n                    throw new RuntimeException(\"Failed to stop threads within 30 seconds, threads may leak into other tests\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSemantics.java",
  "methodName" : "testAppendSemantics",
  "sourceCode" : "@Test\r\npublic void testAppendSemantics() throws Exception {\r\n    //1 try to append to an open file\r\n    Path file1 = new Path(dir.toString() + Path.SEPARATOR_CHAR + \"file1\");\r\n    try (FSDataOutputStream os1 = fs.create(file1, false)) {\r\n        // should fail\r\n        fs.append(file1);\r\n        fail(\"Append did not throw an exception\");\r\n    } catch (RemoteException e) {\r\n        // expecting AlreadyBeingCreatedException inside RemoteException\r\n        assertEquals(AlreadyBeingCreatedException.class, e.unwrapRemoteException().getClass());\r\n    }\r\n    //2 try to append to a closed file\r\n    try (FSDataOutputStream os2 = fs.append(file1)) {\r\n        assertThat(os2, notNullValue());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSemantics.java",
  "methodName" : "testDoubleCreateSemantics",
  "sourceCode" : "@Test\r\npublic void testDoubleCreateSemantics() throws Exception {\r\n    //1 create an already existing open file w/o override flag\r\n    Path file1 = new Path(dir.toString() + Path.SEPARATOR_CHAR + \"file1\");\r\n    try (FSDataOutputStream os1 = fs.create(file1, false)) {\r\n        // should fail\r\n        fs.create(file1, false);\r\n        fail(\"Create did not throw an exception\");\r\n    } catch (RemoteException e) {\r\n        assertEquals(AlreadyBeingCreatedException.class, e.unwrapRemoteException().getClass());\r\n    }\r\n    //2 close file and retry creation\r\n    try {\r\n        // should still fail\r\n        fs.create(file1, false);\r\n        fail(\"Create did not throw an exception\");\r\n    } catch (FileAlreadyExistsException e) {\r\n        // expecting this exception\r\n    }\r\n    //3 delete file and retry creation\r\n    fs.delete(file1, false);\r\n    try (FSDataOutputStream os2 = fs.create(file1, false)) {\r\n        assertNotNull(os2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testSimpleText_noACK",
  "sourceCode" : "@Test\r\npublic void testSimpleText_noACK() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 5);\r\n    Path file2 = new Path(source.toString() + \"/file2.txt\");\r\n    createTextFile(file2, 5);\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        spout.setCommitFrequencySec(1);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        runSpout(spout, \"r11\");\r\n        Path arc1 = new Path(archive.toString() + \"/file1.txt\");\r\n        Path arc2 = new Path(archive.toString() + \"/file2.txt\");\r\n        checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1, arc2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testSimpleText_ACK",
  "sourceCode" : "@Test\r\npublic void testSimpleText_ACK() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 5);\r\n    Path file2 = new Path(source.toString() + \"/file2.txt\");\r\n    createTextFile(file2, 5);\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        spout.setCommitFrequencySec(1);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        // enable ACKing\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, \"1\");\r\n        openSpout(spout, 0, conf);\r\n        // consume file 1\r\n        runSpout(spout, \"r6\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\r\n        Path arc1 = new Path(archive.toString() + \"/file1.txt\");\r\n        checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1);\r\n        // consume file 2\r\n        runSpout(spout, \"r6\", \"a5\", \"a6\", \"a7\", \"a8\", \"a9\");\r\n        Path arc2 = new Path(archive.toString() + \"/file2.txt\");\r\n        checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1, arc2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testEmptySimpleText_ACK",
  "sourceCode" : "@Test\r\npublic void testEmptySimpleText_ACK() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file_empty.txt\");\r\n    createTextFile(file1, 0);\r\n    //Ensure the second file has a later modified timestamp, as the spout should pick the first file first.\r\n    Thread.sleep(2);\r\n    Path file2 = new Path(source.toString() + \"/file.txt\");\r\n    createTextFile(file2, 5);\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        // enable ACKing\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, \"1\");\r\n        openSpout(spout, 0, conf);\r\n        // Read once. Since the first file is empty, the spout should continue with file 2\r\n        runSpout(spout, \"r6\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\r\n        //File 1 should be moved to archive\r\n        assertThat(fs.isFile(new Path(archive.toString() + \"/file_empty.txt\")), is(true));\r\n        //File 2 should be read\r\n        Path arc2 = new Path(archive.toString() + \"/file.txt\");\r\n        checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testResumeAbandoned_Text_NoAck",
  "sourceCode" : "@Test\r\npublic void testResumeAbandoned_Text_NoAck() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 6);\r\n    final Integer lockExpirySec = 1;\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        // effectively disable commits based on time\r\n        spout.setCommitFrequencySec(1000);\r\n        spout.setLockTimeoutSec(lockExpirySec);\r\n        try (AutoCloseableHdfsSpout closeableSpout2 = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n            HdfsSpout spout2 = closeableSpout2.spout;\r\n            spout2.setCommitFrequencyCount(1);\r\n            // effectively disable commits based on time\r\n            spout2.setCommitFrequencySec(1000);\r\n            spout2.setLockTimeoutSec(lockExpirySec);\r\n            Map<String, Object> conf = getCommonConfigs();\r\n            openSpout(spout, 0, conf);\r\n            openSpout(spout2, 1, conf);\r\n            // consume file 1 partially\r\n            List<String> res = runSpout(spout, \"r2\");\r\n            assertEquals(2, res.size());\r\n            // abandon file\r\n            FileLock lock = getField(spout, \"lock\");\r\n            TestFileLock.closeUnderlyingLockFile(lock);\r\n            Thread.sleep(lockExpirySec * 2 * 1000);\r\n            // check lock file presence\r\n            assertTrue(fs.exists(lock.getLockFile()));\r\n            // create another spout to take over processing and read a few lines\r\n            List<String> res2 = runSpout(spout2, \"r3\");\r\n            assertEquals(3, res2.size());\r\n            // check lock file presence\r\n            assertTrue(fs.exists(lock.getLockFile()));\r\n            // check lock file contents\r\n            List<String> contents = readTextFile(fs, lock.getLockFile().toString());\r\n            assertFalse(contents.isEmpty());\r\n            // finish up reading the file\r\n            res2 = runSpout(spout2, \"r2\");\r\n            assertEquals(4, res2.size());\r\n            // check lock file is gone\r\n            assertFalse(fs.exists(lock.getLockFile()));\r\n            FileReader rdr = getField(spout2, \"reader\");\r\n            assertNull(rdr);\r\n            assertTrue(getBoolField(spout2, \"fileReadCompletely\"));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testResumeAbandoned_Seq_NoAck",
  "sourceCode" : "@Test\r\npublic void testResumeAbandoned_Seq_NoAck() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.seq\");\r\n    createSeqFile(fs, file1, 6);\r\n    final Integer lockExpirySec = 1;\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.SEQ, SequenceFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        // effectively disable commits based on time\r\n        spout.setCommitFrequencySec(1000);\r\n        spout.setLockTimeoutSec(lockExpirySec);\r\n        try (AutoCloseableHdfsSpout closeableSpout2 = makeSpout(Configs.SEQ, SequenceFileReader.defaultFields)) {\r\n            HdfsSpout spout2 = closeableSpout2.spout;\r\n            spout2.setCommitFrequencyCount(1);\r\n            // effectively disable commits based on time\r\n            spout2.setCommitFrequencySec(1000);\r\n            spout2.setLockTimeoutSec(lockExpirySec);\r\n            Map<String, Object> conf = getCommonConfigs();\r\n            openSpout(spout, 0, conf);\r\n            openSpout(spout2, 1, conf);\r\n            // consume file 1 partially\r\n            List<String> res = runSpout(spout, \"r2\");\r\n            assertEquals(2, res.size());\r\n            // abandon file\r\n            FileLock lock = getField(spout, \"lock\");\r\n            TestFileLock.closeUnderlyingLockFile(lock);\r\n            Thread.sleep(lockExpirySec * 2 * 1000);\r\n            // check lock file presence\r\n            assertTrue(fs.exists(lock.getLockFile()));\r\n            // create another spout to take over processing and read a few lines\r\n            List<String> res2 = runSpout(spout2, \"r3\");\r\n            assertEquals(3, res2.size());\r\n            // check lock file presence\r\n            assertTrue(fs.exists(lock.getLockFile()));\r\n            // check lock file contents\r\n            List<String> contents = getTextFileContents(fs, lock.getLockFile());\r\n            assertFalse(contents.isEmpty());\r\n            // finish up reading the file\r\n            res2 = runSpout(spout2, \"r3\");\r\n            assertEquals(4, res2.size());\r\n            // check lock file is gone\r\n            assertFalse(fs.exists(lock.getLockFile()));\r\n            FileReader rdr = getField(spout2, \"reader\");\r\n            assertNull(rdr);\r\n            assertTrue(getBoolField(spout2, \"fileReadCompletely\"));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testMultipleFileConsumption_Ack",
  "sourceCode" : "@Test\r\npublic void testMultipleFileConsumption_Ack() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 5);\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        spout.setCommitFrequencySec(1);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        // enable ACKing\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, \"1\");\r\n        openSpout(spout, 0, conf);\r\n        // read few lines from file1 dont ack\r\n        runSpout(spout, \"r3\");\r\n        FileReader reader = getField(spout, \"reader\");\r\n        assertNotNull(reader);\r\n        assertFalse(getBoolField(spout, \"fileReadCompletely\"));\r\n        // read remaining lines\r\n        runSpout(spout, \"r3\");\r\n        reader = getField(spout, \"reader\");\r\n        assertNotNull(reader);\r\n        assertTrue(getBoolField(spout, \"fileReadCompletely\"));\r\n        // ack few\r\n        runSpout(spout, \"a0\", \"a1\", \"a2\");\r\n        reader = getField(spout, \"reader\");\r\n        assertNotNull(reader);\r\n        assertTrue(getBoolField(spout, \"fileReadCompletely\"));\r\n        //ack rest\r\n        runSpout(spout, \"a3\", \"a4\");\r\n        reader = getField(spout, \"reader\");\r\n        assertNull(reader);\r\n        assertTrue(getBoolField(spout, \"fileReadCompletely\"));\r\n        // go to next file\r\n        Path file2 = new Path(source.toString() + \"/file2.txt\");\r\n        createTextFile(file2, 5);\r\n        // Read 1 line\r\n        runSpout(spout, \"r1\");\r\n        assertNotNull(getField(spout, \"reader\"));\r\n        assertFalse(getBoolField(spout, \"fileReadCompletely\"));\r\n        // ack 1 tuple\r\n        runSpout(spout, \"a5\");\r\n        assertNotNull(getField(spout, \"reader\"));\r\n        assertFalse(getBoolField(spout, \"fileReadCompletely\"));\r\n        // read and ack remaining lines\r\n        runSpout(spout, \"r5\", \"a6\", \"a7\", \"a8\", \"a9\");\r\n        assertNull(getField(spout, \"reader\"));\r\n        assertTrue(getBoolField(spout, \"fileReadCompletely\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testSimpleSequenceFile",
  "sourceCode" : "@Test\r\npublic void testSimpleSequenceFile() throws Exception {\r\n    //1) create a couple files to consume\r\n    source = new Path(\"/tmp/hdfsspout/source\");\r\n    fs.mkdirs(source);\r\n    archive = new Path(\"/tmp/hdfsspout/archive\");\r\n    fs.mkdirs(archive);\r\n    Path file1 = new Path(source + \"/file1.seq\");\r\n    createSeqFile(fs, file1, 5);\r\n    Path file2 = new Path(source + \"/file2.seq\");\r\n    createSeqFile(fs, file2, 5);\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.SEQ, SequenceFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        // consume both files\r\n        List<String> res = runSpout(spout, \"r11\");\r\n        assertEquals(10, res.size());\r\n        assertEquals(2, listDir(archive).size());\r\n        Path f1 = new Path(archive + \"/file1.seq\");\r\n        Path f2 = new Path(archive + \"/file2.seq\");\r\n        checkCollectorOutput_seq((MockCollector) spout.getCollector(), f1, f2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testReadFailures",
  "sourceCode" : "@Test\r\npublic void testReadFailures() throws Exception {\r\n    // 1) create couple of input files to read\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    Path file2 = new Path(source.toString() + \"/file2.txt\");\r\n    createTextFile(file1, 6);\r\n    createTextFile(file2, 7);\r\n    assertEquals(2, listDir(source).size());\r\n    // 2) run spout\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(MockTextFailingReader.class.getName(), MockTextFailingReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        List<String> res = runSpout(spout, \"r11\");\r\n        String[] expected = new String[] { \"[line 0]\", \"[line 1]\", \"[line 2]\", \"[line 0]\", \"[line 1]\", \"[line 2]\" };\r\n        assertArrayEquals(expected, res.toArray());\r\n        // 3) make sure 6 lines (3 from each file) were read in all\r\n        assertEquals(((MockCollector) spout.getCollector()).lines.size(), 6);\r\n        ArrayList<Path> badFiles = HdfsUtils.listFilesByModificationTime(fs, badfiles, 0);\r\n        assertEquals(badFiles.size(), 2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testLocking",
  "sourceCode" : "// check lock creation/deletion and contents\r\n@Test\r\npublic void testLocking() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 10);\r\n    // 0) config spout to log progress in lock file for each tuple\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        spout.setCommitFrequencyCount(1);\r\n        // effectively disable commits based on time\r\n        spout.setCommitFrequencySec(1000);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        // 1) read initial lines in file, then check if lock exists\r\n        List<String> res = runSpout(spout, \"r5\");\r\n        assertEquals(5, res.size());\r\n        List<String> lockFiles = listDir(spout.getLockDirPath());\r\n        assertEquals(1, lockFiles.size());\r\n        // 2) check log file content line count == tuples emitted + 1\r\n        List<String> lines = readTextFile(fs, lockFiles.get(0));\r\n        assertEquals(lines.size(), res.size() + 1);\r\n        // 3) read remaining lines in file, then ensure lock is gone\r\n        runSpout(spout, \"r6\");\r\n        lockFiles = listDir(spout.getLockDirPath());\r\n        assertEquals(0, lockFiles.size());\r\n        // 4)  --- Create another input file and reverify same behavior ---\r\n        Path file2 = new Path(source.toString() + \"/file2.txt\");\r\n        createTextFile(file2, 10);\r\n        // 5) read initial lines in file, then check if lock exists\r\n        res = runSpout(spout, \"r5\");\r\n        assertEquals(15, res.size());\r\n        lockFiles = listDir(spout.getLockDirPath());\r\n        assertEquals(1, lockFiles.size());\r\n        // 6) check log file content line count == tuples emitted + 1\r\n        lines = readTextFile(fs, lockFiles.get(0));\r\n        assertEquals(6, lines.size());\r\n        // 7) read remaining lines in file, then ensure lock is gone\r\n        runSpout(spout, \"r6\");\r\n        lockFiles = listDir(spout.getLockDirPath());\r\n        assertEquals(0, lockFiles.size());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testLockLoggingFreqCount",
  "sourceCode" : "@Test\r\npublic void testLockLoggingFreqCount() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 10);\r\n    // 0) config spout to log progress in lock file for each tuple\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        // 1 lock log entry every 2 tuples\r\n        spout.setCommitFrequencyCount(2);\r\n        // Effectively disable commits based on time\r\n        spout.setCommitFrequencySec(1000);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        // 1) read 5 lines in file,\r\n        runSpout(spout, \"r5\");\r\n        // 2) check log file contents\r\n        String lockFile = listDir(spout.getLockDirPath()).get(0);\r\n        List<String> lines = readTextFile(fs, lockFile);\r\n        assertEquals(lines.size(), 3);\r\n        // 3) read 6th line and see if another log entry was made\r\n        runSpout(spout, \"r1\");\r\n        lines = readTextFile(fs, lockFile);\r\n        assertEquals(lines.size(), 4);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestHdfsSpout.java",
  "methodName" : "testLockLoggingFreqSec",
  "sourceCode" : "@Test\r\npublic void testLockLoggingFreqSec() throws Exception {\r\n    Path file1 = new Path(source.toString() + \"/file1.txt\");\r\n    createTextFile(file1, 10);\r\n    // 0) config spout to log progress in lock file for each tuple\r\n    try (AutoCloseableHdfsSpout closeableSpout = makeSpout(Configs.TEXT, TextFileReader.defaultFields)) {\r\n        HdfsSpout spout = closeableSpout.spout;\r\n        // disable it\r\n        spout.setCommitFrequencyCount(0);\r\n        // log every 2 sec\r\n        spout.setCommitFrequencySec(2);\r\n        Map<String, Object> conf = getCommonConfigs();\r\n        openSpout(spout, 0, conf);\r\n        // 1) read 5 lines in file\r\n        runSpout(spout, \"r5\");\r\n        // 2) check log file contents\r\n        String lockFile = listDir(spout.getLockDirPath()).get(0);\r\n        List<String> lines = readTextFile(fs, lockFile);\r\n        assertEquals(lines.size(), 1);\r\n        // allow freq_sec to expire\r\n        Thread.sleep(3000);\r\n        // 3) read another line and see if another log entry was made\r\n        runSpout(spout, \"r1\");\r\n        lines = readTextFile(fs, lockFile);\r\n        assertEquals(2, lines.size());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\spout\\TestProgressTracker.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() throws Exception {\r\n    ProgressTracker tracker = new ProgressTracker();\r\n    baseFolder = new File(tempFolder, \"trackertest\");\r\n    baseFolder.mkdir();\r\n    Path file = new Path(baseFolder.toString() + Path.SEPARATOR + \"testHeadTrimming.txt\");\r\n    createTextFile(file, 10);\r\n    // create reader and do some checks\r\n    TextFileReader reader = new TextFileReader(fs, file, null);\r\n    FileOffset pos0 = tracker.getCommitPosition();\r\n    assertNull(pos0);\r\n    TextFileReader.Offset currOffset = reader.getFileOffset();\r\n    assertNotNull(currOffset);\r\n    assertEquals(0, currOffset.charOffset);\r\n    // read 1st line and ack\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos1 = reader.getFileOffset();\r\n    tracker.recordAckedOffset(pos1);\r\n    TextFileReader.Offset pos1b = (TextFileReader.Offset) tracker.getCommitPosition();\r\n    assertEquals(pos1, pos1b);\r\n    // read 2nd line and ACK\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos2 = reader.getFileOffset();\r\n    tracker.recordAckedOffset(pos2);\r\n    tracker.dumpState(System.err);\r\n    TextFileReader.Offset pos2b = (TextFileReader.Offset) tracker.getCommitPosition();\r\n    assertEquals(pos2, pos2b);\r\n    // read lines 3..7, don't ACK .. commit pos should remain same\r\n    //3\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos3 = reader.getFileOffset();\r\n    //4\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos4 = reader.getFileOffset();\r\n    //5\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos5 = reader.getFileOffset();\r\n    //6\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos6 = reader.getFileOffset();\r\n    //7\r\n    assertNotNull(reader.next());\r\n    TextFileReader.Offset pos7 = reader.getFileOffset();\r\n    // now ack msg 5 and check\r\n    tracker.recordAckedOffset(pos5);\r\n    // should remain unchanged @ 2\r\n    assertEquals(pos2, tracker.getCommitPosition());\r\n    tracker.recordAckedOffset(pos4);\r\n    // should remain unchanged @ 2\r\n    assertEquals(pos2, tracker.getCommitPosition());\r\n    tracker.recordAckedOffset(pos3);\r\n    // should be at 5\r\n    assertEquals(pos5, tracker.getCommitPosition());\r\n    tracker.recordAckedOffset(pos6);\r\n    // should be at 6\r\n    assertEquals(pos6, tracker.getCommitPosition());\r\n    // double ack on same msg\r\n    tracker.recordAckedOffset(pos6);\r\n    // should still be at 6\r\n    assertEquals(pos6, tracker.getCommitPosition());\r\n    tracker.recordAckedOffset(pos7);\r\n    // should be at 7\r\n    assertEquals(pos7, tracker.getCommitPosition());\r\n    tracker.dumpState(System.err);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\format\\TestSimpleFileNameFormat.java",
  "methodName" : "testDefaults",
  "sourceCode" : "@Test\r\npublic void testDefaults() {\r\n    SimpleFileNameFormat format = new SimpleFileNameFormat();\r\n    format.prepare(null, 3, 5);\r\n    long now = System.currentTimeMillis();\r\n    String path = format.getPath();\r\n    String name = format.getName(1, now);\r\n    assertEquals(\"/storm\", path);\r\n    String time = new SimpleDateFormat(\"yyyyMMddHHmmss\").format(now);\r\n    assertEquals(time + \".1.txt\", name);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\format\\TestSimpleFileNameFormat.java",
  "methodName" : "testParameters",
  "sourceCode" : "@Test\r\npublic void testParameters() {\r\n    SimpleFileNameFormat format = new SimpleFileNameFormat().withName(\"$TIME.$HOST.$PARTITION.$NUM.txt\").withPath(\"/mypath\").withTimeFormat(\"yyyy-MM-dd HH:mm:ss\");\r\n    format.prepare(null, 3, 5);\r\n    long now = System.currentTimeMillis();\r\n    String path = format.getPath();\r\n    String name = format.getName(1, now);\r\n    assertEquals(\"/mypath\", path);\r\n    String time = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(now);\r\n    String host = null;\r\n    try {\r\n        host = Utils.localHostname();\r\n    } catch (UnknownHostException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertEquals(time + \".\" + host + \".3.1.txt\", name);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\format\\TestSimpleFileNameFormat.java",
  "methodName" : "testTimeFormat",
  "sourceCode" : "@Test\r\npublic void testTimeFormat() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        SimpleFileNameFormat format = new SimpleFileNameFormat().withTimeFormat(\"xyz\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\HdfsStateTest.java",
  "methodName" : "testPrepare",
  "sourceCode" : "@Test\r\npublic void testPrepare() {\r\n    HdfsState state = createHdfsState();\r\n    Collection<File> files = FileUtils.listFiles(new File(TEST_OUT_DIR), null, false);\r\n    File hdfsDataFile = Paths.get(TEST_OUT_DIR, FILE_NAME_PREFIX + \"0\").toFile();\r\n    assertTrue(files.contains(hdfsDataFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\HdfsStateTest.java",
  "methodName" : "testIndexFileCreation",
  "sourceCode" : "@Test\r\npublic void testIndexFileCreation() {\r\n    HdfsState state = createHdfsState();\r\n    state.beginCommit(1L);\r\n    Collection<File> files = FileUtils.listFiles(new File(TEST_OUT_DIR), null, false);\r\n    File hdfsIndexFile = Paths.get(TEST_OUT_DIR, INDEX_FILE_PREFIX + TEST_TOPOLOGY_NAME + \".0\").toFile();\r\n    assertTrue(files.contains(hdfsIndexFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\HdfsStateTest.java",
  "methodName" : "testUpdateState",
  "sourceCode" : "@Test\r\npublic void testUpdateState() throws Exception {\r\n    HdfsState state = createHdfsState();\r\n    state.beginCommit(1L);\r\n    int tupleCount = 100;\r\n    state.updateState(createMockTridentTuples(tupleCount), null);\r\n    state.commit(1L);\r\n    state.close();\r\n    List<String> lines = getLinesFromCurrentDataFile();\r\n    List<String> expected = new ArrayList<>();\r\n    for (int i = 0; i < tupleCount; i++) {\r\n        expected.add(\"data\");\r\n    }\r\n    assertEquals(tupleCount, lines.size());\r\n    assertEquals(expected, lines);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\HdfsStateTest.java",
  "methodName" : "testRecoverOneBatch",
  "sourceCode" : "@Test\r\npublic void testRecoverOneBatch() throws Exception {\r\n    HdfsState state = createHdfsState();\r\n    // batch 1 is played with 25 tuples initially.\r\n    state.beginCommit(1L);\r\n    state.updateState(createMockTridentTuples(25), null);\r\n    // batch 1 is replayed with 50 tuples.\r\n    int replayBatchSize = 50;\r\n    state.beginCommit(1L);\r\n    state.updateState(createMockTridentTuples(replayBatchSize), null);\r\n    state.commit(1L);\r\n    // close the state to force flush\r\n    state.close();\r\n    // Ensure that the original batch1 is discarded and new one is persisted.\r\n    List<String> lines = getLinesFromCurrentDataFile();\r\n    assertEquals(replayBatchSize, lines.size());\r\n    List<String> expected = new ArrayList<>();\r\n    for (int i = 0; i < replayBatchSize; i++) {\r\n        expected.add(\"data\");\r\n    }\r\n    assertEquals(expected, lines);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs\\src\\test\\java\\org\\apache\\storm\\hdfs\\trident\\HdfsStateTest.java",
  "methodName" : "testRecoverMultipleBatches",
  "sourceCode" : "@Test\r\npublic void testRecoverMultipleBatches() throws Exception {\r\n    HdfsState state = createHdfsState();\r\n    // batch 1\r\n    int batch1Count = 10;\r\n    state.beginCommit(1L);\r\n    state.updateState(createMockTridentTuples(batch1Count), null);\r\n    state.commit(1L);\r\n    // batch 2\r\n    int batch2Count = 20;\r\n    state.beginCommit(2L);\r\n    state.updateState(createMockTridentTuples(batch2Count), null);\r\n    state.commit(2L);\r\n    // batch 3\r\n    int batch3Count = 30;\r\n    state.beginCommit(3L);\r\n    state.updateState(createMockTridentTuples(batch3Count), null);\r\n    state.commit(3L);\r\n    // batch 3 replayed with 40 tuples\r\n    int batch3ReplayCount = 40;\r\n    state.beginCommit(3L);\r\n    state.updateState(createMockTridentTuples(batch3ReplayCount), null);\r\n    state.commit(3L);\r\n    state.close();\r\n    /*\r\n         * total tuples should be\r\n         * recovered (batch-1 + batch-2) + replayed (batch-3)\r\n         */\r\n    List<String> lines = getLinesFromCurrentDataFile();\r\n    int preReplayCount = batch1Count + batch2Count + batch3Count;\r\n    int expectedTupleCount = batch1Count + batch2Count + batch3ReplayCount;\r\n    assertNotEquals(preReplayCount, lines.size());\r\n    assertEquals(expectedTupleCount, lines.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testHdfsReplication",
  "sourceCode" : "@Test\r\npublic void testHdfsReplication() throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstoreReplication\")) {\r\n        testReplication(\"/storm/blobstoreReplication/test\", container.blobStore);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testBasicHdfs",
  "sourceCode" : "@Test\r\npublic void testBasicHdfs() throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore1\")) {\r\n        testBasic(container.blobStore);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testMultipleHdfs",
  "sourceCode" : "@Test\r\npublic void testMultipleHdfs() throws Exception {\r\n    // use different blobstore dir, so it doesn't conflict with other test\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore2\")) {\r\n        testMultiple(container.blobStore);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testWithAuthentication",
  "sourceCode" : "@ParameterizedTest\r\n@EnumSource(value = AuthenticationTestSubject.class)\r\nvoid testWithAuthentication(AuthenticationTestSubject testSubject) throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-\" + testSubject.name())) {\r\n        BlobStore store = container.blobStore;\r\n        assertStoreHasExactly(store);\r\n        SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\r\n        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, testSubject.subject)) {\r\n            assertStoreHasExactly(store, \"test\");\r\n            out.write(1);\r\n        }\r\n        store.deleteBlob(\"test\", testSubject.subject);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testWithAuthenticationDummy",
  "sourceCode" : "@ParameterizedTest\r\n@ValueSource(booleans = { true, false })\r\nvoid testWithAuthenticationDummy(boolean securityEnabled) throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-dummy-sec-\" + securityEnabled)) {\r\n        BlobStore store = container.blobStore;\r\n        Subject who = getSubject(\"test_subject\");\r\n        assertStoreHasExactly(store);\r\n        // Tests for case when subject != null (security turned on) and\r\n        // acls for the blob are set to WORLD_EVERYTHING\r\n        SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\r\n        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\r\n            out.write(1);\r\n        }\r\n        assertStoreHasExactly(store, \"test\");\r\n        if (securityEnabled) {\r\n            // Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because\r\n            // the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have\r\n            // complete access to the blob\r\n            assertFalse(metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"), \"ACL contains WORLD_EVERYTHING\");\r\n        } else {\r\n            // Testing whether acls are set to WORLD_EVERYTHING\r\n            assertTrue(metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"), \"ACL does not contain WORLD_EVERYTHING\");\r\n        }\r\n        readAssertEqualsWithAuth(store, who, \"test\", 1);\r\n        LOG.info(\"Deleting test\");\r\n        store.deleteBlob(\"test\", who);\r\n        assertStoreHasExactly(store);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testWithAuthenticationUpdate",
  "sourceCode" : "@Test\r\nvoid testWithAuthenticationUpdate() throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-update\")) {\r\n        BlobStore store = container.blobStore;\r\n        Subject who = getSubject(\"test_subject\");\r\n        assertStoreHasExactly(store);\r\n        SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\r\n        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\r\n            out.write(1);\r\n        }\r\n        assertStoreHasExactly(store, \"test\");\r\n        readAssertEqualsWithAuth(store, who, \"test\", 1);\r\n        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\r\n            out.write(2);\r\n        }\r\n        assertStoreHasExactly(store, \"test\");\r\n        readAssertEqualsWithAuth(store, who, \"test\", 2);\r\n        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\r\n            out.write(3);\r\n        }\r\n        assertStoreHasExactly(store, \"test\");\r\n        readAssertEqualsWithAuth(store, who, \"test\", 3);\r\n        LOG.info(\"Deleting test\");\r\n        store.deleteBlob(\"test\", who);\r\n        assertStoreHasExactly(store);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\BlobStoreTest.java",
  "methodName" : "testWithAuthenticationNoPrincipal",
  "sourceCode" : "@ParameterizedTest\r\n@ValueSource(booleans = { true, false })\r\nvoid testWithAuthenticationNoPrincipal(boolean securityEnabled) throws Exception {\r\n    try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-no-principal-sec-\" + securityEnabled)) {\r\n        BlobStore store = container.blobStore;\r\n        //Test for subject with no principals\r\n        Subject who = new Subject();\r\n        assertStoreHasExactly(store);\r\n        // Tests for case when subject != null (security turned on) and\r\n        // acls for the blob are set to WORLD_EVERYTHING\r\n        SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\r\n        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\r\n            out.write(1);\r\n        }\r\n        assertStoreHasExactly(store, \"test\");\r\n        // With no principals in the subject ACL should always be set to WORLD_EVERYTHING\r\n        assertTrue(metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"), \"ACL does not contain WORLD_EVERYTHING\");\r\n        readAssertEqualsWithAuth(store, who, \"test\", 1);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\HdfsBlobStoreImplTest.java",
  "methodName" : "testMultiple",
  "sourceCode" : "// Be careful about adding additional tests as the dfscluster will be shared\r\n@Test\r\npublic void testMultiple() throws Exception {\r\n    String testString = \"testingblob\";\r\n    String validKey = \"validkeyBasic\";\r\n    //Will be closed automatically when shutting down the DFS cluster\r\n    FileSystem fs = DFS_CLUSTER_EXTENSION.getDfscluster().getFileSystem();\r\n    Map<String, Object> conf = new HashMap<>();\r\n    try (TestHdfsBlobStoreImpl hbs = new TestHdfsBlobStoreImpl(blobDir, conf, DFS_CLUSTER_EXTENSION.getHadoopConf())) {\r\n        // should have created blobDir\r\n        assertTrue(fs.exists(blobDir), \"BlobStore dir wasn't created\");\r\n        assertEquals(HdfsBlobStoreImpl.BLOBSTORE_DIR_PERMISSION, fs.getFileStatus(blobDir).getPermission(), \"BlobStore dir was created with wrong permissions\");\r\n        // test exist with non-existent key\r\n        assertFalse(hbs.exists(\"bogus\"), \"file exists but shouldn't\");\r\n        // test write\r\n        BlobStoreFile pfile = hbs.write(validKey, false);\r\n        // Adding metadata to avoid null pointer exception\r\n        SettableBlobMeta meta = new SettableBlobMeta();\r\n        meta.set_replication_factor(1);\r\n        pfile.setMetadata(meta);\r\n        try (OutputStream ios = pfile.getOutputStream()) {\r\n            ios.write(testString.getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        // test modTime can change\r\n        long initialModTime = pfile.getModTime();\r\n        try (OutputStream ios = pfile.getOutputStream()) {\r\n            ios.write(testString.getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        long nextModTime = pfile.getModTime();\r\n        assertTrue(nextModTime > initialModTime);\r\n        // test commit creates properly\r\n        assertTrue(fs.exists(fullKeyDir), \"BlobStore key dir wasn't created\");\r\n        pfile.commit();\r\n        Path dataFile = new Path(new Path(fullKeyDir, validKey), BLOBSTORE_DATA);\r\n        assertTrue(fs.exists(dataFile), \"blob data not committed\");\r\n        assertEquals(HdfsBlobStoreFile.BLOBSTORE_FILE_PERMISSION, fs.getFileStatus(dataFile).getPermission(), \"BlobStore dir was created with wrong permissions\");\r\n        assertTrue(hbs.exists(validKey), \"key doesn't exist but should\");\r\n        // test read\r\n        BlobStoreFile readpFile = hbs.read(validKey);\r\n        try (InputStream inStream = readpFile.getInputStream()) {\r\n            String readString = IOUtils.toString(inStream, StandardCharsets.UTF_8);\r\n            assertEquals(testString, readString, \"string read from blob doesn't match\");\r\n        }\r\n        // test listkeys\r\n        Iterator<String> keys = hbs.listKeys();\r\n        assertTrue(keys.hasNext(), \"blob has one key\");\r\n        assertEquals(validKey, keys.next(), \"one key in blobstore\");\r\n        // delete\r\n        hbs.deleteKey(validKey);\r\n        assertFalse(fs.exists(dataFile), \"key not deleted\");\r\n        assertFalse(hbs.exists(validKey), \"key not deleted\");\r\n        // Now do multiple\r\n        String testString2 = \"testingblob2\";\r\n        String validKey2 = \"validkey2\";\r\n        // test write\r\n        pfile = hbs.write(validKey, false);\r\n        pfile.setMetadata(meta);\r\n        try (OutputStream ios = pfile.getOutputStream()) {\r\n            ios.write(testString.getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        // test commit creates properly\r\n        assertTrue(fs.exists(fullKeyDir), \"BlobStore key dir wasn't created\");\r\n        pfile.commit();\r\n        assertTrue(fs.exists(dataFile), \"blob data not committed\");\r\n        assertEquals(HdfsBlobStoreFile.BLOBSTORE_FILE_PERMISSION, fs.getFileStatus(dataFile).getPermission(), \"BlobStore dir was created with wrong permissions\");\r\n        assertTrue(hbs.exists(validKey), \"key doesn't exist but should\");\r\n        // test write again\r\n        pfile = hbs.write(validKey2, false);\r\n        pfile.setMetadata(meta);\r\n        try (OutputStream ios2 = pfile.getOutputStream()) {\r\n            ios2.write(testString2.getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        // test commit second creates properly\r\n        pfile.commit();\r\n        Path dataFile2 = new Path(new Path(fullKeyDir, validKey2), BLOBSTORE_DATA);\r\n        assertTrue(fs.exists(dataFile2), \"blob data not committed\");\r\n        assertEquals(HdfsBlobStoreFile.BLOBSTORE_FILE_PERMISSION, fs.getFileStatus(dataFile2).getPermission(), \"BlobStore dir was created with wrong permissions\");\r\n        assertTrue(hbs.exists(validKey2), \"key doesn't exist but should\");\r\n        // test listkeys\r\n        keys = hbs.listKeys();\r\n        int total = 0;\r\n        boolean key1Found = false;\r\n        boolean key2Found = false;\r\n        while (keys.hasNext()) {\r\n            total++;\r\n            String key = keys.next();\r\n            if (key.equals(validKey)) {\r\n                key1Found = true;\r\n            } else if (key.equals(validKey2)) {\r\n                key2Found = true;\r\n            } else {\r\n                fail(\"Found key that wasn't expected: \" + key);\r\n            }\r\n        }\r\n        assertEquals(2, total, \"number of keys is wrong\");\r\n        assertTrue(key1Found, \"blobstore missing key1\");\r\n        assertTrue(key2Found, \"blobstore missing key2\");\r\n        // test read\r\n        readpFile = hbs.read(validKey);\r\n        try (InputStream inStream = readpFile.getInputStream()) {\r\n            String readString = IOUtils.toString(inStream, StandardCharsets.UTF_8);\r\n            assertEquals(testString, readString, \"string read from blob doesn't match\");\r\n        }\r\n        // test read\r\n        readpFile = hbs.read(validKey2);\r\n        try (InputStream inStream = readpFile.getInputStream()) {\r\n            String readString = IOUtils.toString(inStream, StandardCharsets.UTF_8);\r\n            assertEquals(testString2, readString, \"string read from blob doesn't match\");\r\n        }\r\n        hbs.deleteKey(validKey);\r\n        assertFalse(hbs.exists(validKey), \"key not deleted\");\r\n        hbs.deleteKey(validKey2);\r\n        assertFalse(hbs.exists(validKey2), \"key not deleted\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\HdfsBlobStoreImplTest.java",
  "methodName" : "testGetFileLength",
  "sourceCode" : "@Test\r\npublic void testGetFileLength() throws Exception {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    String validKey = \"validkeyBasic\";\r\n    String testString = \"testingblob\";\r\n    try (TestHdfsBlobStoreImpl hbs = new TestHdfsBlobStoreImpl(blobDir, conf, DFS_CLUSTER_EXTENSION.getHadoopConf())) {\r\n        BlobStoreFile pfile = hbs.write(validKey, false);\r\n        // Adding metadata to avoid null pointer exception\r\n        SettableBlobMeta meta = new SettableBlobMeta();\r\n        meta.set_replication_factor(1);\r\n        pfile.setMetadata(meta);\r\n        try (OutputStream ios = pfile.getOutputStream()) {\r\n            ios.write(testString.getBytes(StandardCharsets.UTF_8));\r\n        }\r\n        assertEquals(testString.getBytes(StandardCharsets.UTF_8).length, pfile.getFileLength());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-hdfs-blobstore\\src\\test\\java\\org\\apache\\storm\\hdfs\\blobstore\\HdfsBlobStoreImplTest.java",
  "methodName" : "testConcurrentIteration",
  "sourceCode" : "/**\r\n * Test by listing keys {@link HdfsBlobStoreImpl#listKeys()} in multiple concurrent threads and then ensure that\r\n * same keys are retrived in all the threads without any exceptions.\r\n */\r\n@Test\r\npublic void testConcurrentIteration() throws Exception {\r\n    int concurrency = 100;\r\n    int keyCount = 10;\r\n    class ConcurrentListerRunnable implements Runnable {\r\n\r\n        TestHdfsBlobStoreImpl hbs;\r\n\r\n        int instanceNum;\r\n\r\n        List<String> keys = new ArrayList<>();\r\n\r\n        public ConcurrentListerRunnable(TestHdfsBlobStoreImpl hbs, int instanceNum) {\r\n            this.hbs = hbs;\r\n            this.instanceNum = instanceNum;\r\n        }\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                Iterator<String> iterator = hbs.listKeys(concurrentTestFullKeyDir);\r\n                while (iterator.hasNext()) {\r\n                    keys.add(iterator.next());\r\n                }\r\n            } catch (Exception ex) {\r\n                ex.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n    Map<String, Object> conf = new HashMap<>();\r\n    try (TestHdfsBlobStoreImpl hbs = new TestHdfsBlobStoreImpl(concurrentTestBlobDir, conf, DFS_CLUSTER_EXTENSION.getHadoopConf())) {\r\n        // test write again\r\n        for (int i = 0; i < keyCount; i++) {\r\n            String key = CONCURRENT_TEST_KEY_PREFIX + i;\r\n            String val = \"This is string \" + i;\r\n            BlobStoreFile pfile = hbs.write(key, false);\r\n            SettableBlobMeta meta = new SettableBlobMeta();\r\n            meta.set_replication_factor(1);\r\n            pfile.setMetadata(meta);\r\n            try (OutputStream ios = pfile.getOutputStream()) {\r\n                ios.write(val.getBytes(StandardCharsets.UTF_8));\r\n            }\r\n        }\r\n        ConcurrentListerRunnable[] runnables = new ConcurrentListerRunnable[concurrency];\r\n        Thread[] threads = new Thread[concurrency];\r\n        for (int i = 0; i < concurrency; i++) {\r\n            runnables[i] = new ConcurrentListerRunnable(hbs, i);\r\n            threads[i] = new Thread(runnables[i]);\r\n        }\r\n        for (int i = 0; i < concurrency; i++) {\r\n            threads[i].start();\r\n        }\r\n        for (int i = 0; i < concurrency; i++) {\r\n            threads[i].join();\r\n        }\r\n        List<String> keys = runnables[0].keys;\r\n        assertEquals(keyCount, keys.size(), \"Number of keys (values=\" + keys + \")\");\r\n        for (int i = 1; i < concurrency; i++) {\r\n            ConcurrentListerRunnable otherRunnable = runnables[i];\r\n            assertEquals(keys, otherRunnable.keys);\r\n        }\r\n        for (int i = 0; i < keyCount; i++) {\r\n            String key = CONCURRENT_TEST_KEY_PREFIX + i;\r\n            hbs.deleteKey(key);\r\n        }\r\n        LOG.info(\"All %d threads have %d keys=[%s]\\n\", concurrency, keys.size(), String.join(\",\", keys));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\bolt\\JdbcInsertBoltTest.java",
  "methodName" : "testValidation",
  "sourceCode" : "@Test\r\npublic void testValidation() {\r\n    ConnectionProvider provider = new HikariCPConnectionProvider(new HashMap<>());\r\n    JdbcMapper mapper = new SimpleJdbcMapper(Lists.newArrayList(new Column<String>(\"test\", 0)));\r\n    expectNullPointerException(null, mapper);\r\n    expectNullPointerException(provider, null);\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        JdbcInsertBolt bolt = new JdbcInsertBolt(provider, mapper);\r\n        bolt.withInsertQuery(\"test\");\r\n        bolt.withTableName(\"test\");\r\n    });\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        JdbcInsertBolt bolt = new JdbcInsertBolt(provider, mapper);\r\n        bolt.withTableName(\"test\");\r\n        bolt.withInsertQuery(\"test\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\bolt\\JdbcLookupBoltTest.java",
  "methodName" : "testValidation",
  "sourceCode" : "@Test\r\npublic void testValidation() {\r\n    ConnectionProvider provider = new HikariCPConnectionProvider(new HashMap<>());\r\n    JdbcLookupMapper mapper = new SimpleJdbcLookupMapper(new Fields(\"test\"), Lists.newArrayList(new Column<String>(\"test\", 0)));\r\n    String selectQuery = \"select * from dual\";\r\n    expectNullPointerException(null, selectQuery, mapper);\r\n    expectNullPointerException(provider, null, mapper);\r\n    expectNullPointerException(provider, selectQuery, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\common\\JdbcClientTest.java",
  "methodName" : "testInsertAndSelect",
  "sourceCode" : "@Test\r\npublic void testInsertAndSelect() {\r\n    List<Column> row1 = createRow(1, \"bob\");\r\n    List<Column> row2 = createRow(2, \"alice\");\r\n    List<List<Column>> rows = Lists.newArrayList(row1, row2);\r\n    client.insert(tableName, rows);\r\n    List<List<Column>> selectedRows = client.select(\"select * from user_details where id = ?\", Lists.newArrayList(new Column(\"id\", 1, Types.INTEGER)));\r\n    List<List<Column>> expectedRows = Lists.newArrayList();\r\n    expectedRows.add(row1);\r\n    assertEquals(expectedRows, selectedRows);\r\n    List<Column> row3 = createRow(3, \"frank\");\r\n    List<List<Column>> moreRows = new ArrayList<List<Column>>();\r\n    moreRows.add(row3);\r\n    client.executeInsertQuery(\"insert into user_details values(?,?,?)\", moreRows);\r\n    selectedRows = client.select(\"select * from user_details where id = ?\", Lists.newArrayList(new Column(\"id\", 3, Types.INTEGER)));\r\n    expectedRows = Lists.newArrayList();\r\n    expectedRows.add(row3);\r\n    assertEquals(expectedRows, selectedRows);\r\n    selectedRows = client.select(\"select * from user_details order by id\", Lists.<Column>newArrayList());\r\n    rows.add(row3);\r\n    assertEquals(rows, selectedRows);\r\n    client.executeSql(\"drop table \" + tableName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\common\\JdbcClientTest.java",
  "methodName" : "testInsertConnectionError",
  "sourceCode" : "@Test\r\npublic void testInsertConnectionError() {\r\n    ConnectionProvider connectionProvider = new ThrowingConnectionProvider(null);\r\n    this.client = new JdbcClient(connectionProvider, 60);\r\n    List<Column> row = createRow(1, \"frank\");\r\n    List<List<Column>> rows = new ArrayList<>();\r\n    rows.add(row);\r\n    String query = \"insert into user_details values(?,?,?)\";\r\n    assertThrows(RuntimeException.class, () -> client.executeInsertQuery(query, rows));\r\n    assertThrows(RuntimeException.class, () -> client.executeSql(\"drop table \" + tableName));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\common\\UtilTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    assertEquals(String.class, Util.getJavaType(Types.CHAR));\r\n    assertEquals(String.class, Util.getJavaType(Types.VARCHAR));\r\n    assertEquals(String.class, Util.getJavaType(Types.LONGVARCHAR));\r\n    assertEquals(byte[].class, Util.getJavaType(Types.BINARY));\r\n    assertEquals(byte[].class, Util.getJavaType(Types.VARBINARY));\r\n    assertEquals(byte[].class, Util.getJavaType(Types.LONGVARBINARY));\r\n    assertEquals(Boolean.class, Util.getJavaType(Types.BIT));\r\n    assertEquals(Short.class, Util.getJavaType(Types.TINYINT));\r\n    assertEquals(Short.class, Util.getJavaType(Types.SMALLINT));\r\n    assertEquals(Integer.class, Util.getJavaType(Types.INTEGER));\r\n    assertEquals(Long.class, Util.getJavaType(Types.BIGINT));\r\n    assertEquals(Float.class, Util.getJavaType(Types.REAL));\r\n    assertEquals(Double.class, Util.getJavaType(Types.DOUBLE));\r\n    assertEquals(Double.class, Util.getJavaType(Types.FLOAT));\r\n    assertEquals(Date.class, Util.getJavaType(Types.DATE));\r\n    assertEquals(Time.class, Util.getJavaType(Types.TIME));\r\n    assertEquals(Timestamp.class, Util.getJavaType(Types.TIMESTAMP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jdbc\\src\\test\\java\\org\\apache\\storm\\jdbc\\common\\UtilTest.java",
  "methodName" : "testError",
  "sourceCode" : "@Test\r\npublic void testError() {\r\n    Exception e = assertThrows(Exception.class, () -> Util.getJavaType(Types.REF));\r\n    assertEquals(\"We do not support tables with SqlType: REF\", e.getMessage());\r\n    e = assertThrows(Exception.class, () -> Util.getJavaType(-1000));\r\n    assertEquals(\"Unknown sqlType -1000\", e.getMessage());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jms\\src\\test\\java\\org\\apache\\storm\\jms\\spout\\JmsSpoutTest.java",
  "methodName" : "testFailure",
  "sourceCode" : "@Test\r\npublic void testFailure() throws Exception {\r\n    JmsSpout spout = new JmsSpout();\r\n    try {\r\n        JmsProvider mockProvider = new MockJmsProvider();\r\n        MockSpoutOutputCollector mockCollector = new MockSpoutOutputCollector();\r\n        SpoutOutputCollector collector = new SpoutOutputCollector(mockCollector);\r\n        spout.setJmsProvider(new MockJmsProvider());\r\n        spout.setJmsTupleProducer(new MockTupleProducer());\r\n        spout.setJmsAcknowledgeMode(Session.CLIENT_ACKNOWLEDGE);\r\n        spout.open(new HashMap<>(), null, collector);\r\n        ConnectionFactory connectionFactory = mockProvider.connectionFactory();\r\n        Destination destination = mockProvider.destination();\r\n        Message msg = this.sendMessage(connectionFactory, destination);\r\n        Thread.sleep(100);\r\n        LOG.info(\"Calling nextTuple on the spout...\");\r\n        // Pretend to be storm.\r\n        spout.nextTuple();\r\n        assertTrue(mockCollector.emitted);\r\n        mockCollector.reset();\r\n        // Mock failure\r\n        spout.fail(msg.getJMSMessageID());\r\n        Thread.sleep(5000);\r\n        // Pretend to be storm.\r\n        spout.nextTuple();\r\n        Thread.sleep(5000);\r\n        // Should have been re-emitted\r\n        assertTrue(mockCollector.emitted);\r\n    } finally {\r\n        spout.close();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jms\\src\\test\\java\\org\\apache\\storm\\jms\\spout\\JmsSpoutTest.java",
  "methodName" : "testSerializability",
  "sourceCode" : "@Test\r\npublic void testSerializability() throws IOException {\r\n    JmsSpout spout = new JmsSpout();\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    ObjectOutputStream oos = new ObjectOutputStream(out);\r\n    oos.writeObject(spout);\r\n    oos.close();\r\n    assertTrue(out.toByteArray().length > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-jms\\src\\test\\java\\org\\apache\\storm\\jms\\spout\\JmsSpoutTest.java",
  "methodName" : "testOpenWorksMultipleTypesOfNumberObjects",
  "sourceCode" : "/**\r\n * Make sure that {@link JmsSpout#open} returns correctly regardless of the type of {@link Number} that is the value of {@link\r\n * Config#TOPOLOGY_MESSAGE_TIMEOUT_SECS}.\r\n */\r\n@Test\r\npublic void testOpenWorksMultipleTypesOfNumberObjects() throws Exception {\r\n    JmsSpout spout = new JmsSpout();\r\n    try {\r\n        spout.setJmsProvider(new MockJmsProvider());\r\n        spout.setJmsTupleProducer(new MockTupleProducer());\r\n        Map<String, Object> configuration = new HashMap<>();\r\n        MockSpoutOutputCollector delegateCollector = new MockSpoutOutputCollector();\r\n        SpoutOutputCollector collector = new SpoutOutputCollector(delegateCollector);\r\n        // Test with long value\r\n        configuration.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 1000L);\r\n        spout.open(configuration, null, collector);\r\n        spout.close();\r\n        // Test with integer value\r\n        configuration.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 1000);\r\n        spout.open(configuration, null, collector);\r\n    } finally {\r\n        spout.close();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\main\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManager.java",
  "methodName" : "containsEmitted",
  "sourceCode" : "@VisibleForTesting\r\nboolean containsEmitted(long offset) {\r\n    return emittedOffsets.contains(offset);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\main\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpout.java",
  "methodName" : "getKafkaOffsetMetricManager",
  "sourceCode" : "@VisibleForTesting\r\nKafkaOffsetMetricManager<K, V> getKafkaOffsetMetricManager() {\r\n    return kafkaOffsetMetricManager;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\bolt\\KafkaBoltTest.java",
  "methodName" : "testSimple",
  "sourceCode" : "@Test\r\npublic void testSimple() {\r\n    MockProducer<String, String> producer = new MockProducer<>(Cluster.empty(), false, null, new StringSerializer(), new StringSerializer());\r\n    KafkaBolt<String, String> bolt = makeBolt(producer);\r\n    OutputCollector collector = mock(OutputCollector.class);\r\n    TopologyContext context = mock(TopologyContext.class);\r\n    Map<String, Object> conf = new HashMap<>();\r\n    bolt.prepare(conf, context, collector);\r\n    String key = \"KEY\";\r\n    String value = \"VALUE\";\r\n    Tuple testTuple = createTestTuple(key, value);\r\n    bolt.execute(testTuple);\r\n    assertThat(producer.history().size(), is(1));\r\n    ProducerRecord<String, String> arg = producer.history().get(0);\r\n    LOG.info(\"GOT {} ->\", arg);\r\n    LOG.info(\"{}, {}, {}\", arg.topic(), arg.key(), arg.value());\r\n    assertThat(arg.topic(), is(\"MY_TOPIC\"));\r\n    assertThat(arg.key(), is(key));\r\n    assertThat(arg.value(), is(value));\r\n    // Complete the send\r\n    producer.completeNext();\r\n    verify(collector).ack(testTuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\bolt\\KafkaBoltTest.java",
  "methodName" : "testSimpleWithError",
  "sourceCode" : "@Test\r\npublic void testSimpleWithError() {\r\n    MockProducer<String, String> producer = new MockProducer<>(Cluster.empty(), false, null, new StringSerializer(), new StringSerializer());\r\n    KafkaBolt<String, String> bolt = makeBolt(producer);\r\n    OutputCollector collector = mock(OutputCollector.class);\r\n    TopologyContext context = mock(TopologyContext.class);\r\n    Map<String, Object> conf = new HashMap<>();\r\n    bolt.prepare(conf, context, collector);\r\n    String key = \"KEY\";\r\n    String value = \"VALUE\";\r\n    Tuple testTuple = createTestTuple(key, value);\r\n    bolt.execute(testTuple);\r\n    assertThat(producer.history().size(), is(1));\r\n    ProducerRecord<String, String> arg = producer.history().get(0);\r\n    LOG.info(\"GOT {} ->\", arg);\r\n    LOG.info(\"{}, {}, {}\", arg.topic(), arg.key(), arg.value());\r\n    assertThat(arg.topic(), is(\"MY_TOPIC\"));\r\n    assertThat(arg.key(), is(key));\r\n    assertThat(arg.value(), is(value));\r\n    // Force a send error\r\n    KafkaException ex = new KafkaException();\r\n    producer.errorNext(ex);\r\n    verify(collector).reportError(ex);\r\n    verify(collector).fail(testTuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\bolt\\KafkaBoltTest.java",
  "methodName" : "testCustomCallbackIsWrappedByDefaultCallbackBehavior",
  "sourceCode" : "@Test\r\npublic void testCustomCallbackIsWrappedByDefaultCallbackBehavior() {\r\n    MockProducer<String, String> producer = new MockProducer<>(Cluster.empty(), false, null, new StringSerializer(), new StringSerializer());\r\n    KafkaBolt<String, String> bolt = makeBolt(producer);\r\n    PreparableCallback customCallback = mock(PreparableCallback.class);\r\n    bolt.withProducerCallback(customCallback);\r\n    OutputCollector collector = mock(OutputCollector.class);\r\n    TopologyContext context = mock(TopologyContext.class);\r\n    Map<String, Object> topoConfig = new HashMap<>();\r\n    bolt.prepare(topoConfig, context, collector);\r\n    verify(customCallback).prepare(topoConfig, context);\r\n    String key = \"KEY\";\r\n    String value = \"VALUE\";\r\n    Tuple testTuple = createTestTuple(key, value);\r\n    bolt.execute(testTuple);\r\n    assertThat(producer.history().size(), is(1));\r\n    ProducerRecord<String, String> arg = producer.history().get(0);\r\n    LOG.info(\"GOT {} ->\", arg);\r\n    LOG.info(\"{}, {}, {}\", arg.topic(), arg.key(), arg.value());\r\n    assertThat(arg.topic(), is(\"MY_TOPIC\"));\r\n    assertThat(arg.key(), is(key));\r\n    assertThat(arg.value(), is(value));\r\n    // Force a send error\r\n    KafkaException ex = new KafkaException();\r\n    producer.errorNext(ex);\r\n    verify(customCallback).onCompletion(any(), eq(ex));\r\n    verify(collector).reportError(ex);\r\n    verify(collector).fail(testTuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\ByTopicRecordTranslatorTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    ByTopicRecordTranslator<String, String> trans = new ByTopicRecordTranslator<>((r) -> new Values(r.key()), new Fields(\"key\"));\r\n    trans.forTopic(\"TOPIC 1\", (r) -> new Values(r.value()), new Fields(\"value\"), \"value-stream\");\r\n    trans.forTopic(\"TOPIC 2\", (r) -> new Values(r.key(), r.value()), new Fields(\"key\", \"value\"), \"key-value-stream\");\r\n    HashSet<String> expectedStreams = new HashSet<>();\r\n    expectedStreams.add(\"default\");\r\n    expectedStreams.add(\"value-stream\");\r\n    expectedStreams.add(\"key-value-stream\");\r\n    assertEquals(expectedStreams, new HashSet<>(trans.streams()));\r\n    ConsumerRecord<String, String> cr1 = new ConsumerRecord<>(\"TOPIC OTHER\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertEquals(new Fields(\"key\"), trans.getFieldsFor(\"default\"));\r\n    assertEquals(Collections.singletonList(\"THE KEY\"), trans.apply(cr1));\r\n    ConsumerRecord<String, String> cr2 = new ConsumerRecord<>(\"TOPIC 1\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertEquals(new Fields(\"value\"), trans.getFieldsFor(\"value-stream\"));\r\n    assertEquals(Collections.singletonList(\"THE VALUE\"), trans.apply(cr2));\r\n    ConsumerRecord<String, String> cr3 = new ConsumerRecord<>(\"TOPIC 2\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertEquals(new Fields(\"key\", \"value\"), trans.getFieldsFor(\"key-value-stream\"));\r\n    assertEquals(Arrays.asList(\"THE KEY\", \"THE VALUE\"), trans.apply(cr3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\ByTopicRecordTranslatorTest.java",
  "methodName" : "testNullTranslation",
  "sourceCode" : "@Test\r\npublic void testNullTranslation() {\r\n    ByTopicRecordTranslator<String, String> trans = new ByTopicRecordTranslator<>((r) -> null, new Fields(\"key\"));\r\n    ConsumerRecord<String, String> cr = new ConsumerRecord<>(\"TOPIC 1\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertNull(trans.apply(cr));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\ByTopicRecordTranslatorTest.java",
  "methodName" : "testFieldCollision",
  "sourceCode" : "@Test\r\npublic void testFieldCollision() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        ByTopicRecordTranslator<String, String> trans = new ByTopicRecordTranslator<>((r) -> new Values(r.key()), new Fields(\"key\"));\r\n        trans.forTopic(\"foo\", (r) -> new Values(r.value()), new Fields(\"value\"));\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\ByTopicRecordTranslatorTest.java",
  "methodName" : "testTopicCollision",
  "sourceCode" : "@Test\r\npublic void testTopicCollision() {\r\n    assertThrows(IllegalStateException.class, () -> {\r\n        ByTopicRecordTranslator<String, String> trans = new ByTopicRecordTranslator<>((r) -> new Values(r.key()), new Fields(\"key\"));\r\n        trans.forTopic(\"foo\", (r) -> new Values(r.value()), new Fields(\"value\"), \"foo1\");\r\n        trans.forTopic(\"foo\", (r) -> new Values(r.key(), r.value()), new Fields(\"key\", \"value\"), \"foo2\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\DefaultRecordTranslatorTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    DefaultRecordTranslator<String, String> trans = new DefaultRecordTranslator<>();\r\n    assertEquals(Collections.singletonList(\"default\"), trans.streams());\r\n    assertEquals(new Fields(\"topic\", \"partition\", \"offset\", \"key\", \"value\"), trans.getFieldsFor(\"default\"));\r\n    ConsumerRecord<String, String> cr = new ConsumerRecord<>(\"TOPIC\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertEquals(Arrays.asList(\"TOPIC\", 100, 100L, \"THE KEY\", \"THE VALUE\"), trans.apply(cr));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testSkipMissingOffsetsWhenFindingNextCommitOffsetWithGapInMiddleOfAcked",
  "sourceCode" : "@Test\r\npublic void testSkipMissingOffsetsWhenFindingNextCommitOffsetWithGapInMiddleOfAcked() {\r\n    /* If topic compaction is enabled in Kafka, we sometimes need to commit past a gap of deleted offsets\r\n         * Since the Kafka consumer should return offsets in order, we can assume that if a message is acked\r\n         * then any prior message will have been emitted at least once.\r\n         * If we see an acked message and some of the offsets preceding it were not emitted, they must have been compacted away and should be skipped.\r\n         */\r\n    manager.addToEmitMsgs(0);\r\n    manager.addToEmitMsgs(1);\r\n    manager.addToEmitMsgs(2);\r\n    //3, 4 compacted away\r\n    manager.addToEmitMsgs(initialFetchOffset + 5);\r\n    manager.addToEmitMsgs(initialFetchOffset + 6);\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset));\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 1));\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 2));\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 6));\r\n    assertThat(\"The offset manager should not skip past offset 5 which is still pending\", manager.findNextCommitOffset(COMMIT_METADATA).offset(), is(initialFetchOffset + 3));\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 5));\r\n    assertThat(\"The offset manager should skip past the gap in acked messages, since the messages were not emitted\", manager.findNextCommitOffset(COMMIT_METADATA), is(new OffsetAndMetadata(initialFetchOffset + 7, COMMIT_METADATA)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testSkipMissingOffsetsWhenFindingNextCommitOffsetWithGapBeforeAcked",
  "sourceCode" : "@Test\r\npublic void testSkipMissingOffsetsWhenFindingNextCommitOffsetWithGapBeforeAcked() {\r\n    //0-4 compacted away\r\n    manager.addToEmitMsgs(initialFetchOffset + 5);\r\n    manager.addToEmitMsgs(initialFetchOffset + 6);\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 6));\r\n    assertThat(\"The offset manager should not skip past offset 5 which is still pending\", manager.findNextCommitOffset(COMMIT_METADATA), is(nullValue()));\r\n    manager.addToAckMsgs(getMessageId(initialFetchOffset + 5));\r\n    assertThat(\"The offset manager should skip past the gap in acked messages, since the messages were not emitted\", manager.findNextCommitOffset(COMMIT_METADATA), is(new OffsetAndMetadata(initialFetchOffset + 7, COMMIT_METADATA)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommittedOffsetWithNoAcks",
  "sourceCode" : "@Test\r\npublic void testFindNextCommittedOffsetWithNoAcks() {\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"There shouldn't be a next commit offset when nothing has been acked\", nextCommitOffset, is(nullValue()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommitOffsetWithOneAck",
  "sourceCode" : "@Test\r\npublic void testFindNextCommitOffsetWithOneAck() {\r\n    /*\r\n         * The KafkaConsumer commitSync API docs: \"The committed offset should be the next message your application will consume, i.e.\r\n         * lastProcessedMessageOffset + 1. \"\r\n         */\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"The next commit offset should be one past the processed message offset\", nextCommitOffset.offset(), is(initialFetchOffset + 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommitOffsetWithMultipleOutOfOrderAcks",
  "sourceCode" : "@Test\r\npublic void testFindNextCommitOffsetWithMultipleOutOfOrderAcks() {\r\n    emitAndAckMessage(getMessageId(initialFetchOffset + 1));\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"The next commit offset should be one past the processed message offset\", nextCommitOffset.offset(), is(initialFetchOffset + 2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommitOffsetWithAckedOffsetGap",
  "sourceCode" : "@Test\r\npublic void testFindNextCommitOffsetWithAckedOffsetGap() {\r\n    emitAndAckMessage(getMessageId(initialFetchOffset + 2));\r\n    manager.addToEmitMsgs(initialFetchOffset + 1);\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"The next commit offset should cover the sequential acked offsets\", nextCommitOffset.offset(), is(initialFetchOffset + 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextOffsetWithAckedButNotEmittedOffsetGap",
  "sourceCode" : "@Test\r\npublic void testFindNextOffsetWithAckedButNotEmittedOffsetGap() {\r\n    /**\r\n     * If topic compaction is enabled in Kafka some offsets may be deleted.\r\n     * We distinguish this case from regular gaps in the acked offset sequence caused by out of order acking\r\n     * by checking that offsets in the gap have been emitted at some point previously.\r\n     * If they haven't then they can't exist in Kafka, since the spout emits tuples in order.\r\n     */\r\n    emitAndAckMessage(getMessageId(initialFetchOffset + 2));\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"The next commit offset should cover all the acked offsets, since the offset in the gap hasn't been emitted and doesn't exist\", nextCommitOffset.offset(), is(initialFetchOffset + 3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommitOffsetWithUnackedOffsetGap",
  "sourceCode" : "@Test\r\npublic void testFindNextCommitOffsetWithUnackedOffsetGap() {\r\n    manager.addToEmitMsgs(initialFetchOffset + 1);\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    OffsetAndMetadata nextCommitOffset = manager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"The next commit offset should cover the contiguously acked offsets\", nextCommitOffset.offset(), is(initialFetchOffset + 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testFindNextCommitOffsetWhenTooLowOffsetIsAcked",
  "sourceCode" : "@Test\r\npublic void testFindNextCommitOffsetWhenTooLowOffsetIsAcked() {\r\n    OffsetManager startAtHighOffsetManager = new OffsetManager(testTp, 10);\r\n    emitAndAckMessage(getMessageId(0));\r\n    OffsetAndMetadata nextCommitOffset = startAtHighOffsetManager.findNextCommitOffset(COMMIT_METADATA);\r\n    assertThat(\"Acking an offset earlier than the committed offset should have no effect\", nextCommitOffset, is(nullValue()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testCommit",
  "sourceCode" : "@Test\r\npublic void testCommit() {\r\n    emitAndAckMessage(getMessageId(initialFetchOffset));\r\n    emitAndAckMessage(getMessageId(initialFetchOffset + 1));\r\n    emitAndAckMessage(getMessageId(initialFetchOffset + 2));\r\n    long committedMessages = manager.commit(new OffsetAndMetadata(initialFetchOffset + 2));\r\n    assertThat(\"Should have committed all messages to the left of the earliest uncommitted offset\", committedMessages, is(2L));\r\n    assertThat(\"The committed messages should not be in the acked list anymore\", manager.contains(getMessageId(initialFetchOffset)), is(false));\r\n    assertThat(\"The committed messages should not be in the emitted list anymore\", manager.containsEmitted(initialFetchOffset), is(false));\r\n    assertThat(\"The committed messages should not be in the acked list anymore\", manager.contains(getMessageId(initialFetchOffset + 1)), is(false));\r\n    assertThat(\"The committed messages should not be in the emitted list anymore\", manager.containsEmitted(initialFetchOffset + 1), is(false));\r\n    assertThat(\"The uncommitted message should still be in the acked list\", manager.contains(getMessageId(initialFetchOffset + 2)), is(true));\r\n    assertThat(\"The uncommitted message should still be in the emitted list\", manager.containsEmitted(initialFetchOffset + 2), is(true));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testGetNthUncommittedOffsetAfterCommittedOffset",
  "sourceCode" : "@Test\r\npublic void testGetNthUncommittedOffsetAfterCommittedOffset() {\r\n    manager.addToEmitMsgs(initialFetchOffset + 1);\r\n    manager.addToEmitMsgs(initialFetchOffset + 2);\r\n    manager.addToEmitMsgs(initialFetchOffset + 5);\r\n    manager.addToEmitMsgs(initialFetchOffset + 30);\r\n    assertThat(\"The third uncommitted offset should be 5\", manager.getNthUncommittedOffsetAfterCommittedOffset(3), is(initialFetchOffset + 5L));\r\n    assertThat(\"The fourth uncommitted offset should be 30\", manager.getNthUncommittedOffsetAfterCommittedOffset(4), is(initialFetchOffset + 30L));\r\n    Assertions.assertThrows(NoSuchElementException.class, () -> manager.getNthUncommittedOffsetAfterCommittedOffset(5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\internal\\OffsetManagerTest.java",
  "methodName" : "testCommittedFlagSetOnCommit",
  "sourceCode" : "@Test\r\npublic void testCommittedFlagSetOnCommit() throws Exception {\r\n    assertFalse(manager.hasCommitted());\r\n    manager.commit(mock(OffsetAndMetadata.class));\r\n    assertTrue(manager.hasCommitted());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").build();\r\n    assertEquals(conf.getFirstPollOffsetStrategy(), FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST);\r\n    assertNull(conf.getConsumerGroupId());\r\n    assertTrue(conf.getTranslator() instanceof DefaultRecordTranslator);\r\n    HashMap<String, Object> expected = new HashMap<>();\r\n    expected.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\r\n    expected.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\r\n    expected.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\r\n    expected.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\r\n    expected.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\r\n    assertEquals(conf.getKafkaProps(), expected);\r\n    assertEquals(conf.getMetricsTimeBucketSizeInSecs(), KafkaSpoutConfig.DEFAULT_METRICS_TIME_BUCKET_SIZE_SECONDS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testSetEmitNullTuplesToTrue",
  "sourceCode" : "@Test\r\npublic void testSetEmitNullTuplesToTrue() {\r\n    final KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").setEmitNullTuples(true).build();\r\n    assertTrue(conf.isEmitNullTuples(), \"Failed to set emit null tuples to true\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testShouldNotChangeAutoOffsetResetPolicyWhenNotUsingAtLeastOnce",
  "sourceCode" : "@Test\r\npublic void testShouldNotChangeAutoOffsetResetPolicyWhenNotUsingAtLeastOnce() {\r\n    KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE).build();\r\n    assertThat(\"When at-least-once is not specified, the spout should use the Kafka default auto offset reset policy\", conf.getKafkaProps().get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), nullValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testWillRespectExplicitAutoOffsetResetPolicy",
  "sourceCode" : "@Test\r\npublic void testWillRespectExplicitAutoOffsetResetPolicy() {\r\n    KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").setProp(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"none\").build();\r\n    assertThat(\"Should allow users to pick a different auto offset reset policy than the one recommended for the at-least-once processing guarantee\", conf.getKafkaProps().get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), is(\"none\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testMetricsTimeBucketSizeInSecs",
  "sourceCode" : "@Test\r\npublic void testMetricsTimeBucketSizeInSecs() {\r\n    KafkaSpoutConfig<String, String> conf = KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").setMetricsTimeBucketSizeInSecs(100).build();\r\n    assertEquals(conf.getMetricsTimeBucketSizeInSecs(), 100);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutConfigTest.java",
  "methodName" : "testThrowsIfEnableAutoCommitIsSet",
  "sourceCode" : "@Test\r\npublic void testThrowsIfEnableAutoCommitIsSet() {\r\n    Assertions.assertThrows(IllegalStateException.class, () -> KafkaSpoutConfig.builder(\"localhost:1234\", \"topic\").setProp(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true).build());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutEmitTest.java",
  "methodName" : "testNextTupleEmitsAtMostOneTuple",
  "sourceCode" : "@Test\r\npublic void testNextTupleEmitsAtMostOneTuple() {\r\n    //The spout should emit at most one message per call to nextTuple\r\n    //This is necessary for Storm to be able to throttle the spout according to maxSpoutPending\r\n    KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n    Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\r\n    records.put(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 10));\r\n    when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(records));\r\n    spout.nextTuple();\r\n    verify(collectorMock, times(1)).emit(anyString(), anyList(), any(KafkaSpoutMessageId.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutEmitTest.java",
  "methodName" : "testNextTupleEmitsFailedMessagesEvenWhenMaxUncommittedOffsetsIsExceeded",
  "sourceCode" : "@Test\r\npublic void testNextTupleEmitsFailedMessagesEvenWhenMaxUncommittedOffsetsIsExceeded() throws IOException {\r\n    //The spout must reemit failed messages waiting for retry even if it is not allowed to poll for new messages due to maxUncommittedOffsets being exceeded\r\n    //Emit maxUncommittedOffsets messages, and fail all of them. Then ensure that the spout will retry them when the retry backoff has passed\r\n    try (SimulatedTime simulatedTime = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\r\n        int numRecords = spoutConfig.getMaxUncommittedOffsets();\r\n        //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\r\n        records.put(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, numRecords));\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(records));\r\n        for (int i = 0; i < numRecords; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock, times(numRecords)).emit(anyString(), anyList(), messageIds.capture());\r\n        for (KafkaSpoutMessageId messageId : messageIds.getAllValues()) {\r\n            spout.fail(messageId);\r\n        }\r\n        reset(collectorMock);\r\n        Time.advanceTime(50);\r\n        //No backoff for test retry service, just check that messages will retry immediately\r\n        for (int i = 0; i < numRecords; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> retryMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock, times(numRecords)).emit(anyString(), anyList(), retryMessageIds.capture());\r\n        //Verify that the poll started at the earliest retriable tuple offset\r\n        List<Long> failedOffsets = new ArrayList<>();\r\n        for (KafkaSpoutMessageId msgId : messageIds.getAllValues()) {\r\n            failedOffsets.add(msgId.offset());\r\n        }\r\n        InOrder inOrder = inOrder(consumerMock);\r\n        inOrder.verify(consumerMock).seek(partition, failedOffsets.get(0));\r\n        inOrder.verify(consumerMock).poll(any(Duration.class));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutEmitTest.java",
  "methodName" : "testSpoutWillSkipPartitionsAtTheMaxUncommittedOffsetsLimit",
  "sourceCode" : "@Test\r\npublic void testSpoutWillSkipPartitionsAtTheMaxUncommittedOffsetsLimit() {\r\n    //This verifies that partitions can't prevent each other from retrying tuples due to the maxUncommittedOffsets limit.\r\n    try (SimulatedTime simulatedTime = new SimulatedTime()) {\r\n        TopicPartition partitionTwo = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 2);\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition, partitionTwo);\r\n        Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\r\n        //This is cheating a bit since maxPollRecords would normally spread this across multiple polls\r\n        records.put(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, spoutConfig.getMaxUncommittedOffsets()));\r\n        records.put(partitionTwo, SpoutWithMockedConsumerSetupHelper.createRecords(partitionTwo, 0, spoutConfig.getMaxUncommittedOffsets() + 1));\r\n        int numMessages = spoutConfig.getMaxUncommittedOffsets() * 2 + 1;\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(records));\r\n        for (int i = 0; i < numMessages; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock, times(numMessages)).emit(anyString(), anyList(), messageIds.capture());\r\n        //Now fail a tuple on partition one and verify that it is allowed to retry, because the failed tuple is below the maxUncommittedOffsets limit\r\n        Optional<KafkaSpoutMessageId> failedMessageIdPartitionOne = messageIds.getAllValues().stream().filter(messageId -> messageId.partition() == partition.partition()).findAny();\r\n        spout.fail(failedMessageIdPartitionOne.get());\r\n        //Also fail the last tuple from partition two. Since the failed tuple is beyond the maxUncommittedOffsets limit, it should not be retried until earlier messages are acked.\r\n        Optional<KafkaSpoutMessageId> failedMessagePartitionTwo = messageIds.getAllValues().stream().filter(messageId -> messageId.partition() == partitionTwo.partition()).max((msgId, msgId2) -> (int) (msgId.offset() - msgId2.offset()));\r\n        spout.fail(failedMessagePartitionTwo.get());\r\n        reset(collectorMock);\r\n        Time.advanceTime(50);\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, failedMessageIdPartitionOne.get().offset(), 1))));\r\n        spout.nextTuple();\r\n        verify(collectorMock, times(1)).emit(anyString(), anyList(), any());\r\n        InOrder inOrder = inOrder(consumerMock);\r\n        inOrder.verify(consumerMock).seek(partition, failedMessageIdPartitionOne.get().offset());\r\n        //Should not seek on the paused partition\r\n        inOrder.verify(consumerMock, never()).seek(eq(partitionTwo), anyLong());\r\n        inOrder.verify(consumerMock).pause(Collections.singleton(partitionTwo));\r\n        inOrder.verify(consumerMock).poll(any(Duration.class));\r\n        inOrder.verify(consumerMock).resume(Collections.singleton(partitionTwo));\r\n        reset(collectorMock);\r\n        //Now also check that no more tuples are polled for, since both partitions are at their limits\r\n        spout.nextTuple();\r\n        verify(collectorMock, never()).emit(anyString(), anyList(), any());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutLogCompactionSupportTest.java",
  "methodName" : "testCommitSuccessWithOffsetVoids",
  "sourceCode" : "@Test\r\npublic void testCommitSuccessWithOffsetVoids() {\r\n    //Verify that the commit logic can handle offset voids due to log compaction\r\n    try (SimulatedTime simulatedTime = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\r\n        List<ConsumerRecord<String, String>> recordsForPartition = new ArrayList<>();\r\n        // Offsets emitted are 0,1,2,3,4,<void>,8,9\r\n        recordsForPartition.addAll(SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 5));\r\n        recordsForPartition.addAll(SpoutWithMockedConsumerSetupHelper.createRecords(partition, 8, 2));\r\n        records.put(partition, recordsForPartition);\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(records));\r\n        for (int i = 0; i < recordsForPartition.size(); i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock, times(recordsForPartition.size())).emit(anyString(), anyList(), messageIds.capture());\r\n        for (KafkaSpoutMessageId messageId : messageIds.getAllValues()) {\r\n            spout.ack(messageId);\r\n        }\r\n        // Advance time and then trigger first call to kafka consumer commit; the commit must progress to offset 9\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.emptyMap()));\r\n        spout.nextTuple();\r\n        InOrder inOrder = inOrder(consumerMock);\r\n        inOrder.verify(consumerMock).commitSync(commitCapture.capture());\r\n        inOrder.verify(consumerMock).poll(any(Duration.class));\r\n        //verify that Offset 10 was last committed offset, since this is the offset the spout should resume at\r\n        Map<TopicPartition, OffsetAndMetadata> commits = commitCapture.getValue();\r\n        assertTrue(commits.containsKey(partition));\r\n        assertEquals(10, commits.get(partition).offset());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutLogCompactionSupportTest.java",
  "methodName" : "testWillSkipRetriableTuplesIfOffsetsAreCompactedAway",
  "sourceCode" : "/**\r\n *     Verify that failed offsets will only retry if the corresponding message exists.\r\n *     When log compaction is enabled in Kafka it is possible that a tuple can fail,\r\n *     and then be impossible to retry because the message in Kafka has been deleted.\r\n *     The spout needs to quietly ack such tuples to allow commits to progress past the deleted offset.\r\n */\r\n@Test\r\npublic void testWillSkipRetriableTuplesIfOffsetsAreCompactedAway() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        TopicPartition partitionTwo = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 2);\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition, partitionTwo);\r\n        List<KafkaSpoutMessageId> firstPartitionMsgIds = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 3, collectorMock, partition, 0, 1, 2);\r\n        reset(collectorMock);\r\n        List<KafkaSpoutMessageId> secondPartitionMsgIds = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 3, collectorMock, partitionTwo, 0, 1, 2);\r\n        reset(collectorMock);\r\n        for (int i = 0; i < 3; i++) {\r\n            spout.fail(firstPartitionMsgIds.get(i));\r\n            spout.fail(secondPartitionMsgIds.get(i));\r\n        }\r\n        Time.advanceTime(50);\r\n        //The failed tuples are ready for retry. Make it appear like 0 and 1 on the first partition were compacted away.\r\n        //In this case the second partition acts as control to verify that we only skip past offsets that are no longer present.\r\n        Map<TopicPartition, int[]> retryOffsets = new HashMap<>();\r\n        retryOffsets.put(partition, new int[] { 2 });\r\n        retryOffsets.put(partitionTwo, new int[] { 0, 1, 2 });\r\n        //2 on first partition, 0-2 on second partition\r\n        int expectedEmits = 4;\r\n        List<KafkaSpoutMessageId> retryMessageIds = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, expectedEmits, collectorMock, retryOffsets);\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock).commitSync(commitCapture.capture());\r\n        Map<TopicPartition, OffsetAndMetadata> committed = commitCapture.getValue();\r\n        assertThat(committed.keySet(), is(Collections.singleton(partition)));\r\n        assertThat(\"The first partition should have committed up to the first retriable tuple that is not missing\", committed.get(partition).offset(), is(2L));\r\n        for (KafkaSpoutMessageId msgId : retryMessageIds) {\r\n            spout.ack(msgId);\r\n        }\r\n        //The spout should now commit all the offsets, since all offsets are either acked or were missing when retrying\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock, times(2)).commitSync(commitCapture.capture());\r\n        committed = commitCapture.getValue();\r\n        assertThat(committed, hasKey(partition));\r\n        assertThat(committed, hasKey(partitionTwo));\r\n        assertThat(committed.get(partition).offset(), is(3L));\r\n        assertThat(committed.get(partitionTwo).offset(), is(3L));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutLogCompactionSupportTest.java",
  "methodName" : "testWillSkipRetriableTuplesIfOffsetsAreCompactedAwayWithoutAckingPendingTuples",
  "sourceCode" : "@Test\r\npublic void testWillSkipRetriableTuplesIfOffsetsAreCompactedAwayWithoutAckingPendingTuples() {\r\n    //Demonstrate that the spout doesn't ack pending tuples when skipping compacted tuples. The pending tuples should be allowed to finish normally.\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        List<KafkaSpoutMessageId> firstPartitionMsgIds = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 3, collectorMock, partition, 0, 1, 2);\r\n        reset(collectorMock);\r\n        spout.fail(firstPartitionMsgIds.get(0));\r\n        spout.fail(firstPartitionMsgIds.get(2));\r\n        Time.advanceTime(50);\r\n        //The failed tuples are ready for retry. Make it appear like 0 and 1 were compacted away.\r\n        List<KafkaSpoutMessageId> retryMessageIds = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 1, collectorMock, partition, 2);\r\n        for (KafkaSpoutMessageId msgId : retryMessageIds) {\r\n            spout.ack(msgId);\r\n        }\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock).commitSync(commitCapture.capture());\r\n        Map<TopicPartition, OffsetAndMetadata> committed = commitCapture.getValue();\r\n        assertThat(committed.keySet(), is(Collections.singleton(partition)));\r\n        assertThat(\"The first partition should have committed the missing offset, but no further since the next tuple is pending\", committed.get(partition).offset(), is(1L));\r\n        spout.ack(firstPartitionMsgIds.get(1));\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock, times(2)).commitSync(commitCapture.capture());\r\n        committed = commitCapture.getValue();\r\n        assertThat(committed.keySet(), is(Collections.singleton(partition)));\r\n        assertThat(\"The first partition should have committed all offsets\", committed.get(partition).offset(), is(3L));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutLogCompactionSupportTest.java",
  "methodName" : "testCommitTupleAfterCompactionGap",
  "sourceCode" : "@Test\r\npublic void testCommitTupleAfterCompactionGap() {\r\n    //If there is an acked tupled after a compaction gap, the spout should commit it immediately\r\n    try (SimulatedTime simulatedTime = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        List<KafkaSpoutMessageId> firstMessage = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 1, collectorMock, partition, 0);\r\n        reset(collectorMock);\r\n        List<KafkaSpoutMessageId> messageAfterGap = SpoutWithMockedConsumerSetupHelper.pollAndEmit(spout, consumerMock, 1, collectorMock, partition, 2);\r\n        reset(collectorMock);\r\n        spout.ack(firstMessage.get(0));\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock).commitSync(commitCapture.capture());\r\n        Map<TopicPartition, OffsetAndMetadata> committed = commitCapture.getValue();\r\n        assertThat(committed.keySet(), is(Collections.singleton(partition)));\r\n        assertThat(\"The consumer should have committed the offset before the gap\", committed.get(partition).offset(), is(1L));\r\n        reset(consumerMock);\r\n        spout.ack(messageAfterGap.get(0));\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        verify(consumerMock).commitSync(commitCapture.capture());\r\n        committed = commitCapture.getValue();\r\n        assertThat(committed.keySet(), is(Collections.singleton(partition)));\r\n        assertThat(\"The consumer should have committed the offset after the gap, since offset 1 wasn't emitted and both 0 and 2 are acked\", committed.get(partition).offset(), is(3L));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testAtMostOnceModeCommitsBeforeEmit",
  "sourceCode" : "@Test\r\npublic void testAtMostOnceModeCommitsBeforeEmit() {\r\n    //At-most-once mode must commit tuples before they are emitted to the topology to ensure that a spout crash won't cause replays.\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE).build();\r\n    KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n    when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\r\n    spout.nextTuple();\r\n    //The spout should have emitted the tuple, and must have committed it before emit\r\n    InOrder inOrder = inOrder(consumerMock, collectorMock);\r\n    inOrder.verify(consumerMock).poll(any(Duration.class));\r\n    inOrder.verify(consumerMock).commitSync(commitCapture.capture());\r\n    inOrder.verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList());\r\n    CommitMetadataManager metadataManager = new CommitMetadataManager(contextMock, KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE);\r\n    Map<TopicPartition, OffsetAndMetadata> committedOffsets = commitCapture.getValue();\r\n    assertThat(committedOffsets.get(partition).offset(), is(0L));\r\n    assertThat(committedOffsets.get(partition).metadata(), is(metadataManager.getCommitMetadata()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testAtMostOnceModeDisregardsMaxUncommittedOffsets",
  "sourceCode" : "@Test\r\npublic void testAtMostOnceModeDisregardsMaxUncommittedOffsets() {\r\n    //The maxUncommittedOffsets limit should not be enforced, since it is only meaningful in at-least-once mode\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE).build();\r\n    doTestModeDisregardsMaxUncommittedOffsets(spoutConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testNoGuaranteeModeDisregardsMaxUncommittedOffsets",
  "sourceCode" : "@Test\r\npublic void testNoGuaranteeModeDisregardsMaxUncommittedOffsets() {\r\n    //The maxUncommittedOffsets limit should not be enforced, since it is only meaningful in at-least-once mode\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE).build();\r\n    doTestModeDisregardsMaxUncommittedOffsets(spoutConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testAtMostOnceModeCannotReplayTuples",
  "sourceCode" : "@Test\r\npublic void testAtMostOnceModeCannotReplayTuples() {\r\n    //When tuple tracking is enabled, the spout must not replay tuples in at-most-once mode\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE).setTupleTrackingEnforced(true).build();\r\n    doTestModeCannotReplayTuples(spoutConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testNoGuaranteeModeCannotReplayTuples",
  "sourceCode" : "@Test\r\npublic void testNoGuaranteeModeCannotReplayTuples() {\r\n    //When tuple tracking is enabled, the spout must not replay tuples in no guarantee mode\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE).setTupleTrackingEnforced(true).build();\r\n    doTestModeCannotReplayTuples(spoutConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testAtMostOnceModeDoesNotCommitAckedTuples",
  "sourceCode" : "@Test\r\npublic void testAtMostOnceModeDoesNotCommitAckedTuples() {\r\n    //When tuple tracking is enabled, the spout must not commit acked tuples in at-most-once mode because they were committed before being emitted\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE).setTupleTrackingEnforced(true).build();\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\r\n        spout.nextTuple();\r\n        clearInvocations(consumerMock);\r\n        ArgumentCaptor<KafkaSpoutMessageId> msgIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), msgIdCaptor.capture());\r\n        assertThat(\"Should have captured a message id\", msgIdCaptor.getValue(), not(nullValue()));\r\n        spout.ack(msgIdCaptor.getValue());\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + spoutConfig.getOffsetsCommitPeriodMs());\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.emptyMap()));\r\n        spout.nextTuple();\r\n        verify(consumerMock, never()).commitSync(argThat((Map<TopicPartition, OffsetAndMetadata> arg) -> !arg.containsKey(partition)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testNoGuaranteeModeCommitsPolledTuples",
  "sourceCode" : "@Test\r\npublic void testNoGuaranteeModeCommitsPolledTuples() {\r\n    //When using the no guarantee mode, the spout must commit tuples periodically, regardless of whether they've been acked\r\n    KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1).setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE).setTupleTrackingEnforced(true).build();\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\r\n        spout.nextTuple();\r\n        when(consumerMock.position(partition)).thenReturn(1L);\r\n        ArgumentCaptor<KafkaSpoutMessageId> msgIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), msgIdCaptor.capture());\r\n        assertThat(\"Should have captured a message id\", msgIdCaptor.getValue(), not(nullValue()));\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + spoutConfig.getOffsetsCommitPeriodMs());\r\n        spout.nextTuple();\r\n        verify(consumerMock).commitAsync(commitCapture.capture(), isNull());\r\n        CommitMetadataManager metadataManager = new CommitMetadataManager(contextMock, KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE);\r\n        Map<TopicPartition, OffsetAndMetadata> committedOffsets = commitCapture.getValue();\r\n        assertThat(committedOffsets.get(partition).offset(), is(1L));\r\n        assertThat(committedOffsets.get(partition).metadata(), is(metadataManager.getCommitMetadata()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testAtMostOnceModeCanFilterNullTuples",
  "sourceCode" : "@Test\r\npublic void testAtMostOnceModeCanFilterNullTuples() {\r\n    doFilterNullTupleTest(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutMessagingGuaranteeTest.java",
  "methodName" : "testNoGuaranteeModeCanFilterNullTuples",
  "sourceCode" : "@Test\r\npublic void testNoGuaranteeModeCanFilterNullTuples() {\r\n    doFilterNullTupleTest(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutNullTupleTest.java",
  "methodName" : "testShouldCommitAllMessagesIfNotSetToEmitNullTuples",
  "sourceCode" : "@Test\r\npublic void testShouldCommitAllMessagesIfNotSetToEmitNullTuples() throws Exception {\r\n    final int messageCount = 10;\r\n    prepareSpout(messageCount);\r\n    //All null tuples should be commited, meaning they were considered by to be emitted and acked\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    verify(collectorMock, never()).emit(anyString(), anyList(), any());\r\n    Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n    //Commit offsets\r\n    spout.nextTuple();\r\n    verifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutReactivationTest.java",
  "methodName" : "testSpoutShouldResumeWhereItLeftOffWithUncommittedEarliestStrategy",
  "sourceCode" : "@Test\r\npublic void testSpoutShouldResumeWhereItLeftOffWithUncommittedEarliestStrategy() throws Exception {\r\n    //With uncommitted earliest the spout should pick up where it left off when reactivating.\r\n    doReactivationTest(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutReactivationTest.java",
  "methodName" : "testSpoutShouldResumeWhereItLeftOffWithEarliestStrategy",
  "sourceCode" : "@Test\r\npublic void testSpoutShouldResumeWhereItLeftOffWithEarliestStrategy() throws Exception {\r\n    //With earliest, the spout should also resume where it left off, rather than restart at the earliest offset.\r\n    doReactivationTest(FirstPollOffsetStrategy.EARLIEST);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutReactivationTest.java",
  "methodName" : "testSpoutMustHandleGettingMetricsWhileDeactivated",
  "sourceCode" : "@Test\r\npublic void testSpoutMustHandleGettingMetricsWhileDeactivated() throws Exception {\r\n    //Storm will try to get metrics from the spout even while deactivated, the spout must be able to handle this\r\n    prepareSpout(10, FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST);\r\n    for (int i = 0; i < 5; i++) {\r\n        KafkaSpoutMessageId msgId = emitOne();\r\n        spout.ack(msgId);\r\n    }\r\n    spout.deactivate();\r\n    Map<String, Metric> partitionsOffsetMetric = spout.getKafkaOffsetMetricManager().getTopicPartitionMetricsMap().get(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0)).getMetrics();\r\n    Long partitionLag = (Long) ((Gauge) partitionsOffsetMetric.get(SingleTopicKafkaSpoutConfiguration.TOPIC + \"/partition_0/spoutLag\")).getValue();\r\n    assertThat(partitionLag, is(5L));\r\n    Map<String, Metric> topicOffsetMetric = spout.getKafkaOffsetMetricManager().getTopicMetricsMap().get(SingleTopicKafkaSpoutConfiguration.TOPIC).getMetrics();\r\n    Long totalSpoutLag = (Long) ((Gauge) topicOffsetMetric.get(SingleTopicKafkaSpoutConfiguration.TOPIC + \"/totalSpoutLag\")).getValue();\r\n    assertThat(totalSpoutLag, is(5L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRebalanceTest.java",
  "methodName" : "spoutMustIgnoreAcksForTuplesItIsNotAssignedAfterRebalance",
  "sourceCode" : "@Test\r\npublic void spoutMustIgnoreAcksForTuplesItIsNotAssignedAfterRebalance() {\r\n    // Acking tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        TopicAssigner assignerMock = mock(TopicAssigner.class);\r\n        KafkaSpout<String, String> spout = new KafkaSpout<>(createKafkaSpoutConfigBuilder(topicFilterMock, partitionerMock, -1).setOffsetCommitPeriodMs(offsetCommitPeriodMs).build(), clientFactory, assignerMock);\r\n        String topic = SingleTopicKafkaSpoutConfiguration.TOPIC;\r\n        TopicPartition partitionThatWillBeRevoked = new TopicPartition(topic, 1);\r\n        TopicPartition assignedPartition = new TopicPartition(topic, 2);\r\n        //Emit a message on each partition and revoke the first partition\r\n        List<KafkaSpoutMessageId> emittedMessageIds = emitOneMessagePerPartitionThenRevokeOnePartition(spout, partitionThatWillBeRevoked, assignedPartition, assignerMock);\r\n        //Ack both emitted tuples\r\n        spout.ack(emittedMessageIds.get(0));\r\n        spout.ack(emittedMessageIds.get(1));\r\n        //Ensure the commit timer has expired\r\n        Time.advanceTime(offsetCommitPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n        //Make the spout commit any acked tuples\r\n        spout.nextTuple();\r\n        //Verify that it only committed the message on the assigned partition\r\n        verify(consumerMock, times(1)).commitSync(commitCapture.capture());\r\n        Map<TopicPartition, OffsetAndMetadata> commitCaptureMap = commitCapture.getValue();\r\n        assertThat(commitCaptureMap, hasKey(assignedPartition));\r\n        assertThat(commitCaptureMap, not(hasKey(partitionThatWillBeRevoked)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRebalanceTest.java",
  "methodName" : "spoutMustIgnoreFailsForTuplesItIsNotAssignedAfterRebalance",
  "sourceCode" : "@Test\r\npublic void spoutMustIgnoreFailsForTuplesItIsNotAssignedAfterRebalance() {\r\n    //Failing tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them if they later pass\r\n    TopicAssigner assignerMock = mock(TopicAssigner.class);\r\n    KafkaSpoutRetryService retryServiceMock = mock(KafkaSpoutRetryService.class);\r\n    KafkaSpout<String, String> spout = new KafkaSpout<>(createKafkaSpoutConfigBuilder(topicFilterMock, partitionerMock, -1).setOffsetCommitPeriodMs(10).setRetry(retryServiceMock).build(), clientFactory, assignerMock);\r\n    String topic = SingleTopicKafkaSpoutConfiguration.TOPIC;\r\n    TopicPartition partitionThatWillBeRevoked = new TopicPartition(topic, 1);\r\n    TopicPartition assignedPartition = new TopicPartition(topic, 2);\r\n    when(retryServiceMock.getMessageId(any(TopicPartition.class), anyLong())).thenReturn(new KafkaSpoutMessageId(partitionThatWillBeRevoked, 0)).thenReturn(new KafkaSpoutMessageId(assignedPartition, 0));\r\n    //Emit a message on each partition and revoke the first partition\r\n    List<KafkaSpoutMessageId> emittedMessageIds = emitOneMessagePerPartitionThenRevokeOnePartition(spout, partitionThatWillBeRevoked, assignedPartition, assignerMock);\r\n    //Check that only two message ids were generated\r\n    verify(retryServiceMock, times(2)).getMessageId(any(TopicPartition.class), anyLong());\r\n    //Fail both emitted tuples\r\n    spout.fail(emittedMessageIds.get(0));\r\n    spout.fail(emittedMessageIds.get(1));\r\n    //Check that only the tuple on the currently assigned partition is retried\r\n    verify(retryServiceMock, never()).schedule(emittedMessageIds.get(0));\r\n    verify(retryServiceMock).schedule(emittedMessageIds.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRebalanceTest.java",
  "methodName" : "testReassignPartitionSeeksForOnlyNewPartitions",
  "sourceCode" : "@Test\r\npublic void testReassignPartitionSeeksForOnlyNewPartitions() {\r\n    /*\r\n         * When partitions are reassigned, the spout should seek with the first poll offset strategy for new partitions.\r\n         * Previously assigned partitions should be left alone, since the spout keeps the emitted and acked state for those.\r\n         */\r\n    TopicAssigner assignerMock = mock(TopicAssigner.class);\r\n    KafkaSpout<String, String> spout = new KafkaSpout<>(createKafkaSpoutConfigBuilder(topicFilterMock, partitionerMock, -1).setFirstPollOffsetStrategy(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST).build(), clientFactory, assignerMock);\r\n    String topic = SingleTopicKafkaSpoutConfiguration.TOPIC;\r\n    TopicPartition assignedPartition = new TopicPartition(topic, 1);\r\n    TopicPartition newPartition = new TopicPartition(topic, 2);\r\n    //Setup spout with mock consumer so we can get at the rebalance listener\r\n    spout.open(conf, contextMock, collectorMock);\r\n    spout.activate();\r\n    ArgumentCaptor<ConsumerRebalanceListener> rebalanceListenerCapture = ArgumentCaptor.forClass(ConsumerRebalanceListener.class);\r\n    verify(assignerMock).assignPartitions(any(), any(), rebalanceListenerCapture.capture());\r\n    //Assign partitions to the spout\r\n    ConsumerRebalanceListener consumerRebalanceListener = rebalanceListenerCapture.getValue();\r\n    Set<TopicPartition> assignedPartitions = new HashSet<>();\r\n    assignedPartitions.add(assignedPartition);\r\n    consumerRebalanceListener.onPartitionsAssigned(assignedPartitions);\r\n    reset(consumerMock);\r\n    //Set up committed so it looks like some messages have been committed on each partition\r\n    long committedOffset = 500;\r\n    final Map<TopicPartition, OffsetAndMetadata> mapAnswer = new HashMap<>();\r\n    mapAnswer.put(newPartition, new OffsetAndMetadata(committedOffset));\r\n    final Answer<Object> objectAnswer = invocation -> mapAnswer;\r\n    lenient().doAnswer(objectAnswer).when(consumerMock).committed(Collections.singleton(newPartition));\r\n    doAnswer(objectAnswer).when(consumerMock).committed(Collections.singleton(newPartition));\r\n    //Now rebalance and add a new partition\r\n    consumerRebalanceListener.onPartitionsRevoked(assignedPartitions);\r\n    Set<TopicPartition> newAssignedPartitions = new HashSet<>();\r\n    newAssignedPartitions.add(assignedPartition);\r\n    newAssignedPartitions.add(newPartition);\r\n    consumerRebalanceListener.onPartitionsAssigned(newAssignedPartitions);\r\n    //This partition was previously assigned, so the consumer position shouldn't change\r\n    verify(consumerMock, never()).seek(eq(assignedPartition), anyLong());\r\n    //This partition is new, and should start at the committed offset\r\n    verify(consumerMock).seek(newPartition, committedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCanScheduleRetry",
  "sourceCode" : "@Test\r\npublic void testCanScheduleRetry() {\r\n    KafkaSpoutRetryExponentialBackoff retryService = createNoWaitRetryService();\r\n    long offset = 0;\r\n    KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n    msgId.incrementNumFails();\r\n    boolean scheduled = retryService.schedule(msgId);\r\n    assertThat(\"The service must schedule the message for retry\", scheduled, is(true));\r\n    KafkaSpoutMessageId retrievedMessageId = retryService.getMessageId(testTopic, offset);\r\n    assertThat(\"The service should return the original message id when asked for the same tp/offset twice\", retrievedMessageId, sameInstance(msgId));\r\n    assertThat(retryService.isScheduled(msgId), is(true));\r\n    assertThat(retryService.isReady(msgId), is(true));\r\n    assertThat(retryService.readyMessageCount(), is(1));\r\n    assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, msgId.offset())));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCanRescheduleRetry",
  "sourceCode" : "@Test\r\npublic void testCanRescheduleRetry() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpoutRetryExponentialBackoff retryService = createOneSecondWaitRetryService();\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n        msgId.incrementNumFails();\r\n        retryService.schedule(msgId);\r\n        Time.advanceTime(500);\r\n        boolean scheduled = retryService.schedule(msgId);\r\n        assertThat(\"The service must be able to reschedule an already scheduled id\", scheduled, is(true));\r\n        Time.advanceTime(500);\r\n        assertThat(\"The message should not be ready for retry yet since it was rescheduled\", retryService.isReady(msgId), is(false));\r\n        assertThat(retryService.isScheduled(msgId), is(true));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.emptyMap()));\r\n        assertThat(retryService.readyMessageCount(), is(0));\r\n        Time.advanceTime(500);\r\n        assertThat(\"The message should be ready for retry once the full delay has passed\", retryService.isReady(msgId), is(true));\r\n        assertThat(retryService.isScheduled(msgId), is(true));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, msgId.offset())));\r\n        assertThat(retryService.readyMessageCount(), is(1));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCannotContainMultipleSchedulesForId",
  "sourceCode" : "@Test\r\npublic void testCannotContainMultipleSchedulesForId() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpoutRetryExponentialBackoff retryService = createOneSecondWaitRetryService();\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n        msgId.incrementNumFails();\r\n        retryService.schedule(msgId);\r\n        Time.advanceTime(500);\r\n        boolean scheduled = retryService.schedule(msgId);\r\n        retryService.remove(msgId);\r\n        assertThat(\"The message should no longer be scheduled\", retryService.isScheduled(msgId), is(false));\r\n        Time.advanceTime(500);\r\n        assertThat(\"The message should not be ready for retry because it isn't scheduled\", retryService.isReady(msgId), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCanRemoveRetry",
  "sourceCode" : "@Test\r\npublic void testCanRemoveRetry() {\r\n    KafkaSpoutRetryExponentialBackoff retryService = createNoWaitRetryService();\r\n    long offset = 0;\r\n    KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n    msgId.incrementNumFails();\r\n    retryService.schedule(msgId);\r\n    boolean removed = retryService.remove(msgId);\r\n    assertThat(removed, is(true));\r\n    assertThat(retryService.isScheduled(msgId), is(false));\r\n    assertThat(retryService.isReady(msgId), is(false));\r\n    assertThat(retryService.earliestRetriableOffsets(), is(Collections.emptyMap()));\r\n    assertThat(retryService.readyMessageCount(), is(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCanHandleMultipleTopics",
  "sourceCode" : "@Test\r\npublic void testCanHandleMultipleTopics() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        //Tests that isScheduled, isReady and earliestRetriableOffsets are mutually consistent when there are messages from multiple partitions scheduled\r\n        KafkaSpoutRetryExponentialBackoff retryService = createOneSecondWaitRetryService();\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgIdTp1 = retryService.getMessageId(testTopic, offset);\r\n        KafkaSpoutMessageId msgIdTp2 = retryService.getMessageId(testTopic2, offset);\r\n        msgIdTp1.incrementNumFails();\r\n        msgIdTp2.incrementNumFails();\r\n        boolean scheduledOne = retryService.schedule(msgIdTp1);\r\n        Time.advanceTime(500);\r\n        boolean scheduledTwo = retryService.schedule(msgIdTp2);\r\n        //The retry schedules for two messages should be unrelated\r\n        assertThat(scheduledOne, is(true));\r\n        assertThat(retryService.isScheduled(msgIdTp1), is(true));\r\n        assertThat(scheduledTwo, is(true));\r\n        assertThat(retryService.isScheduled(msgIdTp2), is(true));\r\n        assertThat(retryService.isReady(msgIdTp1), is(false));\r\n        assertThat(retryService.isReady(msgIdTp2), is(false));\r\n        Time.advanceTime(500);\r\n        assertThat(retryService.isReady(msgIdTp1), is(true));\r\n        assertThat(retryService.isReady(msgIdTp2), is(false));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, offset)));\r\n        Time.advanceTime(500);\r\n        assertThat(retryService.isReady(msgIdTp2), is(true));\r\n        Map<TopicPartition, Long> earliestOffsets = new HashMap<>();\r\n        earliestOffsets.put(testTopic, offset);\r\n        earliestOffsets.put(testTopic2, offset);\r\n        assertThat(retryService.earliestRetriableOffsets(), is(earliestOffsets));\r\n        //The service must be able to remove retry schedules for unnecessary partitions\r\n        retryService.retainAll(Collections.singleton(testTopic2));\r\n        assertThat(retryService.isScheduled(msgIdTp1), is(false));\r\n        assertThat(retryService.isScheduled(msgIdTp2), is(true));\r\n        assertThat(retryService.isReady(msgIdTp1), is(false));\r\n        assertThat(retryService.isReady(msgIdTp2), is(true));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic2, offset)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testCanHandleMultipleMessagesOnPartition",
  "sourceCode" : "@Test\r\npublic void testCanHandleMultipleMessagesOnPartition() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        //Tests that isScheduled, isReady and earliestRetriableOffsets are mutually consistent when there are multiple messages scheduled on a partition\r\n        KafkaSpoutRetryExponentialBackoff retryService = createOneSecondWaitRetryService();\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgIdEarliest = retryService.getMessageId(testTopic, offset);\r\n        KafkaSpoutMessageId msgIdLatest = retryService.getMessageId(testTopic, offset + 1);\r\n        msgIdEarliest.incrementNumFails();\r\n        msgIdLatest.incrementNumFails();\r\n        retryService.schedule(msgIdEarliest);\r\n        Time.advanceTime(500);\r\n        retryService.schedule(msgIdLatest);\r\n        assertThat(retryService.isScheduled(msgIdEarliest), is(true));\r\n        assertThat(retryService.isScheduled(msgIdLatest), is(true));\r\n        Time.advanceTime(500);\r\n        assertThat(retryService.isReady(msgIdEarliest), is(true));\r\n        assertThat(retryService.isReady(msgIdLatest), is(false));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, msgIdEarliest.offset())));\r\n        Time.advanceTime(500);\r\n        assertThat(retryService.isReady(msgIdEarliest), is(true));\r\n        assertThat(retryService.isReady(msgIdLatest), is(true));\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, msgIdEarliest.offset())));\r\n        retryService.remove(msgIdEarliest);\r\n        assertThat(retryService.earliestRetriableOffsets(), is(Collections.singletonMap(testTopic, msgIdLatest.offset())));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testMaxRetries",
  "sourceCode" : "@Test\r\npublic void testMaxRetries() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        int maxRetries = 3;\r\n        KafkaSpoutRetryExponentialBackoff retryService = new KafkaSpoutRetryExponentialBackoff(TimeInterval.seconds(0), TimeInterval.seconds(0), maxRetries, TimeInterval.seconds(0));\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n        for (int i = 0; i < maxRetries; i++) {\r\n            msgId.incrementNumFails();\r\n        }\r\n        //Should be allowed to retry 3 times, in addition to original try\r\n        boolean scheduled = retryService.schedule(msgId);\r\n        assertThat(scheduled, is(true));\r\n        assertThat(retryService.isScheduled(msgId), is(true));\r\n        retryService.remove(msgId);\r\n        msgId.incrementNumFails();\r\n        boolean rescheduled = retryService.schedule(msgId);\r\n        assertThat(\"The message should not be allowed to retry once the limit is reached\", rescheduled, is(false));\r\n        assertThat(retryService.isScheduled(msgId), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testMaxDelay",
  "sourceCode" : "@Test\r\npublic void testMaxDelay() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        int maxDelaySecs = 2;\r\n        KafkaSpoutRetryExponentialBackoff retryService = new KafkaSpoutRetryExponentialBackoff(TimeInterval.seconds(500), TimeInterval.seconds(0), 1, TimeInterval.seconds(maxDelaySecs));\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n        msgId.incrementNumFails();\r\n        retryService.schedule(msgId);\r\n        Time.advanceTimeSecs(maxDelaySecs);\r\n        assertThat(\"The message should be ready for retry after the max delay\", retryService.isReady(msgId), is(true));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryExponentialBackoffTest.java",
  "methodName" : "testExponentialBackoff",
  "sourceCode" : "@Test\r\npublic void testExponentialBackoff() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpoutRetryExponentialBackoff retryService = new KafkaSpoutRetryExponentialBackoff(TimeInterval.seconds(0), TimeInterval.seconds(4), Integer.MAX_VALUE, TimeInterval.seconds(Integer.MAX_VALUE));\r\n        long offset = 0;\r\n        KafkaSpoutMessageId msgId = retryService.getMessageId(testTopic, offset);\r\n        msgId.incrementNumFails();\r\n        //First failure is the initial delay, so not interesting\r\n        msgId.incrementNumFails();\r\n        //Expecting 4*2^(failCount-1)\r\n        Integer[] expectedBackoffsSecs = new Integer[] { 8, 16, 32 };\r\n        for (Integer expectedBackoffSecs : expectedBackoffsSecs) {\r\n            retryService.schedule(msgId);\r\n            Time.advanceTimeSecs(expectedBackoffSecs - 1);\r\n            assertThat(\"The message should not be ready for retry until backoff \" + expectedBackoffSecs + \" has expired\", retryService.isReady(msgId), is(false));\r\n            Time.advanceTimeSecs(1);\r\n            assertThat(\"The message should be ready for retry once backoff \" + expectedBackoffSecs + \" has expired\", retryService.isReady(msgId), is(true));\r\n            msgId.incrementNumFails();\r\n            retryService.remove(msgId);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutRetryLimitTest.java",
  "methodName" : "testFailingTupleCompletesAckAfterRetryLimitIsMet",
  "sourceCode" : "@Test\r\npublic void testFailingTupleCompletesAckAfterRetryLimitIsMet() {\r\n    //Spout should ack failed messages after they hit the retry limit\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\r\n        Map<TopicPartition, List<ConsumerRecord<String, String>>> records = new HashMap<>();\r\n        int lastOffset = 3;\r\n        int numRecords = lastOffset + 1;\r\n        records.put(partition, SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, numRecords));\r\n        when(consumerMock.poll(any(Duration.class))).thenReturn(new ConsumerRecords<>(records));\r\n        for (int i = 0; i < numRecords; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collectorMock, times(numRecords)).emit(anyString(), anyList(), messageIds.capture());\r\n        for (KafkaSpoutMessageId messageId : messageIds.getAllValues()) {\r\n            spout.fail(messageId);\r\n        }\r\n        // Advance time and then trigger call to kafka consumer commit\r\n        Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + offsetCommitPeriodMs);\r\n        spout.nextTuple();\r\n        InOrder inOrder = inOrder(consumerMock);\r\n        inOrder.verify(consumerMock).commitSync(commitCapture.capture());\r\n        inOrder.verify(consumerMock).poll(any(Duration.class));\r\n        //verify that offset 4 was committed for the given TopicPartition, since processing should resume at 4.\r\n        assertTrue(commitCapture.getValue().containsKey(partition));\r\n        assertEquals(lastOffset + 1, commitCapture.getValue().get(partition).offset());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testSeekToCommittedOffsetIfConsumerPositionIsBehindWhenCommitting",
  "sourceCode" : "@Test\r\npublic void testSeekToCommittedOffsetIfConsumerPositionIsBehindWhenCommitting() throws Exception {\r\n    final int messageCount = maxPollRecords * 2;\r\n    prepareSpout(messageCount);\r\n    //Emit all messages and fail the first one while acking the rest\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    ArgumentCaptor<KafkaSpoutMessageId> messageIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock, times(messageCount)).emit(anyString(), anyList(), messageIdCaptor.capture());\r\n    List<KafkaSpoutMessageId> messageIds = messageIdCaptor.getAllValues();\r\n    for (int i = 1; i < messageIds.size(); i++) {\r\n        spout.ack(messageIds.get(i));\r\n    }\r\n    KafkaSpoutMessageId failedTuple = messageIds.get(0);\r\n    spout.fail(failedTuple);\r\n    //Advance the time and replay the failed tuple.\r\n    reset(collectorMock);\r\n    spout.nextTuple();\r\n    ArgumentCaptor<KafkaSpoutMessageId> failedIdReplayCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock).emit(anyString(), anyList(), failedIdReplayCaptor.capture());\r\n    assertThat(\"Expected replay of failed tuple\", failedIdReplayCaptor.getValue(), is(failedTuple));\r\n    /* Ack the tuple, and commit.\r\n         * Since the tuple is more than max poll records behind the most recent emitted tuple, the consumer won't catch up in this poll.\r\n         */\r\n    clearInvocations(collectorMock);\r\n    Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + commitOffsetPeriodMs);\r\n    spout.ack(failedIdReplayCaptor.getValue());\r\n    spout.nextTuple();\r\n    verify(getKafkaConsumer()).commitSync(commitCapture.capture());\r\n    Map<TopicPartition, OffsetAndMetadata> capturedCommit = commitCapture.getValue();\r\n    TopicPartition expectedTp = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0);\r\n    assertThat(\"Should have committed to the right topic\", capturedCommit, Matchers.hasKey(expectedTp));\r\n    assertThat(\"Should have committed all the acked messages\", capturedCommit.get(expectedTp).offset(), is((long) messageCount));\r\n    /* Verify that the following acked (now committed) tuples are not emitted again\r\n         * Since the consumer position was somewhere in the middle of the acked tuples when the commit happened,\r\n         * this verifies that the spout keeps the consumer position ahead of the committed offset when committing\r\n         */\r\n    //Just do a few polls to check that nothing more is emitted\r\n    for (int i = 0; i < 3; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    verify(collectorMock, never()).emit(anyString(), anyList(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testClearingWaitingToEmitIfConsumerPositionIsNotBehindWhenCommitting",
  "sourceCode" : "@Test\r\npublic void testClearingWaitingToEmitIfConsumerPositionIsNotBehindWhenCommitting() throws Exception {\r\n    final int messageCountExcludingLast = maxPollRecords;\r\n    int messagesInKafka = messageCountExcludingLast + 1;\r\n    prepareSpout(messagesInKafka);\r\n    //Emit all messages and fail the first one while acking the rest\r\n    for (int i = 0; i < messageCountExcludingLast; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    ArgumentCaptor<KafkaSpoutMessageId> messageIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock, times(messageCountExcludingLast)).emit(anyString(), anyList(), messageIdCaptor.capture());\r\n    List<KafkaSpoutMessageId> messageIds = messageIdCaptor.getAllValues();\r\n    for (int i = 1; i < messageIds.size(); i++) {\r\n        spout.ack(messageIds.get(i));\r\n    }\r\n    KafkaSpoutMessageId failedTuple = messageIds.get(0);\r\n    spout.fail(failedTuple);\r\n    //Advance the time and replay the failed tuple.\r\n    //Since the last tuple on the partition is more than maxPollRecords ahead of the failed tuple, it shouldn't be emitted here\r\n    reset(collectorMock);\r\n    spout.nextTuple();\r\n    ArgumentCaptor<KafkaSpoutMessageId> failedIdReplayCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock).emit(anyString(), anyList(), failedIdReplayCaptor.capture());\r\n    assertThat(\"Expected replay of failed tuple\", failedIdReplayCaptor.getValue(), is(failedTuple));\r\n    /* Ack the tuple, and commit.\r\n         * \r\n         * The waiting to emit list should now be cleared, and the next emitted tuple should be the last tuple on the partition,\r\n         * which hasn't been emitted yet\r\n         */\r\n    reset(collectorMock);\r\n    Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + commitOffsetPeriodMs);\r\n    spout.ack(failedIdReplayCaptor.getValue());\r\n    spout.nextTuple();\r\n    verify(getKafkaConsumer()).commitSync(commitCapture.capture());\r\n    Map<TopicPartition, OffsetAndMetadata> capturedCommit = commitCapture.getValue();\r\n    TopicPartition expectedTp = new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0);\r\n    assertThat(\"Should have committed to the right topic\", capturedCommit, Matchers.hasKey(expectedTp));\r\n    assertThat(\"Should have committed all the acked messages\", capturedCommit.get(expectedTp).offset(), is((long) messageCountExcludingLast));\r\n    ArgumentCaptor<KafkaSpoutMessageId> lastOffsetMessageCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock).emit(anyString(), anyList(), lastOffsetMessageCaptor.capture());\r\n    assertThat(\"Expected emit of the final tuple in the partition\", lastOffsetMessageCaptor.getValue().offset(), is(messagesInKafka - 1L));\r\n    reset(collectorMock);\r\n    //Nothing else should be emitted, all tuples are acked except for the final tuple, which is pending.\r\n    for (int i = 0; i < 3; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    verify(collectorMock, never()).emit(anyString(), anyList(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldContinueWithSlowDoubleAcks",
  "sourceCode" : "@Test\r\npublic void testShouldContinueWithSlowDoubleAcks() throws Exception {\r\n    final int messageCount = 20;\r\n    prepareSpout(messageCount);\r\n    //play 1st tuple\r\n    ArgumentCaptor<Object> messageIdToDoubleAck = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), messageIdToDoubleAck.capture());\r\n    spout.ack(messageIdToDoubleAck.getValue());\r\n    //Emit some more messages\r\n    for (int i = 0; i < messageCount / 2; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    spout.ack(messageIdToDoubleAck.getValue());\r\n    //Emit any remaining messages\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    //Verify that all messages are emitted, ack all the messages\r\n    ArgumentCaptor<Object> messageIds = ArgumentCaptor.forClass(Object.class);\r\n    verify(collectorMock, times(messageCount)).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), messageIds.capture());\r\n    for (Object id : messageIds.getAllValues()) {\r\n        spout.ack(id);\r\n    }\r\n    Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n    //Commit offsets\r\n    spout.nextTuple();\r\n    verifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldEmitAllMessages",
  "sourceCode" : "@Test\r\npublic void testShouldEmitAllMessages() throws Exception {\r\n    final int messageCount = 10;\r\n    prepareSpout(messageCount);\r\n    //Emit all messages and check that they are emitted. Ack the messages too\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n        ArgumentCaptor<Object> messageId = ArgumentCaptor.forClass(Object.class);\r\n        verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), eq(new Values(SingleTopicKafkaSpoutConfiguration.TOPIC, Integer.toString(i), Integer.toString(i))), messageId.capture());\r\n        spout.ack(messageId.getValue());\r\n        reset(collectorMock);\r\n    }\r\n    Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n    //Commit offsets\r\n    spout.nextTuple();\r\n    verifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldReplayInOrderFailedMessages",
  "sourceCode" : "@Test\r\npublic void testShouldReplayInOrderFailedMessages() throws Exception {\r\n    final int messageCount = 10;\r\n    prepareSpout(messageCount);\r\n    //play and ack 1 tuple\r\n    ArgumentCaptor<Object> messageIdAcked = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), messageIdAcked.capture());\r\n    spout.ack(messageIdAcked.getValue());\r\n    reset(collectorMock);\r\n    //play and fail 1 tuple\r\n    ArgumentCaptor<Object> messageIdFailed = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), messageIdFailed.capture());\r\n    spout.fail(messageIdFailed.getValue());\r\n    reset(collectorMock);\r\n    //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    ArgumentCaptor<Object> remainingMessageIds = ArgumentCaptor.forClass(Object.class);\r\n    //All messages except the first acked message should have been emitted\r\n    verify(collectorMock, times(messageCount - 1)).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), remainingMessageIds.capture());\r\n    for (Object id : remainingMessageIds.getAllValues()) {\r\n        spout.ack(id);\r\n    }\r\n    Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n    //Commit offsets\r\n    spout.nextTuple();\r\n    verifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldReplayFirstTupleFailedOutOfOrder",
  "sourceCode" : "@Test\r\npublic void testShouldReplayFirstTupleFailedOutOfOrder() throws Exception {\r\n    final int messageCount = 10;\r\n    prepareSpout(messageCount);\r\n    //play 1st tuple\r\n    ArgumentCaptor<Object> messageIdToFail = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), messageIdToFail.capture());\r\n    reset(collectorMock);\r\n    //play 2nd tuple\r\n    ArgumentCaptor<Object> messageIdToAck = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), messageIdToAck.capture());\r\n    reset(collectorMock);\r\n    //ack 2nd tuple\r\n    spout.ack(messageIdToAck.getValue());\r\n    //fail 1st tuple\r\n    spout.fail(messageIdToFail.getValue());\r\n    //Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait.\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    ArgumentCaptor<Object> remainingIds = ArgumentCaptor.forClass(Object.class);\r\n    //All messages except the first acked message should have been emitted\r\n    verify(collectorMock, times(messageCount - 1)).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), remainingIds.capture());\r\n    for (Object id : remainingIds.getAllValues()) {\r\n        spout.ack(id);\r\n    }\r\n    Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n    //Commit offsets\r\n    spout.nextTuple();\r\n    verifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldReplayAllFailedTuplesWhenFailedOutOfOrder",
  "sourceCode" : "@Test\r\npublic void testShouldReplayAllFailedTuplesWhenFailedOutOfOrder() throws Exception {\r\n    //The spout must reemit retriable tuples, even if they fail out of order.\r\n    //The spout should be able to skip tuples it has already emitted when retrying messages, even if those tuples are also retries.\r\n    final int messageCount = 10;\r\n    prepareSpout(messageCount);\r\n    //play all tuples\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    ArgumentCaptor<KafkaSpoutMessageId> messageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    verify(collectorMock, times(messageCount)).emit(anyString(), anyList(), messageIds.capture());\r\n    reset(collectorMock);\r\n    //Fail tuple 5 and 3, call nextTuple, then fail tuple 2\r\n    List<KafkaSpoutMessageId> capturedMessageIds = messageIds.getAllValues();\r\n    spout.fail(capturedMessageIds.get(5));\r\n    spout.fail(capturedMessageIds.get(3));\r\n    spout.nextTuple();\r\n    spout.fail(capturedMessageIds.get(2));\r\n    //Check that the spout will reemit all 3 failed tuples and no other tuples\r\n    ArgumentCaptor<KafkaSpoutMessageId> reemittedMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n    for (int i = 0; i < messageCount; i++) {\r\n        spout.nextTuple();\r\n    }\r\n    verify(collectorMock, times(3)).emit(anyString(), anyList(), reemittedMessageIds.capture());\r\n    Set<KafkaSpoutMessageId> expectedReemitIds = new HashSet<>();\r\n    expectedReemitIds.add(capturedMessageIds.get(5));\r\n    expectedReemitIds.add(capturedMessageIds.get(3));\r\n    expectedReemitIds.add(capturedMessageIds.get(2));\r\n    assertThat(\"Expected reemits to be the 3 failed tuples\", new HashSet<>(reemittedMessageIds.getAllValues()), is(expectedReemitIds));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testShouldDropMessagesAfterMaxRetriesAreReached",
  "sourceCode" : "@Test\r\npublic void testShouldDropMessagesAfterMaxRetriesAreReached() throws Exception {\r\n    //Check that if one message fails repeatedly, the retry cap limits how many times the message can be reemitted\r\n    final int messageCount = 1;\r\n    prepareSpout(messageCount);\r\n    //Emit and fail the same tuple until we've reached retry limit\r\n    for (int i = 0; i <= maxRetries; i++) {\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIdFailed = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        spout.nextTuple();\r\n        verify(collectorMock).emit(anyString(), anyList(), messageIdFailed.capture());\r\n        KafkaSpoutMessageId msgId = messageIdFailed.getValue();\r\n        spout.fail(msgId);\r\n        assertThat(\"Expected message id number of failures to match the number of times the message has failed\", msgId.numFails(), is(i + 1));\r\n        reset(collectorMock);\r\n    }\r\n    //Verify that the tuple is not emitted again\r\n    spout.nextTuple();\r\n    verify(collectorMock, never()).emit(anyString(), anyList(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutSingleTopicTest.java",
  "methodName" : "testSpoutMustRefreshPartitionsEvenIfNotPolling",
  "sourceCode" : "@Test\r\npublic void testSpoutMustRefreshPartitionsEvenIfNotPolling() throws Exception {\r\n    SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collectorMock);\r\n    //Nothing is assigned yet, should emit nothing\r\n    spout.nextTuple();\r\n    verify(collectorMock, never()).emit(anyString(), anyList(), any(KafkaSpoutMessageId.class));\r\n    SingleTopicKafkaUnitSetupHelper.populateTopicData(kafkaUnitExtension.getKafkaUnit(), SingleTopicKafkaSpoutConfiguration.TOPIC, 1);\r\n    Time.advanceTime(KafkaSpoutConfig.DEFAULT_PARTITION_REFRESH_PERIOD_MS + KafkaSpout.TIMER_DELAY_MS);\r\n    //The new partition should be discovered and the message should be emitted\r\n    spout.nextTuple();\r\n    verify(collectorMock).emit(anyString(), anyList(), any(KafkaSpoutMessageId.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutTopologyDeployActivateDeactivateTest.java",
  "methodName" : "test_FirstPollStrategy_Earliest_NotEnforced_OnTopologyActivateDeactivate",
  "sourceCode" : "@Test\r\npublic void test_FirstPollStrategy_Earliest_NotEnforced_OnTopologyActivateDeactivate() throws Exception {\r\n    final int messageCount = 2;\r\n    prepareSpout(messageCount);\r\n    nextTuple_verifyEmitted_ack_resetCollector(0);\r\n    //Commits offsets during deactivation\r\n    spout.deactivate();\r\n    verifyAllMessagesCommitted(1);\r\n    spout.activate();\r\n    nextTuple_verifyEmitted_ack_resetCollector(1);\r\n    commitAndVerifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutTopologyDeployActivateDeactivateTest.java",
  "methodName" : "test_FirstPollStrategy_Earliest_NotEnforced_OnPartitionReassignment",
  "sourceCode" : "@Test\r\npublic void test_FirstPollStrategy_Earliest_NotEnforced_OnPartitionReassignment() throws Exception {\r\n    when(topologyContext.getStormId()).thenReturn(\"topology-1\");\r\n    final int messageCount = 2;\r\n    prepareSpout(messageCount);\r\n    nextTuple_verifyEmitted_ack_resetCollector(0);\r\n    //Commits offsets during deactivation\r\n    spout.deactivate();\r\n    verifyAllMessagesCommitted(1);\r\n    // Restart topology with the same topology id, which mimics the behavior of partition reassignment\r\n    setUp();\r\n    // Initialize spout using the same populated data (i.e same kafkaUnitRule)\r\n    SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collectorMock);\r\n    nextTuple_verifyEmitted_ack_resetCollector(1);\r\n    commitAndVerifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\KafkaSpoutTopologyDeployActivateDeactivateTest.java",
  "methodName" : "test_FirstPollStrategy_Earliest_Enforced_OnlyOnTopologyDeployment",
  "sourceCode" : "@Test\r\npublic void test_FirstPollStrategy_Earliest_Enforced_OnlyOnTopologyDeployment() throws Exception {\r\n    when(topologyContext.getStormId()).thenReturn(\"topology-1\");\r\n    final int messageCount = 2;\r\n    prepareSpout(messageCount);\r\n    nextTuple_verifyEmitted_ack_resetCollector(0);\r\n    //Commits offsets during deactivation\r\n    spout.deactivate();\r\n    verifyAllMessagesCommitted(1);\r\n    // Restart topology with a different topology id\r\n    setUp();\r\n    when(topologyContext.getStormId()).thenReturn(\"topology-2\");\r\n    // Initialize spout using the same populated data (i.e same kafkaUnitRule)\r\n    SingleTopicKafkaUnitSetupHelper.initializeSpout(spout, conf, topologyContext, collectorMock);\r\n    //Emit all messages and check that they are emitted. Ack the messages too\r\n    for (int i = 0; i < messageCount; i++) {\r\n        nextTuple_verifyEmitted_ack_resetCollector(i);\r\n    }\r\n    commitAndVerifyAllMessagesCommitted(messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\MaxUncommittedOffsetTest.java",
  "methodName" : "testNextTupleCanEmitMoreMessagesWhenDroppingBelowMaxUncommittedOffsetsDueToCommit",
  "sourceCode" : "@Test\r\npublic void testNextTupleCanEmitMoreMessagesWhenDroppingBelowMaxUncommittedOffsetsDueToCommit() throws Exception {\r\n    //The spout must respect maxUncommittedOffsets after committing a set of records\r\n    try (Time.SimulatedTime simulatedTime = new Time.SimulatedTime()) {\r\n        //First check that maxUncommittedOffsets is respected when emitting from scratch\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = emitMaxUncommittedOffsetsMessagesAndCheckNoMoreAreEmitted(numMessages);\r\n        reset(collector);\r\n        //Ack all emitted messages and commit them\r\n        for (KafkaSpoutMessageId messageId : messageIds.getAllValues()) {\r\n            spout.ack(messageId);\r\n        }\r\n        Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n        spout.nextTuple();\r\n        //Now check that the spout will emit another maxUncommittedOffsets messages\r\n        for (int i = 0; i < numMessages; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        verify(collector, times(maxUncommittedOffsets)).emit(any(), any(), any());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\MaxUncommittedOffsetTest.java",
  "methodName" : "testNextTupleWillRespectMaxUncommittedOffsetsWhenThereAreAckedUncommittedTuples",
  "sourceCode" : "@Test\r\npublic void testNextTupleWillRespectMaxUncommittedOffsetsWhenThereAreAckedUncommittedTuples() throws Exception {\r\n    //The spout must respect maxUncommittedOffsets even if some tuples have been acked but not committed\r\n    try (Time.SimulatedTime simulatedTime = new Time.SimulatedTime()) {\r\n        //First check that maxUncommittedOffsets is respected when emitting from scratch\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = emitMaxUncommittedOffsetsMessagesAndCheckNoMoreAreEmitted(numMessages);\r\n        reset(collector);\r\n        //Fail all emitted messages except the last one. Try to commit.\r\n        List<KafkaSpoutMessageId> messageIdList = messageIds.getAllValues();\r\n        for (int i = 0; i < messageIdList.size() - 1; i++) {\r\n            spout.fail(messageIdList.get(i));\r\n        }\r\n        spout.ack(messageIdList.get(messageIdList.size() - 1));\r\n        Time.advanceTime(commitOffsetPeriodMs + KafkaSpout.TIMER_DELAY_MS);\r\n        spout.nextTuple();\r\n        //Now check that the spout will not emit anything else since nothing has been committed\r\n        for (int i = 0; i < numMessages; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        verify(collector, times(0)).emit(any(), any(), any());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\MaxUncommittedOffsetTest.java",
  "methodName" : "testNextTupleWillNotEmitMoreThanMaxUncommittedOffsetsPlusMaxPollRecordsMessages",
  "sourceCode" : "@Test\r\npublic void testNextTupleWillNotEmitMoreThanMaxUncommittedOffsetsPlusMaxPollRecordsMessages() throws Exception {\r\n    /*\r\n        For each partition the spout is allowed to retry all tuples between the committed offset, and maxUncommittedOffsets ahead.\r\n        It is not allowed to retry tuples past that limit.\r\n        This makes the actual limit per partition maxUncommittedOffsets + maxPollRecords - 1,\r\n        reached if the tuple at the maxUncommittedOffsets limit is the earliest retriable tuple,\r\n        or if the spout is 1 tuple below the limit, and receives a full maxPollRecords tuples in the poll.\r\n         */\r\n    try (Time.SimulatedTime simulatedTime = new Time.SimulatedTime()) {\r\n        //First check that maxUncommittedOffsets is respected when emitting from scratch\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = emitMaxUncommittedOffsetsMessagesAndCheckNoMoreAreEmitted(numMessages);\r\n        reset(collector);\r\n        //Fail only the last tuple\r\n        List<KafkaSpoutMessageId> messageIdList = messageIds.getAllValues();\r\n        KafkaSpoutMessageId failedMessageId = messageIdList.get(messageIdList.size() - 1);\r\n        spout.fail(failedMessageId);\r\n        //Offset 0 to maxUncommittedOffsets - 2 are pending, maxUncommittedOffsets - 1 is failed but not retriable\r\n        //The spout should not emit any more tuples.\r\n        spout.nextTuple();\r\n        verify(collector, never()).emit(any(), any(), any());\r\n        //Allow the failed record to retry\r\n        Time.advanceTimeSecs(initialRetryDelaySecs);\r\n        for (int i = 0; i < maxPollRecords; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> secondRunMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collector, times(maxPollRecords)).emit(any(), any(), secondRunMessageIds.capture());\r\n        reset(collector);\r\n        assertThat(secondRunMessageIds.getAllValues().get(0), is(failedMessageId));\r\n        //There should now be maxUncommittedOffsets + maxPollRecords emitted in all.\r\n        //Fail the last emitted tuple and verify that the spout won't retry it because it's above the emit limit.\r\n        spout.fail(secondRunMessageIds.getAllValues().get(secondRunMessageIds.getAllValues().size() - 1));\r\n        Time.advanceTimeSecs(initialRetryDelaySecs);\r\n        spout.nextTuple();\r\n        verify(collector, never()).emit(any(), any(), any());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\MaxUncommittedOffsetTest.java",
  "methodName" : "testNextTupleWillAllowRetryForTuplesBelowEmitLimit",
  "sourceCode" : "@Test\r\npublic void testNextTupleWillAllowRetryForTuplesBelowEmitLimit() throws Exception {\r\n    /*\r\n        For each partition the spout is allowed to retry all tuples between the committed offset, and maxUncommittedOffsets ahead.\r\n        It must retry tuples within that limit, even if more tuples were emitted.\r\n         */\r\n    try (Time.SimulatedTime simulatedTime = new Time.SimulatedTime()) {\r\n        //First check that maxUncommittedOffsets is respected when emitting from scratch\r\n        ArgumentCaptor<KafkaSpoutMessageId> messageIds = emitMaxUncommittedOffsetsMessagesAndCheckNoMoreAreEmitted(numMessages);\r\n        reset(collector);\r\n        failAllExceptTheFirstMessageThenCommit(messageIds);\r\n        //Offset 0 is committed, 1 to maxUncommittedOffsets - 1 are failed but not retriable\r\n        //The spout should now emit another maxPollRecords messages\r\n        //This is allowed because the committed message brings the numUncommittedOffsets below the cap\r\n        for (int i = 0; i < maxUncommittedOffsets; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> secondRunMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collector, times(maxPollRecords)).emit(any(), any(), secondRunMessageIds.capture());\r\n        reset(collector);\r\n        List<Long> firstRunOffsets = messageIds.getAllValues().stream().map(messageId -> messageId.offset()).collect(Collectors.toList());\r\n        List<Long> secondRunOffsets = secondRunMessageIds.getAllValues().stream().map(messageId -> messageId.offset()).collect(Collectors.toList());\r\n        assertThat(\"Expected the newly emitted messages to have no overlap with the first batch\", secondRunOffsets.removeAll(firstRunOffsets), is(false));\r\n        //Offset 0 is committed, 1 to maxUncommittedOffsets-1 are failed, maxUncommittedOffsets to maxUncommittedOffsets + maxPollRecords-1 are emitted\r\n        //Fail the last tuples so only offset 0 is not failed.\r\n        //Advance time so the failed tuples become ready for retry, and check that the spout will emit retriable tuples\r\n        //for all the failed tuples that are within maxUncommittedOffsets tuples of the committed offset\r\n        //This means 1 to maxUncommitteddOffsets, but not maxUncommittedOffsets+1...maxUncommittedOffsets+maxPollRecords-1\r\n        for (KafkaSpoutMessageId msgId : secondRunMessageIds.getAllValues()) {\r\n            spout.fail(msgId);\r\n        }\r\n        Time.advanceTimeSecs(initialRetryDelaySecs);\r\n        for (int i = 0; i < numMessages; i++) {\r\n            spout.nextTuple();\r\n        }\r\n        ArgumentCaptor<KafkaSpoutMessageId> thirdRunMessageIds = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\r\n        verify(collector, times(maxUncommittedOffsets)).emit(anyString(), anyList(), thirdRunMessageIds.capture());\r\n        reset(collector);\r\n        List<Long> thirdRunOffsets = thirdRunMessageIds.getAllValues().stream().map(msgId -> msgId.offset()).collect(Collectors.toList());\r\n        assertThat(\"Expected the emitted messages to be retries of the failed tuples from the first batch, plus the first failed tuple from the second batch\", thirdRunOffsets, everyItem(either(isIn(firstRunOffsets)).or(is(secondRunMessageIds.getAllValues().get(0).offset()))));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\metric2\\KafkaOffsetPartitionMetricsTest.java",
  "methodName" : "registerMetricsGetSpoutLagAndPartitionRecords",
  "sourceCode" : "@Test\r\npublic void registerMetricsGetSpoutLagAndPartitionRecords() throws ExecutionException, InterruptedException {\r\n    TopicPartition topicAPartition1 = new TopicPartition(\"topicA\", 1);\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(100, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionLatestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1LatestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap);\r\n    ListOffsetsResult listOffsetsResult = mock(ListOffsetsResult.class);\r\n    when(listOffsetsResult.all()).thenReturn(kafkaFuture);\r\n    admin = mock(Admin.class);\r\n    when(admin.listOffsets(anyMap())).thenReturn(listOffsetsResult);\r\n    OffsetManager offsetManagertopicAPartition1 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition1.getCommittedOffset()).thenReturn(90L);\r\n    offsetManagers = new HashMap<>();\r\n    offsetManagers.put(topicAPartition1, offsetManagertopicAPartition1);\r\n    KafkaOffsetPartitionMetrics kafkaOffsetPartitionAndTopicMetrics = new KafkaOffsetPartitionMetrics(() -> Collections.unmodifiableMap(offsetManagers), () -> admin, topicAPartition1);\r\n    Map<String, Metric> result = kafkaOffsetPartitionAndTopicMetrics.getMetrics();\r\n    Gauge g1 = (Gauge) result.get(\"topicA/partition_1/spoutLag\");\r\n    assertEquals(10L, g1.getValue());\r\n    //get partition records\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionEarliestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1EarliestListOffsetsResultInfo);\r\n    //mock consecutive calls. Each call to the recordsInPartition gauge will call kafkaFuture.get() twice\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap, topicPartitionEarliestListOffsetsResultInfoMap);\r\n    result = kafkaOffsetPartitionAndTopicMetrics.getMetrics();\r\n    g1 = (Gauge) result.get(\"topicA/partition_1/recordsInPartition\");\r\n    assertEquals(99L, g1.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\metric2\\KafkaOffsetPartitionMetricsTest.java",
  "methodName" : "registerMetricsGetEarliestAndLatest",
  "sourceCode" : "@Test\r\npublic void registerMetricsGetEarliestAndLatest() throws ExecutionException, InterruptedException {\r\n    TopicPartition topicAPartition1 = new TopicPartition(\"topicA\", 1);\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionEarliestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1EarliestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionEarliestListOffsetsResultInfoMap);\r\n    ListOffsetsResult listOffsetsResult = mock(ListOffsetsResult.class);\r\n    when(listOffsetsResult.all()).thenReturn(kafkaFuture);\r\n    admin = mock(Admin.class);\r\n    when(admin.listOffsets(anyMap())).thenReturn(listOffsetsResult);\r\n    OffsetManager offsetManagertopicAPartition1 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition1.getLatestEmittedOffset()).thenReturn(50L);\r\n    when(offsetManagertopicAPartition1.getCommittedOffset()).thenReturn(40L);\r\n    offsetManagers = new HashMap<>();\r\n    offsetManagers.put(topicAPartition1, offsetManagertopicAPartition1);\r\n    assignment = new HashSet<>();\r\n    assignment.add(topicAPartition1);\r\n    KafkaOffsetPartitionMetrics kafkaOffsetPartitionAndTopicMetrics = new KafkaOffsetPartitionMetrics(() -> Collections.unmodifiableMap(offsetManagers), () -> admin, topicAPartition1);\r\n    Map<String, Metric> result = kafkaOffsetPartitionAndTopicMetrics.getMetrics();\r\n    Gauge g1 = (Gauge) result.get(\"topicA/partition_1/earliestTimeOffset\");\r\n    assertEquals(g1.getValue(), 1L);\r\n    //get the latest offsets\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(100, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionLatestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1LatestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap);\r\n    result = kafkaOffsetPartitionAndTopicMetrics.getMetrics();\r\n    g1 = (Gauge) result.get(\"topicA/partition_1/latestTimeOffset\");\r\n    assertEquals(100L, g1.getValue());\r\n    g1 = (Gauge) result.get(\"topicA/partition_1/latestEmittedOffset\");\r\n    assertEquals(50L, g1.getValue());\r\n    g1 = (Gauge) result.get(\"topicA/partition_1/latestCompletedOffset\");\r\n    assertEquals(40L, g1.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\metric2\\KafkaOffsetTopicMetricsTest.java",
  "methodName" : "registerMetricsGetSpoutLagAndPartitionRecords",
  "sourceCode" : "@Test\r\npublic void registerMetricsGetSpoutLagAndPartitionRecords() throws ExecutionException, InterruptedException {\r\n    TopicPartition topicAPartition1 = new TopicPartition(\"topicA\", 1);\r\n    TopicPartition topicAPartition2 = new TopicPartition(\"topicA\", 2);\r\n    TopicPartition topicBPartition1 = new TopicPartition(\"topicB\", 1);\r\n    TopicPartition topicBPartition2 = new TopicPartition(\"topicB\", 2);\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(100, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition2LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(200, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(300, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition2LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(400, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionLatestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition2, topicAPartition2LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicBPartition1, topicBPartition1LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicBPartition2, topicBPartition2LatestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap);\r\n    ListOffsetsResult listOffsetsResult = mock(ListOffsetsResult.class);\r\n    when(listOffsetsResult.all()).thenReturn(kafkaFuture);\r\n    admin = mock(Admin.class);\r\n    when(admin.listOffsets(anyMap())).thenReturn(listOffsetsResult);\r\n    OffsetManager offsetManagertopicAPartition1 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition1.getCommittedOffset()).thenReturn(90L);\r\n    OffsetManager offsetManagertopicAPartition2 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition2.getCommittedOffset()).thenReturn(170L);\r\n    offsetManagers = new HashMap<>();\r\n    offsetManagers.put(topicAPartition1, offsetManagertopicAPartition1);\r\n    offsetManagers.put(topicAPartition2, offsetManagertopicAPartition2);\r\n    assignment = new HashSet<>();\r\n    assignment.add(topicAPartition1);\r\n    assignment.add(topicAPartition2);\r\n    assignment.add(topicBPartition1);\r\n    assignment.add(topicBPartition2);\r\n    KafkaOffsetTopicMetrics kafkaOffsetTopicMetricsA = new KafkaOffsetTopicMetrics(\"topicA\", () -> Collections.unmodifiableMap(offsetManagers), () -> admin, assignment);\r\n    Map<String, Metric> result = kafkaOffsetTopicMetricsA.getMetrics();\r\n    Gauge g1 = (Gauge) result.get(\"topicA/totalSpoutLag\");\r\n    assertEquals(40L, g1.getValue());\r\n    //get again the values from the Gauge. Values cannot change\r\n    g1 = (Gauge) result.get(\"topicA/totalSpoutLag\");\r\n    assertEquals(40L, g1.getValue());\r\n    assertNull(result.get(\"topicB/totalSpoutLag\"));\r\n    //get topic records\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition2EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(2, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(3, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition2EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(4, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionEarliestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition2, topicAPartition2EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicBPartition1, topicBPartition1EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicBPartition2, topicBPartition2EarliestListOffsetsResultInfo);\r\n    //mock consecutive calls. Each call to the recordsInPartition gauge will call kafkaFuture.get() twice\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap, topicPartitionEarliestListOffsetsResultInfoMap, topicPartitionLatestListOffsetsResultInfoMap, topicPartitionEarliestListOffsetsResultInfoMap, topicPartitionLatestListOffsetsResultInfoMap, topicPartitionEarliestListOffsetsResultInfoMap, topicPartitionLatestListOffsetsResultInfoMap, topicPartitionEarliestListOffsetsResultInfoMap);\r\n    Gauge gATotal = (Gauge) result.get(\"topicA/totalRecordsInPartitions\");\r\n    assertEquals(297L, gATotal.getValue());\r\n    //get again the values from the Gauge. Values cannot change\r\n    gATotal = (Gauge) result.get(\"topicA/totalRecordsInPartitions\");\r\n    assertEquals(297L, gATotal.getValue());\r\n    assertNull(result.get(\"topicB/totalRecordsInPartitions\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\metric2\\KafkaOffsetTopicMetricsTest.java",
  "methodName" : "registerMetricsGetEarliestAndLatest",
  "sourceCode" : "@Test\r\npublic void registerMetricsGetEarliestAndLatest() throws ExecutionException, InterruptedException {\r\n    TopicPartition topicAPartition1 = new TopicPartition(\"topicA\", 1);\r\n    TopicPartition topicAPartition2 = new TopicPartition(\"topicA\", 2);\r\n    TopicPartition topicBPartition1 = new TopicPartition(\"topicB\", 1);\r\n    TopicPartition topicBPartition2 = new TopicPartition(\"topicB\", 2);\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition2EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition1EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition2EarliestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(1, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionEarliestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicAPartition2, topicAPartition2EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicBPartition1, topicBPartition1EarliestListOffsetsResultInfo);\r\n    topicPartitionEarliestListOffsetsResultInfoMap.put(topicBPartition2, topicBPartition2EarliestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionEarliestListOffsetsResultInfoMap);\r\n    ListOffsetsResult listOffsetsResult = mock(ListOffsetsResult.class);\r\n    when(listOffsetsResult.all()).thenReturn(kafkaFuture);\r\n    admin = mock(Admin.class);\r\n    when(admin.listOffsets(anyMap())).thenReturn(listOffsetsResult);\r\n    OffsetManager offsetManagertopicAPartition1 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition1.getLatestEmittedOffset()).thenReturn(50L);\r\n    when(offsetManagertopicAPartition1.getCommittedOffset()).thenReturn(40L);\r\n    OffsetManager offsetManagertopicAPartition2 = mock(OffsetManager.class);\r\n    when(offsetManagertopicAPartition2.getLatestEmittedOffset()).thenReturn(100L);\r\n    when(offsetManagertopicAPartition2.getCommittedOffset()).thenReturn(90L);\r\n    offsetManagers = new HashMap<>();\r\n    offsetManagers.put(topicAPartition1, offsetManagertopicAPartition1);\r\n    offsetManagers.put(topicAPartition2, offsetManagertopicAPartition2);\r\n    assignment = new HashSet<>();\r\n    assignment.add(topicAPartition1);\r\n    assignment.add(topicAPartition2);\r\n    assignment.add(topicBPartition1);\r\n    assignment.add(topicBPartition2);\r\n    KafkaOffsetTopicMetrics kafkaOffsetPartitionAndTopicMetrics = new KafkaOffsetTopicMetrics(\"topicA\", () -> Collections.unmodifiableMap(offsetManagers), () -> admin, assignment);\r\n    Map<String, Metric> result = kafkaOffsetPartitionAndTopicMetrics.getMetrics();\r\n    Gauge gATotal = (Gauge) result.get(\"topicA/totalEarliestTimeOffset\");\r\n    assertEquals(2L, gATotal.getValue());\r\n    assertNull(result.get(\"topicB/totalEarliestTimeOffset\"));\r\n    //get the metrics a second time. Values should be the same\r\n    gATotal = (Gauge) result.get(\"topicA/totalEarliestTimeOffset\");\r\n    assertEquals(2L, gATotal.getValue());\r\n    assertNull(result.get(\"topicB/totalEarliestTimeOffset\"));\r\n    //get the latest offsets\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(100, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicAPartition2LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(200, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition1LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(300, System.currentTimeMillis(), Optional.empty());\r\n    ListOffsetsResult.ListOffsetsResultInfo topicBPartition2LatestListOffsetsResultInfo = new ListOffsetsResult.ListOffsetsResultInfo(400, System.currentTimeMillis(), Optional.empty());\r\n    Map<TopicPartition, ListOffsetsResult.ListOffsetsResultInfo> topicPartitionLatestListOffsetsResultInfoMap = new HashMap<>();\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition1, topicAPartition1LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicAPartition2, topicAPartition2LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicBPartition1, topicBPartition1LatestListOffsetsResultInfo);\r\n    topicPartitionLatestListOffsetsResultInfoMap.put(topicBPartition2, topicBPartition2LatestListOffsetsResultInfo);\r\n    when(kafkaFuture.get()).thenReturn(topicPartitionLatestListOffsetsResultInfoMap);\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestTimeOffset\");\r\n    assertEquals(300L, gATotal.getValue());\r\n    assertNull((result.get(\"topicB/totalLatestTimeOffset\")));\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestEmittedOffset\");\r\n    assertEquals(150L, gATotal.getValue());\r\n    assertNull(result.get(\"topicB/totalLatestEmittedOffset\"));\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestCompletedOffset\");\r\n    assertEquals(130L, gATotal.getValue());\r\n    assertNull(result.get(\"topiBA/totalLatestCompletedOffset\"));\r\n    //get the metrics a second time. Values should be the same\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestTimeOffset\");\r\n    assertEquals(300L, gATotal.getValue());\r\n    assertNull((result.get(\"topicB/totalLatestTimeOffset\")));\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestEmittedOffset\");\r\n    assertEquals(150L, gATotal.getValue());\r\n    assertNull(result.get(\"topicB/totalLatestEmittedOffset\"));\r\n    gATotal = (Gauge) result.get(\"topicA/totalLatestCompletedOffset\");\r\n    assertEquals(130L, gATotal.getValue());\r\n    assertNull(result.get(\"topiBA/totalLatestCompletedOffset\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\SimpleRecordTranslatorTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    SimpleRecordTranslator<String, String> trans = new SimpleRecordTranslator<>((r) -> new Values(r.value()), new Fields(\"value\"));\r\n    assertEquals(Collections.singletonList(\"default\"), trans.streams());\r\n    ConsumerRecord<String, String> cr = new ConsumerRecord<>(\"TOPIC\", 100, 100, \"THE KEY\", \"THE VALUE\");\r\n    assertEquals(Collections.singletonList(\"THE VALUE\"), trans.apply(cr));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\SimpleRecordTranslatorTest.java",
  "methodName" : "testNullTranslation",
  "sourceCode" : "@Test\r\npublic void testNullTranslation() {\r\n    SimpleRecordTranslator<String, String> trans = new SimpleRecordTranslator<>((r) -> null, new Fields(\"key\"));\r\n    assertNull(trans.apply(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\subscription\\NamedTopicFilterTest.java",
  "methodName" : "testFilter",
  "sourceCode" : "@Test\r\npublic void testFilter() {\r\n    String matchingTopicOne = \"test-1\";\r\n    String matchingTopicTwo = \"test-11\";\r\n    String unmatchedTopic = \"unmatched\";\r\n    NamedTopicFilter filter = new NamedTopicFilter(matchingTopicOne, matchingTopicTwo);\r\n    when(consumerMock.partitionsFor(matchingTopicOne)).thenReturn(Collections.singletonList(createPartitionInfo(matchingTopicOne, 0)));\r\n    List<PartitionInfo> partitionTwoPartitions = new ArrayList<>();\r\n    partitionTwoPartitions.add(createPartitionInfo(matchingTopicTwo, 0));\r\n    partitionTwoPartitions.add(createPartitionInfo(matchingTopicTwo, 1));\r\n    when(consumerMock.partitionsFor(matchingTopicTwo)).thenReturn(partitionTwoPartitions);\r\n    when(consumerMock.partitionsFor(unmatchedTopic)).thenReturn(Collections.singletonList(createPartitionInfo(unmatchedTopic, 0)));\r\n    Set<TopicPartition> matchedPartitions = filter.getAllSubscribedPartitions(consumerMock);\r\n    assertThat(\"Expected filter to pass only topics with exact name matches\", matchedPartitions, containsInAnyOrder(new TopicPartition(matchingTopicOne, 0), new TopicPartition(matchingTopicTwo, 0), new TopicPartition(matchingTopicTwo, 1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\subscription\\NamedTopicFilterTest.java",
  "methodName" : "testFilterOnAbsentTopic",
  "sourceCode" : "@Test\r\npublic void testFilterOnAbsentTopic() {\r\n    String presentTopic = \"present\";\r\n    String absentTopic = \"absent\";\r\n    NamedTopicFilter filter = new NamedTopicFilter(presentTopic, absentTopic);\r\n    when(consumerMock.partitionsFor(presentTopic)).thenReturn(Collections.singletonList(createPartitionInfo(presentTopic, 2)));\r\n    when(consumerMock.partitionsFor(absentTopic)).thenReturn(null);\r\n    Set<TopicPartition> presentPartitions = filter.getAllSubscribedPartitions(consumerMock);\r\n    assertThat(\"Expected filter to pass only topics which are present\", presentPartitions, contains(new TopicPartition(presentTopic, 2)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\subscription\\PatternTopicFilterTest.java",
  "methodName" : "testFilter",
  "sourceCode" : "@Test\r\npublic void testFilter() {\r\n    Pattern pattern = Pattern.compile(\"test-\\\\d+\");\r\n    PatternTopicFilter filter = new PatternTopicFilter(pattern);\r\n    String matchingTopicOne = \"test-1\";\r\n    String matchingTopicTwo = \"test-11\";\r\n    String unmatchedTopic = \"unmatched\";\r\n    Map<String, List<PartitionInfo>> allTopics = new HashMap<>();\r\n    allTopics.put(matchingTopicOne, Collections.singletonList(createPartitionInfo(matchingTopicOne, 0)));\r\n    List<PartitionInfo> testTwoPartitions = new ArrayList<>();\r\n    testTwoPartitions.add(createPartitionInfo(matchingTopicTwo, 0));\r\n    testTwoPartitions.add(createPartitionInfo(matchingTopicTwo, 1));\r\n    allTopics.put(matchingTopicTwo, testTwoPartitions);\r\n    allTopics.put(unmatchedTopic, Collections.singletonList(createPartitionInfo(unmatchedTopic, 0)));\r\n    when(consumerMock.listTopics()).thenReturn(allTopics);\r\n    Set<TopicPartition> matchedPartitions = filter.getAllSubscribedPartitions(consumerMock);\r\n    assertThat(\"Expected topic partitions matching the pattern to be passed by the filter\", matchedPartitions, containsInAnyOrder(new TopicPartition(matchingTopicOne, 0), new TopicPartition(matchingTopicTwo, 0), new TopicPartition(matchingTopicTwo, 1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\subscription\\RoundRobinManualPartitionerTest.java",
  "methodName" : "testRoundRobinPartitioning",
  "sourceCode" : "@Test\r\npublic void testRoundRobinPartitioning() {\r\n    List<TopicPartition> allPartitions = new ArrayList<>();\r\n    for (int i = 0; i < 11; i++) {\r\n        allPartitions.add(createTp(i));\r\n    }\r\n    List<TopologyContext> contextMocks = new ArrayList<>();\r\n    String thisComponentId = \"A spout\";\r\n    List<Integer> allTasks = Arrays.asList(0, 1, 2);\r\n    for (int i = 0; i < 3; i++) {\r\n        TopologyContext contextMock = mock(TopologyContext.class);\r\n        when(contextMock.getThisTaskIndex()).thenReturn(i);\r\n        when(contextMock.getThisComponentId()).thenReturn(thisComponentId);\r\n        when(contextMock.getComponentTasks(thisComponentId)).thenReturn(allTasks);\r\n        contextMocks.add(contextMock);\r\n    }\r\n    RoundRobinManualPartitioner partitioner = new RoundRobinManualPartitioner();\r\n    Set<TopicPartition> partitionsForFirstTask = partitioner.getPartitionsForThisTask(allPartitions, contextMocks.get(0));\r\n    assertThat(partitionsForFirstTask, is(partitionsToTps(new int[] { 0, 3, 6, 9 })));\r\n    Set<TopicPartition> partitionsForSecondTask = partitioner.getPartitionsForThisTask(allPartitions, contextMocks.get(1));\r\n    assertThat(partitionsForSecondTask, is(partitionsToTps(new int[] { 1, 4, 7, 10 })));\r\n    Set<TopicPartition> partitionsForThirdTask = partitioner.getPartitionsForThisTask(allPartitions, contextMocks.get(2));\r\n    assertThat(partitionsForThirdTask, is(partitionsToTps(new int[] { 2, 5, 8 })));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\subscription\\TopicAssignerTest.java",
  "methodName" : "testCanReassignPartitions",
  "sourceCode" : "@Test\r\npublic void testCanReassignPartitions() {\r\n    Set<TopicPartition> onePartition = Collections.singleton(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0));\r\n    Set<TopicPartition> twoPartitions = new HashSet<>();\r\n    twoPartitions.add(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 0));\r\n    twoPartitions.add(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, 1));\r\n    KafkaConsumer<String, String> consumerMock = mock(KafkaConsumer.class);\r\n    ConsumerRebalanceListener listenerMock = mock(ConsumerRebalanceListener.class);\r\n    TopicAssigner assigner = new TopicAssigner();\r\n    //Set the first assignment\r\n    assigner.assignPartitions(consumerMock, onePartition, listenerMock);\r\n    InOrder inOrder = inOrder(consumerMock, listenerMock);\r\n    inOrder.verify(listenerMock).onPartitionsRevoked(Collections.emptySet());\r\n    inOrder.verify(consumerMock).assign(new HashSet<>(onePartition));\r\n    inOrder.verify(listenerMock).onPartitionsAssigned(new HashSet<>(onePartition));\r\n    clearInvocations(consumerMock, listenerMock);\r\n    when(consumerMock.assignment()).thenReturn(new HashSet<>(onePartition));\r\n    //Update to set the second assignment\r\n    assigner.assignPartitions(consumerMock, twoPartitions, listenerMock);\r\n    //The partition revocation hook must be called before the new partitions are assigned to the consumer,\r\n    //to allow the revocation hook to commit offsets for the revoked partitions.\r\n    inOrder.verify(listenerMock).onPartitionsRevoked(new HashSet<>(onePartition));\r\n    inOrder.verify(consumerMock).assign(new HashSet<>(twoPartitions));\r\n    inOrder.verify(listenerMock).onPartitionsAssigned(new HashSet<>(twoPartitions));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutBatchMetadataTest.java",
  "methodName" : "testMetadataIsRoundTripSerializableWithJsonSimple",
  "sourceCode" : "/**\r\n * Tests that the metadata object can be converted to and from a Map. This is needed because Trident metadata is written to\r\n * Zookeeper as JSON with the json-simple library, so the spout converts the metadata to Map before returning it to Trident.\r\n * It is important that all map entries are types json-simple knows about,\r\n * since otherwise the library just calls toString on them which will likely produce invalid JSON.\r\n */\r\n@SuppressWarnings(\"rawtypes\")\r\n@Test\r\npublic void testMetadataIsRoundTripSerializableWithJsonSimple() throws Exception {\r\n    long startOffset = 10;\r\n    long endOffset = 20;\r\n    String topologyId = \"topologyId\";\r\n    KafkaTridentSpoutBatchMetadata metadata = new KafkaTridentSpoutBatchMetadata(startOffset, endOffset, topologyId);\r\n    Map<String, Object> map = metadata.toMap();\r\n    Map<String, Object> deserializedMap = (Map) JSONValue.parseWithException(JSONValue.toJSONString(map));\r\n    KafkaTridentSpoutBatchMetadata deserializedMetadata = KafkaTridentSpoutBatchMetadata.fromMap(deserializedMap);\r\n    assertThat(deserializedMetadata.getFirstOffset(), is(metadata.getFirstOffset()));\r\n    assertThat(deserializedMetadata.getLastOffset(), is(metadata.getLastOffset()));\r\n    assertThat(deserializedMetadata.getTopologyId(), is(metadata.getTopologyId()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEmitNewBatchWithNullMetaUncommittedEarliest",
  "sourceCode" : "@Test\r\npublic void testEmitNewBatchWithNullMetaUncommittedEarliest() {\r\n    //Check that null meta makes the spout seek to EARLIEST, and that the returned meta is correct\r\n    Map<String, Object> batchMeta = doEmitNewBatchTest(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST, collectorMock, partition, null);\r\n    verify(collectorMock, times(recordsInKafka)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstOffsetInKafka));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(lastOffsetInKafka));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(batchMeta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstOffsetInKafka));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEmitNewBatchWithNullMetaUncommittedLatest",
  "sourceCode" : "@Test\r\npublic void testEmitNewBatchWithNullMetaUncommittedLatest() {\r\n    //Check that null meta makes the spout seek to LATEST, and that the returned meta is correct\r\n    Map<String, Object> batchMeta = doEmitNewBatchTest(FirstPollOffsetStrategy.UNCOMMITTED_LATEST, collectorMock, partition, null);\r\n    verify(collectorMock, never()).emit(anyList());\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(batchMeta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(lastOffsetInKafka));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEmitNewBatchWithPreviousMeta",
  "sourceCode" : "@ParameterizedTest\r\n@EnumSource(value = FirstPollOffsetStrategy.class, names = { \"EARLIEST\", \"LATEST\", \"TIMESTAMP\" })\r\npublic void testEmitNewBatchWithPreviousMeta(FirstPollOffsetStrategy firstPollOffsetStrategy) {\r\n    //Check that non-null meta makes the spout seek according to the provided metadata, and that the returned meta is correct\r\n    long firstExpectedEmittedOffset = 50;\r\n    int expectedEmittedRecords = 50;\r\n    KafkaTridentSpoutBatchMetadata previousBatchMeta = new KafkaTridentSpoutBatchMetadata(firstOffsetInKafka, firstExpectedEmittedOffset - 1, topologyId);\r\n    Map<String, Object> batchMeta = doEmitNewBatchTest(firstPollOffsetStrategy, collectorMock, partition, previousBatchMeta.toMap());\r\n    verify(collectorMock, times(expectedEmittedRecords)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstExpectedEmittedOffset));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(lastOffsetInKafka));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(batchMeta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstExpectedEmittedOffset));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEmitEmptyBatches",
  "sourceCode" : "@Test\r\npublic void testEmitEmptyBatches() {\r\n    //Check that the emitter can handle emitting empty batches on a new partition.\r\n    //If the spout is configured to seek to LATEST, or the partition is empty, the initial batches may be empty\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.LATEST);\r\n    KafkaTridentSpoutTopicPartition kttp = new KafkaTridentSpoutTopicPartition(partition);\r\n    Map<String, Object> lastBatchMeta = null;\r\n    //Emit 10 empty batches, simulating no new records being present in Kafka\r\n    for (int i = 0; i < 10; i++) {\r\n        TransactionAttempt txid = new TransactionAttempt((long) i, 0);\r\n        lastBatchMeta = emitBatchNew(emitter, txid, collectorMock, partition, lastBatchMeta);\r\n        KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(lastBatchMeta);\r\n        assertThat(\"Since the first poll strategy is LATEST, the meta should indicate that the last message has already been emitted\", deserializedMeta.getFirstOffset(), is(lastOffsetInKafka));\r\n        assertThat(\"Since the first poll strategy is LATEST, the meta should indicate that the last message has already been emitted\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n    }\r\n    //Add new records to Kafka, and check that the next batch contains these records\r\n    long firstNewRecordOffset = lastOffsetInKafka + 1;\r\n    int numNewRecords = 10;\r\n    List<ConsumerRecord<String, String>> newRecords = SpoutWithMockedConsumerSetupHelper.createRecords(partition, firstNewRecordOffset, numNewRecords);\r\n    newRecords.forEach(consumer::addRecord);\r\n    TransactionAttempt txid = new TransactionAttempt(11L, 0);\r\n    lastBatchMeta = emitBatchNew(emitter, txid, collectorMock, partition, lastBatchMeta);\r\n    verify(collectorMock, times(numNewRecords)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstNewRecordOffset));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(firstNewRecordOffset + numNewRecords - 1));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(lastBatchMeta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstNewRecordOffset));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(firstNewRecordOffset + numNewRecords - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testReEmitBatch",
  "sourceCode" : "@Test\r\npublic void testReEmitBatch() {\r\n    //Check that a reemit emits exactly the same tuples as the last batch, even if Kafka returns more messages\r\n    long firstEmittedOffset = 50;\r\n    int numEmittedRecords = 10;\r\n    KafkaTridentSpoutBatchMetadata batchMeta = new KafkaTridentSpoutBatchMetadata(firstEmittedOffset, firstEmittedOffset + numEmittedRecords - 1, topologyId);\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST);\r\n    TransactionAttempt txid = new TransactionAttempt(10L, 0);\r\n    KafkaTridentSpoutTopicPartition kttp = new KafkaTridentSpoutTopicPartition(partition);\r\n    emitter.reEmitPartitionBatch(txid, collectorMock, kttp, batchMeta.toMap());\r\n    verify(collectorMock, times(numEmittedRecords)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstEmittedOffset));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(firstEmittedOffset + numEmittedRecords - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testReEmitBatchForOldTopologyWhenIgnoringCommittedOffsets",
  "sourceCode" : "@Test\r\npublic void testReEmitBatchForOldTopologyWhenIgnoringCommittedOffsets() {\r\n    //In some cases users will want to drop retrying old batches, e.g. if the topology should start over from scratch.\r\n    //If the FirstPollOffsetStrategy ignores committed offsets, we should not retry batches for old topologies\r\n    //The batch retry should be skipped entirely\r\n    KafkaTridentSpoutBatchMetadata batchMeta = new KafkaTridentSpoutBatchMetadata(firstOffsetInKafka, lastOffsetInKafka, \"a new storm id\");\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.EARLIEST);\r\n    TransactionAttempt txid = new TransactionAttempt(10L, 0);\r\n    KafkaTridentSpoutTopicPartition kttp = new KafkaTridentSpoutTopicPartition(partition);\r\n    emitter.reEmitPartitionBatch(txid, collectorMock, kttp, batchMeta.toMap());\r\n    verify(collectorMock, never()).emit(anyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEmitEmptyFirstBatch",
  "sourceCode" : "@Test\r\npublic void testEmitEmptyFirstBatch() {\r\n    /*\r\n         * Check that when the first batch after a redeploy is empty, the emitter does not restart at the pre-redeploy offset. STORM-3279.\r\n         */\r\n    long firstEmittedOffset = 50;\r\n    int emittedRecords = 10;\r\n    KafkaTridentSpoutBatchMetadata preRedeployLastMeta = new KafkaTridentSpoutBatchMetadata(firstEmittedOffset, firstEmittedOffset + emittedRecords - 1, \"an old topology\");\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.LATEST);\r\n    TransactionAttempt txid = new TransactionAttempt(0L, 0);\r\n    Map<String, Object> meta = emitBatchNew(emitter, txid, collectorMock, partition, preRedeployLastMeta.toMap());\r\n    verify(collectorMock, never()).emit(anyList());\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(meta);\r\n    assertThat(deserializedMeta.getFirstOffset(), is(lastOffsetInKafka));\r\n    assertThat(deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n    long firstNewRecordOffset = lastOffsetInKafka + 1;\r\n    int numNewRecords = 10;\r\n    List<ConsumerRecord<String, String>> newRecords = SpoutWithMockedConsumerSetupHelper.createRecords(partition, firstNewRecordOffset, numNewRecords);\r\n    newRecords.forEach(consumer::addRecord);\r\n    meta = emitBatchNew(emitter, txid, collectorMock, partition, meta);\r\n    verify(collectorMock, times(numNewRecords)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstNewRecordOffset));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(firstNewRecordOffset + numNewRecords - 1));\r\n    deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(meta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstNewRecordOffset));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(firstNewRecordOffset + numNewRecords - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testUnconditionalStrategyWhenSpoutWorkerIsRestarted",
  "sourceCode" : "@ParameterizedTest\r\n@EnumSource(value = FirstPollOffsetStrategy.class, names = { \"EARLIEST\", \"LATEST\", \"TIMESTAMP\" })\r\npublic void testUnconditionalStrategyWhenSpoutWorkerIsRestarted(FirstPollOffsetStrategy firstPollOffsetStrategy) {\r\n    /*\r\n         * EARLIEST/LATEST/TIMESTAMP should act like UNCOMMITTED_EARLIEST/LATEST/TIMESTAMP if the emitter is new but the\r\n         * topology has not restarted (storm id has not changed)\r\n         */\r\n    long preRestartEmittedOffset = 20;\r\n    int lastBatchEmittedRecords = 10;\r\n    int preRestartEmittedRecords = 30;\r\n    KafkaTridentSpoutBatchMetadata preExecutorRestartLastMeta = new KafkaTridentSpoutBatchMetadata(preRestartEmittedOffset, preRestartEmittedOffset + lastBatchEmittedRecords - 1, topologyId);\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(firstPollOffsetStrategy);\r\n    TransactionAttempt txid = new TransactionAttempt(0L, 0);\r\n    Map<String, Object> meta = emitBatchNew(emitter, txid, collectorMock, partition, preExecutorRestartLastMeta.toMap());\r\n    long firstEmittedOffset = preRestartEmittedOffset + lastBatchEmittedRecords;\r\n    int emittedRecords = recordsInKafka - preRestartEmittedRecords;\r\n    verify(collectorMock, times(emittedRecords)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstEmittedOffset));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(lastOffsetInKafka));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(meta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstEmittedOffset));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testEarliestStrategyWhenTopologyIsRedeployed",
  "sourceCode" : "@Test\r\npublic void testEarliestStrategyWhenTopologyIsRedeployed() {\r\n    /*\r\n         * EARLIEST should be applied if the emitter is new and the topology has been redeployed (storm id has changed)\r\n         */\r\n    long preRestartEmittedOffset = 20;\r\n    int preRestartEmittedRecords = 10;\r\n    KafkaTridentSpoutBatchMetadata preExecutorRestartLastMeta = new KafkaTridentSpoutBatchMetadata(preRestartEmittedOffset, preRestartEmittedOffset + preRestartEmittedRecords - 1, \"Some older topology\");\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.EARLIEST);\r\n    TransactionAttempt txid = new TransactionAttempt(0L, 0);\r\n    Map<String, Object> meta = emitBatchNew(emitter, txid, collectorMock, partition, preExecutorRestartLastMeta.toMap());\r\n    verify(collectorMock, times(recordsInKafka)).emit(emitCaptor.capture());\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(firstOffsetInKafka));\r\n    assertThat(emits.get(emits.size() - 1).get(0), is(lastOffsetInKafka));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(meta);\r\n    assertThat(\"The batch should start at the first offset of the polled records\", deserializedMeta.getFirstOffset(), is(firstOffsetInKafka));\r\n    assertThat(\"The batch should end at the last offset of the polled messages\", deserializedMeta.getLastOffset(), is(lastOffsetInKafka));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testLatestStrategyWhenTopologyIsRedeployed",
  "sourceCode" : "@Test\r\npublic void testLatestStrategyWhenTopologyIsRedeployed() {\r\n    /*\r\n         * EARLIEST should be applied if the emitter is new and the topology has been redeployed (storm id has changed)\r\n         */\r\n    long preRestartEmittedOffset = 20;\r\n    int preRestartEmittedRecords = 10;\r\n    KafkaTridentSpoutBatchMetadata preExecutorRestartLastMeta = new KafkaTridentSpoutBatchMetadata(preRestartEmittedOffset, preRestartEmittedOffset + preRestartEmittedRecords - 1, \"Some older topology\");\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(FirstPollOffsetStrategy.LATEST);\r\n    TransactionAttempt txid = new TransactionAttempt(0L, 0);\r\n    KafkaTridentSpoutTopicPartition kttp = new KafkaTridentSpoutTopicPartition(partition);\r\n    Map<String, Object> meta = emitBatchNew(emitter, txid, collectorMock, partition, preExecutorRestartLastMeta.toMap());\r\n    verify(collectorMock, never()).emit(anyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterEmitTest.java",
  "methodName" : "testTimeStampStrategyWhenTopologyIsRedeployed",
  "sourceCode" : "@Test\r\npublic void testTimeStampStrategyWhenTopologyIsRedeployed() {\r\n    /*\r\n         * TIMESTAMP strategy should be applied if the emitter is new and the topology has been redeployed (storm id has changed)\r\n         * Offset should be reset according to the offset corresponding to startTimeStamp\r\n         */\r\n    long preRestartEmittedOffset = 20;\r\n    int preRestartEmittedRecords = 10;\r\n    long timeStampStartOffset = 2L;\r\n    long pollTimeout = 1L;\r\n    KafkaTridentSpoutBatchMetadata preExecutorRestartLastMeta = new KafkaTridentSpoutBatchMetadata(preRestartEmittedOffset, preRestartEmittedOffset + preRestartEmittedRecords - 1, \"Some older topology\");\r\n    KafkaConsumer<String, String> kafkaConsumer = Mockito.mock(KafkaConsumer.class);\r\n    when(kafkaConsumer.assignment()).thenReturn(Collections.singleton(partition));\r\n    OffsetAndTimestamp offsetAndTimestamp = new OffsetAndTimestamp(timeStampStartOffset, startTimeStamp);\r\n    HashMap<TopicPartition, OffsetAndTimestamp> map = new HashMap<>();\r\n    map.put(partition, offsetAndTimestamp);\r\n    when(kafkaConsumer.offsetsForTimes(Collections.singletonMap(partition, startTimeStamp))).thenReturn(map);\r\n    HashMap<TopicPartition, List<ConsumerRecord<String, String>>> topicPartitionMap = new HashMap<>();\r\n    List<ConsumerRecord<String, String>> newRecords = SpoutWithMockedConsumerSetupHelper.createRecords(partition, timeStampStartOffset, recordsInKafka);\r\n    topicPartitionMap.put(partition, newRecords);\r\n    when(kafkaConsumer.poll(Duration.ofMillis(pollTimeout))).thenReturn(new ConsumerRecords<>(topicPartitionMap));\r\n    KafkaTridentSpoutEmitter<String, String> emitter = createEmitter(kafkaConsumer, adminClient, FirstPollOffsetStrategy.TIMESTAMP);\r\n    TransactionAttempt txid = new TransactionAttempt(0L, 0);\r\n    Map<String, Object> meta = emitBatchNew(emitter, txid, collectorMock, partition, preExecutorRestartLastMeta.toMap());\r\n    verify(collectorMock, times(recordsInKafka)).emit(emitCaptor.capture());\r\n    verify(kafkaConsumer, times(1)).seek(partition, timeStampStartOffset);\r\n    List<List<Object>> emits = emitCaptor.getAllValues();\r\n    assertThat(emits.get(0).get(0), is(timeStampStartOffset));\r\n    KafkaTridentSpoutBatchMetadata deserializedMeta = KafkaTridentSpoutBatchMetadata.fromMap(meta);\r\n    assertThat(\"The batch should start at the first offset for startTimestamp\", deserializedMeta.getFirstOffset(), is(timeStampStartOffset));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterPartitioningTest.java",
  "methodName" : "testGetOrderedPartitionsIsConsistent",
  "sourceCode" : "@Test\r\npublic void testGetOrderedPartitionsIsConsistent() {\r\n    KafkaTridentSpoutEmitter<String, String> emitter = new KafkaTridentSpoutEmitter<>(SingleTopicKafkaTridentSpoutConfiguration.createKafkaSpoutConfigBuilder(-1).build(), topologyContextMock, new ClientFactory<String, String>() {\r\n\r\n        @Override\r\n        public Consumer<String, String> createConsumer(Map<String, Object> consumerProps) {\r\n            return consumer;\r\n        }\r\n\r\n        @Override\r\n        public Admin createAdmin(Map<String, Object> adminProps) {\r\n            return adminClient;\r\n        }\r\n    }, new TopicAssigner());\r\n    Set<TopicPartition> allPartitions = new HashSet<>();\r\n    int numPartitions = 10;\r\n    for (int i = 0; i < numPartitions; i++) {\r\n        allPartitions.add(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, i));\r\n    }\r\n    List<Map<String, Object>> serializedPartitions = allPartitions.stream().map(tpSerializer::toMap).collect(Collectors.toList());\r\n    List<KafkaTridentSpoutTopicPartition> orderedPartitions = emitter.getOrderedPartitions(serializedPartitions);\r\n    assertThat(\"Should contain all partitions\", orderedPartitions.size(), is(allPartitions.size()));\r\n    Collections.shuffle(serializedPartitions);\r\n    List<KafkaTridentSpoutTopicPartition> secondGetOrderedPartitions = emitter.getOrderedPartitions(serializedPartitions);\r\n    assertThat(\"Ordering must be consistent\", secondGetOrderedPartitions, is(orderedPartitions));\r\n    serializedPartitions.add(tpSerializer.toMap(new TopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, numPartitions)));\r\n    List<KafkaTridentSpoutTopicPartition> orderedPartitionsWithNewPartition = emitter.getOrderedPartitions(serializedPartitions);\r\n    orderedPartitionsWithNewPartition.remove(orderedPartitionsWithNewPartition.size() - 1);\r\n    assertThat(\"Adding new partitions should not shuffle the existing ordering\", orderedPartitionsWithNewPartition, is(orderedPartitions));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterPartitioningTest.java",
  "methodName" : "testGetPartitionsForTask",
  "sourceCode" : "@Test\r\npublic void testGetPartitionsForTask() {\r\n    //Verify correct wrapping/unwrapping of partition and delegation of partition assignment\r\n    ManualPartitioner partitionerMock = mock(ManualPartitioner.class);\r\n    when(partitionerMock.getPartitionsForThisTask(any(), any())).thenAnswer(invocation -> {\r\n        List<TopicPartition> partitions = new ArrayList<>(invocation.getArgument(0));\r\n        partitions.remove(0);\r\n        return new HashSet<>(partitions);\r\n    });\r\n    KafkaTridentSpoutEmitter<String, String> emitter = new KafkaTridentSpoutEmitter<>(SingleTopicKafkaTridentSpoutConfiguration.createKafkaSpoutConfigBuilder(mock(TopicFilter.class), partitionerMock, -1).build(), topologyContextMock, new ClientFactory<String, String>() {\r\n\r\n        @Override\r\n        public Consumer<String, String> createConsumer(Map<String, Object> consumerProps) {\r\n            return consumer;\r\n        }\r\n\r\n        @Override\r\n        public Admin createAdmin(Map<String, Object> adminProps) {\r\n            return adminClient;\r\n        }\r\n    }, new TopicAssigner());\r\n    List<KafkaTridentSpoutTopicPartition> allPartitions = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        allPartitions.add(new KafkaTridentSpoutTopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, i));\r\n    }\r\n    List<TopicPartition> unwrappedPartitions = allPartitions.stream().map(kttp -> kttp.getTopicPartition()).collect(Collectors.toList());\r\n    List<KafkaTridentSpoutTopicPartition> partitionsForTask = emitter.getPartitionsForTask(0, 2, allPartitions);\r\n    verify(partitionerMock).getPartitionsForThisTask(eq(unwrappedPartitions), any(TopologyContext.class));\r\n    allPartitions.remove(0);\r\n    assertThat(\"Should have assigned all except the first partition to this task\", new HashSet<>(partitionsForTask), is(new HashSet<>(allPartitions)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutEmitterPartitioningTest.java",
  "methodName" : "testAssignPartitions",
  "sourceCode" : "@Test\r\npublic void testAssignPartitions() {\r\n    //Verify correct unwrapping of partitions and delegation of assignment\r\n    TopicAssigner assignerMock = mock(TopicAssigner.class);\r\n    KafkaTridentSpoutEmitter<String, String> emitter = new KafkaTridentSpoutEmitter<>(SingleTopicKafkaTridentSpoutConfiguration.createKafkaSpoutConfigBuilder(-1).build(), topologyContextMock, new ClientFactory<String, String>() {\r\n\r\n        @Override\r\n        public Consumer<String, String> createConsumer(Map<String, Object> consumerProps) {\r\n            return consumer;\r\n        }\r\n\r\n        @Override\r\n        public Admin createAdmin(Map<String, Object> adminProps) {\r\n            return adminClient;\r\n        }\r\n    }, assignerMock);\r\n    List<KafkaTridentSpoutTopicPartition> allPartitions = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        allPartitions.add(new KafkaTridentSpoutTopicPartition(SingleTopicKafkaSpoutConfiguration.TOPIC, i));\r\n    }\r\n    Set<TopicPartition> unwrappedPartitions = allPartitions.stream().map(kttp -> kttp.getTopicPartition()).collect(Collectors.toSet());\r\n    emitter.refreshPartitions(allPartitions);\r\n    verify(assignerMock).assignPartitions(eq(consumer), eq(unwrappedPartitions), any(ConsumerRebalanceListener.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutOpaqueCoordinatorTest.java",
  "methodName" : "testCanGetPartitions",
  "sourceCode" : "@Test\r\npublic void testCanGetPartitions() {\r\n    KafkaConsumer<String, String> mockConsumer = mock(KafkaConsumer.class);\r\n    Admin mockAdmin = mock(Admin.class);\r\n    TopicPartition expectedPartition = new TopicPartition(\"test\", 0);\r\n    TopicFilter mockFilter = mock(TopicFilter.class);\r\n    when(mockFilter.getAllSubscribedPartitions(any())).thenReturn(Collections.singleton(expectedPartition));\r\n    KafkaTridentSpoutConfig<String, String> spoutConfig = SingleTopicKafkaTridentSpoutConfiguration.createKafkaSpoutConfigBuilder(mockFilter, mock(ManualPartitioner.class), -1).build();\r\n    KafkaTridentSpoutCoordinator<String, String> coordinator = new KafkaTridentSpoutCoordinator<>(spoutConfig, new ClientFactory<String, String>() {\r\n\r\n        @Override\r\n        public Consumer<String, String> createConsumer(Map<String, Object> consumerProps) {\r\n            return mockConsumer;\r\n        }\r\n\r\n        @Override\r\n        public Admin createAdmin(Map<String, Object> adminProps) {\r\n            return mockAdmin;\r\n        }\r\n    });\r\n    List<Map<String, Object>> partitionsForBatch = coordinator.getPartitionsForBatch();\r\n    List<TopicPartition> tps = deserializePartitions(partitionsForBatch);\r\n    verify(mockFilter).getAllSubscribedPartitions(mockConsumer);\r\n    assertThat(tps, contains(expectedPartition));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-kafka-client\\src\\test\\java\\org\\apache\\storm\\kafka\\spout\\trident\\KafkaTridentSpoutOpaqueCoordinatorTest.java",
  "methodName" : "testCanUpdatePartitions",
  "sourceCode" : "@Test\r\npublic void testCanUpdatePartitions() {\r\n    try (SimulatedTime ignored1 = new SimulatedTime()) {\r\n        KafkaConsumer<String, String> mockConsumer = mock(KafkaConsumer.class);\r\n        Admin mockAdmin = mock(Admin.class);\r\n        TopicPartition expectedPartition = new TopicPartition(\"test\", 0);\r\n        TopicPartition addedLaterPartition = new TopicPartition(\"test-2\", 0);\r\n        HashSet<TopicPartition> allPartitions = new HashSet<>();\r\n        allPartitions.add(expectedPartition);\r\n        allPartitions.add(addedLaterPartition);\r\n        TopicFilter mockFilter = mock(TopicFilter.class);\r\n        when(mockFilter.getAllSubscribedPartitions(any())).thenReturn(Collections.singleton(expectedPartition)).thenReturn(allPartitions);\r\n        KafkaTridentSpoutConfig<String, String> spoutConfig = SingleTopicKafkaTridentSpoutConfiguration.createKafkaSpoutConfigBuilder(mockFilter, mock(ManualPartitioner.class), -1).build();\r\n        KafkaTridentSpoutCoordinator<String, String> coordinator = new KafkaTridentSpoutCoordinator<>(spoutConfig, new ClientFactory<String, String>() {\r\n\r\n            @Override\r\n            public Consumer<String, String> createConsumer(Map<String, Object> consumerProps) {\r\n                return mockConsumer;\r\n            }\r\n\r\n            @Override\r\n            public Admin createAdmin(Map<String, Object> adminProps) {\r\n                return mockAdmin;\r\n            }\r\n        });\r\n        List<Map<String, Object>> partitionsForBatch = coordinator.getPartitionsForBatch();\r\n        List<TopicPartition> firstBatchTps = deserializePartitions(partitionsForBatch);\r\n        verify(mockFilter).getAllSubscribedPartitions(mockConsumer);\r\n        assertThat(firstBatchTps, contains(expectedPartition));\r\n        Time.advanceTime(KafkaTridentSpoutCoordinator.TIMER_DELAY_MS + spoutConfig.getPartitionRefreshPeriodMs());\r\n        List<Map<String, Object>> partitionsForSecondBatch = coordinator.getPartitionsForBatch();\r\n        List<TopicPartition> secondBatchTps = deserializePartitions(partitionsForSecondBatch);\r\n        verify(mockFilter, times(2)).getAllSubscribedPartitions(mockConsumer);\r\n        assertThat(new HashSet<>(secondBatchTps), is(allPartitions));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-metrics-prometheus\\src\\test\\java\\org\\apache\\storm\\metrics\\prometheus\\PrometheusPreparableReporterTest.java",
  "methodName" : "testSimple",
  "sourceCode" : "@Test\r\npublic void testSimple() throws IOException {\r\n    pushGatewayContainer.start();\r\n    final PrometheusPreparableReporter sut = new PrometheusPreparableReporter();\r\n    final Map<String, Object> daemonConf = Map.of(\"storm.daemon.metrics.reporter.plugin.prometheus.job\", \"test_simple\", \"storm.daemon.metrics.reporter.plugin.prometheus.endpoint\", \"localhost:\" + pushGatewayContainer.getMappedPort(9091), \"storm.daemon.metrics.reporter.plugin.prometheus.scheme\", \"http\");\r\n    runTest(sut, daemonConf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-metrics-prometheus\\src\\test\\java\\org\\apache\\storm\\metrics\\prometheus\\PrometheusPreparableReporterTest.java",
  "methodName" : "testBasicAuth",
  "sourceCode" : "@Test\r\npublic void testBasicAuth() throws IOException {\r\n    pushGatewayContainer.withCopyFileToContainer(MountableFile.forClasspathResource(\"/pushgateway-basicauth.yaml\"), \"/pushgateway/pushgateway-basicauth.yaml\").withCommand(\"--web.config.file\", \"pushgateway-basicauth.yaml\").start();\r\n    final PrometheusPreparableReporter sut = new PrometheusPreparableReporter();\r\n    final Map<String, Object> daemonConf = Map.of(\"storm.daemon.metrics.reporter.plugin.prometheus.job\", \"test_simple\", \"storm.daemon.metrics.reporter.plugin.prometheus.endpoint\", \"localhost:\" + pushGatewayContainer.getMappedPort(9091), \"storm.daemon.metrics.reporter.plugin.prometheus.scheme\", \"http\", \"storm.daemon.metrics.reporter.plugin.prometheus.basic_auth_user\", \"my_user\", \"storm.daemon.metrics.reporter.plugin.prometheus.basic_auth_password\", \"secret_password\");\r\n    runTest(sut, daemonConf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-metrics-prometheus\\src\\test\\java\\org\\apache\\storm\\metrics\\prometheus\\PrometheusPreparableReporterTest.java",
  "methodName" : "testTls",
  "sourceCode" : "@Test\r\npublic void testTls() throws IOException {\r\n    pushGatewayContainer.withCopyFileToContainer(MountableFile.forClasspathResource(\"/pushgateway-ssl.yaml\"), \"/pushgateway/pushgateway-ssl.yaml\").withCommand(\"--web.config.file\", \"pushgateway-ssl.yaml\").start();\r\n    final PrometheusPreparableReporter sut = new PrometheusPreparableReporter();\r\n    final Map<String, Object> daemonConf = Map.of(\"storm.daemon.metrics.reporter.plugin.prometheus.job\", \"test_simple\", \"storm.daemon.metrics.reporter.plugin.prometheus.endpoint\", \"localhost:\" + pushGatewayContainer.getMappedPort(9091), \"storm.daemon.metrics.reporter.plugin.prometheus.scheme\", \"https\", \"storm.daemon.metrics.reporter.plugin.prometheus.skip_tls_validation\", true);\r\n    runTest(sut, daemonConf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_exists_keyNotFound",
  "sourceCode" : "/**\r\n * Smoke test the exists check when the key is NOT found.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_exists_keyNotFound() {\r\n    // Define input key\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does not exist in redis\r\n    jedisHelper.delete(inputKey);\r\n    assertFalse(jedisHelper.exists(inputKey), \"Sanity check key should not exist\");\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(STRING);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_exists_keyFound",
  "sourceCode" : "/**\r\n * Smoke test the exists check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_exists_keyFound() {\r\n    // Define input key\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.set(inputKey, \"some-value\");\r\n    assertTrue(jedisHelper.exists(inputKey), \"Sanity check key exists.\");\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(STRING);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_sismember_notMember",
  "sourceCode" : "/**\r\n * Smoke test the sismember check when the key is NOT found in the set.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_sismember_notMember() {\r\n    // Define input key\r\n    final String setKey = \"ThisIsMySet\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(SET, setKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_sismember_isMember",
  "sourceCode" : "/**\r\n * Smoke test the exists check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_sismember_isMember() {\r\n    // Define input key\r\n    final String setKey = \"ThisIsMySet\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.smember(setKey, inputKey);\r\n    assertTrue(jedisHelper.sismember(setKey, inputKey), \"Sanity check, should be a member\");\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(SET, setKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_hexists_notMember",
  "sourceCode" : "/**\r\n * Smoke test the hexists check when the key is NOT found in the set.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_hexists_notMember() {\r\n    // Define input key\r\n    final String hashKey = \"ThisIsMyHash\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(HASH, hashKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_hexists_isMember",
  "sourceCode" : "/**\r\n * Smoke test the hexists check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_hexists_isMember() {\r\n    // Define input key\r\n    final String hashKey = \"ThisIsMyHash\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.hset(hashKey, inputKey, \"value\");\r\n    assertTrue(jedisHelper.hexists(hashKey, inputKey), \"Sanity check, should be a member\");\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(HASH, hashKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_zrank_notMember",
  "sourceCode" : "/**\r\n * Smoke test the zrank check when the key is NOT found in the set.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_zrank_notMember() {\r\n    // Define input key\r\n    final String setKey = \"ThisIsMySetKey\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(SORTED_SET, setKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_zrank_isMember",
  "sourceCode" : "/**\r\n * Smoke test the zrank check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_zrank_isMember() {\r\n    // Define input key\r\n    final String setKey = \"ThisIsMySetKey\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.zrank(setKey, 2, inputKey);\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(SORTED_SET, setKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_pfcount_notMember",
  "sourceCode" : "/**\r\n * Smoke test the pfcount check when the key is NOT found in the set.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_pfcount_notMember() {\r\n    // Define input key\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(HYPER_LOG_LOG);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_pfcount_isMember",
  "sourceCode" : "/**\r\n * Smoke test the pfcount check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_pfcount_isMember() {\r\n    // Define input key\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.pfadd(inputKey, \"my value\");\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(HYPER_LOG_LOG);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_geopos_notMember",
  "sourceCode" : "/**\r\n * Smoke test the geopos check when the key is NOT found in the set.\r\n * Expectation is tuple is acked, and nothing is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_geopos_notMember() {\r\n    // Define input key\r\n    final String geoKey = \"ThisIsMyGeoKey\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(GEO, geoKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify the bolt filtered the input tuple.\r\n    verifyTupleFiltered();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\bolt\\RedisFilterBoltTest.java",
  "methodName" : "smokeTest_geopos_isMember",
  "sourceCode" : "/**\r\n * Smoke test the geopos check when the key IS found.\r\n * Expectation is tuple is acked, and tuple is emitted.\r\n */\r\n@Test\r\nvoid smokeTest_geopos_isMember() {\r\n    // Define input key\r\n    final String geoKey = \"ThisIsMyGeoKey\";\r\n    final String inputKey = \"ThisIsMyKey\";\r\n    // Ensure key does exist in redis\r\n    jedisHelper.geoadd(geoKey, 139.731992, 35.709026, inputKey);\r\n    // Create an input tuple\r\n    final Map<String, Object> values = new HashMap<>();\r\n    values.put(\"key\", inputKey);\r\n    values.put(\"value\", \"ThisIsMyValue\");\r\n    final Tuple tuple = new StubTuple(values);\r\n    final JedisPoolConfig config = configBuilder.build();\r\n    final TestMapper mapper = new TestMapper(GEO, geoKey);\r\n    final RedisFilterBolt bolt = new RedisFilterBolt(config, mapper);\r\n    bolt.prepare(new HashMap<>(), topologyContext, new OutputCollector(outputCollector));\r\n    bolt.process(tuple);\r\n    // Verify Tuple passed through the bolt\r\n    verifyTuplePassed(tuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateIteratorTest.java",
  "methodName" : "testGetEntriesFromFirstPartOfChunkInRedis",
  "sourceCode" : "@Test\r\npublic void testGetEntriesFromFirstPartOfChunkInRedis() {\r\n    // pendingPrepare has no entries\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    // pendingCommit has no entries\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    // Redis has a chunk but no more\r\n    NavigableMap<byte[], byte[]> chunkMap = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(chunkMap, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putEncodedKeyValueToMap(chunkMap, \"key2\".getBytes(), \"value2\".getBytes());\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultFirst = new ScanResult<>(\"12345\".getBytes(), new ArrayList<>(chunkMap.entrySet()));\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultSecond = new ScanResult<>(ScanParams.SCAN_POINTER_START_BINARY, new ArrayList<>());\r\n    when(mockJedis.hscan(eq(namespace), any(byte[].class), any(ScanParams.class))).thenReturn(scanResultFirst, scanResultSecond);\r\n    RedisKeyValueStateIterator<byte[], byte[]> kvIterator = new RedisKeyValueStateIterator<>(namespace, mockContainer, pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator(), chunkSize, keySerializer, valueSerializer);\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't in iterator\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateIteratorTest.java",
  "methodName" : "testGetEntriesFromThirdPartOfChunkInRedis",
  "sourceCode" : "@Test\r\npublic void testGetEntriesFromThirdPartOfChunkInRedis() {\r\n    // pendingPrepare has no entries\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    // pendingCommit has no entries\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    // Redis has three chunks which last chunk only has entries\r\n    NavigableMap<byte[], byte[]> chunkMap = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(chunkMap, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putEncodedKeyValueToMap(chunkMap, \"key2\".getBytes(), \"value2\".getBytes());\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultFirst = new ScanResult<>(\"12345\".getBytes(), new ArrayList<>());\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultSecond = new ScanResult<>(\"23456\".getBytes(), new ArrayList<>());\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultThird = new ScanResult<>(ScanParams.SCAN_POINTER_START_BINARY, new ArrayList<>(chunkMap.entrySet()));\r\n    when(mockJedis.hscan(eq(namespace), any(byte[].class), any(ScanParams.class))).thenReturn(scanResultFirst, scanResultSecond, scanResultThird);\r\n    RedisKeyValueStateIterator<byte[], byte[]> kvIterator = new RedisKeyValueStateIterator<>(namespace, mockContainer, pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator(), chunkSize, keySerializer, valueSerializer);\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't in iterator\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateIteratorTest.java",
  "methodName" : "testGetEntriesRemovingDuplicationKeys",
  "sourceCode" : "@Test\r\npublic void testGetEntriesRemovingDuplicationKeys() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingPrepare, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putTombstoneToMap(pendingPrepare, \"key1\".getBytes());\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingCommit, \"key1\".getBytes(), \"value1\".getBytes());\r\n    putEncodedKeyValueToMap(pendingCommit, \"key2\".getBytes(), \"value2\".getBytes());\r\n    NavigableMap<byte[], byte[]> chunkMap = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(chunkMap, \"key2\".getBytes(), \"value2\".getBytes());\r\n    putEncodedKeyValueToMap(chunkMap, \"key3\".getBytes(), \"value3\".getBytes());\r\n    NavigableMap<byte[], byte[]> chunkMap2 = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(chunkMap2, \"key3\".getBytes(), \"value3\".getBytes());\r\n    putEncodedKeyValueToMap(chunkMap2, \"key4\".getBytes(), \"value4\".getBytes());\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultFirst = new ScanResult<>(\"12345\".getBytes(), new ArrayList<>(chunkMap.entrySet()));\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultSecond = new ScanResult<>(\"23456\".getBytes(), new ArrayList<>(chunkMap2.entrySet()));\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResultThird = new ScanResult<>(ScanParams.SCAN_POINTER_START_BINARY, new ArrayList<>());\r\n    when(mockJedis.hscan(eq(namespace), any(byte[].class), any(ScanParams.class))).thenReturn(scanResultFirst, scanResultSecond, scanResultThird);\r\n    RedisKeyValueStateIterator<byte[], byte[]> kvIterator = new RedisKeyValueStateIterator<>(namespace, mockContainer, pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator(), chunkSize, keySerializer, valueSerializer);\r\n    // keys shouldn't appear twice\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't be in iterator since it's marked as deleted\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertNextEntry(kvIterator, \"key3\".getBytes(), \"value3\".getBytes());\r\n    assertNextEntry(kvIterator, \"key4\".getBytes(), \"value4\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateIteratorTest.java",
  "methodName" : "testGetEntryNotAvailable",
  "sourceCode" : "@Test\r\npublic void testGetEntryNotAvailable() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    ScanResult<Map.Entry<byte[], byte[]>> scanResult = new ScanResult<>(ScanParams.SCAN_POINTER_START_BINARY, new ArrayList<>());\r\n    when(mockJedis.hscan(eq(namespace), any(byte[].class), any(ScanParams.class))).thenReturn(scanResult);\r\n    RedisKeyValueStateIterator<byte[], byte[]> kvIterator = new RedisKeyValueStateIterator<>(namespace, mockContainer, pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator(), chunkSize, keySerializer, valueSerializer);\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateProviderTest.java",
  "methodName" : "testgetDefaultConfig",
  "sourceCode" : "@Test\r\npublic void testgetDefaultConfig() throws Exception {\r\n    RedisKeyValueStateProvider provider = new RedisKeyValueStateProvider();\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    //topoConf.put(Config.TOPOLOGY_STATE_PROVIDER_CONFIG, \"{\\\"keyClass\\\":\\\"String\\\"}\");\r\n    RedisKeyValueStateProvider.StateConfig config = provider.getStateConfig(topoConf);\r\n    assertNotNull(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateProviderTest.java",
  "methodName" : "testgetConfigWithProviderConfig",
  "sourceCode" : "@Test\r\npublic void testgetConfigWithProviderConfig() throws Exception {\r\n    RedisKeyValueStateProvider provider = new RedisKeyValueStateProvider();\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    topoConf.put(Config.TOPOLOGY_STATE_PROVIDER_CONFIG, \"{\\\"keyClass\\\":\\\"String\\\", \\\"valueClass\\\":\\\"String\\\",\" + \" \\\"jedisPoolConfig\\\":\" + \"{\\\"host\\\":\\\"localhost\\\", \\\"port\\\":1000}}\");\r\n    RedisKeyValueStateProvider.StateConfig config = provider.getStateConfig(topoConf);\r\n    //System.out.println(config);\r\n    assertEquals(\"String\", config.keyClass);\r\n    assertEquals(\"String\", config.valueClass);\r\n    assertEquals(\"localhost\", config.jedisPoolConfig.getHost());\r\n    assertEquals(1000, config.jedisPoolConfig.getPort());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateTest.java",
  "methodName" : "testPutAndGet",
  "sourceCode" : "@Test\r\npublic void testPutAndGet() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    assertEquals(\"1\", keyValueState.get(\"a\"));\r\n    assertEquals(\"2\", keyValueState.get(\"b\"));\r\n    assertNull(keyValueState.get(\"c\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateTest.java",
  "methodName" : "testPutAndDelete",
  "sourceCode" : "@Test\r\npublic void testPutAndDelete() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    assertEquals(\"1\", keyValueState.get(\"a\"));\r\n    assertEquals(\"2\", keyValueState.get(\"b\"));\r\n    assertNull(keyValueState.get(\"c\"));\r\n    assertEquals(\"1\", keyValueState.delete(\"a\"));\r\n    assertNull(keyValueState.get(\"a\"));\r\n    assertEquals(\"2\", keyValueState.get(\"b\"));\r\n    assertNull(keyValueState.get(\"c\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\external\\storm-redis\\src\\test\\java\\org\\apache\\storm\\redis\\state\\RedisKeyValueStateTest.java",
  "methodName" : "testPrepareCommitRollback",
  "sourceCode" : "@Test\r\npublic void testPrepareCommitRollback() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(1);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { null, null, null }, getValues());\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(1);\r\n    keyValueState.commit(1);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { \"1\", \"2\", null }, getValues());\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertEquals(\"2\", keyValueState.delete(\"b\"));\r\n    assertEquals(\"3\", keyValueState.delete(\"c\"));\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.prepareCommit(2);\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.commit(2);\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(3);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\FluxBuilderTest.java",
  "methodName" : "testIsPrimitiveNumber",
  "sourceCode" : "@Test\r\npublic void testIsPrimitiveNumber() {\r\n    assertTrue(FluxBuilder.isPrimitiveNumber(int.class));\r\n    assertFalse(FluxBuilder.isPrimitiveNumber(boolean.class));\r\n    assertFalse(FluxBuilder.isPrimitiveNumber(String.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\IntegrationTest.java",
  "methodName" : "testRunTopologySource",
  "sourceCode" : "@Test\r\npublic void testRunTopologySource() throws Exception {\r\n    if (!skipTest) {\r\n        Flux.main(new String[] { \"-s\", \"30000\", \"src/test/resources/configs/existing-topology.yaml\" });\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\multilang\\MultilangEnvironmentTest.java",
  "methodName" : "testInvokePython",
  "sourceCode" : "@Test\r\npublic void testInvokePython() throws Exception {\r\n    String[] command = new String[] { \"python3\", \"--version\" };\r\n    int exitVal = invokeCommand(command);\r\n    assertEquals(0, exitVal, \"Exit value for python3 is 0.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\multilang\\MultilangEnvironmentTest.java",
  "methodName" : "testInvokeNode",
  "sourceCode" : "@Test\r\npublic void testInvokeNode() throws Exception {\r\n    String[] command = new String[] { \"node\", \"--version\" };\r\n    int exitVal = invokeCommand(command);\r\n    assertEquals(0, exitVal, \"Exit value for node should be 0.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTCK",
  "sourceCode" : "@Test\r\npublic void testTCK() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/tck.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testShellComponents",
  "sourceCode" : "@Test\r\npublic void testShellComponents() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/shell_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testBadShellComponents",
  "sourceCode" : "@Test\r\npublic void testBadShellComponents() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/bad_shell_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    IllegalArgumentException expectedException = assertThrows(IllegalArgumentException.class, () -> FluxBuilder.buildTopology(context));\r\n    assertTrue(expectedException.getMessage().contains(\"Unable to find configuration method\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testKafkaSpoutConfig",
  "sourceCode" : "@Test\r\npublic void testKafkaSpoutConfig() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/kafka_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testLoadFromResource",
  "sourceCode" : "@Test\r\npublic void testLoadFromResource() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/kafka_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testHdfs",
  "sourceCode" : "@Test\r\npublic void testHdfs() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/hdfs_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testDiamondTopology",
  "sourceCode" : "@Test\r\npublic void testDiamondTopology() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/diamond-topology.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testBadHbase",
  "sourceCode" : "@Test\r\npublic void testBadHbase() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/bad_hbase.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    IllegalArgumentException expectedException = assertThrows(IllegalArgumentException.class, () -> FluxBuilder.buildTopology(context));\r\n    assertTrue(expectedException.getMessage().contains(\"Couldn't find a suitable constructor\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testIncludes",
  "sourceCode" : "@Test\r\npublic void testIncludes() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/include_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    assertTrue(topologyDef.getName().equals(\"include-topology\"));\r\n    assertTrue(topologyDef.getBolts().size() > 0);\r\n    assertTrue(topologyDef.getSpouts().size() > 0);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySource",
  "sourceCode" : "@Test\r\npublic void testTopologySource() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySourceWithReflection",
  "sourceCode" : "@Test\r\npublic void testTopologySourceWithReflection() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology-reflection.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySourceWithConfigParam",
  "sourceCode" : "@Test\r\npublic void testTopologySourceWithConfigParam() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology-reflection-config.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySourceWithMethodName",
  "sourceCode" : "@Test\r\npublic void testTopologySourceWithMethodName() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology-method-override.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTridentTopologySource",
  "sourceCode" : "@Test\r\npublic void testTridentTopologySource() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology-trident.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySourceWithGetMethodName",
  "sourceCode" : "@Test\r\npublic void testTopologySourceWithGetMethodName() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/existing-topology-reflection.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologySourceWithConfigMethods",
  "sourceCode" : "@Test\r\npublic void testTopologySourceWithConfigMethods() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/config-methods-test.yaml\", false, true, null, false);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n    // make sure the property was actually set\r\n    TestBolt bolt = (TestBolt) context.getBolt(\"bolt-1\");\r\n    assertTrue(bolt.getFoo().equals(\"foo\"));\r\n    assertTrue(bolt.getBar().equals(\"bar\"));\r\n    assertTrue(bolt.getFooBar().equals(\"foobar\"));\r\n    assertNotNull(context.getBolt(\"bolt-2\"));\r\n    assertNotNull(context.getBolt(\"bolt-3\"));\r\n    assertNotNull(context.getBolt(\"bolt-4\"));\r\n    assertArrayEquals(new TestBolt.TestClass[] { new TestBolt.TestClass(\"foo\"), new TestBolt.TestClass(\"bar\"), new TestBolt.TestClass(\"baz\") }, bolt.getClasses());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testVariableSubstitution",
  "sourceCode" : "@Test\r\npublic void testVariableSubstitution() throws Exception {\r\n    Properties properties = FluxParser.parseProperties(\"/configs/test.properties\", true);\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/substitution-test.yaml\", false, true, properties, true);\r\n    assertTrue(topologyDef.validate());\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    topology.validate();\r\n    // test basic substitution\r\n    assertEquals(\"substitution-topology\", context.getTopologyDef().getName(), \"Property not replaced.\");\r\n    // test environment variable substitution\r\n    // $PATH should be defined on most systems\r\n    String envPath = System.getenv().get(\"PATH\");\r\n    assertEquals(envPath, context.getTopologyDef().getConfig().get(\"test.env.value\"), \"ENV variable not replaced.\");\r\n    //Test substitution where the target type is List\r\n    assertThat(\"List property is not replaced by the expected value\", Collections.singletonList(\"A string list\"), is(context.getTopologyDef().getConfig().get(\"list.property.target\")));\r\n    //Test substitution where the target type is a List element\r\n    assertThat(\"List element property is not replaced by the expected value\", \"A string list\", is(context.getTopologyDef().getConfig().get(\"list.element.property.target\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologyWithInvalidStaticFactoryArgument",
  "sourceCode" : "@Test\r\npublic void testTopologyWithInvalidStaticFactoryArgument() throws Exception {\r\n    //STORM-3087.\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/bad_static_factory_test.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    IllegalArgumentException expectedException = assertThrows(IllegalArgumentException.class, () -> FluxBuilder.buildTopology(context));\r\n    assertTrue(expectedException.getMessage().contains(\"Couldn't find a suitable static method\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\flux\\flux-core\\src\\test\\java\\org\\apache\\storm\\flux\\TCKTest.java",
  "methodName" : "testTopologyWithWorkerHook",
  "sourceCode" : "@Test\r\npublic void testTopologyWithWorkerHook() throws Exception {\r\n    TopologyDef topologyDef = FluxParser.parseResource(\"/configs/worker_hook.yaml\", false, true, null, false);\r\n    Config conf = FluxBuilder.buildConfig(topologyDef);\r\n    ExecutionContext context = new ExecutionContext(topologyDef, conf);\r\n    StormTopology topology = FluxBuilder.buildTopology(context);\r\n    assertNotNull(topology);\r\n    assertTrue(topologyDef.getName().equals(\"worker-hook-topology\"));\r\n    assertTrue(topologyDef.getWorkerHooks().size() > 0);\r\n    assertTrue(topology.get_worker_hooks_size() > 0);\r\n    topology.validate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\integration-test\\src\\test\\java\\org\\apache\\storm\\st\\DemoTest.java",
  "methodName" : "testExclamationTopology",
  "sourceCode" : "@Test\r\npublic void testExclamationTopology() throws Exception {\r\n    topo = new TopoWrap(cluster, topologyName, ExclamationTopology.getStormTopology());\r\n    topo.submitSuccessfully();\r\n    final int minExclaim2Emits = 500;\r\n    final int minSpoutEmits = 10000;\r\n    topo.assertProgress(minSpoutEmits, ExclamationTopology.SPOUT_EXECUTORS, ExclamationTopology.WORD, 180);\r\n    topo.assertProgress(minExclaim2Emits, ExclamationTopology.EXCLAIM_2_EXECUTORS, ExclamationTopology.EXCLAIM_2, 180);\r\n    Set<TopoWrap.ExecutorURL> boltUrls = topo.getLogUrls(ExclamationTopology.WORD);\r\n    log.info(boltUrls.toString());\r\n    final String actualOutput = topo.getLogs(ExclamationTopology.EXCLAIM_2);\r\n    for (String oneExpectedOutput : exclaim2Output) {\r\n        assertTrue(actualOutput.contains(oneExpectedOutput), \"Couldn't find \" + oneExpectedOutput + \" in urls\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\integration-test\\src\\test\\java\\org\\apache\\storm\\st\\tests\\window\\SlidingWindowTest.java",
  "methodName" : "testWindowCount",
  "sourceCode" : "@Test(dataProvider = \"generateCountWindows\")\r\npublic void testWindowCount(int windowSize, int slideSize) throws Exception {\r\n    final SlidingWindowCorrectness testable = new SlidingWindowCorrectness(windowSize, slideSize);\r\n    final String topologyName = this.getClass().getSimpleName() + \"-size-window\" + windowSize + \"-slide\" + slideSize;\r\n    if (windowSize <= 0 || slideSize <= 0) {\r\n        assertThrows(IllegalArgumentException.class, () -> testable.newTopology());\r\n    }\r\n    topo = new TopoWrap(cluster, topologyName, testable.newTopology());\r\n    windowVerifier.runAndVerifyCount(windowSize, slideSize, testable, topo);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\integration-test\\src\\test\\java\\org\\apache\\storm\\st\\tests\\window\\SlidingWindowTest.java",
  "methodName" : "testTimeWindow",
  "sourceCode" : "@Test(dataProvider = \"generateTimeWindows\")\r\npublic void testTimeWindow(int windowSec, int slideSec) throws Exception {\r\n    final SlidingTimeCorrectness testable = new SlidingTimeCorrectness(windowSec, slideSec);\r\n    final String topologyName = this.getClass().getSimpleName() + \"-sec-window\" + windowSec + \"-slide\" + slideSec;\r\n    if (windowSec <= 0 || slideSec <= 0) {\r\n        assertThrows(IllegalArgumentException.class, () -> testable.newTopology());\r\n    }\r\n    topo = new TopoWrap(cluster, topologyName, testable.newTopology());\r\n    windowVerifier.runAndVerifyTime(windowSec, slideSec, testable, topo);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\integration-test\\src\\test\\java\\org\\apache\\storm\\st\\tests\\window\\TumblingWindowTest.java",
  "methodName" : "testTumbleCount",
  "sourceCode" : "@Test(dataProvider = \"generateWindows\")\r\npublic void testTumbleCount(int tumbleSize) throws Exception {\r\n    final TumblingWindowCorrectness testable = new TumblingWindowCorrectness(tumbleSize);\r\n    final String topologyName = this.getClass().getSimpleName() + \"-size\" + tumbleSize;\r\n    if (tumbleSize <= 0) {\r\n        try {\r\n            testable.newTopology();\r\n            fail(\"Expected IllegalArgumentException was not thrown.\");\r\n        } catch (IllegalArgumentException ignore) {\r\n            return;\r\n        }\r\n    }\r\n    topo = new TopoWrap(cluster, topologyName, testable.newTopology());\r\n    windowVerifier.runAndVerifyCount(tumbleSize, tumbleSize, testable, topo);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\integration-test\\src\\test\\java\\org\\apache\\storm\\st\\tests\\window\\TumblingWindowTest.java",
  "methodName" : "testTumbleTime",
  "sourceCode" : "@Test(dataProvider = \"generateTumbleTimes\")\r\npublic void testTumbleTime(int tumbleSec) throws Exception {\r\n    final TumblingTimeCorrectness testable = new TumblingTimeCorrectness(tumbleSec);\r\n    final String topologyName = this.getClass().getSimpleName() + \"-sec\" + tumbleSec;\r\n    if (tumbleSec <= 0) {\r\n        assertThrows(IllegalArgumentException.class, () -> testable.newTopology());\r\n    }\r\n    topo = new TopoWrap(cluster, topologyName, testable.newTopology());\r\n    windowVerifier.runAndVerifyTime(tumbleSec, tumbleSec, testable, topo);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\dependency\\DependencyUploader.java",
  "methodName" : "setBlobStore",
  "sourceCode" : "@VisibleForTesting\r\nvoid setBlobStore(ClientBlobStore blobStore) {\r\n    this.blobStore = blobStore;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\executor\\Executor.java",
  "methodName" : "setLocalExecutorTransfer",
  "sourceCode" : "@VisibleForTesting\r\npublic void setLocalExecutorTransfer(ExecutorTransfer executorTransfer) {\r\n    this.executorTransfer = executorTransfer;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGrouping.java",
  "methodName" : "getCurrentScope",
  "sourceCode" : "@VisibleForTesting\r\npublic LocalityScope getCurrentScope() {\r\n    return currentScope;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\nimbus\\ILeaderElector.java",
  "methodName" : "awaitLeadership",
  "sourceCode" : "/**\r\n * Wait for the caller to gain leadership. This should only be used in single-Nimbus clusters, and is only useful to allow testing\r\n * code to wait for a LocalCluster's Nimbus to gain leadership before trying to submit topologies.\r\n *\r\n * @return true is leadership was acquired, false otherwise\r\n */\r\n@VisibleForTesting\r\nboolean awaitLeadership(long timeout, TimeUnit timeUnit) throws InterruptedException;",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\security\\auth\\AutoSSL.java",
  "methodName" : "getSSLWriteDirFromConf",
  "sourceCode" : "@VisibleForTesting\r\n@SuppressWarnings(\"checkstyle:AbbreviationAsWordInName\")\r\nprotected String getSSLWriteDirFromConf(Map<String, Object> conf) {\r\n    return \"./\";\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\security\\auth\\AutoSSL.java",
  "methodName" : "getSSLFilesFromConf",
  "sourceCode" : "@VisibleForTesting\r\n@SuppressWarnings(\"checkstyle:AbbreviationAsWordInName\")\r\nCollection<String> getSSLFilesFromConf(Map<String, Object> conf) {\r\n    Object sslConf = conf.get(SSL_FILES_CONF);\r\n    if (sslConf == null) {\r\n        LOG.info(\"No ssl files requested, if you want to use SSL please set {} to the list of files\", SSL_FILES_CONF);\r\n        return null;\r\n    }\r\n    Collection<String> sslFiles = null;\r\n    if (sslConf instanceof Collection) {\r\n        sslFiles = (Collection<String>) sslConf;\r\n    } else if (sslConf instanceof String) {\r\n        sslFiles = Arrays.asList(((String) sslConf).split(\",\"));\r\n    } else {\r\n        throw new RuntimeException(SSL_FILES_CONF + \" is not set to something that I know how to use \" + sslConf);\r\n    }\r\n    return sslFiles;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\security\\auth\\workertoken\\WorkerTokenAuthorizer.java",
  "methodName" : "getSignedPasswordFor",
  "sourceCode" : "@VisibleForTesting\r\nbyte[] getSignedPasswordFor(byte[] user, WorkerTokenInfo deser) {\r\n    assert keyCache != null;\r\n    if (deser.is_set_expirationTimeMillis() && deser.get_expirationTimeMillis() <= Time.currentTimeMillis()) {\r\n        throw new IllegalArgumentException(\"Token is not valid, token has expired.\");\r\n    }\r\n    PrivateWorkerKey key;\r\n    try {\r\n        key = keyCache.getUnchecked(deser);\r\n    } catch (CacheLoader.InvalidCacheLoadException e) {\r\n        //This happens when the key is not found, the cache loader returns a null and this exception is thrown.\r\n        // because the cache cannot store a null.\r\n        throw new IllegalArgumentException(\"Token is not valid, private key not found.\", e);\r\n    }\r\n    if (key.is_set_expirationTimeMillis() && key.get_expirationTimeMillis() <= Time.currentTimeMillis()) {\r\n        throw new IllegalArgumentException(\"Token is not valid, key has expired.\");\r\n    }\r\n    return WorkerTokenSigner.createPassword(user, new SecretKeySpec(key.get_key(), WorkerTokenSigner.DEFAULT_HMAC_ALGORITHM));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\utils\\Utils.java",
  "methodName" : "setClassLoaderForJavaDeSerialize",
  "sourceCode" : "@VisibleForTesting\r\npublic static void setClassLoaderForJavaDeSerialize(ClassLoader cl) {\r\n    Utils.cl = cl;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\utils\\Utils.java",
  "methodName" : "resetClassLoaderForJavaDeSerialize",
  "sourceCode" : "@VisibleForTesting\r\npublic static void resetClassLoaderForJavaDeSerialize() {\r\n    Utils.cl = ClassLoader.getSystemClassLoader();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\utils\\Utils.java",
  "methodName" : "isValidConf",
  "sourceCode" : "@VisibleForTesting\r\nstatic boolean isValidConf(Map<String, Object> orig, Map<String, Object> deser) {\r\n    MapDifference<String, Object> diff = Maps.difference(orig, deser);\r\n    if (diff.areEqual()) {\r\n        return true;\r\n    }\r\n    for (Map.Entry<String, Object> entryOnLeft : diff.entriesOnlyOnLeft().entrySet()) {\r\n        LOG.warn(\"Config property ({}) is found in original config, but missing from the \" + \"serialized-deserialized config. This is due to an internal error in \" + \"serialization. Name: {} - Value: {}\", entryOnLeft.getKey(), entryOnLeft.getKey(), entryOnLeft.getValue());\r\n    }\r\n    for (Map.Entry<String, Object> entryOnRight : diff.entriesOnlyOnRight().entrySet()) {\r\n        LOG.warn(\"Config property ({}) is not found in original config, but present in \" + \"serialized-deserialized config. This is due to an internal error in \" + \"serialization. Name: {} - Value: {}\", entryOnRight.getKey(), entryOnRight.getKey(), entryOnRight.getValue());\r\n    }\r\n    for (Map.Entry<String, MapDifference.ValueDifference<Object>> entryDiffers : diff.entriesDiffering().entrySet()) {\r\n        Object leftValue = entryDiffers.getValue().leftValue();\r\n        Object rightValue = entryDiffers.getValue().rightValue();\r\n        LOG.warn(\"Config value differs after json serialization. Name: {} - Original Value: {} - DeSer. Value: {}\", entryDiffers.getKey(), leftValue, rightValue);\r\n    }\r\n    return false;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\src\\jvm\\org\\apache\\storm\\utils\\Utils.java",
  "methodName" : "findComponentCycles",
  "sourceCode" : "/**\r\n * Find and return components cycles in the topology graph when starting from spout.\r\n * Return a list of cycles. Each cycle may consist of one or more components.\r\n * Components that cannot be reached from any of the spouts are ignored.\r\n *\r\n * @return a List of cycles. Each cycle has a list of component names.\r\n */\r\n@VisibleForTesting\r\npublic static List<List<String>> findComponentCycles(StormTopology topology, String topoId) {\r\n    List<List<String>> ret = new ArrayList<>();\r\n    Map<String, Set<String>> edgesOut = getStormTopologyForwardGraph(topology);\r\n    Set<String> allComponentIds = new HashSet<>();\r\n    edgesOut.forEach((k, v) -> {\r\n        allComponentIds.add(k);\r\n        allComponentIds.addAll(v);\r\n    });\r\n    if (topology.get_spouts_size() == 0) {\r\n        LOG.error(\"Topology {} does not contain any spouts, cannot traverse graph to determine cycles\", topoId);\r\n        return ret;\r\n    }\r\n    Set<String> unreachable = new HashSet<>(edgesOut.keySet());\r\n    topology.get_spouts().forEach((spoutId, spout) -> {\r\n        Stack<String> dfsStack = new Stack<>();\r\n        dfsStack.push(spoutId);\r\n        Set<String> seen = new HashSet<>();\r\n        seen.add(spoutId);\r\n        findComponentCyclesRecursion(dfsStack, edgesOut, seen, ret);\r\n        unreachable.removeAll(seen);\r\n    });\r\n    // warning about unreachable components\r\n    if (!unreachable.isEmpty()) {\r\n        LOG.warn(\"Topology {} contains unreachable components \\\"{}\\\"\", topoId, String.join(\",\", unreachable));\r\n    }\r\n    return ret;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\assignments\\LocalAssignmentsBackendTest.java",
  "methodName" : "testLocalAssignment",
  "sourceCode" : "@Test\r\npublic void testLocalAssignment() {\r\n    String storm1 = \"storm1\";\r\n    String storm2 = \"storm2\";\r\n    Assignment ass1 = mockedAssignment(1);\r\n    Assignment ass2 = mockedAssignment(2);\r\n    ILocalAssignmentsBackend backend = LocalAssignmentsBackendFactory.getBackend(ConfigUtils.readStormConfig());\r\n    assertNull(backend.getAssignment(storm1));\r\n    backend.keepOrUpdateAssignment(storm1, ass1);\r\n    backend.keepOrUpdateAssignment(storm2, ass2);\r\n    assertEquals(ass1, backend.getAssignment(storm1));\r\n    assertEquals(ass2, backend.getAssignment(storm2));\r\n    backend.clearStateForStorm(storm1);\r\n    assertNull(backend.getAssignment(storm1));\r\n    backend.keepOrUpdateAssignment(storm1, ass1);\r\n    backend.keepOrUpdateAssignment(storm1, ass2);\r\n    assertEquals(ass2, backend.getAssignment(storm1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\assignments\\LocalAssignmentsBackendTest.java",
  "methodName" : "testLocalIdInfo",
  "sourceCode" : "@Test\r\npublic void testLocalIdInfo() {\r\n    String name1 = \"name1\";\r\n    String name2 = \"name2\";\r\n    String name3 = \"name3\";\r\n    String id1 = \"id1\";\r\n    String id2 = \"id2\";\r\n    String id3 = \"id3\";\r\n    ILocalAssignmentsBackend backend = LocalAssignmentsBackendFactory.getBackend(ConfigUtils.readStormConfig());\r\n    assertNull(backend.getStormId(name3));\r\n    backend.keepStormId(name1, id1);\r\n    backend.keepStormId(name2, id2);\r\n    assertEquals(id1, backend.getStormId(name1));\r\n    assertEquals(id2, backend.getStormId(name2));\r\n    backend.deleteStormId(name1);\r\n    assertNull(backend.getStormId(name1));\r\n    backend.clearStateForStorm(id2);\r\n    assertNull(backend.getStormId(name2));\r\n    backend.keepStormId(name1, id1);\r\n    backend.keepStormId(name1, id3);\r\n    assertEquals(id3, backend.getStormId(name1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\blobstore\\ClientBlobStoreTest.java",
  "methodName" : "testDuplicateACLsForCreate",
  "sourceCode" : "@Test\r\npublic void testDuplicateACLsForCreate() {\r\n    assertThrows(AuthorizationException.class, () -> {\r\n        SettableBlobMeta meta = new SettableBlobMeta();\r\n        AccessControl submitterAcl = BlobStoreAclHandler.parseAccessControl(\"u:tester:rwa\");\r\n        meta.add_to_acl(submitterAcl);\r\n        AccessControl duplicateAcl = BlobStoreAclHandler.parseAccessControl(\"u:tester:r--\");\r\n        meta.add_to_acl(duplicateAcl);\r\n        String testKey = \"testDuplicateACLsBlobKey\";\r\n        client.createBlob(testKey, meta);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\blobstore\\ClientBlobStoreTest.java",
  "methodName" : "testGoodACLsForCreate",
  "sourceCode" : "@Test\r\npublic void testGoodACLsForCreate() throws Exception {\r\n    SettableBlobMeta meta = new SettableBlobMeta();\r\n    AccessControl submitterAcl = BlobStoreAclHandler.parseAccessControl(\"u:tester:rwa\");\r\n    meta.add_to_acl(submitterAcl);\r\n    String testKey = \"testBlobKey\";\r\n    client.createBlob(testKey, meta);\r\n    validatedBlobAcls(testKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\blobstore\\ClientBlobStoreTest.java",
  "methodName" : "testDuplicateACLsForSetBlobMeta",
  "sourceCode" : "@Test\r\npublic void testDuplicateACLsForSetBlobMeta() {\r\n    assertThrows(AuthorizationException.class, () -> {\r\n        String testKey = \"testDuplicateACLsBlobKey\";\r\n        SettableBlobMeta meta = new SettableBlobMeta();\r\n        createTestBlob(testKey, meta);\r\n        AccessControl duplicateAcl = BlobStoreAclHandler.parseAccessControl(\"u:tester:r--\");\r\n        meta.add_to_acl(duplicateAcl);\r\n        client.setBlobMeta(testKey, meta);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\blobstore\\ClientBlobStoreTest.java",
  "methodName" : "testGoodACLsForSetBlobMeta",
  "sourceCode" : "@Test\r\npublic void testGoodACLsForSetBlobMeta() throws Exception {\r\n    String testKey = \"testBlobKey\";\r\n    SettableBlobMeta meta = new SettableBlobMeta();\r\n    createTestBlob(testKey, meta);\r\n    meta.add_to_acl(BlobStoreAclHandler.parseAccessControl(\"u:nextuser:r--\"));\r\n    client.setBlobMeta(testKey, meta);\r\n    validatedBlobAcls(testKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\blobstore\\ClientBlobStoreTest.java",
  "methodName" : "testBloblStoreKeyWithUnicodesValidation",
  "sourceCode" : "@Test\r\npublic void testBloblStoreKeyWithUnicodesValidation() {\r\n    BlobStore.validateKey(\"msg-kafka-unicodewriter-11-1483434711-stormconf.ser\");\r\n    BlobStore.validateKey(\"msg-kafka-ascii-11-148343436363-stormconf.ser\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testTrivial",
  "sourceCode" : "@Test\r\npublic void testTrivial() throws Exception {\r\n    ArrayList<Tuple> orderStream = makeStream(\"orders\", orderFields, orders, \"ordersSpout\");\r\n    TupleWindow window = makeTupleWindow(orderStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"orders\", orderFields[0]).select(\"orderId,userId,itemId,price\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(orderStream.size(), collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testNestedKeys",
  "sourceCode" : "@Test\r\npublic void testNestedKeys() throws Exception {\r\n    ArrayList<Tuple> userStream = makeNestedEventsStream(\"users\", userFields, users, \"usersSpout\");\r\n    TupleWindow window = makeTupleWindow(userStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", \"outer.userId\").select(\"outer.name, outer.city\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(userStream.size(), collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testProjection_FieldsWithStreamName",
  "sourceCode" : "@Test\r\npublic void testProjection_FieldsWithStreamName() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> storeStream = makeStream(\"stores\", storeFields, stores, \"storesSpout\");\r\n    TupleWindow window = makeTupleWindow(storeStream, userStream);\r\n    // join users and stores on city name\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[2]).join(\"stores\", \"city\", \"users\").select(\"userId,name,storeName,users:city,stores:city\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(storeStream.size() + 1, collector.actualResults.size());\r\n    // ensure 5 fields per tuple and no null fields\r\n    for (List<Object> tuple : collector.actualResults) {\r\n        assertEquals(5, tuple.size());\r\n        for (Object o : tuple) {\r\n            assertNotNull(o);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testInnerJoin",
  "sourceCode" : "@Test\r\npublic void testInnerJoin() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> orderStream = makeStream(\"orders\", orderFields, orders, \"ordersSpout\");\r\n    TupleWindow window = makeTupleWindow(orderStream, userStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[0]).join(\"orders\", \"userId\", \"users\").select(\"userId,name,price\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(orders.length, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testLeftJoin",
  "sourceCode" : "@Test\r\npublic void testLeftJoin() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> orderStream = makeStream(\"orders\", orderFields, orders, \"ordersSpout\");\r\n    TupleWindow window = makeTupleWindow(orderStream, userStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[0]).leftJoin(\"orders\", \"userId\", \"users\").select(\"userId,name,price\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(12, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testThreeStreamInnerJoin",
  "sourceCode" : "@Test\r\npublic void testThreeStreamInnerJoin() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> storesStream = makeStream(\"stores\", storeFields, stores, \"storesSpout\");\r\n    ArrayList<Tuple> cityStream = makeStream(\"cities\", cityFields, cities, \"citiesSpout\");\r\n    TupleWindow window = makeTupleWindow(userStream, storesStream, cityStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[2]).join(\"stores\", \"city\", \"users\").join(\"cities\", \"cityName\", \"stores\").select(\"name,storeName,city,country\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(6, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testThreeStreamLeftJoin_1",
  "sourceCode" : "@Test\r\npublic void testThreeStreamLeftJoin_1() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> storesStream = makeStream(\"stores\", storeFields, stores, \"storesSpout\");\r\n    ArrayList<Tuple> cityStream = makeStream(\"cities\", cityFields, cities, \"citiesSpout\");\r\n    TupleWindow window = makeTupleWindow(userStream, cityStream, storesStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[2]).leftJoin(\"stores\", \"city\", \"users\").leftJoin(\"cities\", \"cityName\", \"users\").select(\"name,storeName,city,country\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    assertEquals(users.length, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testThreeStreamLeftJoin_2",
  "sourceCode" : "@Test\r\npublic void testThreeStreamLeftJoin_2() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> storesStream = makeStream(\"stores\", storeFields, stores, \"storesSpout\");\r\n    ArrayList<Tuple> cityStream = makeStream(\"cities\", cityFields, cities, \"citiesSpout\");\r\n    TupleWindow window = makeTupleWindow(userStream, cityStream, storesStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", \"city\").leftJoin(\"stores\", \"city\", \"users\").leftJoin(\"cities\", \"cityName\", // join against diff stream compared to testThreeStreamLeftJoin_1\r\n    \"stores\").select(\"name,storeName,city,country\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    // stores.length+1 as 2 users in Bengaluru\r\n    assertEquals(stores.length + 1, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\bolt\\TestJoinBolt.java",
  "methodName" : "testThreeStreamMixedJoin",
  "sourceCode" : "@Test\r\npublic void testThreeStreamMixedJoin() throws Exception {\r\n    ArrayList<Tuple> userStream = makeStream(\"users\", userFields, users, \"usersSpout\");\r\n    ArrayList<Tuple> storesStream = makeStream(\"stores\", storeFields, stores, \"storesSpout\");\r\n    ArrayList<Tuple> cityStream = makeStream(\"cities\", cityFields, cities, \"citiesSpout\");\r\n    TupleWindow window = makeTupleWindow(userStream, cityStream, storesStream);\r\n    JoinBolt bolt = new JoinBolt(JoinBolt.Selector.STREAM, \"users\", userFields[2]).join(\"stores\", \"city\", \"users\").leftJoin(\"cities\", \"cityName\", \"users\").select(\"name,storeName,city,country\");\r\n    MockCollector collector = new MockCollector();\r\n    bolt.prepare(null, null, collector);\r\n    bolt.execute(window);\r\n    printResults(collector);\r\n    // stores.length+1 as 2 users in Bengaluru\r\n    assertEquals(stores.length + 1, collector.actualResults.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\cluster\\DaemonTypeTest.java",
  "methodName" : "getDefaultZkAclsDefaultConf",
  "sourceCode" : "@Test\r\npublic void getDefaultZkAclsDefaultConf() {\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    assertNull(DaemonType.UNKNOWN.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.PACEMAKER.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.SUPERVISOR.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.NIMBUS.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.WORKER.getDefaultZkAcls(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\cluster\\DaemonTypeTest.java",
  "methodName" : "getDefaultZkAclsSecureServerConf",
  "sourceCode" : "@Test\r\npublic void getDefaultZkAclsSecureServerConf() {\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"digest\");\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_PAYLOAD, \"storm:thisisapoorpassword\");\r\n    conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, DefaultPrincipalToLocal.class.getName());\r\n    conf.put(Config.NIMBUS_THRIFT_PORT, 6666);\r\n    assertNull(DaemonType.UNKNOWN.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.PACEMAKER.getDefaultZkAcls(conf));\r\n    assertEquals(DaemonType.NIMBUS_SUPERVISOR_ZK_ACLS, DaemonType.SUPERVISOR.getDefaultZkAcls(conf));\r\n    assertEquals(DaemonType.NIMBUS_SUPERVISOR_ZK_ACLS, DaemonType.NIMBUS.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.WORKER.getDefaultZkAcls(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\cluster\\DaemonTypeTest.java",
  "methodName" : "getDefaultZkAclsSecureWorkerConf",
  "sourceCode" : "@Test\r\npublic void getDefaultZkAclsSecureWorkerConf() {\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME, \"digest\");\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, \"storm:thisisapoorpassword\");\r\n    conf.put(Config.STORM_ZOOKEEPER_SUPERACL, \"sasl:nimbus\");\r\n    conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, DefaultPrincipalToLocal.class.getName());\r\n    conf.put(Config.NIMBUS_THRIFT_PORT, 6666);\r\n    assertNull(DaemonType.UNKNOWN.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.PACEMAKER.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.SUPERVISOR.getDefaultZkAcls(conf));\r\n    assertNull(DaemonType.NIMBUS.getDefaultZkAcls(conf));\r\n    List<ACL> expected = new ArrayList<>(ZooDefs.Ids.CREATOR_ALL_ACL);\r\n    expected.add(new ACL(ZooDefs.Perms.ALL, new Id(\"sasl\", \"nimbus\")));\r\n    assertEquals(expected, DaemonType.WORKER.getDefaultZkAcls(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\cluster\\StormClusterStateImplTest.java",
  "methodName" : "registeredCallback",
  "sourceCode" : "@Test\r\npublic void registeredCallback() {\r\n    Mockito.verify(storage).register(ArgumentMatchers.any(ZKStateChangedCallback.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\cluster\\StormClusterStateImplTest.java",
  "methodName" : "createdZNodes",
  "sourceCode" : "@Test\r\npublic void createdZNodes() {\r\n    for (String path : pathlist) {\r\n        Mockito.verify(storage).mkdirs(path, null);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\metrics\\ClientMetricsUtilsTest.java",
  "methodName" : "getMetricsRateUnit",
  "sourceCode" : "@Test\r\npublic void getMetricsRateUnit() {\r\n    Map<String, Object> reporterConf = new HashMap<>();\r\n    assertNull(ClientMetricsUtils.getMetricsRateUnit(reporterConf));\r\n    reporterConf.put(\"rate.unit\", \"SECONDS\");\r\n    assertEquals(TimeUnit.SECONDS, ClientMetricsUtils.getMetricsRateUnit(reporterConf));\r\n    reporterConf.put(\"rate.unit\", \"MINUTES\");\r\n    assertEquals(TimeUnit.MINUTES, ClientMetricsUtils.getMetricsRateUnit(reporterConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\metrics\\ClientMetricsUtilsTest.java",
  "methodName" : "getMetricsDurationUnit",
  "sourceCode" : "@Test\r\npublic void getMetricsDurationUnit() {\r\n    Map<String, Object> reporterConf = new HashMap<>();\r\n    assertNull(ClientMetricsUtils.getMetricsDurationUnit(reporterConf));\r\n    reporterConf.put(\"duration.unit\", \"SECONDS\");\r\n    assertEquals(TimeUnit.SECONDS, ClientMetricsUtils.getMetricsDurationUnit(reporterConf));\r\n    reporterConf.put(\"duration.unit\", \"MINUTES\");\r\n    assertEquals(TimeUnit.MINUTES, ClientMetricsUtils.getMetricsDurationUnit(reporterConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\metrics\\ClientMetricsUtilsTest.java",
  "methodName" : "getMetricsReporterLocale",
  "sourceCode" : "@Test\r\npublic void getMetricsReporterLocale() {\r\n    Map<String, Object> reporterConf = new HashMap<>();\r\n    assertNull(ClientMetricsUtils.getMetricsReporterLocale(reporterConf));\r\n    reporterConf.put(\"locale\", \"en-US\");\r\n    assertEquals(Locale.US, ClientMetricsUtils.getMetricsReporterLocale(reporterConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\metrics\\ClientMetricsUtilsTest.java",
  "methodName" : "getTimeUnitForConfig",
  "sourceCode" : "@Test\r\npublic void getTimeUnitForConfig() {\r\n    Map<String, Object> reporterConf = new HashMap<>();\r\n    String dummyKey = \"dummy.unit\";\r\n    assertNull(ClientMetricsUtils.getTimeUnitForConfig(reporterConf, dummyKey));\r\n    reporterConf.put(dummyKey, \"SECONDS\");\r\n    assertEquals(TimeUnit.SECONDS, ClientMetricsUtils.getTimeUnitForConfig(reporterConf, dummyKey));\r\n    reporterConf.put(dummyKey, \"MINUTES\");\r\n    assertEquals(TimeUnit.MINUTES, ClientMetricsUtils.getTimeUnitForConfig(reporterConf, dummyKey));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testGetBackpressure",
  "sourceCode" : "@Test\r\npublic void testGetBackpressure() {\r\n    int taskIdNoBackPressure = 1;\r\n    JCQueue noBackPressureQueue = mock(JCQueue.class);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, Collections.singletonMap(taskIdNoBackPressure, noBackPressureQueue), new StormMetricRegistry(), Collections.singletonMap(taskIdNoBackPressure, \"testComponent\"));\r\n    BackPressureStatus status = tracker.getCurrStatus();\r\n    assertThat(status.workerId, is(WORKER_ID));\r\n    assertThat(status.nonBpTasks, contains(taskIdNoBackPressure));\r\n    assertThat(status.bpTasks, is(empty()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testSetBackpressure",
  "sourceCode" : "@Test\r\npublic void testSetBackpressure() {\r\n    int taskIdNoBackPressure = 1;\r\n    JCQueue noBackPressureQueue = mock(JCQueue.class);\r\n    int taskIdBackPressure = 2;\r\n    JCQueue backPressureQueue = mock(JCQueue.class);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(taskIdNoBackPressure, noBackPressureQueue, taskIdBackPressure, backPressureQueue), new StormMetricRegistry(), ImmutableMap.of(taskIdNoBackPressure, \"NoBackPressureComponent\", taskIdBackPressure, \"BackPressureComponent\"));\r\n    BackpressureState state = tracker.getBackpressureState(taskIdBackPressure);\r\n    boolean backpressureChanged = tracker.recordBackPressure(state);\r\n    BackPressureStatus status = tracker.getCurrStatus();\r\n    assertThat(backpressureChanged, is(true));\r\n    assertThat(status.workerId, is(WORKER_ID));\r\n    assertThat(status.nonBpTasks, contains(taskIdNoBackPressure));\r\n    assertThat(status.bpTasks, contains(taskIdBackPressure));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testSetBackpressureWithExistingBackpressure",
  "sourceCode" : "@Test\r\npublic void testSetBackpressureWithExistingBackpressure() {\r\n    int taskId = 1;\r\n    JCQueue queue = mock(JCQueue.class);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(taskId, queue), new StormMetricRegistry(), ImmutableMap.of(taskId, \"component-1\"));\r\n    BackpressureState state = tracker.getBackpressureState(taskId);\r\n    tracker.recordBackPressure(state);\r\n    boolean backpressureChanged = tracker.recordBackPressure(state);\r\n    BackPressureStatus status = tracker.getCurrStatus();\r\n    assertThat(backpressureChanged, is(false));\r\n    assertThat(status.workerId, is(WORKER_ID));\r\n    assertThat(status.bpTasks, contains(taskId));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testRefreshBackpressureWithEmptyOverflow",
  "sourceCode" : "@Test\r\npublic void testRefreshBackpressureWithEmptyOverflow() {\r\n    int taskId = 1;\r\n    JCQueue queue = mock(JCQueue.class);\r\n    when(queue.isEmptyOverflow()).thenReturn(true);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(taskId, queue), new StormMetricRegistry(), ImmutableMap.of(taskId, \"component-1\"));\r\n    BackpressureState state = tracker.getBackpressureState(taskId);\r\n    tracker.recordBackPressure(state);\r\n    boolean backpressureChanged = tracker.refreshBpTaskList();\r\n    BackPressureStatus status = tracker.getCurrStatus();\r\n    assertThat(backpressureChanged, is(true));\r\n    assertThat(status.workerId, is(WORKER_ID));\r\n    assertThat(status.nonBpTasks, contains(taskId));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testRefreshBackPressureWithNonEmptyOverflow",
  "sourceCode" : "@Test\r\npublic void testRefreshBackPressureWithNonEmptyOverflow() {\r\n    int taskId = 1;\r\n    JCQueue queue = mock(JCQueue.class);\r\n    when(queue.isEmptyOverflow()).thenReturn(false);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(taskId, queue), new StormMetricRegistry(), ImmutableMap.of(taskId, \"component-1\"));\r\n    BackpressureState state = tracker.getBackpressureState(taskId);\r\n    tracker.recordBackPressure(state);\r\n    boolean backpressureChanged = tracker.refreshBpTaskList();\r\n    BackPressureStatus status = tracker.getCurrStatus();\r\n    assertThat(backpressureChanged, is(false));\r\n    assertThat(status.workerId, is(WORKER_ID));\r\n    assertThat(status.bpTasks, contains(taskId));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\BackPressureTrackerTest.java",
  "methodName" : "testSetLastOverflowCount",
  "sourceCode" : "@Test\r\npublic void testSetLastOverflowCount() {\r\n    int taskId = 1;\r\n    int overflow = 5;\r\n    JCQueue queue = mock(JCQueue.class);\r\n    BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(taskId, queue), new StormMetricRegistry(), ImmutableMap.of(taskId, \"component-1\"));\r\n    BackpressureState state = tracker.getBackpressureState(taskId);\r\n    tracker.recordBackPressure(state);\r\n    tracker.setLastOverflowCount(state, overflow);\r\n    BackpressureState retrievedState = tracker.getBackpressureState(taskId);\r\n    int lastOverflowCount = tracker.getLastOverflowCount(retrievedState);\r\n    assertThat(lastOverflowCount, is(overflow));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetShouldNotTriggerForFutureTime",
  "sourceCode" : "@Test\r\npublic void testLogResetShouldNotTriggerForFutureTime() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long theFuture = Time.currentTimeMillis() + 1000;\r\n        TreeMap<String, LogLevel> config = new TreeMap<>();\r\n        config.put(\"foo\", ll(theFuture));\r\n        AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n        LogConfigManager underTest = new LogConfigManagerUnderTest(atomConf);\r\n        underTest.resetLogLevels();\r\n        assertNotNull(atomConf.get());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetTriggersForPastTime",
  "sourceCode" : "@Test\r\npublic void testLogResetTriggersForPastTime() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long past = Time.currentTimeMillis() - 1000;\r\n        TreeMap<String, LogLevel> config = new TreeMap<>();\r\n        config.put(\"foo\", ll(\"INFO\", \"WARN\", past));\r\n        AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n        LogConfigManager underTest = new LogConfigManagerUnderTest(atomConf);\r\n        underTest.resetLogLevels();\r\n        assertEquals(new TreeMap<>(), atomConf.get());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetResetsDoesNothingForEmptyLogConfig",
  "sourceCode" : "@Test\r\npublic void testLogResetResetsDoesNothingForEmptyLogConfig() {\r\n    TreeMap<String, LogLevel> config = new TreeMap<>();\r\n    AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n    LogConfigManager underTest = spy(new LogConfigManagerUnderTest(atomConf));\r\n    underTest.resetLogLevels();\r\n    assertEquals(new TreeMap<>(), atomConf.get());\r\n    verify(underTest, never()).setLoggerLevel(any(LoggerContext.class), anyString(), anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetResetsRootLoggerIfSet",
  "sourceCode" : "@Test\r\npublic void testLogResetResetsRootLoggerIfSet() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long past = Time.currentTimeMillis() - 1000;\r\n        TreeMap<String, LogLevel> config = new TreeMap<>();\r\n        config.put(LogManager.ROOT_LOGGER_NAME, ll(\"DEBUG\", \"WARN\", past));\r\n        AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n        LogConfigManager underTest = spy(new LogConfigManagerUnderTest(atomConf));\r\n        underTest.resetLogLevels();\r\n        assertEquals(new TreeMap<>(), atomConf.get());\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(LogManager.ROOT_LOGGER_NAME), eq(\"WARN\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetProperlyResetLogLevelAfterTimeout",
  "sourceCode" : "@Test\r\npublic void testLogResetProperlyResetLogLevelAfterTimeout() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long inThirtySeconds = Time.currentTimeMillis() + 30_000;\r\n        TreeMap<String, LogLevel> config = new TreeMap<>();\r\n        config.put(LogManager.ROOT_LOGGER_NAME, ll(\"DEBUG\", \"WARN\", inThirtySeconds));\r\n        AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n        LogConfigManager underTest = spy(new LogConfigManagerUnderTest(atomConf));\r\n        TreeMap<String, LogLevel> expected = new TreeMap<>();\r\n        LogLevel logLevel = new LogLevel(LogLevelAction.UPDATE);\r\n        logLevel.set_target_log_level(\"DEBUG\");\r\n        logLevel.set_reset_log_level(\"WARN\");\r\n        logLevel.set_reset_log_level_timeout_epoch(30_000);\r\n        expected.put(LogManager.ROOT_LOGGER_NAME, logLevel);\r\n        underTest.resetLogLevels();\r\n        assertEquals(expected, atomConf.get());\r\n        verify(underTest, never()).setLoggerLevel(any(LoggerContext.class), eq(LogManager.ROOT_LOGGER_NAME), anyString());\r\n        // 11 seconds passed by, not timing out\r\n        Time.advanceTimeSecs(11);\r\n        underTest.resetLogLevels();\r\n        assertEquals(expected, atomConf.get());\r\n        verify(underTest, never()).setLoggerLevel(any(LoggerContext.class), eq(LogManager.ROOT_LOGGER_NAME), anyString());\r\n        // 22 seconds passed by, still not timing out\r\n        Time.advanceTimeSecs(11);\r\n        underTest.resetLogLevels();\r\n        assertEquals(expected, atomConf.get());\r\n        verify(underTest, never()).setLoggerLevel(any(LoggerContext.class), eq(LogManager.ROOT_LOGGER_NAME), anyString());\r\n        // 33 seconds passed by, timed out\r\n        Time.advanceTimeSecs(11);\r\n        underTest.resetLogLevels();\r\n        assertEquals(new TreeMap<>(), atomConf.get());\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(LogManager.ROOT_LOGGER_NAME), eq(\"WARN\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testLogResetsNamedLoggersWithPastTimeout",
  "sourceCode" : "@Test\r\npublic void testLogResetsNamedLoggersWithPastTimeout() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long past = Time.currentTimeMillis() - 1000;\r\n        TreeMap<String, LogLevel> config = new TreeMap<>();\r\n        config.put(\"my_debug_logger\", ll(\"DEBUG\", \"INFO\", past));\r\n        config.put(\"my_info_logger\", ll(\"INFO\", \"WARN\", past));\r\n        config.put(\"my_error_logger\", ll(\"ERROR\", \"INFO\", past));\r\n        AtomicReference<TreeMap<String, LogLevel>> atomConf = new AtomicReference<>(config);\r\n        LogConfigManager underTest = spy(new LogConfigManagerUnderTest(atomConf));\r\n        underTest.resetLogLevels();\r\n        assertEquals(new TreeMap<>(), atomConf.get());\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_debug_logger\"), eq(\"INFO\"));\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_info_logger\"), eq(\"WARN\"));\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_error_logger\"), eq(\"INFO\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testProcessRootLogLevelToDebugSetsLoggerAndTimeout2",
  "sourceCode" : "@Test\r\npublic void testProcessRootLogLevelToDebugSetsLoggerAndTimeout2() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        LogConfig mockConfig = new LogConfig();\r\n        AtomicReference<TreeMap<String, LogLevel>> mockConfigAtom = new AtomicReference<>(null);\r\n        long inThirtySeconds = Time.currentTimeMillis() + 30_000;\r\n        mockConfig.put_to_named_logger_level(\"ROOT\", ll(\"DEBUG\", inThirtySeconds));\r\n        LogConfigManager underTest = spy(new LogConfigManagerUnderTest(mockConfigAtom));\r\n        underTest.processLogConfigChange(mockConfig);\r\n        // test that the set-logger-level function was not called\r\n        LOG.info(\"Tests {}\", mockConfigAtom.get());\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"\"), eq(\"DEBUG\"));\r\n        LogLevel rootResult = mockConfigAtom.get().get(LogManager.ROOT_LOGGER_NAME);\r\n        assertNotNull(rootResult);\r\n        assertEquals(LogLevelAction.UPDATE, rootResult.get_action());\r\n        assertEquals(\"DEBUG\", rootResult.get_target_log_level());\r\n        // defaults to INFO level when the logger isn't found previously\r\n        assertEquals(\"INFO\", rootResult.get_reset_log_level());\r\n        assertEquals(inThirtySeconds, rootResult.get_reset_log_level_timeout_epoch());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testProcessLogConfigChangeThrowsIllegalArgumentExceptionWhenTargetLogLevelIsNotSet",
  "sourceCode" : "@Test\r\npublic void testProcessLogConfigChangeThrowsIllegalArgumentExceptionWhenTargetLogLevelIsNotSet() {\r\n    LogConfigManager logConfigManager = new LogConfigManager();\r\n    LogConfig logConfig = new LogConfig();\r\n    LogLevel logLevel = new LogLevel();\r\n    logLevel.set_action(LogLevelAction.UPDATE);\r\n    logLevel.set_reset_log_level(\"INFO\");\r\n    logConfig.put_to_named_logger_level(\"RESET_LOG\", logLevel);\r\n    assertThrows(IllegalArgumentException.class, () -> logConfigManager.processLogConfigChange(logConfig));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testProcessLogConfigChangeExecutesSuccessfullyWhenTargetLogLevelIsSet",
  "sourceCode" : "@Test\r\npublic void testProcessLogConfigChangeExecutesSuccessfullyWhenTargetLogLevelIsSet() {\r\n    LogConfigManager logConfigManager = new LogConfigManager();\r\n    LogConfig logConfig = new LogConfig();\r\n    LogLevel logLevel = new LogLevel();\r\n    logLevel.set_action(LogLevelAction.UPDATE);\r\n    logLevel.set_target_log_level(\"DEBUG\");\r\n    logConfig.put_to_named_logger_level(\"TARGET_LOG\", logLevel);\r\n    logConfigManager.processLogConfigChange(logConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\LogConfigManagerTest.java",
  "methodName" : "testProcessRootLogLevelToDebugSetsLoggerAndTimeout",
  "sourceCode" : "@Test\r\npublic void testProcessRootLogLevelToDebugSetsLoggerAndTimeout() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        LogConfig mockConfig = new LogConfig();\r\n        AtomicReference<TreeMap<String, LogLevel>> mockConfigAtom = new AtomicReference<>(null);\r\n        long inThirtySeconds = Time.currentTimeMillis() + 30_000;\r\n        mockConfig.put_to_named_logger_level(\"ROOT\", ll(\"DEBUG\", inThirtySeconds));\r\n        mockConfig.put_to_named_logger_level(\"my_debug_logger\", ll(\"DEBUG\", inThirtySeconds));\r\n        mockConfig.put_to_named_logger_level(\"my_info_logger\", ll(\"INFO\", inThirtySeconds));\r\n        mockConfig.put_to_named_logger_level(\"my_error_logger\", ll(\"ERROR\", inThirtySeconds));\r\n        LOG.info(\"Tests {}\", mockConfigAtom.get());\r\n        LogConfigManager underTest = spy(new LogConfigManagerUnderTest(mockConfigAtom));\r\n        underTest.processLogConfigChange(mockConfig);\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"\"), eq(\"DEBUG\"));\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_debug_logger\"), eq(\"DEBUG\"));\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_info_logger\"), eq(\"INFO\"));\r\n        verify(underTest).setLoggerLevel(any(LoggerContext.class), eq(\"my_error_logger\"), eq(\"ERROR\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\WorkerStateTest.java",
  "methodName" : "testWorkerHooksLifecycle",
  "sourceCode" : "@Test\r\npublic void testWorkerHooksLifecycle() throws TException, IOException {\r\n    ConfigUtils mockedConfigUtils = mock(ConfigUtils.class);\r\n    ConfigUtils previousConfigUtils = ConfigUtils.setInstance(mockedConfigUtils);\r\n    try {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(Config.TOPOLOGY_WORKER_SHARED_THREAD_POOL_SIZE, 1);\r\n        List<ByteBuffer> workerHookBuffers = Collections.singletonList(ByteBuffer.wrap(Utils.javaSerialize(new TestUtilsForWorkerState.StateTrackingWorkerHook())));\r\n        String topologyId = \"1\";\r\n        StormTopology topology = mock(StormTopology.class);\r\n        when(topology.deepCopy()).thenReturn(topology);\r\n        when(topology.is_set_worker_hooks()).thenReturn(true);\r\n        when(topology.get_worker_hooks()).thenReturn(workerHookBuffers);\r\n        when(mockedConfigUtils.readSupervisorTopologyImpl(eq(conf), eq(topologyId), any(AdvancedFSOps.class))).thenReturn(topology);\r\n        WorkerState workerState = TestUtilsForWorkerState.getWorkerState(conf, topologyId);\r\n        TestUtilsForWorkerState.StateTrackingWorkerHook workerHook = workerState.getDeserializedWorkerHooks().stream().filter(iwh -> iwh instanceof TestUtilsForWorkerState.StateTrackingWorkerHook).map(iwh -> (TestUtilsForWorkerState.StateTrackingWorkerHook) iwh).findFirst().get();\r\n        assertFalse(workerHook.isStartCalled());\r\n        assertFalse(workerHook.isShutdownCalled());\r\n        workerState.runWorkerStartHooks();\r\n        assertTrue(workerHook.isStartCalled());\r\n        workerState.runWorkerShutdownHooks();\r\n        assertTrue(workerHook.isShutdownCalled());\r\n    } finally {\r\n        ConfigUtils.setInstance(previousConfigUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\WorkerStateTest.java",
  "methodName" : "testVisibilityOfUserResource",
  "sourceCode" : "@Test\r\npublic void testVisibilityOfUserResource() throws IOException, TException {\r\n    ConfigUtils mockedConfigUtils = mock(ConfigUtils.class);\r\n    ConfigUtils previousConfigUtils = ConfigUtils.setInstance(mockedConfigUtils);\r\n    try {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(Config.TOPOLOGY_WORKER_SHARED_THREAD_POOL_SIZE, 1);\r\n        TestUtilsForWorkerState.ResourceInitializingWorkerHook workerHook = new TestUtilsForWorkerState.ResourceInitializingWorkerHook();\r\n        List<ByteBuffer> workerHookBuffers = Collections.singletonList(ByteBuffer.wrap(Utils.javaSerialize(workerHook)));\r\n        String topologyId = \"1\";\r\n        StormTopology topology = mock(StormTopology.class);\r\n        when(topology.deepCopy()).thenReturn(topology);\r\n        when(topology.is_set_worker_hooks()).thenReturn(true);\r\n        when(topology.get_worker_hooks()).thenReturn(workerHookBuffers);\r\n        when(mockedConfigUtils.readSupervisorTopologyImpl(eq(conf), eq(topologyId), any(AdvancedFSOps.class))).thenReturn(topology);\r\n        WorkerState workerState = TestUtilsForWorkerState.getWorkerState(conf, topologyId);\r\n        assertNull(workerState.getWorkerTopologyContext().getResource(TestUtilsForWorkerState.RESOURCE_KEY));\r\n        workerState.runWorkerStartHooks();\r\n        assertEquals(TestUtilsForWorkerState.RESOURCE_VALUE, workerState.getWorkerTopologyContext().getResource(TestUtilsForWorkerState.RESOURCE_KEY));\r\n        workerState.runWorkerShutdownHooks();\r\n    } finally {\r\n        ConfigUtils.setInstance(previousConfigUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\daemon\\worker\\WorkerTest.java",
  "methodName" : "testWorkerIsConnectionReady",
  "sourceCode" : "@Test\r\npublic void testWorkerIsConnectionReady() {\r\n    ConnectionWithStatus connection = Mockito.mock(ConnectionWithStatus.class);\r\n    Mockito.when(connection.status()).thenReturn(ConnectionWithStatus.Status.Ready);\r\n    assertTrue(WorkerState.isConnectionReady(connection));\r\n    Mockito.when(connection.status()).thenReturn(ConnectionWithStatus.Status.Connecting);\r\n    assertFalse(WorkerState.isConnectionReady(connection));\r\n    Mockito.when(connection.status()).thenReturn(ConnectionWithStatus.Status.Closed);\r\n    assertFalse(WorkerState.isConnectionReady(connection));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyPropertiesParserTest.java",
  "methodName" : "parseJarsProperties",
  "sourceCode" : "@Test\r\npublic void parseJarsProperties() {\r\n    List<File> parsed = sut.parseJarsProperties(\"storm-core-1.0.0.jar,json-simple-1.1.jar\");\r\n    assertEquals(2, parsed.size());\r\n    assertEquals(\"storm-core-1.0.0.jar\", parsed.get(0).getName());\r\n    assertEquals(\"json-simple-1.1.jar\", parsed.get(1).getName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyPropertiesParserTest.java",
  "methodName" : "parseEmptyJarsProperties",
  "sourceCode" : "@Test\r\npublic void parseEmptyJarsProperties() {\r\n    List<File> parsed = sut.parseJarsProperties(\"\");\r\n    assertEquals(0, parsed.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyPropertiesParserTest.java",
  "methodName" : "parsePackagesProperties",
  "sourceCode" : "@Test\r\npublic void parsePackagesProperties() {\r\n    Map<String, String> testInputMap = new HashMap<>();\r\n    testInputMap.put(\"org.apache.storm:storm-core:1.0.0\", \"storm-core-1.0.0.jar\");\r\n    testInputMap.put(\"com.googlecode.json-simple:json-simple:1.1\", \"json-simple-1.1.jar\");\r\n    String testJson = JSONValue.toJSONString(testInputMap);\r\n    Map<String, File> parsed = sut.parseArtifactsProperties(testJson);\r\n    assertEquals(2, parsed.size());\r\n    assertEquals(\"storm-core-1.0.0.jar\", parsed.get(\"org.apache.storm:storm-core:1.0.0\").getName());\r\n    assertEquals(\"json-simple-1.1.jar\", parsed.get(\"com.googlecode.json-simple:json-simple:1.1\").getName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyPropertiesParserTest.java",
  "methodName" : "parseEmptyPackagesProperties",
  "sourceCode" : "@Test\r\npublic void parseEmptyPackagesProperties() {\r\n    Map<String, File> parsed = sut.parseArtifactsProperties(\"{}\");\r\n    assertEquals(0, parsed.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyPropertiesParserTest.java",
  "methodName" : "parsePackagesPropertiesWithBrokenJSON",
  "sourceCode" : "@Test\r\npublic void parsePackagesPropertiesWithBrokenJSON() {\r\n    assertThrows(RuntimeException.class, () -> sut.parseArtifactsProperties(\"{\\\"group:artifact:version\\\": \\\"a.jar\\\"\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadFilesWhichOneOfThemIsNotFoundInLocal",
  "sourceCode" : "@Test\r\npublic void uploadFilesWhichOneOfThemIsNotFoundInLocal() {\r\n    assertThrows(FileNotAvailableException.class, () -> {\r\n        File mockFile = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        File mockFile2 = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(false);\r\n        List<File> dependencies = new ArrayList<>();\r\n        dependencies.add(mockFile);\r\n        dependencies.add(mockFile2);\r\n        sut.uploadFiles(dependencies, false);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadFilesWhichOneOfThemIsNotFile",
  "sourceCode" : "@Test\r\npublic void uploadFilesWhichOneOfThemIsNotFile() {\r\n    assertThrows(FileNotAvailableException.class, () -> {\r\n        File mockFile = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        File mockFile2 = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(false);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        List<File> dependencies = Lists.newArrayList(mockFile, mockFile2);\r\n        sut.uploadFiles(dependencies, false);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadFilesWhichOneOfThemIsFailedToBeUploaded",
  "sourceCode" : "@Test\r\npublic void uploadFilesWhichOneOfThemIsFailedToBeUploaded() throws Exception {\r\n    File mockFile = createTemporaryDummyFile();\r\n    File mockFile2 = mock(File.class);\r\n    when(mockFile2.getName()).thenReturn(\"dummy.jar\");\r\n    when(mockFile2.isFile()).thenReturn(true);\r\n    when(mockFile2.exists()).thenReturn(true);\r\n    when(mockFile2.getPath()).thenThrow(new RuntimeException(\"just for test!\"));\r\n    when(mockFile2.toPath()).thenThrow(new RuntimeException(\"just for test!\"));\r\n    String mockFileFileNameWithoutExtension = Files.getNameWithoutExtension(mockFile.getName());\r\n    String mockFile2FileNameWithoutExtension = Files.getNameWithoutExtension(mockFile2.getName());\r\n    // we skip uploading first one since we want to test rollback, not upload\r\n    when(mockBlobStore.getBlobMeta(contains(mockFileFileNameWithoutExtension))).thenReturn(new ReadableBlobMeta());\r\n    // we try uploading second one and it should be failed throwing RuntimeException\r\n    when(mockBlobStore.getBlobMeta(contains(mockFile2FileNameWithoutExtension))).thenThrow(new KeyNotFoundException());\r\n    List<File> dependencies = Lists.newArrayList(mockFile, mockFile2);\r\n    try {\r\n        sut.uploadFiles(dependencies, true);\r\n        fail(\"Should pass RuntimeException\");\r\n    } catch (RuntimeException ignore) {\r\n        // intended behavior\r\n    }\r\n    verify(mockBlobStore).getBlobMeta(contains(mockFileFileNameWithoutExtension));\r\n    verify(mockBlobStore).getBlobMeta(contains(mockFile2FileNameWithoutExtension));\r\n    verify(mockBlobStore).deleteBlob(contains(mockFileFileNameWithoutExtension));\r\n    verify(mockBlobStore, never()).deleteBlob(contains(mockFile2FileNameWithoutExtension));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadFiles",
  "sourceCode" : "@Test\r\npublic void uploadFiles() throws Exception {\r\n    AtomicOutputStream mockOutputStream = mock(AtomicOutputStream.class);\r\n    doNothing().when(mockOutputStream).cancel();\r\n    final AtomicInteger counter = new AtomicInteger();\r\n    final Answer<Object> incrementCounter = invocation -> {\r\n        counter.addAndGet(1);\r\n        return null;\r\n    };\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(anyInt());\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(any(byte[].class));\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(any(byte[].class), anyInt(), anyInt());\r\n    doNothing().when(mockOutputStream).close();\r\n    when(mockBlobStore.getBlobMeta(anyString())).thenThrow(new KeyNotFoundException());\r\n    when(mockBlobStore.createBlob(anyString(), any(SettableBlobMeta.class))).thenReturn(mockOutputStream);\r\n    File mockFile = createTemporaryDummyFile();\r\n    String mockFileFileNameWithoutExtension = Files.getNameWithoutExtension(mockFile.getName());\r\n    List<String> keys = sut.uploadFiles(Lists.newArrayList(mockFile), false);\r\n    assertEquals(1, keys.size());\r\n    assertTrue(keys.get(0).contains(mockFileFileNameWithoutExtension));\r\n    assertTrue(counter.get() > 0);\r\n    verify(mockOutputStream).close();\r\n    ArgumentCaptor<SettableBlobMeta> blobMetaArgumentCaptor = ArgumentCaptor.forClass(SettableBlobMeta.class);\r\n    verify(mockBlobStore).createBlob(anyString(), blobMetaArgumentCaptor.capture());\r\n    SettableBlobMeta actualBlobMeta = blobMetaArgumentCaptor.getValue();\r\n    List<AccessControl> actualAcls = actualBlobMeta.get_acl();\r\n    assertTrue(actualAcls.contains(new AccessControl(AccessControlType.USER, BlobStoreAclHandler.READ | BlobStoreAclHandler.WRITE | BlobStoreAclHandler.ADMIN)));\r\n    assertTrue(actualAcls.contains(new AccessControl(AccessControlType.OTHER, BlobStoreAclHandler.READ)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadArtifactsWhichOneOfThemIsNotFoundInLocal",
  "sourceCode" : "@Test\r\npublic void uploadArtifactsWhichOneOfThemIsNotFoundInLocal() {\r\n    assertThrows(FileNotAvailableException.class, () -> {\r\n        File mockFile = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        File mockFile2 = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(false);\r\n        Map<String, File> artifacts = new LinkedHashMap<>();\r\n        artifacts.put(\"group:artifact:1.0.0\", mockFile);\r\n        artifacts.put(\"group:artifact:1.1.0\", mockFile2);\r\n        sut.uploadArtifacts(artifacts);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadArtifactsWhichOneOfThemIsNotFile",
  "sourceCode" : "@Test\r\npublic void uploadArtifactsWhichOneOfThemIsNotFile() {\r\n    assertThrows(FileNotAvailableException.class, () -> {\r\n        File mockFile = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(true);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        File mockFile2 = mock(File.class);\r\n        when(mockFile.isFile()).thenReturn(false);\r\n        when(mockFile.exists()).thenReturn(true);\r\n        Map<String, File> artifacts = new LinkedHashMap<>();\r\n        artifacts.put(\"group:artifact:1.0.0\", mockFile);\r\n        artifacts.put(\"group:artifact:1.1.0\", mockFile2);\r\n        sut.uploadArtifacts(artifacts);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadArtifactsWhichOneOfThemIsFailedToBeUploaded",
  "sourceCode" : "@Test\r\npublic void uploadArtifactsWhichOneOfThemIsFailedToBeUploaded() throws Exception {\r\n    String artifact = \"group:artifact:1.0.0\";\r\n    String expectedBlobKeyForArtifact = \"group-artifact-1.0.0.jar\";\r\n    File mockFile = createTemporaryDummyFile();\r\n    String artifact2 = \"group:artifact2:2.0.0\";\r\n    String expectedBlobKeyForArtifact2 = \"group-artifact2-2.0.0.jar\";\r\n    File mockFile2 = mock(File.class);\r\n    when(mockFile2.getName()).thenReturn(\"dummy.jar\");\r\n    when(mockFile2.isFile()).thenReturn(true);\r\n    when(mockFile2.exists()).thenReturn(true);\r\n    when(mockFile2.getPath()).thenThrow(new RuntimeException(\"just for test!\"));\r\n    when(mockFile2.toPath()).thenThrow(new RuntimeException(\"just for test!\"));\r\n    // we skip uploading first one since we don't test upload for now\r\n    when(mockBlobStore.getBlobMeta(contains(expectedBlobKeyForArtifact))).thenReturn(new ReadableBlobMeta());\r\n    // we try uploading second one and it should be failed throwing RuntimeException\r\n    when(mockBlobStore.getBlobMeta(contains(expectedBlobKeyForArtifact2))).thenThrow(new KeyNotFoundException());\r\n    Map<String, File> artifacts = new LinkedHashMap<>();\r\n    artifacts.put(artifact, mockFile);\r\n    artifacts.put(artifact2, mockFile2);\r\n    try {\r\n        sut.uploadArtifacts(artifacts);\r\n        fail(\"Should pass RuntimeException\");\r\n    } catch (RuntimeException e) {\r\n        // intended behavior\r\n    }\r\n    verify(mockBlobStore).getBlobMeta(contains(expectedBlobKeyForArtifact));\r\n    verify(mockBlobStore).getBlobMeta(contains(expectedBlobKeyForArtifact2));\r\n    // never rollback\r\n    verify(mockBlobStore, never()).deleteBlob(contains(expectedBlobKeyForArtifact));\r\n    verify(mockBlobStore, never()).deleteBlob(contains(expectedBlobKeyForArtifact2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\dependency\\DependencyUploaderTest.java",
  "methodName" : "uploadArtifacts",
  "sourceCode" : "@Test\r\npublic void uploadArtifacts() throws Exception {\r\n    AtomicOutputStream mockOutputStream = mock(AtomicOutputStream.class);\r\n    doNothing().when(mockOutputStream).cancel();\r\n    final AtomicInteger counter = new AtomicInteger();\r\n    final Answer<Object> incrementCounter = invocation -> {\r\n        counter.addAndGet(1);\r\n        return null;\r\n    };\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(anyInt());\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(any(byte[].class));\r\n    doAnswer(incrementCounter).when(mockOutputStream).write(any(byte[].class), anyInt(), anyInt());\r\n    doNothing().when(mockOutputStream).close();\r\n    when(mockBlobStore.getBlobMeta(anyString())).thenThrow(new KeyNotFoundException());\r\n    when(mockBlobStore.createBlob(anyString(), any(SettableBlobMeta.class))).thenReturn(mockOutputStream);\r\n    String artifact = \"group:artifact:1.0.0\";\r\n    String expectedBlobKeyForArtifact = \"group-artifact-1.0.0.jar\";\r\n    File mockFile = createTemporaryDummyFile();\r\n    Map<String, File> artifacts = new LinkedHashMap<>();\r\n    artifacts.put(artifact, mockFile);\r\n    List<String> keys = sut.uploadArtifacts(artifacts);\r\n    assertEquals(1, keys.size());\r\n    assertTrue(keys.get(0).contains(expectedBlobKeyForArtifact));\r\n    assertTrue(counter.get() > 0);\r\n    verify(mockOutputStream).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\executor\\error\\ReportErrorTest.java",
  "methodName" : "testReport",
  "sourceCode" : "@Test\r\npublic void testReport() {\r\n    final String topo = \"topology\";\r\n    final String comp = \"component\";\r\n    final Long port = 8080L;\r\n    final AtomicLong errorCount = new AtomicLong(0L);\r\n    WorkerTopologyContext context = mock(WorkerTopologyContext.class);\r\n    when(context.getThisWorkerPort()).thenReturn(port.intValue());\r\n    IStormClusterState state = mock(IStormClusterState.class);\r\n    doAnswer((invocation) -> errorCount.incrementAndGet()).when(state).reportError(eq(topo), eq(comp), anyString(), eq(port), any(Throwable.class));\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_ERROR_THROTTLE_INTERVAL_SECS, 10);\r\n    conf.put(Config.TOPOLOGY_MAX_ERROR_REPORT_PER_INTERVAL, 4);\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        ReportError report = new ReportError(conf, state, topo, comp, context);\r\n        report.report(new RuntimeException(\"ERROR-1\"));\r\n        assertEquals(1, errorCount.get());\r\n        report.report(new RuntimeException(\"ERROR-2\"));\r\n        assertEquals(2, errorCount.get());\r\n        report.report(new RuntimeException(\"ERROR-3\"));\r\n        assertEquals(3, errorCount.get());\r\n        report.report(new RuntimeException(\"ERROR-4\"));\r\n        assertEquals(4, errorCount.get());\r\n        //Too fast not reported\r\n        report.report(new RuntimeException(\"ERROR-5\"));\r\n        assertEquals(4, errorCount.get());\r\n        Time.advanceTime(9000);\r\n        report.report(new RuntimeException(\"ERROR-6\"));\r\n        assertEquals(4, errorCount.get());\r\n        Time.advanceTime(2000);\r\n        report.report(new RuntimeException(\"ERROR-7\"));\r\n        assertEquals(5, errorCount.get());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\executor\\ExecutorTransferMultiThreadingTest.java",
  "methodName" : "testExecutorTransfer",
  "sourceCode" : "@Test\r\npublic void testExecutorTransfer() throws InterruptedException {\r\n    //There is one ExecutorTransfer per executor\r\n    ExecutorTransfer executorTransfer = new ExecutorTransfer(workerState, topoConf);\r\n    executorTransfer.initLocalRecvQueues();\r\n    ExecutorService executorService = Executors.newFixedThreadPool(5);\r\n    //There can be multiple producer threads sending out tuples inside each executor\r\n    //This mimics the case of multi-threading components where a component spawns extra threads to emit tuples.\r\n    int producerTaskNum = 10;\r\n    Runnable[] producerTasks = new Runnable[producerTaskNum];\r\n    for (int i = 0; i < producerTaskNum; i++) {\r\n        producerTasks[i] = createProducerTask(executorTransfer);\r\n    }\r\n    for (Runnable task : producerTasks) {\r\n        executorService.submit(task);\r\n    }\r\n    //give producers enough time to insert messages into the queue\r\n    executorService.awaitTermination(1000, TimeUnit.MILLISECONDS);\r\n    //consume all the tuples in the queue and deserialize them one by one\r\n    //this mimics a remote worker.\r\n    KryoTupleDeserializer deserializer = new KryoTupleDeserializer(topoConf, workerState.getWorkerTopologyContext());\r\n    SingleThreadedConsumer consumer = new SingleThreadedConsumer(deserializer, producerTaskNum);\r\n    transferQueue.consume(consumer);\r\n    consumer.finalCheck();\r\n    executorService.shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\executor\\SpoutExecutorTest.java",
  "methodName" : "testPendingTuplesRotateShouldBeCalledOnlyOnce",
  "sourceCode" : "@Test\r\npublic void testPendingTuplesRotateShouldBeCalledOnlyOnce() throws Exception {\r\n    RateCounter rateCounter = Mockito.mock(RateCounter.class);\r\n    StormMetricRegistry stormMetricRegistry = Mockito.mock(StormMetricRegistry.class);\r\n    Mockito.when(stormMetricRegistry.rateCounter(anyString(), anyString(), anyInt())).thenReturn(rateCounter);\r\n    Map<String, Object> hashmap = Utils.readDefaultConfig();\r\n    IStateStorage stateStorage = Mockito.mock(IStateStorage.class);\r\n    ComponentCommon componentCommon = Mockito.mock(ComponentCommon.class);\r\n    Mockito.when(componentCommon.get_json_conf()).thenReturn(null);\r\n    WorkerTopologyContext workerTopologyContext = Mockito.mock(WorkerTopologyContext.class);\r\n    Mockito.when(workerTopologyContext.getComponentId(anyInt())).thenReturn(\"1\");\r\n    Mockito.when(workerTopologyContext.getComponentCommon(anyString())).thenReturn(componentCommon);\r\n    WorkerState workerState = Mockito.mock(WorkerState.class);\r\n    Mockito.when(workerState.getWorkerTopologyContext()).thenReturn(workerTopologyContext);\r\n    Mockito.when(workerState.getStateStorage()).thenReturn(stateStorage);\r\n    Mockito.when(workerState.getTopologyConf()).thenReturn(hashmap);\r\n    Mockito.when(workerState.getMetricRegistry()).thenReturn(stormMetricRegistry);\r\n    SpoutExecutor spoutExecutor = new SpoutExecutor(workerState, List.of(1L, 5L), new HashMap<>());\r\n    TupleImpl tuple = Mockito.mock(TupleImpl.class);\r\n    Mockito.when(tuple.getSourceStreamId()).thenReturn(Constants.SYSTEM_TICK_STREAM_ID);\r\n    AddressedTuple addressedTuple = Mockito.mock(AddressedTuple.class);\r\n    Mockito.when(addressedTuple.getDest()).thenReturn(AddressedTuple.BROADCAST_DEST);\r\n    Mockito.when(addressedTuple.getTuple()).thenReturn(tuple);\r\n    RotatingMap rotatingMap = Mockito.mock(RotatingMap.class);\r\n    Field fieldRotatingMap = ReflectionUtils.findFields(SpoutExecutor.class, f -> f.getName().equals(\"pending\"), ReflectionUtils.HierarchyTraversalMode.TOP_DOWN).get(0);\r\n    fieldRotatingMap.setAccessible(true);\r\n    fieldRotatingMap.set(spoutExecutor, rotatingMap);\r\n    spoutExecutor.accept(addressedTuple);\r\n    Mockito.verify(rotatingMap, Mockito.times(1)).rotate();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testUnevenLoadOverTime",
  "sourceCode" : "@Test\r\npublic void testUnevenLoadOverTime() {\r\n    LoadAwareShuffleGrouping grouping = new LoadAwareShuffleGrouping();\r\n    WorkerTopologyContext context = mockContext(Arrays.asList(1, 2));\r\n    grouping.prepare(context, new GlobalStreamId(\"a\", \"default\"), Arrays.asList(1, 2));\r\n    double expectedOneWeight = 100.0;\r\n    double expectedTwoWeight = 100.0;\r\n    Map<Integer, Double> localLoad = new HashMap<>();\r\n    localLoad.put(1, 1.0);\r\n    localLoad.put(2, 0.0);\r\n    LoadMapping lm = new LoadMapping();\r\n    lm.setLocal(localLoad);\r\n    //First verify that if something has a high load it's distribution will drop over time\r\n    for (int i = 9; i >= 0; i--) {\r\n        grouping.refreshLoad(lm);\r\n        expectedOneWeight -= 10.0;\r\n        Map<Integer, Double> countByType = count(grouping.choices, grouping.rets);\r\n        LOG.info(\"contByType = {}\", countByType);\r\n        double expectedOnePercentage = expectedOneWeight / (expectedOneWeight + expectedTwoWeight);\r\n        double expectedTwoPercentage = expectedTwoWeight / (expectedOneWeight + expectedTwoWeight);\r\n        assertEquals(expectedOnePercentage, countByType.getOrDefault(1, 0.0) / grouping.getCapacity(), 0.01, \"i = \" + i);\r\n        assertEquals(expectedTwoPercentage, countByType.getOrDefault(2, 0.0) / grouping.getCapacity(), 0.01, \"i = \" + i);\r\n    }\r\n    //Now verify that when it is switched we can recover\r\n    localLoad.put(1, 0.0);\r\n    localLoad.put(2, 1.0);\r\n    lm.setLocal(localLoad);\r\n    while (expectedOneWeight < 100.0) {\r\n        grouping.refreshLoad(lm);\r\n        expectedOneWeight += 1.0;\r\n        expectedTwoWeight = Math.max(0.0, expectedTwoWeight - 10.0);\r\n        Map<Integer, Double> countByType = count(grouping.choices, grouping.rets);\r\n        LOG.info(\"contByType = {}\", countByType);\r\n        double expectedOnePercentage = expectedOneWeight / (expectedOneWeight + expectedTwoWeight);\r\n        double expectedTwoPercentage = expectedTwoWeight / (expectedOneWeight + expectedTwoWeight);\r\n        assertEquals(expectedOnePercentage, countByType.getOrDefault(1, 0.0) / grouping.getCapacity(), 0.01);\r\n        assertEquals(expectedTwoPercentage, countByType.getOrDefault(2, 0.0) / grouping.getCapacity(), 0.01);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testLoadAwareShuffleGroupingWithEvenLoadWithManyTargets",
  "sourceCode" : "@Test\r\npublic void testLoadAwareShuffleGroupingWithEvenLoadWithManyTargets() {\r\n    testLoadAwareShuffleGroupingWithEvenLoad(1000);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testLoadAwareShuffleGroupingWithEvenLoadWithLessTargets",
  "sourceCode" : "@Test\r\npublic void testLoadAwareShuffleGroupingWithEvenLoadWithLessTargets() {\r\n    testLoadAwareShuffleGroupingWithEvenLoad(7);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testLoadAwareShuffleGroupingWithEvenLoadMultiThreadedWithManyTargets",
  "sourceCode" : "@Test\r\npublic void testLoadAwareShuffleGroupingWithEvenLoadMultiThreadedWithManyTargets() throws ExecutionException, InterruptedException {\r\n    testLoadAwareShuffleGroupingWithEvenLoadMultiThreaded(1000);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testLoadAwareShuffleGroupingWithEvenLoadMultiThreadedWithLessTargets",
  "sourceCode" : "@Test\r\npublic void testLoadAwareShuffleGroupingWithEvenLoadMultiThreadedWithLessTargets() throws ExecutionException, InterruptedException {\r\n    testLoadAwareShuffleGroupingWithEvenLoadMultiThreaded(7);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testShuffleLoadEven",
  "sourceCode" : "@Test\r\npublic void testShuffleLoadEven() {\r\n    // port test-shuffle-load-even\r\n    LoadAwareCustomStreamGrouping shuffler = GrouperFactory.mkGrouper(mockContext(Lists.newArrayList(1, 2)), \"comp\", \"stream\", null, Grouping.shuffle(new NullStruct()), Lists.newArrayList(1, 2), Collections.emptyMap());\r\n    int numMessages = 100000;\r\n    int minPrCount = (int) (numMessages * (0.5 - ACCEPTABLE_MARGIN));\r\n    int maxPrCount = (int) (numMessages * (0.5 + ACCEPTABLE_MARGIN));\r\n    LoadMapping load = new LoadMapping();\r\n    Map<Integer, Double> loadInfoMap = new HashMap<>();\r\n    loadInfoMap.put(1, 0.0);\r\n    loadInfoMap.put(2, 0.0);\r\n    load.setLocal(loadInfoMap);\r\n    // force triggers building ring\r\n    shuffler.refreshLoad(load);\r\n    List<Object> data = Lists.newArrayList(1, 2);\r\n    int[] frequencies = new int[3];\r\n    for (int i = 0; i < numMessages; i++) {\r\n        List<Integer> tasks = shuffler.chooseTasks(1, data);\r\n        for (int task : tasks) {\r\n            frequencies[task]++;\r\n        }\r\n    }\r\n    int load1 = frequencies[1];\r\n    int load2 = frequencies[2];\r\n    LOG.info(\"Frequency info: load1 = {}, load2 = {}\", load1, load2);\r\n    assertTrue(load1 >= minPrCount);\r\n    assertTrue(load1 <= maxPrCount);\r\n    assertTrue(load2 >= minPrCount);\r\n    assertTrue(load2 <= maxPrCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testBenchmarkLoadAwareShuffleGroupingEvenLoad",
  "sourceCode" : "@Disabled\r\n@Test\r\npublic void testBenchmarkLoadAwareShuffleGroupingEvenLoad() {\r\n    final int numTasks = 10;\r\n    List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);\r\n    runSimpleBenchmark(new LoadAwareShuffleGrouping(), availableTaskIds, buildLocalTasksEvenLoadMapping(availableTaskIds));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testBenchmarkLoadAwareShuffleGroupingUnevenLoad",
  "sourceCode" : "@Disabled\r\n@Test\r\npublic void testBenchmarkLoadAwareShuffleGroupingUnevenLoad() {\r\n    final int numTasks = 10;\r\n    List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);\r\n    runSimpleBenchmark(new LoadAwareShuffleGrouping(), availableTaskIds, buildLocalTasksUnevenLoadMapping(availableTaskIds));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testBenchmarkLoadAwareShuffleGroupingEvenLoadAndMultiThreaded",
  "sourceCode" : "@Disabled\r\n@Test\r\npublic void testBenchmarkLoadAwareShuffleGroupingEvenLoadAndMultiThreaded() throws ExecutionException, InterruptedException {\r\n    final int numTasks = 10;\r\n    final int numThreads = 2;\r\n    List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);\r\n    runMultithreadedBenchmark(new LoadAwareShuffleGrouping(), availableTaskIds, buildLocalTasksEvenLoadMapping(availableTaskIds), numThreads);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testBenchmarkLoadAwareShuffleGroupingUnevenLoadAndMultiThreaded",
  "sourceCode" : "@Disabled\r\n@Test\r\npublic void testBenchmarkLoadAwareShuffleGroupingUnevenLoadAndMultiThreaded() throws ExecutionException, InterruptedException {\r\n    final int numTasks = 10;\r\n    final int numThreads = 2;\r\n    List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);\r\n    runMultithreadedBenchmark(new LoadAwareShuffleGrouping(), availableTaskIds, buildLocalTasksUnevenLoadMapping(availableTaskIds), numThreads);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\LoadAwareShuffleGroupingTest.java",
  "methodName" : "testLoadSwitching",
  "sourceCode" : "@Test\r\npublic void testLoadSwitching() {\r\n    LoadAwareShuffleGrouping grouping = new LoadAwareShuffleGrouping();\r\n    WorkerTopologyContext context = createLoadSwitchingContext();\r\n    grouping.prepare(context, new GlobalStreamId(\"a\", \"default\"), Arrays.asList(1, 2, 3));\r\n    // startup should default to worker local\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.WORKER_LOCAL, grouping.getCurrentScope());\r\n    // with high load, switch to host local\r\n    LoadMapping lm = createLoadMapping(1.0, 1.0, 1.0);\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.HOST_LOCAL, grouping.getCurrentScope());\r\n    // load remains high, switch to rack local\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.RACK_LOCAL, grouping.getCurrentScope());\r\n    // load remains high. switch to everything\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.EVERYTHING, grouping.getCurrentScope());\r\n    // lower load below low water threshold, but worker local load remains too high\r\n    // should switch to rack local\r\n    lm = createLoadMapping(0.2, 0.1, 0.1);\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.RACK_LOCAL, grouping.getCurrentScope());\r\n    // lower load continues, switch to host local\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.HOST_LOCAL, grouping.getCurrentScope());\r\n    // lower load continues, should NOT be able to switch to worker local yet\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.HOST_LOCAL, grouping.getCurrentScope());\r\n    // reduce load on local worker task, should switch to worker local\r\n    lm = createLoadMapping(0.1, 0.1, 0.1);\r\n    grouping.refreshLoad(lm);\r\n    assertEquals(LoadAwareShuffleGrouping.LocalityScope.WORKER_LOCAL, grouping.getCurrentScope());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\BalancedTargetSelectorTest.java",
  "methodName" : "classIsSerializable",
  "sourceCode" : "@Test\r\npublic void classIsSerializable() {\r\n    Utils.javaSerialize(targetSelector);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\BalancedTargetSelectorTest.java",
  "methodName" : "selectorReturnsTasksInAssignment",
  "sourceCode" : "@Test\r\npublic void selectorReturnsTasksInAssignment() {\r\n    // select tasks once more than the number of tasks available\r\n    for (int i = 0; i < TASK_LIST.length + 1; i++) {\r\n        int selectedTask = targetSelector.chooseTask(TASK_LIST);\r\n        assertThat(selectedTask, Matchers.in(Arrays.stream(TASK_LIST).boxed().collect(Collectors.toList())));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\BalancedTargetSelectorTest.java",
  "methodName" : "selectsTaskThatHasBeenUsedTheLeast",
  "sourceCode" : "@Test\r\npublic void selectsTaskThatHasBeenUsedTheLeast() {\r\n    // ensure that the first three tasks have been selected before\r\n    targetSelector.chooseTask(new int[] { TASK_LIST[0] });\r\n    targetSelector.chooseTask(new int[] { TASK_LIST[1] });\r\n    targetSelector.chooseTask(new int[] { TASK_LIST[2] });\r\n    // now, selecting from the full set should cause the fourth task to be chosen.\r\n    int selectedTask = targetSelector.chooseTask(TASK_LIST);\r\n    assertThat(selectedTask, equalTo(TASK_LIST[3]));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\PartialKeyGroupingTest.java",
  "methodName" : "testGroupingIsSerializable",
  "sourceCode" : "@Test\r\npublic void testGroupingIsSerializable() {\r\n    PartialKeyGrouping grouping = new PartialKeyGrouping(new Fields(\"some_field\"));\r\n    Utils.javaSerialize(grouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\PartialKeyGroupingTest.java",
  "methodName" : "testChooseTasks",
  "sourceCode" : "@Test\r\npublic void testChooseTasks() {\r\n    PartialKeyGrouping pkg = new PartialKeyGrouping();\r\n    pkg.prepare(null, null, Lists.newArrayList(0, 1, 2, 3, 4, 5));\r\n    Values message = new Values(\"key1\");\r\n    List<Integer> choice1 = pkg.chooseTasks(0, message);\r\n    assertThat(choice1.size(), is(1));\r\n    List<Integer> choice2 = pkg.chooseTasks(0, message);\r\n    assertThat(choice2, is(not(choice1)));\r\n    List<Integer> choice3 = pkg.chooseTasks(0, message);\r\n    assertThat(choice3, is(not(choice2)));\r\n    assertThat(choice3, is(choice1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\PartialKeyGroupingTest.java",
  "methodName" : "testChooseTasksWithoutConsecutiveTaskIds",
  "sourceCode" : "@Test\r\npublic void testChooseTasksWithoutConsecutiveTaskIds() {\r\n    PartialKeyGrouping pkg = new PartialKeyGrouping();\r\n    pkg.prepare(null, null, Lists.newArrayList(9, 8, 7, 1, 2, 3));\r\n    Values message = new Values(\"key1\");\r\n    List<Integer> choice1 = pkg.chooseTasks(0, message);\r\n    assertThat(choice1.size(), is(1));\r\n    List<Integer> choice2 = pkg.chooseTasks(0, message);\r\n    assertThat(choice2, is(not(choice1)));\r\n    List<Integer> choice3 = pkg.chooseTasks(0, message);\r\n    assertThat(choice3, is(not(choice2)));\r\n    assertThat(choice3, is(choice1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\PartialKeyGroupingTest.java",
  "methodName" : "testChooseTasksFields",
  "sourceCode" : "@Test\r\npublic void testChooseTasksFields() {\r\n    PartialKeyGrouping pkg = new PartialKeyGrouping(new Fields(\"test\"));\r\n    WorkerTopologyContext context = mock(WorkerTopologyContext.class);\r\n    when(context.getComponentOutputFields(any(GlobalStreamId.class))).thenReturn(new Fields(\"test\"));\r\n    pkg.prepare(context, mock(GlobalStreamId.class), Lists.newArrayList(0, 1, 2, 3, 4, 5));\r\n    Values message = new Values(\"key1\");\r\n    List<Integer> choice1 = pkg.chooseTasks(0, message);\r\n    assertThat(choice1.size(), is(1));\r\n    List<Integer> choice2 = pkg.chooseTasks(0, message);\r\n    assertThat(choice2, is(not(choice1)));\r\n    List<Integer> choice3 = pkg.chooseTasks(0, message);\r\n    assertThat(choice3, is(not(choice2)));\r\n    assertThat(choice3, is(choice1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\RandomTwoTaskAssignmentCreatorTest.java",
  "methodName" : "classIsSerializable",
  "sourceCode" : "@Test\r\npublic void classIsSerializable() throws Exception {\r\n    PartialKeyGrouping.AssignmentCreator assignmentCreator = new PartialKeyGrouping.RandomTwoTaskAssignmentCreator();\r\n    Utils.javaSerialize(assignmentCreator);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\RandomTwoTaskAssignmentCreatorTest.java",
  "methodName" : "returnsAssignmentOfExpectedSize",
  "sourceCode" : "@Test\r\npublic void returnsAssignmentOfExpectedSize() {\r\n    PartialKeyGrouping.AssignmentCreator assignmentCreator = new PartialKeyGrouping.RandomTwoTaskAssignmentCreator();\r\n    int[] assignedTasks = assignmentCreator.createAssignment(Lists.newArrayList(9, 8, 7, 6), GROUPING_KEY_ONE);\r\n    assertThat(assignedTasks.length, equalTo(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\RandomTwoTaskAssignmentCreatorTest.java",
  "methodName" : "returnsDifferentAssignmentForDifferentKeys",
  "sourceCode" : "@Test\r\npublic void returnsDifferentAssignmentForDifferentKeys() {\r\n    PartialKeyGrouping.AssignmentCreator assignmentCreator = new PartialKeyGrouping.RandomTwoTaskAssignmentCreator();\r\n    int[] assignmentOne = assignmentCreator.createAssignment(Lists.newArrayList(9, 8, 7, 6), GROUPING_KEY_ONE);\r\n    int[] assignmentTwo = assignmentCreator.createAssignment(Lists.newArrayList(9, 8, 7, 6), GROUPING_KEY_TWO);\r\n    assertThat(assignmentOne, not(equalTo(assignmentTwo)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\partialKeyGrouping\\RandomTwoTaskAssignmentCreatorTest.java",
  "methodName" : "returnsSameAssignmentForSameKey",
  "sourceCode" : "@Test\r\npublic void returnsSameAssignmentForSameKey() {\r\n    PartialKeyGrouping.AssignmentCreator assignmentCreator = new PartialKeyGrouping.RandomTwoTaskAssignmentCreator();\r\n    int[] assignmentOne = assignmentCreator.createAssignment(Lists.newArrayList(9, 8, 7, 6), GROUPING_KEY_ONE);\r\n    int[] assignmentOneAgain = assignmentCreator.createAssignment(Lists.newArrayList(9, 8, 7, 6), GROUPING_KEY_ONE);\r\n    assertThat(assignmentOne, equalTo(assignmentOneAgain));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\ShuffleGroupingTest.java",
  "methodName" : "testShuffleGrouping",
  "sourceCode" : "/**\r\n * Tests that we round robbin correctly using ShuffleGrouping implementation.\r\n */\r\n@Test\r\npublic void testShuffleGrouping() {\r\n    final int numTasks = 6;\r\n    final ShuffleGrouping grouper = new ShuffleGrouping();\r\n    // Task Id not used, so just pick a static value\r\n    final int inputTaskId = 100;\r\n    // Define our taskIds\r\n    final List<Integer> availableTaskIds = Lists.newArrayList();\r\n    for (int i = 0; i < numTasks; i++) {\r\n        availableTaskIds.add(i);\r\n    }\r\n    WorkerTopologyContext context = mock(WorkerTopologyContext.class);\r\n    grouper.prepare(context, null, availableTaskIds);\r\n    // Keep track of how many times we see each taskId\r\n    int[] taskCounts = new int[numTasks];\r\n    for (int i = 1; i <= 30000; i++) {\r\n        List<Integer> taskIds = grouper.chooseTasks(inputTaskId, Lists.newArrayList());\r\n        // Validate a single task id return\r\n        assertNotNull(taskIds, \"Not null taskId list returned\");\r\n        assertEquals(1, taskIds.size(), \"Single task Id not returned\");\r\n        int taskId = taskIds.get(0);\r\n        assertTrue(taskId >= 0 && taskId < numTasks, \"TaskId should exist\");\r\n        taskCounts[taskId]++;\r\n    }\r\n    for (int i = 0; i < numTasks; i++) {\r\n        assertEquals(5000, taskCounts[i], \"Distribution should be even for all nodes\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\grouping\\ShuffleGroupingTest.java",
  "methodName" : "testShuffleGroupMultiThreaded",
  "sourceCode" : "/**\r\n * Tests that we round robbin correctly with multiple threads using ShuffleGrouping implementation.\r\n */\r\n@Test\r\npublic void testShuffleGroupMultiThreaded() throws InterruptedException, ExecutionException {\r\n    final int numTasks = 6;\r\n    final int groupingExecutionsPerThread = 30000;\r\n    final int numThreads = 10;\r\n    final CustomStreamGrouping grouper = new ShuffleGrouping();\r\n    // Task Id not used, so just pick a static value\r\n    final int inputTaskId = 100;\r\n    // Define our taskIds - the test expects these to be incrementing by one up from zero\r\n    final List<Integer> availableTaskIds = Lists.newArrayList();\r\n    for (int i = 0; i < numTasks; i++) {\r\n        availableTaskIds.add(i);\r\n    }\r\n    final WorkerTopologyContext context = mock(WorkerTopologyContext.class);\r\n    // Call prepare with our available taskIds\r\n    grouper.prepare(context, null, availableTaskIds);\r\n    List<Callable<int[]>> threadTasks = Lists.newArrayList();\r\n    for (int x = 0; x < numThreads; x++) {\r\n        Callable<int[]> threadTask = () -> {\r\n            int[] taskCounts = new int[availableTaskIds.size()];\r\n            for (int i = 1; i <= groupingExecutionsPerThread; i++) {\r\n                List<Integer> taskIds = grouper.chooseTasks(inputTaskId, Lists.newArrayList());\r\n                // Validate a single task id return\r\n                assertNotNull(taskIds, \"Not null taskId list returned\");\r\n                assertEquals(1, taskIds.size(), \"Single task Id not returned\");\r\n                int taskId = taskIds.get(0);\r\n                assertTrue(taskId >= 0 && taskId < availableTaskIds.size(), \"TaskId should exist\");\r\n                taskCounts[taskId]++;\r\n            }\r\n            return taskCounts;\r\n        };\r\n        // Add to our collection.\r\n        threadTasks.add(threadTask);\r\n    }\r\n    ExecutorService executor = Executors.newFixedThreadPool(threadTasks.size());\r\n    List<Future<int[]>> taskResults = executor.invokeAll(threadTasks);\r\n    // Wait for all tasks to complete\r\n    int[] taskIdTotals = new int[numTasks];\r\n    for (Future<int[]> taskResult : taskResults) {\r\n        while (!taskResult.isDone()) {\r\n            Thread.sleep(1000);\r\n        }\r\n        int[] taskDistributions = taskResult.get();\r\n        for (int i = 0; i < taskDistributions.length; i++) {\r\n            taskIdTotals[i] += taskDistributions[i];\r\n        }\r\n    }\r\n    for (int i = 0; i < numTasks; i++) {\r\n        assertEquals(numThreads * groupingExecutionsPerThread / numTasks, taskIdTotals[i]);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\messaging\\DeserializingConnectionCallbackTest.java",
  "methodName" : "testUpdateMetricsConfigOff",
  "sourceCode" : "@Test\r\npublic void testUpdateMetricsConfigOff() {\r\n    Map<String, Object> config = new HashMap<>();\r\n    config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.FALSE);\r\n    DeserializingConnectionCallback withoutMetrics = new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(WorkerState.ILocalTransferCallback.class));\r\n    // Metrics are off, verify null\r\n    assertNull(withoutMetrics.getValueAndReset());\r\n    // Add our messages and verify no metrics are recorded\r\n    withoutMetrics.updateMetrics(123, message);\r\n    assertNull(withoutMetrics.getValueAndReset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\messaging\\DeserializingConnectionCallbackTest.java",
  "methodName" : "testUpdateMetricsConfigOn",
  "sourceCode" : "@Test\r\npublic void testUpdateMetricsConfigOn() {\r\n    Map<String, Object> config = new HashMap<>();\r\n    config.put(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS, Boolean.TRUE);\r\n    DeserializingConnectionCallback withMetrics = new DeserializingConnectionCallback(config, mock(GeneralTopologyContext.class), mock(WorkerState.ILocalTransferCallback.class));\r\n    // Starting empty\r\n    Object metrics = withMetrics.getValueAndReset();\r\n    assertTrue(metrics instanceof Map);\r\n    assertTrue(((Map<?, ?>) metrics).isEmpty());\r\n    // Add messages\r\n    withMetrics.updateMetrics(123, message);\r\n    withMetrics.updateMetrics(123, message);\r\n    // Verify recorded messages size metrics\r\n    metrics = withMetrics.getValueAndReset();\r\n    assertTrue(metrics instanceof Map);\r\n    assertEquals(6L, ((Map<?, ?>) metrics).get(\"123-456\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\filter\\FilterByMetricNameTest.java",
  "methodName" : "testWhitelist",
  "sourceCode" : "@Test\r\npublic void testWhitelist() {\r\n    List<String> whitelistPattern = Lists.newArrayList(\"^metric\\\\.\", \"test\\\\.hello\\\\.[0-9]+\");\r\n    FilterByMetricName sut = new FilterByMetricName(whitelistPattern, null);\r\n    Map<String, Boolean> testMetricNamesAndExpected = Maps.newHashMap();\r\n    testMetricNamesAndExpected.put(\"storm.metric.hello\", false);\r\n    testMetricNamesAndExpected.put(\"test.hello.world\", false);\r\n    testMetricNamesAndExpected.put(\"test.hello.123\", true);\r\n    testMetricNamesAndExpected.put(\"test.metric.world\", false);\r\n    testMetricNamesAndExpected.put(\"metric.world\", true);\r\n    assertTests(sut, testMetricNamesAndExpected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\filter\\FilterByMetricNameTest.java",
  "methodName" : "testBlacklist",
  "sourceCode" : "@Test\r\npublic void testBlacklist() {\r\n    List<String> blacklistPattern = Lists.newArrayList(\"^__\", \"test\\\\.\");\r\n    FilterByMetricName sut = new FilterByMetricName(null, blacklistPattern);\r\n    Map<String, Boolean> testMetricNamesAndExpected = Maps.newHashMap();\r\n    testMetricNamesAndExpected.put(\"__storm.metric.hello\", false);\r\n    testMetricNamesAndExpected.put(\"storm.metric.__hello\", true);\r\n    testMetricNamesAndExpected.put(\"test.hello.world\", false);\r\n    testMetricNamesAndExpected.put(\"storm.test.123\", false);\r\n    testMetricNamesAndExpected.put(\"metric.world\", true);\r\n    assertTests(sut, testMetricNamesAndExpected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\filter\\FilterByMetricNameTest.java",
  "methodName" : "testBothWhitelistAndBlacklistAreSpecified",
  "sourceCode" : "@Test\r\npublic void testBothWhitelistAndBlacklistAreSpecified() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        List<String> whitelistPattern = Lists.newArrayList(\"^metric\\\\.\", \"test\\\\.hello\\\\.[0-9]+\");\r\n        List<String> blacklistPattern = Lists.newArrayList(\"^__\", \"test\\\\.\");\r\n        new FilterByMetricName(whitelistPattern, blacklistPattern);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\filter\\FilterByMetricNameTest.java",
  "methodName" : "testNoneIsSpecified",
  "sourceCode" : "@Test\r\npublic void testNoneIsSpecified() {\r\n    FilterByMetricName sut = new FilterByMetricName(null, null);\r\n    Map<String, Boolean> testMetricNamesAndExpected = Maps.newHashMap();\r\n    testMetricNamesAndExpected.put(\"__storm.metric.hello\", true);\r\n    testMetricNamesAndExpected.put(\"storm.metric.__hello\", true);\r\n    testMetricNamesAndExpected.put(\"test.hello.world\", true);\r\n    testMetricNamesAndExpected.put(\"storm.test.123\", true);\r\n    testMetricNamesAndExpected.put(\"metric.world\", true);\r\n    assertTests(sut, testMetricNamesAndExpected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\CountStatTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    long time = 0L;\r\n    CountStat count = new CountStat(10, time);\r\n    while (time < TEN_MIN) {\r\n        //For this part of the test we interleave the different rotation types.\r\n        count.incBy(50);\r\n        time += THIRTY_SEC / 2;\r\n        count.rotateSched(time);\r\n        count.incBy(50);\r\n        time += THIRTY_SEC / 2;\r\n    }\r\n    long val = 100 * TEN_MIN / THIRTY_SEC;\r\n    Map<String, Long> expected = new HashMap<>();\r\n    expected.put(\"600\", val);\r\n    expected.put(\"10800\", val);\r\n    expected.put(\"86400\", val);\r\n    expected.put(\":all-time\", val);\r\n    assertEquals(expected, count.getTimeCounts(time));\r\n    while (time < THREE_HOUR) {\r\n        count.incBy(100);\r\n        time += THIRTY_SEC;\r\n    }\r\n    val = 100 * THREE_HOUR / THIRTY_SEC;\r\n    expected = new HashMap<>();\r\n    expected.put(\"600\", 100 * TEN_MIN / THIRTY_SEC);\r\n    expected.put(\"10800\", val);\r\n    expected.put(\"86400\", val);\r\n    expected.put(\":all-time\", val);\r\n    assertEquals(expected, count.getTimeCounts(time));\r\n    while (time < ONE_DAY) {\r\n        count.incBy(100);\r\n        time += THIRTY_SEC;\r\n    }\r\n    val = 100 * ONE_DAY / THIRTY_SEC;\r\n    expected = new HashMap<>();\r\n    expected.put(\"600\", 100 * TEN_MIN / THIRTY_SEC);\r\n    expected.put(\"10800\", 100 * THREE_HOUR / THIRTY_SEC);\r\n    expected.put(\"86400\", val);\r\n    expected.put(\":all-time\", val);\r\n    assertEquals(expected, count.getTimeCounts(time));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\LatencyStatTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() {\r\n    long time = 0L;\r\n    LatencyStat lat = new LatencyStat(10, time);\r\n    while (time < TEN_MIN_IN_MS) {\r\n        lat.record(100);\r\n        time += THIRTY_SEC_IN_MS;\r\n        assertEquals(100.0, (Double) lat.getValueAndReset(time), 0.01);\r\n    }\r\n    Map<String, Double> found = lat.getTimeLatAvg(time);\r\n    assertEquals(4, found.size());\r\n    assertEquals(100.0, found.get(\"600\"), 0.01);\r\n    assertEquals(100.0, found.get(\"10800\"), 0.01);\r\n    assertEquals(100.0, found.get(\"86400\"), 0.01);\r\n    assertEquals(100.0, found.get(\":all-time\"), 0.01);\r\n    while (time < THREE_HOUR_IN_MS) {\r\n        lat.record(200);\r\n        time += THIRTY_SEC_IN_MS;\r\n        assertEquals(200.0, (Double) lat.getValueAndReset(time), 0.01);\r\n    }\r\n    double expected = ((100.0 * TEN_MIN_IN_MS / THIRTY_SEC_IN_MS) + (200.0 * (THREE_HOUR_IN_MS - TEN_MIN_IN_MS) / THIRTY_SEC_IN_MS)) / (THREE_HOUR_IN_MS / THIRTY_SEC_IN_MS);\r\n    found = lat.getTimeLatAvg(time);\r\n    assertEquals(4, found.size());\r\n    //flushed the buffers completely\r\n    assertEquals(200.0, found.get(\"600\"), 0.01);\r\n    assertEquals(expected, found.get(\"10800\"), 0.01);\r\n    assertEquals(expected, found.get(\"86400\"), 0.01);\r\n    assertEquals(expected, found.get(\":all-time\"), 0.01);\r\n    while (time < ONE_DAY_IN_MS) {\r\n        lat.record(300);\r\n        time += THIRTY_SEC_IN_MS;\r\n        assertEquals(300.0, (Double) lat.getValueAndReset(time), 0.01);\r\n    }\r\n    expected = ((100.0 * TEN_MIN_IN_MS / THIRTY_SEC_IN_MS) + (200.0 * (THREE_HOUR_IN_MS - TEN_MIN_IN_MS) / THIRTY_SEC_IN_MS) + (300.0 * (ONE_DAY_IN_MS - THREE_HOUR_IN_MS) / THIRTY_SEC_IN_MS)) / (ONE_DAY_IN_MS / THIRTY_SEC_IN_MS);\r\n    found = lat.getTimeLatAvg(time);\r\n    assertEquals(4, found.size());\r\n    //flushed the buffers completely\r\n    assertEquals(300.0, found.get(\"600\"), 0.01);\r\n    assertEquals(300.0, found.get(\"10800\"), 0.01);\r\n    assertEquals(expected, found.get(\"86400\"), 0.01);\r\n    assertEquals(expected, found.get(\":all-time\"), 0.01);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\RateTrackerTest.java",
  "methodName" : "testExactRate",
  "sourceCode" : "@Test\r\npublic void testExactRate() {\r\n    //This test is in two phases.  The first phase fills up the 10 buckets with 10 tuples each\r\n    // We purposely simulate a 1 second bucket size so the rate will always be 10 per second.\r\n    final long interval = 1000L;\r\n    long time = 0L;\r\n    RateTracker rt = new RateTracker(10000, 10, time);\r\n    double[] expected = new double[] { 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0 };\r\n    for (int i = 0; i < expected.length; i++) {\r\n        double exp = expected[i];\r\n        rt.notify(10);\r\n        time += interval;\r\n        double actual = rt.reportRate(time);\r\n        rt.forceRotate(1, interval);\r\n        assertEquals(exp, actual, 0.00001, \"Expected rate on iteration \" + i + \" is wrong.\");\r\n    }\r\n    //In the second part of the test the rate doubles to 20 per second but the rate tracker\r\n    // increases its result slowly as we push the 10 tuples per second buckets out and replace them\r\n    // with 20 tuples per second.\r\n    expected = new double[] { 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0 };\r\n    for (int i = 0; i < expected.length; i++) {\r\n        double exp = expected[i];\r\n        rt.notify(20);\r\n        time += interval;\r\n        double actual = rt.reportRate(time);\r\n        rt.forceRotate(1, interval);\r\n        assertEquals(exp, actual, 0.00001, \"Expected rate on iteration \" + i + \" is wrong.\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\RateTrackerTest.java",
  "methodName" : "testEclipsedAllWindows",
  "sourceCode" : "@Test\r\npublic void testEclipsedAllWindows() {\r\n    long time = 0;\r\n    RateTracker rt = new RateTracker(10000, 10, time);\r\n    rt.notify(10);\r\n    rt.forceRotate(10, 1000L);\r\n    assertEquals(0.0, rt.reportRate(10000L), 0.00001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\RateTrackerTest.java",
  "methodName" : "testEclipsedOneWindow",
  "sourceCode" : "@Test\r\npublic void testEclipsedOneWindow() {\r\n    long time = 0;\r\n    RateTracker rt = new RateTracker(10000, 10, time);\r\n    rt.notify(1);\r\n    double r1 = rt.reportRate(1000L);\r\n    rt.forceRotate(1, 1000L);\r\n    rt.notify(1);\r\n    double r2 = rt.reportRate(2000L);\r\n    assertEquals(r1, r2, 0.00001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\internal\\RateTrackerTest.java",
  "methodName" : "testEclipsedNineWindows",
  "sourceCode" : "@Test\r\npublic void testEclipsedNineWindows() {\r\n    long time = 0;\r\n    RateTracker rt = new RateTracker(10000, 10, time);\r\n    rt.notify(1);\r\n    double r1 = rt.reportRate(1000);\r\n    rt.forceRotate(9, 1000);\r\n    rt.notify(9);\r\n    double r2 = rt.reportRate(10000);\r\n    assertEquals(r1, r2, 0.00001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\util\\DataPointExpanderTest.java",
  "methodName" : "testExpandDataPointWithExpandDisabled",
  "sourceCode" : "@Test\r\npublic void testExpandDataPointWithExpandDisabled() {\r\n    DataPointExpander populator = new DataPointExpander(false, \".\");\r\n    Map<String, Object> value = getDummyMetricMapValue();\r\n    IMetricsConsumer.DataPoint point = new IMetricsConsumer.DataPoint(\"test\", value);\r\n    Collection<IMetricsConsumer.DataPoint> expandedDataPoints = populator.expandDataPoint(point);\r\n    assertEquals(1, expandedDataPoints.size());\r\n    assertEquals(point, expandedDataPoints.iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\util\\DataPointExpanderTest.java",
  "methodName" : "testExpandDataPointsWithExpandDisabled",
  "sourceCode" : "@Test\r\npublic void testExpandDataPointsWithExpandDisabled() {\r\n    DataPointExpander populator = new DataPointExpander(false, \".\");\r\n    Map<String, Object> value = getDummyMetricMapValue();\r\n    IMetricsConsumer.DataPoint point = new IMetricsConsumer.DataPoint(\"test\", value);\r\n    Collection<IMetricsConsumer.DataPoint> expandedDataPoints = populator.expandDataPoints(Collections.singletonList(point));\r\n    assertEquals(1, expandedDataPoints.size());\r\n    assertEquals(point, expandedDataPoints.iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\util\\DataPointExpanderTest.java",
  "methodName" : "testExpandDataPointWithVariousKindOfMetrics",
  "sourceCode" : "@Test\r\npublic void testExpandDataPointWithVariousKindOfMetrics() {\r\n    DataPointExpander populator = new DataPointExpander(true, \".\");\r\n    IMetricsConsumer.DataPoint point = new IMetricsConsumer.DataPoint(\"point\", getDummyMetricMapValue());\r\n    Collection<IMetricsConsumer.DataPoint> expandedDataPoints = populator.expandDataPoints(Collections.singletonList(point));\r\n    assertEquals(4, expandedDataPoints.size());\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point.a\", 1.0)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point.b\", 2.5)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point.c\", null)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point.d\", \"hello\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\util\\DataPointExpanderTest.java",
  "methodName" : "testExpandDataPointsWithVariousKindOfMetrics",
  "sourceCode" : "@Test\r\npublic void testExpandDataPointsWithVariousKindOfMetrics() {\r\n    DataPointExpander populator = new DataPointExpander(true, \":\");\r\n    IMetricsConsumer.DataPoint point1 = new IMetricsConsumer.DataPoint(\"point1\", 2.5);\r\n    IMetricsConsumer.DataPoint point2 = new IMetricsConsumer.DataPoint(\"point2\", getDummyMetricMapValue());\r\n    Collection<IMetricsConsumer.DataPoint> expandedDataPoints = populator.expandDataPoints(Lists.newArrayList(point1, point2));\r\n    // 1 for point1, 4 for point2\r\n    assertEquals(5, expandedDataPoints.size());\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point1\", 2.5)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point2:a\", 1.0)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point2:b\", 2.5)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point2:c\", null)));\r\n    assertTrue(expandedDataPoints.contains(new IMetricsConsumer.DataPoint(\"point2:d\", \"hello\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metric\\util\\DataPointExpanderTest.java",
  "methodName" : "testExpandDataPointWithNullValueMetric",
  "sourceCode" : "@Test\r\npublic void testExpandDataPointWithNullValueMetric() {\r\n    DataPointExpander populator = new DataPointExpander(true, \".\");\r\n    IMetricsConsumer.DataPoint point = new IMetricsConsumer.DataPoint(\"point\", null);\r\n    Collection<IMetricsConsumer.DataPoint> expandedDataPoints = populator.expandDataPoint(point);\r\n    assertEquals(0, expandedDataPoints.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\metrics2\\RollingAverageGaugeTest.java",
  "methodName" : "testAverage",
  "sourceCode" : "@Test\r\npublic void testAverage() {\r\n    RollingAverageGauge gauge = new RollingAverageGauge();\r\n    assertEquals(0.0, gauge.getValue(), 0.001);\r\n    gauge.addValue(30);\r\n    assertEquals(10.0, gauge.getValue(), 0.001);\r\n    gauge.addValue(30);\r\n    assertEquals(20.0, gauge.getValue(), 0.001);\r\n    gauge.addValue(30);\r\n    assertEquals(30.0, gauge.getValue(), 0.001);\r\n    gauge.addValue(90);\r\n    assertEquals(50.0, gauge.getValue(), 0.001);\r\n    gauge.addValue(0);\r\n    assertEquals(40.0, gauge.getValue(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\nimbus\\NimbusInfoTest.java",
  "methodName" : "parseOnePort",
  "sourceCode" : "@Test\r\nvoid parseOnePort() {\r\n    NimbusInfo nimbusInfo = NimbusInfo.parse(\"nimbus.com:6627\");\r\n    assertEquals(6627, nimbusInfo.getPort());\r\n    assertEquals(\"nimbus.com\", nimbusInfo.getHost());\r\n    assertEquals(0, nimbusInfo.getTlsPort());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\nimbus\\NimbusInfoTest.java",
  "methodName" : "parseTwoPort",
  "sourceCode" : "@Test\r\nvoid parseTwoPort() {\r\n    NimbusInfo nimbusInfo = NimbusInfo.parse(\"nimbus.com:6627:6628\");\r\n    assertEquals(6627, nimbusInfo.getPort());\r\n    assertEquals(\"nimbus.com\", nimbusInfo.getHost());\r\n    assertEquals(6628, nimbusInfo.getTlsPort());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\nimbus\\NimbusInfoTest.java",
  "methodName" : "parseInvalidTlsPort",
  "sourceCode" : "@Test\r\nvoid parseInvalidTlsPort() {\r\n    Exception exception = assertThrows(RuntimeException.class, () -> {\r\n        NimbusInfo.parse(\"nimbus.com:6627:abc\");\r\n    });\r\n    String expectedMessage = \"nimbusInfo should have format of host:port:tlsPort or host:port\";\r\n    String actualMessage = exception.getMessage();\r\n    assertTrue(actualMessage.contains(expectedMessage));\r\n    exception = assertThrows(RuntimeException.class, () -> {\r\n        NimbusInfo.parse(\"nimbus.com:6627:\");\r\n    });\r\n    actualMessage = exception.getMessage();\r\n    assertTrue(actualMessage.contains(expectedMessage));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testSetWorkerHb",
  "sourceCode" : "@Test\r\npublic void testSetWorkerHb() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.SEND_PULSE_RESPONSE, null);\r\n    stateStorage.set_worker_hb(\"/foo\", \"data\".getBytes(StandardCharsets.UTF_8), null);\r\n    verify(clientMock).send(hbMessageCaptor.capture());\r\n    HBMessage sent = hbMessageCaptor.getValue();\r\n    HBPulse pulse = sent.get_data().get_pulse();\r\n    assertEquals(HBServerMessageType.SEND_PULSE, sent.get_type());\r\n    assertEquals(\"/foo\", pulse.get_id());\r\n    assertEquals(\"data\", new String(pulse.get_details(), StandardCharsets.UTF_8));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testSetWorkerHbResponseType",
  "sourceCode" : "@Test\r\npublic void testSetWorkerHbResponseType() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.SEND_PULSE, null);\r\n    assertThrows(RuntimeException.class, () -> stateStorage.set_worker_hb(\"/foo\", \"data\".getBytes(StandardCharsets.UTF_8), null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testDeleteWorkerHb",
  "sourceCode" : "@Test\r\npublic void testDeleteWorkerHb() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.DELETE_PATH_RESPONSE, null);\r\n    stateStorage.delete_worker_hb(\"/foo/bar\");\r\n    verify(clientMock).send(hbMessageCaptor.capture());\r\n    HBMessage sent = hbMessageCaptor.getValue();\r\n    assertEquals(HBServerMessageType.DELETE_PATH, sent.get_type());\r\n    assertEquals(\"/foo/bar\", sent.get_data().get_path());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testDeleteWorkerHbResponseType",
  "sourceCode" : "@Test\r\npublic void testDeleteWorkerHbResponseType() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.DELETE_PATH, null);\r\n    assertThrows(RuntimeException.class, () -> stateStorage.delete_worker_hb(\"/foo/bar\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testGetWorkerHb",
  "sourceCode" : "@Test\r\npublic void testGetWorkerHb() throws Exception {\r\n    HBPulse hbPulse = new HBPulse();\r\n    hbPulse.set_id(\"/foo\");\r\n    ClusterWorkerHeartbeat cwh = new ClusterWorkerHeartbeat(\"some-storm-id\", new HashMap<>(), 1, 1);\r\n    hbPulse.set_details(Utils.serialize(cwh));\r\n    createPaceMakerStateStorage(HBServerMessageType.GET_PULSE_RESPONSE, HBMessageData.pulse(hbPulse));\r\n    stateStorage.get_worker_hb(\"/foo\", false);\r\n    verify(clientMock).send(hbMessageCaptor.capture());\r\n    HBMessage sent = hbMessageCaptor.getValue();\r\n    assertEquals(HBServerMessageType.GET_PULSE, sent.get_type());\r\n    assertEquals(\"/foo\", sent.get_data().get_path());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testGetWorkerHbBadResponse",
  "sourceCode" : "@Test\r\npublic void testGetWorkerHbBadResponse() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.GET_PULSE, null);\r\n    assertThrows(RuntimeException.class, () -> stateStorage.get_worker_hb(\"/foo\", false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testGetWorkerHbBadData",
  "sourceCode" : "@Test\r\npublic void testGetWorkerHbBadData() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.GET_PULSE_RESPONSE, null);\r\n    assertThrows(RuntimeException.class, () -> stateStorage.get_worker_hb(\"/foo\", false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testGetWorkerHbChildren",
  "sourceCode" : "@Test\r\npublic void testGetWorkerHbChildren() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, HBMessageData.nodes(new HBNodes()));\r\n    stateStorage.get_worker_hb_children(\"/foo\", false);\r\n    verify(clientMock).send(hbMessageCaptor.capture());\r\n    HBMessage sent = hbMessageCaptor.getValue();\r\n    assertEquals(HBServerMessageType.GET_ALL_NODES_FOR_PATH, sent.get_type());\r\n    assertEquals(\"/foo\", sent.get_data().get_path());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\PaceMakerStateStorageFactoryTest.java",
  "methodName" : "testGetWorkerHbChildrenBadData",
  "sourceCode" : "@Test\r\npublic void testGetWorkerHbChildrenBadData() throws Exception {\r\n    createPaceMakerStateStorage(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, null);\r\n    assertThrows(RuntimeException.class, () -> stateStorage.get_worker_hb_children(\"/foo\", false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_partial_authorization",
  "sourceCode" : "@Test\r\npublic void test_partial_authorization() {\r\n    assertFalse(isPermitted(strictHandler, ReqContext.context(), \"execute\", partialFunction), \"Did not deny execute to unauthorized user\");\r\n    assertTrue(isPermitted(strictHandler, aliceKerbContext, \"execute\", partialFunction), \"Did not allow execute to authorized kerb user for correct function\");\r\n    assertFalse(isPermitted(strictHandler, aliceKerbContext, \"fetchRequest\", partialFunction), \"Did not deny fetchRequest to unauthorized user for correct function\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_client_authorization_strict",
  "sourceCode" : "@Test\r\npublic void test_client_authorization_strict() {\r\n    assertFalse(isPermitted(strictHandler, ReqContext.context(), \"execute\", function), \"Did not deny execute to unauthorized user\");\r\n    assertFalse(isPermitted(strictHandler, aliceContext, \"execute\", wrongFunction), \"Did not deny execute to valid user for incorrect function\");\r\n    assertTrue(isPermitted(strictHandler, aliceKerbContext, \"execute\", function), \"Did not allow execute to authorized kerb user for correct function\");\r\n    assertTrue(isPermitted(strictHandler, aliceContext, \"execute\", function), \"Did not allow execute to authorized user for correct function\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_client_authorization_permissive",
  "sourceCode" : "@Test\r\npublic void test_client_authorization_permissive() {\r\n    assertFalse(isPermitted(permissiveHandler, ReqContext.context(), \"execute\", function), \"Did not deny execute to unauthorized user for correct function\");\r\n    assertTrue(isPermitted(permissiveHandler, aliceContext, \"execute\", wrongFunction), \"Did not allow execute for user for incorrect function when permissive\");\r\n    assertTrue(isPermitted(permissiveHandler, aliceKerbContext, \"execute\", wrongFunction), \"Did not allow execute for user for incorrect function when permissive\");\r\n    assertTrue(isPermitted(permissiveHandler, bobContext, \"execute\", function), \"Did not allow execute to authorized user for correct function\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_invocation_authorization_strict",
  "sourceCode" : "@Test\r\npublic void test_invocation_authorization_strict() {\r\n    for (String operation : new String[] { \"fetchRequest\", \"failRequest\", \"result\" }) {\r\n        assertFalse(isPermitted(strictHandler, aliceContext, operation, function), \"Did not deny \" + operation + \" to unauthorized user for correct function\");\r\n        assertFalse(isPermitted(strictHandler, charlieContext, operation, wrongFunction), \"Did not deny \" + operation + \" to user for incorrect function when strict\");\r\n        assertTrue(isPermitted(strictHandler, charlieContext, operation, function), \"Did not allow \" + operation + \" to authorized user for correct function\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_invocation_authorization_permissive",
  "sourceCode" : "@Test\r\npublic void test_invocation_authorization_permissive() {\r\n    for (String operation : new String[] { \"fetchRequest\", \"failRequest\", \"result\" }) {\r\n        assertFalse(isPermitted(permissiveHandler, bobContext, operation, function), \"Did not deny \" + operation + \" to unauthorized user for correct function\");\r\n        assertTrue(isPermitted(permissiveHandler, charlieContext, operation, wrongFunction), \"Did not allow \" + operation + \" to user for incorrect function when permissive\");\r\n        assertTrue(isPermitted(permissiveHandler, charlieContext, operation, function), \"Did not allow \" + operation + \" to authorized user\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_deny_when_no_function_given",
  "sourceCode" : "@Test\r\npublic void test_deny_when_no_function_given() {\r\n    assertFalse(strictHandler.permit(aliceContext, \"execute\", new HashMap<>()));\r\n    assertFalse(isPermitted(strictHandler, aliceContext, \"execute\", null));\r\n    assertFalse(permissiveHandler.permit(bobContext, \"execute\", new HashMap<>()));\r\n    assertFalse(isPermitted(permissiveHandler, bobContext, \"execute\", null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_deny_when_invalid_user_given",
  "sourceCode" : "@Test\r\npublic void test_deny_when_invalid_user_given() {\r\n    assertFalse(isPermitted(strictHandler, Mockito.mock(ReqContext.class), \"execute\", function));\r\n    assertFalse(isPermitted(strictHandler, null, \"execute\", function));\r\n    assertFalse(isPermitted(permissiveHandler, Mockito.mock(ReqContext.class), \"execute\", function));\r\n    assertFalse(isPermitted(permissiveHandler, null, \"execute\", function));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_read_acl_no_values",
  "sourceCode" : "/**\r\n * {@link DRPCSimpleACLAuthorizer} should still work even if {@link Config#DRPC_AUTHORIZER_ACL} has no values.\r\n * @throws IOException if there is any issue with creating or writing the temp file.\r\n */\r\n@Test\r\npublic void test_read_acl_no_values() throws IOException {\r\n    DRPCSimpleACLAuthorizer authorizer = new DRPCSimpleACLAuthorizer();\r\n    File tempFile = Files.createTempFile(\"drpcacl\", \".yaml\").toFile();\r\n    tempFile.deleteOnExit();\r\n    BufferedWriter writer = new BufferedWriter(new FileWriter(tempFile));\r\n    writer.write(\"drpc.authorizer.acl:\");\r\n    writer.close();\r\n    authorizer.prepare(ImmutableMap.of(Config.DRPC_AUTHORIZER_ACL_STRICT, true, Config.DRPC_AUTHORIZER_ACL_FILENAME, tempFile.toString(), Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, KerberosPrincipalToLocal.class.getName()));\r\n    Map<String, DRPCSimpleACLAuthorizer.AclFunctionEntry> acl = authorizer.readAclFromConfig();\r\n    assertEquals(0, acl.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\DRPCSimpleACLAuthorizerTest.java",
  "methodName" : "test_read_acl_empty_file",
  "sourceCode" : "/**\r\n * The file of {@link Config#DRPC_AUTHORIZER_ACL_FILENAME} can not be empty.\r\n * @throws IOException if there is any issue with creating the temp file.\r\n */\r\n@Test\r\npublic void test_read_acl_empty_file() throws IOException {\r\n    DRPCSimpleACLAuthorizer authorizer = new DRPCSimpleACLAuthorizer();\r\n    File tempFile = Files.createTempFile(\"drpcacl\", \".yaml\").toFile();\r\n    tempFile.deleteOnExit();\r\n    authorizer.prepare(ImmutableMap.of(Config.DRPC_AUTHORIZER_ACL_STRICT, true, Config.DRPC_AUTHORIZER_ACL_FILENAME, tempFile.toString(), Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, KerberosPrincipalToLocal.class.getName()));\r\n    Exception exception = assertThrows(RuntimeException.class, authorizer::readAclFromConfig);\r\n    assertTrue(exception.getMessage().contains(\"doesn't have any valid storm configs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\SimpleACLAuthorizerTest.java",
  "methodName" : "SimpleACLUserAuthTest",
  "sourceCode" : "@Test\r\npublic void SimpleACLUserAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    Collection<String> adminUserSet = new HashSet<>(Collections.singletonList(\"admin\"));\r\n    Collection<String> supervisorUserSet = new HashSet<>(Collections.singletonList(\"supervisor\"));\r\n    clusterConf.put(Config.NIMBUS_ADMINS, adminUserSet);\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, supervisorUserSet);\r\n    IAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    Subject adminUser = createSubject(\"admin\");\r\n    Subject supervisorUser = createSubject(\"supervisor\");\r\n    Subject userA = createSubject(\"user-a\");\r\n    Subject userB = createSubject(\"user-b\");\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"submitTopology\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"submitTopology\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"submitTopology\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userB), \"submitTopology\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"fileUpload\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"fileUpload\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"fileUpload\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userB), \"fileUpload\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getNimbusConf\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getNimbusConf\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getNimbusConf\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userB), \"getNimbusConf\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getClusterInfo\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getClusterInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getClusterInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userB), \"getClusterInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getSupervisorPageInfo\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getSupervisorPageInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getSupervisorPageInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(userB), \"getSupervisorPageInfo\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"fileDownload\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(supervisorUser), \"fileDownload\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(userA), \"fileDownload\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"fileDownload\", new HashMap<>()));\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Collection<String> topologyUserSet = new HashSet<>(Collections.singletonList(\"user-a\"));\r\n    topoConf.put(Config.TOPOLOGY_USERS, topologyUserSet);\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"killTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"killTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"killTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"killTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"rebalance\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"rebalance\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"rebalance\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"rebalance\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"activate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"activate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"activate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"activate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"deactivate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"deactivate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"deactivate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"deactivate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getTopologyConf\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getTopologyConf\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyConf\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyConf\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getUserTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getUserTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getUserTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getUserTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getTopologyInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getTopologyInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getTopologyPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getTopologyPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getComponentPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getComponentPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getComponentPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getComponentPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"uploadNewCredentials\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"uploadNewCredentials\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"uploadNewCredentials\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"uploadNewCredentials\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"setLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"setLogConfig\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"setLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"setLogConfig\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"setWorkerProfiler\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"setWorkerProfiler\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"setWorkerProfiler\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"setWorkerProfiler\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getComponentPendingProfileActions\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getComponentPendingProfileActions\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getComponentPendingProfileActions\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getComponentPendingProfileActions\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"startProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"startProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"startProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"startProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"stopProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"stopProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"stopProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"stopProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"dumpProfile\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"dumpProfile\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpProfile\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpProfile\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"dumpJstack\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"dumpJstack\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpJstack\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpJstack\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"dumpHeap\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"dumpHeap\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpHeap\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpHeap\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"debug\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"debug\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"debug\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"debug\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"getLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(supervisorUser), \"getLogConfig\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getLogConfig\", topoConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\SimpleACLAuthorizerTest.java",
  "methodName" : "SimpleACLNimbusUserAuthTest",
  "sourceCode" : "@Test\r\npublic void SimpleACLNimbusUserAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    Collection<String> adminUserSet = new HashSet<>(Collections.singletonList(\"admin\"));\r\n    Collection<String> supervisorUserSet = new HashSet<>(Collections.singletonList(\"supervisor\"));\r\n    Collection<String> nimbusUserSet = new HashSet<>(Collections.singletonList(\"user-a\"));\r\n    clusterConf.put(Config.NIMBUS_ADMINS, adminUserSet);\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, supervisorUserSet);\r\n    clusterConf.put(Config.NIMBUS_USERS, nimbusUserSet);\r\n    IAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    Subject adminUser = createSubject(\"admin\");\r\n    Subject supervisorUser = createSubject(\"supervisor\");\r\n    Subject userA = createSubject(\"user-a\");\r\n    Subject userB = createSubject(\"user-b\");\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"submitTopology\", new HashMap<>()));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"submitTopology\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(adminUser), \"fileUpload\", new HashMap<>()));\r\n    assertTrue(authorizer.permit(new ReqContext(supervisorUser), \"fileDownload\", new HashMap<>()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\SimpleACLAuthorizerTest.java",
  "methodName" : "SimpleACLTopologyReadOnlyUserAuthTest",
  "sourceCode" : "@Test\r\npublic void SimpleACLTopologyReadOnlyUserAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Collection<String> topologyUserSet = new HashSet<>(Collections.singletonList(\"user-a\"));\r\n    topoConf.put(Config.TOPOLOGY_USERS, topologyUserSet);\r\n    Collection<String> topologyReadOnlyUserSet = new HashSet<>(Collections.singletonList(\"user-readonly\"));\r\n    topoConf.put(Config.TOPOLOGY_READONLY_USERS, topologyReadOnlyUserSet);\r\n    Subject userA = createSubject(\"user-a\");\r\n    Subject userB = createSubject(\"user-b\");\r\n    Subject readOnlyUser = createSubject(\"user-readonly\");\r\n    IAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"killTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"killTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"killTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"rebalance\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"rebalance\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"rebalance\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"activate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"activate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"activate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"deactivate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"deactivate\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"deactivate\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getTopologyConf\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyConf\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyConf\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getUserTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getUserTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getUserTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getTopologyInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getTopologyPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getTopologyPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getComponentPageInfo\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getComponentPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getComponentPageInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"uploadNewCredentials\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"uploadNewCredentials\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"uploadNewCredentials\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"setLogConfig\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"setLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"setLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"setWorkerProfiler\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"setWorkerProfiler\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"setWorkerProfiler\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getWorkerProfileActionExpiry\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getComponentPendingProfileActions\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getComponentPendingProfileActions\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getComponentPendingProfileActions\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"startProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"startProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"startProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"stopProfiling\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"stopProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"stopProfiling\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"dumpProfile\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpProfile\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpProfile\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"dumpJstack\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpJstack\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpJstack\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"dumpHeap\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"dumpHeap\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"dumpHeap\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(readOnlyUser), \"debug\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"debug\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"debug\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(readOnlyUser), \"getLogConfig\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userA), \"getLogConfig\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getLogConfig\", topoConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\authorizer\\SimpleACLAuthorizerTest.java",
  "methodName" : "SimpleACLTopologyReadOnlyGroupAuthTest",
  "sourceCode" : "@Test\r\npublic void SimpleACLTopologyReadOnlyGroupAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, SimpleACLTopologyReadOnlyGroupAuthTestMock.class.getName());\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Collection<String> topologyReadOnlyGroupSet = new HashSet<>(Collections.singletonList(\"group-readonly\"));\r\n    topoConf.put(Config.TOPOLOGY_READONLY_GROUPS, topologyReadOnlyGroupSet);\r\n    Subject userInReadOnlyGroup = createSubject(\"user-in-readonly-group\");\r\n    Subject userB = createSubject(\"user-b\");\r\n    IAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertFalse(authorizer.permit(new ReqContext(userInReadOnlyGroup), \"killTopology\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"killTopology\", topoConf));\r\n    assertTrue(authorizer.permit(new ReqContext(userInReadOnlyGroup), \"getTopologyInfo\", topoConf));\r\n    assertFalse(authorizer.permit(new ReqContext(userB), \"getTopologyInfo\", topoConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\AutoSSLTest.java",
  "methodName" : "testgetSSLFilesFromConf",
  "sourceCode" : "@Test\r\npublic void testgetSSLFilesFromConf() {\r\n    AutoSSL assl = new AutoSSL();\r\n    Map<String, Object> conf = new HashMap<>();\r\n    assertNull(assl.getSSLFilesFromConf(conf));\r\n    conf.put(AutoSSL.SSL_FILES_CONF, \"sslfile1.txt\");\r\n    assl.prepare(conf);\r\n    Collection<String> sslFiles = assl.getSSLFilesFromConf(conf);\r\n    assertNotNull(sslFiles);\r\n    assertEquals(1, sslFiles.size());\r\n    for (String file : sslFiles) {\r\n        assertEquals(\"sslfile1.txt\", file);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\AutoSSLTest.java",
  "methodName" : "testgetSSLFilesFromConfMultipleComma",
  "sourceCode" : "@Test\r\npublic void testgetSSLFilesFromConfMultipleComma() {\r\n    AutoSSL assl = new AutoSSL();\r\n    Map<String, Object> conf = new HashMap<>();\r\n    assertNull(assl.getSSLFilesFromConf(conf));\r\n    conf.put(AutoSSL.SSL_FILES_CONF, \"sslfile1.txt,sslfile2.txt,sslfile3.txt\");\r\n    assl.prepare(conf);\r\n    Collection<String> sslFiles = assl.getSSLFilesFromConf(conf);\r\n    assertNotNull(sslFiles);\r\n    assertEquals(3, sslFiles.size());\r\n    List<String> valid = new ArrayList<>();\r\n    Collections.addAll(valid, \"sslfile1.txt\", \"sslfile2.txt\", \"sslfile3.txt\");\r\n    for (String file : sslFiles) {\r\n        assertTrue(valid.remove(file), \"removing: \" + file);\r\n    }\r\n    assertEquals(0, valid.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\AutoSSLTest.java",
  "methodName" : "testpopulateCredentials",
  "sourceCode" : "@Test\r\npublic void testpopulateCredentials() throws Exception {\r\n    File temp = Files.createTempFile(\"tmp-autossl-test\", \".txt\").toFile();\r\n    temp.deleteOnExit();\r\n    List<String> lines = Arrays.asList(\"The first line\", \"The second line\");\r\n    Files.write(temp.toPath(), lines, StandardCharsets.UTF_8);\r\n    File baseDir = null;\r\n    try {\r\n        baseDir = new File(\"/tmp/autossl-test-\" + UUID.randomUUID());\r\n        if (!baseDir.mkdir()) {\r\n            throw new IOException(\"failed to create base directory\");\r\n        }\r\n        AutoSSL assl = new TestAutoSSL(baseDir.getPath());\r\n        LOG.debug(\"base dir is; \" + baseDir);\r\n        Map<String, Object> sslconf = new HashMap<>();\r\n        sslconf.put(AutoSSL.SSL_FILES_CONF, temp.getPath());\r\n        assl.prepare(sslconf);\r\n        Collection<String> sslFiles = assl.getSSLFilesFromConf(sslconf);\r\n        Map<String, String> creds = new HashMap<>();\r\n        assl.populateCredentials(creds);\r\n        assertTrue(creds.containsKey(temp.getName()));\r\n        Subject unusedSubject = new Subject();\r\n        assl.populateSubject(unusedSubject, creds);\r\n        String[] outputFiles = baseDir.list();\r\n        assertNotNull(outputFiles);\r\n        assertEquals(1, outputFiles.length);\r\n        // compare contents of files\r\n        if (outputFiles.length > 0) {\r\n            List<String> linesWritten = FileUtils.readLines(new File(baseDir, outputFiles[0]), StandardCharsets.UTF_8);\r\n            for (String l : linesWritten) {\r\n                assertTrue(lines.contains(l));\r\n            }\r\n        }\r\n    } finally {\r\n        if (baseDir != null) {\r\n            FileUtils.deleteDirectory(baseDir);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "getOptionsThrowsOnMissingSectionTest",
  "sourceCode" : "@Test\r\npublic void getOptionsThrowsOnMissingSectionTest() {\r\n    Configuration mockConfig = Mockito.mock(Configuration.class);\r\n    assertThrows(IOException.class, () -> ClientAuthUtils.get(mockConfig, \"bogus-section\", \"\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "getNonExistentSectionTest",
  "sourceCode" : "@Test\r\npublic void getNonExistentSectionTest() throws IOException {\r\n    Map<String, String> optionMap = new HashMap<>();\r\n    AppConfigurationEntry entry = Mockito.mock(AppConfigurationEntry.class);\r\n    Mockito.<Map<String, ?>>when(entry.getOptions()).thenReturn(optionMap);\r\n    String section = \"bogus-section\";\r\n    Configuration mockConfig = Mockito.mock(Configuration.class);\r\n    Mockito.when(mockConfig.getAppConfigurationEntry(section)).thenReturn(new AppConfigurationEntry[] { entry });\r\n    assertNull(ClientAuthUtils.get(mockConfig, section, \"nonexistent-key\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "getFirstValueForValidKeyTest",
  "sourceCode" : "@Test\r\npublic void getFirstValueForValidKeyTest() throws IOException {\r\n    String k = \"the-key\";\r\n    String expected = \"good-value\";\r\n    Map<String, String> optionMap = new HashMap<>();\r\n    optionMap.put(k, expected);\r\n    Map<String, String> badOptionMap = new HashMap<>();\r\n    badOptionMap.put(k, \"bad-value\");\r\n    AppConfigurationEntry emptyEntry = Mockito.mock(AppConfigurationEntry.class);\r\n    AppConfigurationEntry badEntry = Mockito.mock(AppConfigurationEntry.class);\r\n    AppConfigurationEntry goodEntry = Mockito.mock(AppConfigurationEntry.class);\r\n    Mockito.<Map<String, ?>>when(emptyEntry.getOptions()).thenReturn(new HashMap<String, String>());\r\n    Mockito.<Map<String, ?>>when(badEntry.getOptions()).thenReturn(badOptionMap);\r\n    Mockito.<Map<String, ?>>when(goodEntry.getOptions()).thenReturn(optionMap);\r\n    String section = \"bogus-section\";\r\n    Configuration mockConfig = Mockito.mock(Configuration.class);\r\n    Mockito.when(mockConfig.getAppConfigurationEntry(section)).thenReturn(new AppConfigurationEntry[] { emptyEntry, goodEntry, badEntry });\r\n    assertEquals(ClientAuthUtils.get(mockConfig, section, k), expected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "objGettersReturnNullWithNullConfigTest",
  "sourceCode" : "@Test\r\npublic void objGettersReturnNullWithNullConfigTest() throws IOException {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    assertNull(ClientAuthUtils.pullConfig(topoConf, \"foo\"));\r\n    assertNull(ClientAuthUtils.get(topoConf, \"foo\", \"bar\"));\r\n    assertNull(ClientAuthUtils.getConfiguration(Collections.emptyMap()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "getAutoCredentialsTest",
  "sourceCode" : "@Test\r\npublic void getAutoCredentialsTest() {\r\n    Map<String, Object> map = new HashMap<>();\r\n    map.put(Config.TOPOLOGY_AUTO_CREDENTIALS, Collections.singletonList(\"org.apache.storm.security.auth.AuthUtilsTestMock\"));\r\n    assertTrue(ClientAuthUtils.getAutoCredentials(Collections.emptyMap()).isEmpty());\r\n    assertEquals(ClientAuthUtils.getAutoCredentials(map).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "getNimbusAutoCredPluginTest",
  "sourceCode" : "@Test\r\npublic void getNimbusAutoCredPluginTest() {\r\n    Map<String, Object> map = new HashMap<>();\r\n    map.put(Config.NIMBUS_AUTO_CRED_PLUGINS, Collections.singletonList(\"org.apache.storm.security.auth.AuthUtilsTestMock\"));\r\n    assertTrue(ClientAuthUtils.getNimbusAutoCredPlugins(Collections.emptyMap()).isEmpty());\r\n    assertEquals(ClientAuthUtils.getNimbusAutoCredPlugins(map).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "GetCredentialRenewersTest",
  "sourceCode" : "@Test\r\npublic void GetCredentialRenewersTest() {\r\n    Map<String, Object> map = new HashMap<>();\r\n    map.put(Config.NIMBUS_CREDENTIAL_RENEWERS, Collections.singletonList(\"org.apache.storm.security.auth.AuthUtilsTestMock\"));\r\n    assertTrue(ClientAuthUtils.getCredentialRenewers(Collections.emptyMap()).isEmpty());\r\n    assertEquals(ClientAuthUtils.getCredentialRenewers(map).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "populateSubjectTest",
  "sourceCode" : "@Test\r\npublic void populateSubjectTest() {\r\n    AuthUtilsTestMock autoCred = Mockito.mock(AuthUtilsTestMock.class);\r\n    Subject subject = new Subject();\r\n    Map<String, String> cred = new HashMap<>();\r\n    Collection<IAutoCredentials> autos = Arrays.asList(new IAutoCredentials[] { autoCred });\r\n    ClientAuthUtils.populateSubject(subject, autos, cred);\r\n    Mockito.verify(autoCred, Mockito.times(1)).populateSubject(subject, cred);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "invalidConfigResultsInIOException",
  "sourceCode" : "@Test\r\npublic void invalidConfigResultsInIOException() throws RuntimeException {\r\n    HashMap<String, Object> conf = new HashMap<>();\r\n    conf.put(\"java.security.auth.login.config\", \"__FAKE_FILE__\");\r\n    assertThrows(RuntimeException.class, () -> assertNotNull(ClientAuthUtils.getConfiguration(conf)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "validConfigResultsInNotNullConfigurationTest",
  "sourceCode" : "@Test\r\npublic void validConfigResultsInNotNullConfigurationTest() throws IOException {\r\n    File file1 = new File(folder, \"mockfile.txt\");\r\n    file1.createNewFile();\r\n    HashMap<String, Object> conf = new HashMap<>();\r\n    conf.put(\"java.security.auth.login.config\", file1.getAbsolutePath());\r\n    assertNotNull(ClientAuthUtils.getConfiguration(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "updateSubjectWithNullThrowsTest",
  "sourceCode" : "@Test\r\npublic void updateSubjectWithNullThrowsTest() {\r\n    assertThrows(RuntimeException.class, () -> ClientAuthUtils.updateSubject(null, null, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "updateSubjectWithNullAutosThrowsTest",
  "sourceCode" : "@Test\r\npublic void updateSubjectWithNullAutosThrowsTest() {\r\n    assertThrows(RuntimeException.class, () -> ClientAuthUtils.updateSubject(new Subject(), null, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "updateSubjectWithNullAutosTest",
  "sourceCode" : "@Test\r\npublic void updateSubjectWithNullAutosTest() {\r\n    AuthUtilsTestMock mock = Mockito.mock(AuthUtilsTestMock.class);\r\n    Collection<IAutoCredentials> autos = Arrays.asList(new IAutoCredentials[] { mock });\r\n    Subject s = new Subject();\r\n    ClientAuthUtils.updateSubject(s, autos, null);\r\n    Mockito.verify(mock, Mockito.times(1)).updateSubject(s, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ClientAuthUtilsTest.java",
  "methodName" : "pluginCreationTest",
  "sourceCode" : "@Test\r\npublic void pluginCreationTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, AuthUtilsTestMock.class.getName());\r\n    conf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, AuthUtilsTestMock.class.getName());\r\n    assertTrue(ClientAuthUtils.getPrincipalToLocalPlugin(conf).getClass() == AuthUtilsTestMock.class);\r\n    assertSame(ClientAuthUtils.getGroupMappingServiceProviderPlugin(conf).getClass(), AuthUtilsTestMock.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleNoSubjNoTgtTest",
  "sourceCode" : "@Test\r\npublic void loginModuleNoSubjNoTgtTest() throws Exception {\r\n    // Behavior is correct when there is no Subject or TGT\r\n    AutoTGTKrb5LoginModule loginModule = new AutoTGTKrb5LoginModule();\r\n    Assertions.assertThrows(LoginException.class, loginModule::login);\r\n    assertFalse(loginModule.commit());\r\n    assertFalse(loginModule.abort());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleReadonlySubjNoTgtTest",
  "sourceCode" : "@Test\r\npublic void loginModuleReadonlySubjNoTgtTest() throws Exception {\r\n    // Behavior is correct when there is a read-only Subject and no TGT\r\n    Subject readonlySubject = new Subject(true, Collections.emptySet(), Collections.emptySet(), Collections.emptySet());\r\n    AutoTGTKrb5LoginModule loginModule = new AutoTGTKrb5LoginModule();\r\n    loginModule.initialize(readonlySubject, null, null, null);\r\n    assertFalse(loginModule.commit());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleWithSubjNoTgtTest",
  "sourceCode" : "@Test\r\npublic void loginModuleWithSubjNoTgtTest() throws Exception {\r\n    // Behavior is correct when there is a Subject and no TGT\r\n    AutoTGTKrb5LoginModule loginModule = new AutoTGTKrb5LoginModule();\r\n    loginModule.initialize(new Subject(), null, null, null);\r\n    Assertions.assertThrows(LoginException.class, loginModule::login);\r\n    assertFalse(loginModule.commit());\r\n    assertFalse(loginModule.abort());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleNoSubjWithTgtTest",
  "sourceCode" : "@Test\r\npublic void loginModuleNoSubjWithTgtTest() throws Exception {\r\n    // Behavior is correct when there is no Subject and a TGT\r\n    AutoTGTKrb5LoginModuleTest loginModule = new AutoTGTKrb5LoginModuleTest();\r\n    loginModule.setKerbTicket(Mockito.mock(KerberosTicket.class));\r\n    assertTrue(loginModule.login());\r\n    Assertions.assertThrows(LoginException.class, loginModule::commit);\r\n    loginModule.setKerbTicket(Mockito.mock(KerberosTicket.class));\r\n    assertTrue(loginModule.abort());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleReadonlySubjWithTgtTest",
  "sourceCode" : "@Test\r\npublic void loginModuleReadonlySubjWithTgtTest() throws Exception {\r\n    // Behavior is correct when there is a read-only Subject and a TGT\r\n    Subject readonlySubject = new Subject(true, Collections.emptySet(), Collections.emptySet(), Collections.emptySet());\r\n    AutoTGTKrb5LoginModuleTest loginModule = new AutoTGTKrb5LoginModuleTest();\r\n    loginModule.initialize(readonlySubject, null, null, null);\r\n    loginModule.setKerbTicket(Mockito.mock(KerberosTicket.class));\r\n    assertTrue(loginModule.login());\r\n    Assertions.assertThrows(LoginException.class, loginModule::commit);\r\n    loginModule.setKerbTicket(Mockito.mock(KerberosTicket.class));\r\n    assertTrue(loginModule.abort());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\kerberos\\AutoLoginModuleTest.java",
  "methodName" : "loginModuleWithSubjAndTgt",
  "sourceCode" : "@Test\r\npublic void loginModuleWithSubjAndTgt() throws Exception {\r\n    // Behavior is correct when there is a Subject and a TGT\r\n    AutoTGTKrb5LoginModuleTest loginModule = new AutoTGTKrb5LoginModuleTest();\r\n    loginModule.client = Mockito.mock(Principal.class);\r\n    Date endTime = new SimpleDateFormat(\"ddMMyyyy\").parse(\"31122030\");\r\n    byte[] asn1Enc = new byte[10];\r\n    Arrays.fill(asn1Enc, (byte) 122);\r\n    byte[] sessionKey = new byte[10];\r\n    Arrays.fill(sessionKey, (byte) 123);\r\n    KerberosTicket ticket = new KerberosTicket(asn1Enc, new KerberosPrincipal(\"client/localhost@local.com\"), new KerberosPrincipal(\"server/localhost@local.com\"), sessionKey, 234, new boolean[] { false, true, false, true, false, true, false }, new Date(), new Date(), endTime, endTime, new InetAddress[] { InetAddress.getByName(\"localhost\") });\r\n    loginModule.initialize(new Subject(), null, null, null);\r\n    loginModule.setKerbTicket(ticket);\r\n    assertTrue(loginModule.login());\r\n    assertTrue(loginModule.commit());\r\n    assertTrue(loginModule.abort());\r\n    assertTrue(loginModule.logout());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\MultiThriftServerTest.java",
  "methodName" : "testAddThriftServer",
  "sourceCode" : "@Test\r\npublic void testAddThriftServer() {\r\n    multiThriftServer.add(serverNonTls);\r\n    assertEquals(serverNonTls, multiThriftServer.get(ThriftConnectionType.NIMBUS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\MultiThriftServerTest.java",
  "methodName" : "testAddThriftServerTls",
  "sourceCode" : "@Test\r\npublic void testAddThriftServerTls() {\r\n    multiThriftServer.add(serverTls);\r\n    assertEquals(serverTls, multiThriftServer.get(ThriftConnectionType.NIMBUS_TLS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\MultiThriftServerTest.java",
  "methodName" : "testAddThriftServerBoth",
  "sourceCode" : "@Test\r\npublic void testAddThriftServerBoth() {\r\n    multiThriftServer.add(serverTls);\r\n    multiThriftServer.add(serverNonTls);\r\n    assertEquals(serverNonTls, multiThriftServer.get(ThriftConnectionType.NIMBUS));\r\n    assertEquals(serverTls, multiThriftServer.get(ThriftConnectionType.NIMBUS_TLS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ReqContextTest.java",
  "methodName" : "testSubject",
  "sourceCode" : "@Test\r\npublic void testSubject() {\r\n    Subject expected = new Subject();\r\n    assertFalse(expected.isReadOnly());\r\n    rc.setSubject(expected);\r\n    assertEquals(expected, rc.subject());\r\n    expected.setReadOnly();\r\n    rc.setSubject(expected);\r\n    assertEquals(expected, rc.subject());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ReqContextTest.java",
  "methodName" : "testRemoteAddress",
  "sourceCode" : "@Test\r\npublic void testRemoteAddress() throws UnknownHostException {\r\n    InetAddress expected = InetAddress.getByAddress(\"ABCD\".getBytes());\r\n    rc.setRemoteAddress(expected);\r\n    assertEquals(expected, rc.remoteAddress());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ReqContextTest.java",
  "methodName" : "testPrincipalReturnsNullWhenNoSubject",
  "sourceCode" : "/**\r\n * If subject has no principals, request context should return null principal\r\n */\r\n@Test\r\npublic void testPrincipalReturnsNullWhenNoSubject() {\r\n    rc.setSubject(new Subject());\r\n    assertNull(rc.principal());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ReqContextTest.java",
  "methodName" : "testPrincipal",
  "sourceCode" : "@Test\r\npublic void testPrincipal() {\r\n    final String principalName = \"Test Principal\";\r\n    Principal testPrincipal = () -> principalName;\r\n    Set<Principal> principals = ImmutableSet.of(testPrincipal);\r\n    Subject subject = new Subject(false, principals, new HashSet<>(), new HashSet<>());\r\n    rc.setSubject(subject);\r\n    assertNotNull(rc.principal());\r\n    assertEquals(principalName, rc.principal().getName());\r\n    rc.setSubject(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\SaslTransportPluginTest.java",
  "methodName" : "testUserName",
  "sourceCode" : "@Test\r\npublic void testUserName() {\r\n    String name = \"Andy\";\r\n    SaslTransportPlugin.User user = new SaslTransportPlugin.User(name);\r\n    assertEquals(name, user.toString());\r\n    assertEquals(user.getName(), user.toString());\r\n    assertEquals(name.hashCode(), user.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\SaslTransportPluginTest.java",
  "methodName" : "testUserEquals",
  "sourceCode" : "@Test\r\npublic void testUserEquals() {\r\n    String name = \"Andy\";\r\n    SaslTransportPlugin.User user1 = new SaslTransportPlugin.User(name);\r\n    SaslTransportPlugin.User user2 = new SaslTransportPlugin.User(name);\r\n    SaslTransportPlugin.User user3 = new SaslTransportPlugin.User(\"Bobby\");\r\n    assertTrue(user1.equals(user1));\r\n    assertTrue(user1.equals(user2));\r\n    assertFalse(user1.equals(null));\r\n    assertFalse(user1.equals(\"Potato\"));\r\n    assertFalse(user1.equals(user3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ShellBasedGroupsMappingTest.java",
  "methodName" : "testCanGetGroups",
  "sourceCode" : "@Test\r\npublic void testCanGetGroups() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        groupsMapping.prepare(topoConf);\r\n        when(mockShell.execCommand(ShellUtils.getGroupsForUserCommand(TEST_USER_1))).thenReturn(TEST_TWO_GROUPS);\r\n        Set<String> groups = groupsMapping.getGroups(TEST_USER_1);\r\n        assertThat(groups, containsInAnyOrder(TEST_TWO_GROUPS.split(GROUP_SEPARATOR_REGEX)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ShellBasedGroupsMappingTest.java",
  "methodName" : "testWillCacheGroups",
  "sourceCode" : "@Test\r\npublic void testWillCacheGroups() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        groupsMapping.prepare(topoConf);\r\n        when(mockShell.execCommand(ShellUtils.getGroupsForUserCommand(TEST_USER_1))).thenReturn(TEST_TWO_GROUPS, TEST_NO_GROUPS);\r\n        Set<String> firstGroups = groupsMapping.getGroups(TEST_USER_1);\r\n        Set<String> secondGroups = groupsMapping.getGroups(TEST_USER_1);\r\n        assertThat(firstGroups, is(secondGroups));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ShellBasedGroupsMappingTest.java",
  "methodName" : "testWillExpireCache",
  "sourceCode" : "@Test\r\npublic void testWillExpireCache() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        groupsMapping.prepare(topoConf);\r\n        when(mockShell.execCommand(ShellUtils.getGroupsForUserCommand(TEST_USER_1))).thenReturn(TEST_TWO_GROUPS, TEST_NO_GROUPS);\r\n        Set<String> firstGroups = groupsMapping.getGroups(TEST_USER_1);\r\n        Time.advanceTimeSecs(CACHE_EXPIRATION_SECS * 2);\r\n        Set<String> secondGroups = groupsMapping.getGroups(TEST_USER_1);\r\n        assertThat(firstGroups, not(secondGroups));\r\n        assertThat(secondGroups, contains(TEST_NO_GROUPS));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ThriftClientTest.java",
  "methodName" : "testConstructorThrowsIfPortNegative",
  "sourceCode" : "@Test\r\npublic void testConstructorThrowsIfPortNegative() {\r\n    assertThrows(IllegalArgumentException.class, () -> new ThriftClient(conf, ThriftConnectionType.DRPC, \"bogushost\", -1, NIMBUS_TIMEOUT));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ThriftClientTest.java",
  "methodName" : "testConstructorThrowsIfPortZero",
  "sourceCode" : "@Test\r\npublic void testConstructorThrowsIfPortZero() {\r\n    assertThrows(IllegalArgumentException.class, () -> new ThriftClient(conf, ThriftConnectionType.DRPC, \"bogushost\", 0, NIMBUS_TIMEOUT));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ThriftClientTest.java",
  "methodName" : "testConstructorThrowsIfHostNull",
  "sourceCode" : "@Test\r\npublic void testConstructorThrowsIfHostNull() {\r\n    assertThrows(IllegalArgumentException.class, () -> new ThriftClient(conf, ThriftConnectionType.DRPC, null, 4242, NIMBUS_TIMEOUT));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\ThriftClientTest.java",
  "methodName" : "testConstructorThrowsIfHostEmpty",
  "sourceCode" : "@Test\r\npublic void testConstructorThrowsIfHostEmpty() {\r\n    Exception e = assertThrows(RuntimeException.class, () -> new ThriftClient(conf, ThriftConnectionType.DRPC, \"\", 4242, NIMBUS_TIMEOUT));\r\n    // Now the cause of the thrown exception must be TTransportException\r\n    assertTrue(e.getCause().getCause() instanceof TTransportException, e.getCause().getMessage());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\TlsTransportPluginTest.java",
  "methodName" : "testNonTlsConnection",
  "sourceCode" : "@Test\r\nvoid testNonTlsConnection() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        tlsTransportPlugin.prepare(type, conf);\r\n        tlsTransportPlugin.getServer(new Nimbus.Processor<>(handler));\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\TlsTransportPluginTest.java",
  "methodName" : "testValidTlsSetup",
  "sourceCode" : "@Test\r\nvoid testValidTlsSetup() {\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_PORT, \"1111\");\r\n    conf.put(Config.STORM_THRIFT_TLS_SOCKET_TIMEOUT_MS, 60);\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_SERVER_KEYSTORE_PATH, testDataPath + \"testKeyStore.jks\");\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_SERVER_KEYSTORE_PASSWORD, \"testpass\");\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_SERVER_TRUSTSTORE_PATH, testDataPath + \"testTrustStore.jks\");\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_SERVER_TRUSTSTORE_PASSWORD, \"testpass\");\r\n    conf.put(Config.NIMBUS_THRIFT_TLS_THREADS, 1);\r\n    conf.put(Config.NIMBUS_QUEUE_SIZE, 1);\r\n    try {\r\n        tlsTransportPlugin.prepare(type, conf);\r\n        tlsTransportPlugin.getServer(new Nimbus.Processor<>(handler));\r\n        assertEquals(1111, tlsTransportPlugin.getPort());\r\n    } catch (Exception e) {\r\n        fail(\"TLS setup failed: \" + e.getMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\X509CertPrincipalToLocalTest.java",
  "methodName" : "toLocalTest",
  "sourceCode" : "@Test\r\npublic void toLocalTest() {\r\n    assertEquals(\"xyz\", x509CertPrincipalToLocal.toLocal(\"CN=test:test_role.uid.xyz\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\auth\\X509CertPrincipalToLocalTest.java",
  "methodName" : "toLocalTestNegative",
  "sourceCode" : "@Test\r\npublic void toLocalTestNegative() {\r\n    try {\r\n        x509CertPrincipalToLocal.toLocal(\"CN=test_not_defined:test_role.uid.xyz\");\r\n        fail(\"Should have got AccessControlException\");\r\n    } catch (AccessControlException expectedException) {\r\n        assertEquals(\"Invalid principal CN=test_not_defined:test_role.uid.xyz\", expectedException.getMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\serialization\\BlowfishTupleSerializerTest.java",
  "methodName" : "testConstructorThrowsOnNullKey",
  "sourceCode" : "/**\r\n * Throws RuntimeException when no encryption key is given.\r\n */\r\n@Test\r\npublic void testConstructorThrowsOnNullKey() {\r\n    assertThrows(RuntimeException.class, () -> new BlowfishTupleSerializer(null, new HashMap<>()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\serialization\\BlowfishTupleSerializerTest.java",
  "methodName" : "testConstructorThrowsOnInvalidKey",
  "sourceCode" : "/**\r\n * Throws RuntimeException when an invalid encryption key is given.\r\n */\r\n@Test\r\npublic void testConstructorThrowsOnInvalidKey() {\r\n    // The encryption key must be hexadecimal.\r\n    assertThrows(RuntimeException.class, () -> new BlowfishTupleSerializer(null, ImmutableMap.of(BlowfishTupleSerializer.SECRET_KEY, \"0123456789abcdefg\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\serialization\\BlowfishTupleSerializerTest.java",
  "methodName" : "testUseBlowfishKey",
  "sourceCode" : "/**\r\n * Test using {@link org.apache.storm.security.serialization.BlowfishTupleSerializer#SECRET_KEY}.\r\n */\r\n@Test\r\npublic void testUseBlowfishKey() {\r\n    String arbitraryKey = \"7dd6fb3203878381b08f9c89d25ed105\";\r\n    Map<String, Object> topoConf = ImmutableMap.of(BlowfishTupleSerializer.SECRET_KEY, arbitraryKey);\r\n    testEncryptsAndDecryptsMessage(topoConf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\security\\serialization\\BlowfishTupleSerializerTest.java",
  "methodName" : "testUseZookeeperSecret",
  "sourceCode" : "/**\r\n * Test using {@link org.apache.storm.Config#STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD}\r\n * when {@link org.apache.storm.security.serialization.BlowfishTupleSerializer#SECRET_KEY} is not present.\r\n */\r\n@Test\r\npublic void testUseZookeeperSecret() {\r\n    Map<String, Object> topoConf = ImmutableMap.of(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, \"user:password\");\r\n    testEncryptsAndDecryptsMessage(topoConf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\GzipBridgeThriftSerializationDelegateTest.java",
  "methodName" : "testDeserialize_readingFromGzip",
  "sourceCode" : "@Test\r\npublic void testDeserialize_readingFromGzip() {\r\n    GlobalStreamId id = new GlobalStreamId(\"first\", \"second\");\r\n    byte[] serialized = new GzipThriftSerializationDelegate().serialize(id);\r\n    GlobalStreamId id2 = testDelegate.deserialize(serialized, GlobalStreamId.class);\r\n    assertEquals(id2.get_componentId(), id.get_componentId());\r\n    assertEquals(id2.get_streamId(), id.get_streamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\GzipBridgeThriftSerializationDelegateTest.java",
  "methodName" : "testDeserialize_readingFromGzipBridge",
  "sourceCode" : "@Test\r\npublic void testDeserialize_readingFromGzipBridge() {\r\n    GlobalStreamId id = new GlobalStreamId(\"first\", \"second\");\r\n    byte[] serialized = new GzipBridgeThriftSerializationDelegate().serialize(id);\r\n    GlobalStreamId id2 = testDelegate.deserialize(serialized, GlobalStreamId.class);\r\n    assertEquals(id2.get_componentId(), id.get_componentId());\r\n    assertEquals(id2.get_streamId(), id.get_streamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\GzipBridgeThriftSerializationDelegateTest.java",
  "methodName" : "testDeserialize_readingFromDefault",
  "sourceCode" : "@Test\r\npublic void testDeserialize_readingFromDefault() {\r\n    GlobalStreamId id = new GlobalStreamId(\"A\", \"B\");\r\n    byte[] serialized = new ThriftSerializationDelegate().serialize(id);\r\n    GlobalStreamId id2 = testDelegate.deserialize(serialized, GlobalStreamId.class);\r\n    assertEquals(id2.get_componentId(), id.get_componentId());\r\n    assertEquals(id2.get_streamId(), id.get_streamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationFactoryTest.java",
  "methodName" : "test_registers_default_when_not_in_conf",
  "sourceCode" : "@Test\r\npublic void test_registers_default_when_not_in_conf() throws ClassNotFoundException {\r\n    Map<String, Object> conf = Utils.readDefaultConfig();\r\n    String className = (String) conf.get(Config.TOPOLOGY_TUPLE_SERIALIZER);\r\n    Class configuredClass = Class.forName(className);\r\n    Kryo kryo = SerializationFactory.getKryo(conf);\r\n    assertEquals(configuredClass, kryo.getSerializer(ListDelegate.class).getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationFactoryTest.java",
  "methodName" : "test_throws_runtimeexception_when_no_such_class",
  "sourceCode" : "@Test\r\npublic void test_throws_runtimeexception_when_no_such_class() {\r\n    assertThrows(RuntimeException.class, () -> {\r\n        Map<String, Object> conf = Utils.readDefaultConfig();\r\n        conf.put(Config.TOPOLOGY_TUPLE_SERIALIZER, \"null.this.class.does.not.exist\");\r\n        SerializationFactory.getKryo(conf);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationFactoryTest.java",
  "methodName" : "test_registers_when_valid_class_name",
  "sourceCode" : "@Test\r\npublic void test_registers_when_valid_class_name() {\r\n    Class arbitraryClass = BlowfishTupleSerializer.class;\r\n    String secretKey = \"0123456789abcdef\";\r\n    Map<String, Object> conf = Utils.readDefaultConfig();\r\n    conf.put(Config.TOPOLOGY_TUPLE_SERIALIZER, arbitraryClass.getName());\r\n    conf.put(BlowfishTupleSerializer.SECRET_KEY, secretKey);\r\n    Kryo kryo = SerializationFactory.getKryo(conf);\r\n    assertEquals(arbitraryClass, kryo.getSerializer(ListDelegate.class).getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\serialization\\ThriftBridgeSerializationDelegateTest.java",
  "methodName" : "testThriftInstance",
  "sourceCode" : "@Test\r\npublic void testThriftInstance() {\r\n    ErrorInfo errorInfo = new ErrorInfo();\r\n    errorInfo.set_error(\"error\");\r\n    errorInfo.set_error_time_secs(1);\r\n    errorInfo.set_host(\"host\");\r\n    errorInfo.set_port(1);\r\n    byte[] serialized = new ThriftSerializationDelegate().serialize(errorInfo);\r\n    ErrorInfo errorInfo2 = testDelegate.deserialize(serialized, ErrorInfo.class);\r\n    assertEquals(errorInfo, errorInfo2);\r\n    serialized = testDelegate.serialize(errorInfo);\r\n    errorInfo2 = new ThriftSerializationDelegate().deserialize(serialized, ErrorInfo.class);\r\n    assertEquals(errorInfo, errorInfo2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testInitState",
  "sourceCode" : "@Test\r\npublic void testInitState() throws Exception {\r\n    spout.open(new HashMap<>(), mockTopologyContext, mockOutputCollector);\r\n    spout.nextTuple();\r\n    Values expectedTuple = new Values(-1L, Action.INITSTATE);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    Mockito.verify(mockOutputCollector).emit(stream.capture(), values.capture(), msgId.capture());\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(-1L, msgId.getValue());\r\n    spout.ack(-1L);\r\n    Mockito.verify(mockOutputCollector).emit(stream.capture(), values.capture(), msgId.capture());\r\n    expectedTuple = new Values(-1L, Action.INITSTATE);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(-1L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testPrepare",
  "sourceCode" : "@Test\r\npublic void testPrepare() {\r\n    spout.open(new HashMap<>(), mockTopologyContext, mockOutputCollector);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    spout.ack(-1L);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(2)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(0L, Action.PREPARE);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(0L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testPrepareWithFail",
  "sourceCode" : "@Test\r\npublic void testPrepareWithFail() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    KeyValueState<String, CheckPointState> state = (KeyValueState<String, CheckPointState>) StateFactory.getState(\"__state\", topoConf, mockTopologyContext);\r\n    CheckPointState txState = new CheckPointState(-1, COMMITTED);\r\n    state.put(\"__state\", txState);\r\n    spout.open(mockTopologyContext, mockOutputCollector, 0, state);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    spout.ack(-1L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.ack(0L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.ack(0L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.fail(1L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.fail(1L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.ack(1L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.ack(0L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(8)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(1L, Action.PREPARE);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(1L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testCommit",
  "sourceCode" : "@Test\r\npublic void testCommit() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    topoConf.put(Config.TOPOLOGY_STATE_CHECKPOINT_INTERVAL, 0);\r\n    spout.open(topoConf, mockTopologyContext, mockOutputCollector);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    spout.ack(-1L);\r\n    spout.nextTuple();\r\n    spout.ack(0L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    spout.fail(0L);\r\n    Utils.sleep(10);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(4)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(0L, Action.COMMIT);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(0L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testRecoveryRollback",
  "sourceCode" : "@Test\r\npublic void testRecoveryRollback() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    KeyValueState<String, CheckPointState> state = (KeyValueState<String, CheckPointState>) StateFactory.getState(\"test-1\", topoConf, mockTopologyContext);\r\n    CheckPointState checkPointState = new CheckPointState(100, CheckPointState.State.PREPARING);\r\n    state.put(\"__state\", checkPointState);\r\n    spout.open(mockTopologyContext, mockOutputCollector, 0, state);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(100L, Action.ROLLBACK);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(100L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testRecoveryRollbackAck",
  "sourceCode" : "@Test\r\npublic void testRecoveryRollbackAck() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    KeyValueState<String, CheckPointState> state = (KeyValueState<String, CheckPointState>) StateFactory.getState(\"test-1\", topoConf, mockTopologyContext);\r\n    CheckPointState checkPointState = new CheckPointState(100, CheckPointState.State.PREPARING);\r\n    state.put(\"__state\", checkPointState);\r\n    spout.open(mockTopologyContext, mockOutputCollector, 0, state);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    spout.ack(100L);\r\n    spout.nextTuple();\r\n    spout.ack(99L);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(3)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(100L, Action.PREPARE);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(100L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\spout\\CheckpointSpoutTest.java",
  "methodName" : "testRecoveryCommit",
  "sourceCode" : "@Test\r\npublic void testRecoveryCommit() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    KeyValueState<String, CheckPointState> state = (KeyValueState<String, CheckPointState>) StateFactory.getState(\"test-1\", topoConf, mockTopologyContext);\r\n    CheckPointState checkPointState = new CheckPointState(100, CheckPointState.State.COMMITTING);\r\n    state.put(\"__state\", checkPointState);\r\n    spout.open(mockTopologyContext, mockOutputCollector, 0, state);\r\n    ArgumentCaptor<String> stream = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<Object> msgId = ArgumentCaptor.forClass(Object.class);\r\n    spout.nextTuple();\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).emit(stream.capture(), values.capture(), msgId.capture());\r\n    Values expectedTuple = new Values(100L, Action.COMMIT);\r\n    assertEquals(CheckpointSpout.CHECKPOINT_STREAM_ID, stream.getValue());\r\n    assertEquals(expectedTuple, values.getValue());\r\n    assertEquals(100L, msgId.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\BaseBinaryStateIteratorTest.java",
  "methodName" : "testGetEntriesFromPendingPrepare",
  "sourceCode" : "@Test\r\npublic void testGetEntriesFromPendingPrepare() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingPrepare, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putTombstoneToMap(pendingPrepare, \"key1\".getBytes());\r\n    putEncodedKeyValueToMap(pendingPrepare, \"key2\".getBytes(), \"value2\".getBytes());\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    MockBinaryStateIterator kvIterator = new MockBinaryStateIterator(pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator());\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't in iterator\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\BaseBinaryStateIteratorTest.java",
  "methodName" : "testGetEntriesFromPendingCommit",
  "sourceCode" : "@Test\r\npublic void testGetEntriesFromPendingCommit() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingCommit, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putTombstoneToMap(pendingCommit, \"key1\".getBytes());\r\n    putEncodedKeyValueToMap(pendingCommit, \"key2\".getBytes(), \"value2\".getBytes());\r\n    MockBinaryStateIterator kvIterator = new MockBinaryStateIterator(pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator());\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't in iterator\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\BaseBinaryStateIteratorTest.java",
  "methodName" : "testGetEntriesRemovingDuplicationKeys",
  "sourceCode" : "@Test\r\npublic void testGetEntriesRemovingDuplicationKeys() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingPrepare, \"key0\".getBytes(), \"value0\".getBytes());\r\n    putTombstoneToMap(pendingPrepare, \"key1\".getBytes());\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    putEncodedKeyValueToMap(pendingCommit, \"key1\".getBytes(), \"value1\".getBytes());\r\n    putEncodedKeyValueToMap(pendingCommit, \"key2\".getBytes(), \"value2\".getBytes());\r\n    MockBinaryStateIterator kvIterator = new MockBinaryStateIterator(pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator());\r\n    // keys shouldn't appear twice\r\n    assertNextEntry(kvIterator, \"key0\".getBytes(), \"value0\".getBytes());\r\n    // key1 shouldn't be in iterator since it's marked as deleted\r\n    assertNextEntry(kvIterator, \"key2\".getBytes(), \"value2\".getBytes());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\BaseBinaryStateIteratorTest.java",
  "methodName" : "testGetEntryNotAvailable",
  "sourceCode" : "@Test\r\npublic void testGetEntryNotAvailable() {\r\n    NavigableMap<byte[], byte[]> pendingPrepare = getBinaryTreeMap();\r\n    NavigableMap<byte[], byte[]> pendingCommit = getBinaryTreeMap();\r\n    MockBinaryStateIterator kvIterator = new MockBinaryStateIterator(pendingPrepare.entrySet().iterator(), pendingCommit.entrySet().iterator());\r\n    assertFalse(kvIterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\DefaultStateSerializerTest.java",
  "methodName" : "testSerializeDeserialize",
  "sourceCode" : "@Test\r\npublic void testSerializeDeserialize() {\r\n    Serializer<Long> s1 = new DefaultStateSerializer<>();\r\n    byte[] bytes;\r\n    long val = 100;\r\n    bytes = s1.serialize(val);\r\n    assertEquals(val, (long) s1.deserialize(bytes));\r\n    CheckPointState cs = new CheckPointState(100, CheckPointState.State.COMMITTED);\r\n    Serializer<CheckPointState> s2 = new DefaultStateSerializer<>();\r\n    bytes = s2.serialize(cs);\r\n    assertEquals(cs, s2.deserialize(bytes));\r\n    List<Class<?>> classesToRegister = new ArrayList<>();\r\n    classesToRegister.add(CheckPointState.class);\r\n    classesToRegister.add(CheckPointState.State.class);\r\n    Serializer<CheckPointState> s3 = new DefaultStateSerializer<>(Collections.emptyMap(), null, classesToRegister);\r\n    bytes = s3.serialize(cs);\r\n    assertEquals(cs, s3.deserialize(bytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\InMemoryKeyValueStateTest.java",
  "methodName" : "testPutAndGet",
  "sourceCode" : "@Test\r\npublic void testPutAndGet() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", null }, getValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\InMemoryKeyValueStateTest.java",
  "methodName" : "testPutAndDelete",
  "sourceCode" : "@Test\r\npublic void testPutAndDelete() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    assertEquals(\"1\", keyValueState.get(\"a\"));\r\n    assertEquals(\"2\", keyValueState.get(\"b\"));\r\n    assertNull(keyValueState.get(\"c\"));\r\n    assertEquals(\"1\", keyValueState.delete(\"a\"));\r\n    assertNull(keyValueState.get(\"a\"));\r\n    assertEquals(\"2\", keyValueState.get(\"b\"));\r\n    assertNull(keyValueState.get(\"c\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\state\\InMemoryKeyValueStateTest.java",
  "methodName" : "testPrepareCommitRollback",
  "sourceCode" : "@Test\r\npublic void testPrepareCommitRollback() {\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(1);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { null, null, null }, getValues());\r\n    keyValueState.put(\"a\", \"1\");\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(1);\r\n    keyValueState.commit(1);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { \"1\", \"2\", null }, getValues());\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertEquals(\"2\", keyValueState.delete(\"b\"));\r\n    assertEquals(\"3\", keyValueState.delete(\"c\"));\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.prepareCommit(2);\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.commit(2);\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n    keyValueState.put(\"b\", \"2\");\r\n    keyValueState.prepareCommit(3);\r\n    keyValueState.put(\"c\", \"3\");\r\n    assertArrayEquals(new String[] { \"1\", \"2\", \"3\" }, getValues());\r\n    keyValueState.rollback();\r\n    assertArrayEquals(new String[] { \"1\", null, null }, getValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\ProcessorBoltTest.java",
  "methodName" : "testEmitAndAck",
  "sourceCode" : "@Test\r\npublic void testEmitAndAck() {\r\n    setUpProcessorBolt(new FilterProcessor<Integer>(x -> true));\r\n    bolt.execute(mockTuple1);\r\n    ArgumentCaptor<Collection> anchor = ArgumentCaptor.forClass(Collection.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<String> os = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector).emit(os.capture(), anchor.capture(), values.capture());\r\n    assertEquals(\"outputstream\", os.getValue());\r\n    assertArrayEquals(new Object[] { mockTuple1 }, anchor.getValue().toArray());\r\n    assertEquals(new Values(100), values.getValue());\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockTuple1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\ProcessorBoltTest.java",
  "methodName" : "testAggResultAndAck",
  "sourceCode" : "@Test\r\npublic void testAggResultAndAck() {\r\n    setUpProcessorBolt(new AggregateProcessor<>(new LongSum()), Collections.singleton(\"inputstream\"), true, null);\r\n    bolt.execute(mockTuple2);\r\n    bolt.execute(mockTuple3);\r\n    bolt.execute(punctuation);\r\n    ArgumentCaptor<Collection> anchor = ArgumentCaptor.forClass(Collection.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<String> os = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(2)).emit(os.capture(), anchor.capture(), values.capture());\r\n    assertArrayEquals(new Object[] { mockTuple2, mockTuple3, punctuation }, anchor.getAllValues().get(0).toArray());\r\n    assertArrayEquals(new Object[] { mockTuple2, mockTuple3, punctuation }, anchor.getAllValues().get(1).toArray());\r\n    assertArrayEquals(new Object[] { new Values(200L), new Values(\"__punctuation\") }, values.getAllValues().toArray());\r\n    assertArrayEquals(new Object[] { \"outputstream\", \"outputstream__punctuation\" }, os.getAllValues().toArray());\r\n    Mockito.verify(mockOutputCollector).ack(mockTuple2);\r\n    Mockito.verify(mockOutputCollector).ack(mockTuple3);\r\n    Mockito.verify(mockOutputCollector).ack(punctuation);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\ProcessorBoltTest.java",
  "methodName" : "testEmitTs",
  "sourceCode" : "@Test\r\npublic void testEmitTs() {\r\n    Tuple tupleWithTs = Mockito.mock(Tuple.class);\r\n    setUpMockTuples(tupleWithTs);\r\n    Mockito.when(tupleWithTs.getLongByField(\"ts\")).thenReturn(12345L);\r\n    setUpProcessorBolt(new FilterProcessor(x -> true), \"ts\");\r\n    bolt.execute(tupleWithTs);\r\n    ArgumentCaptor<Collection> anchor = ArgumentCaptor.forClass(Collection.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<String> os = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector).emit(os.capture(), anchor.capture(), values.capture());\r\n    assertEquals(\"outputstream\", os.getValue());\r\n    assertArrayEquals(new Object[] { tupleWithTs }, anchor.getValue().toArray());\r\n    assertEquals(new Values(100, 12345L), values.getValue());\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(tupleWithTs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\processors\\CoGroupByKeyProcessorTest.java",
  "methodName" : "testCoGroupByKey",
  "sourceCode" : "@Test\r\npublic void testCoGroupByKey() {\r\n    coGroupByKeyProcessor = new CoGroupByKeyProcessor<>(firstStream, secondStream);\r\n    processValues();\r\n    List<Pair<Integer, Pair<Collection<Integer>, Collection<Integer>>>> expected = new ArrayList<>();\r\n    Collection<Integer> list1 = new ArrayList<>();\r\n    list1.add(25);\r\n    Collection<Integer> list2 = new ArrayList<>();\r\n    list2.add(125);\r\n    list2.add(50);\r\n    expected.add(Pair.of(5, Pair.of(list1, list2)));\r\n    assertEquals(expected.get(0), res.get(1));\r\n    list1.clear();\r\n    list2.clear();\r\n    list1.add(49);\r\n    list1.add(87);\r\n    expected.clear();\r\n    expected.add(Pair.of(7, Pair.of(list1, list2)));\r\n    assertEquals(expected.get(0), res.get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\processors\\JoinProcessorTest.java",
  "methodName" : "testInnerJoin",
  "sourceCode" : "@Test\r\npublic void testInnerJoin() {\r\n    joinProcessor = new JoinProcessor<>(leftStream, rightStream, new PairValueJoiner<>());\r\n    processValues();\r\n    assertEquals(Pair.of(2, Pair.of(4, 8)), res.get(0));\r\n    assertEquals(Pair.of(5, Pair.of(25, 125)), res.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\processors\\JoinProcessorTest.java",
  "methodName" : "testLeftOuterJoin",
  "sourceCode" : "@Test\r\npublic void testLeftOuterJoin() {\r\n    joinProcessor = new JoinProcessor<>(leftStream, rightStream, new PairValueJoiner<>(), JoinProcessor.JoinType.OUTER, JoinProcessor.JoinType.INNER);\r\n    processValues();\r\n    assertEquals(Pair.of(2, Pair.of(4, 8)), res.get(0));\r\n    assertEquals(Pair.of(5, Pair.of(25, 125)), res.get(1));\r\n    assertEquals(Pair.of(7, Pair.of(49, null)), res.get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\processors\\JoinProcessorTest.java",
  "methodName" : "testRightOuterJoin",
  "sourceCode" : "@Test\r\npublic void testRightOuterJoin() {\r\n    joinProcessor = new JoinProcessor<>(leftStream, rightStream, new PairValueJoiner<>(), JoinProcessor.JoinType.INNER, JoinProcessor.JoinType.OUTER);\r\n    processValues();\r\n    assertEquals(Pair.of(1, Pair.of(null, 1)), res.get(0));\r\n    assertEquals(Pair.of(2, Pair.of(4, 8)), res.get(1));\r\n    assertEquals(Pair.of(5, Pair.of(25, 125)), res.get(2));\r\n    assertEquals(Pair.of(6, Pair.of(null, 216)), res.get(3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\processors\\JoinProcessorTest.java",
  "methodName" : "testFullOuterJoin",
  "sourceCode" : "@Test\r\npublic void testFullOuterJoin() {\r\n    joinProcessor = new JoinProcessor<>(leftStream, rightStream, new PairValueJoiner<>(), JoinProcessor.JoinType.OUTER, JoinProcessor.JoinType.OUTER);\r\n    processValues();\r\n    assertEquals(Pair.of(1, Pair.of(null, 1)), res.get(0));\r\n    assertEquals(Pair.of(2, Pair.of(4, 8)), res.get(1));\r\n    assertEquals(Pair.of(5, Pair.of(25, 125)), res.get(2));\r\n    assertEquals(Pair.of(6, Pair.of(null, 216)), res.get(3));\r\n    assertEquals(Pair.of(7, Pair.of(49, null)), res.get(4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StatefulProcessorBoltTest.java",
  "methodName" : "testEmitAndAck",
  "sourceCode" : "@Test\r\npublic void testEmitAndAck() {\r\n    setUpStatefulProcessorBolt(new UpdateStateByKeyProcessor<>(new StateUpdater<Object, Long>() {\r\n\r\n        @Override\r\n        public Long init() {\r\n            return 0L;\r\n        }\r\n\r\n        @Override\r\n        public Long apply(Long state, Object value) {\r\n            return state + 1;\r\n        }\r\n    }));\r\n    bolt.execute(mockTuple1);\r\n    ArgumentCaptor<Collection> anchor = ArgumentCaptor.forClass(Collection.class);\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<String> os = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector).emit(os.capture(), anchor.capture(), values.capture());\r\n    assertEquals(\"outputstream\", os.getValue());\r\n    assertArrayEquals(new Object[] { mockTuple1 }, anchor.getValue().toArray());\r\n    assertEquals(new Values(\"k\", 1L), values.getValue());\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockTuple1);\r\n    Mockito.verify(mockKeyValueState, Mockito.times(1)).put(\"k\", 1L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testSpoutNoDefaultStream",
  "sourceCode" : "@Test\r\npublic void testSpoutNoDefaultStream() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Stream<Tuple> stream = streamBuilder.newStream(newSpout(\"test\"));\r\n        stream.filter(x -> true);\r\n        streamBuilder.build();\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testSpoutToBolt",
  "sourceCode" : "@Test\r\npublic void testSpoutToBolt() {\r\n    Stream<Tuple> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID));\r\n    stream.to(newBolt());\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(1, topology.get_spouts_size());\r\n    assertEquals(1, topology.get_bolts_size());\r\n    String spoutId = topology.get_spouts().keySet().iterator().next();\r\n    Map<GlobalStreamId, Grouping> expected = new HashMap<>();\r\n    expected.put(new GlobalStreamId(spoutId, \"default\"), Grouping.shuffle(new NullStruct()));\r\n    assertEquals(expected, topology.get_bolts().values().iterator().next().get_common().get_inputs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testBranch",
  "sourceCode" : "@Test\r\npublic void testBranch() {\r\n    Stream<Tuple> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID));\r\n    Stream<Tuple>[] streams = stream.branch(x -> true);\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(1, topology.get_spouts_size());\r\n    assertEquals(1, topology.get_bolts_size());\r\n    Map<GlobalStreamId, Grouping> expected = new HashMap<>();\r\n    String spoutId = topology.get_spouts().keySet().iterator().next();\r\n    expected.put(new GlobalStreamId(spoutId, \"default\"), Grouping.shuffle(new NullStruct()));\r\n    assertEquals(expected, topology.get_bolts().values().iterator().next().get_common().get_inputs());\r\n    assertEquals(1, streams.length);\r\n    assertEquals(1, streams[0].node.getOutputStreams().size());\r\n    String parentStream = streams[0].node.getOutputStreams().iterator().next() + \"-branch\";\r\n    assertEquals(1, streams[0].node.getParents(parentStream).size());\r\n    Node processorNdoe = streams[0].node.getParents(parentStream).iterator().next();\r\n    assertTrue(processorNdoe instanceof ProcessorNode);\r\n    assertTrue(((ProcessorNode) processorNdoe).getProcessor() instanceof BranchProcessor);\r\n    assertTrue(processorNdoe.getParents(\"default\").iterator().next() instanceof SpoutNode);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testJoin",
  "sourceCode" : "@Test\r\npublic void testJoin() {\r\n    Stream<Integer> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0));\r\n    Stream<Integer>[] streams = stream.branch(x -> x % 2 == 0, x -> x % 3 == 0);\r\n    PairStream<Integer, Integer> s1 = streams[0].mapToPair(x -> Pair.of(x, 1));\r\n    PairStream<Integer, Integer> s2 = streams[1].mapToPair(x -> Pair.of(x, 1));\r\n    PairStream<Integer, Pair<Integer, Integer>> sj = s1.join(s2);\r\n    assertEquals(Collections.singleton(s1.node), sj.node.getParents(s1.stream));\r\n    assertEquals(Collections.singleton(s2.node), sj.node.getParents(s2.stream));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testGroupBy",
  "sourceCode" : "@Test\r\npublic void testGroupBy() {\r\n    PairStream<String, String> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new PairValueMapper<>(0, 1), 2);\r\n    stream.window(TumblingWindows.of(BaseWindowedBolt.Count.of(10))).aggregateByKey(new Count<>());\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(2, topology.get_bolts_size());\r\n    Bolt bolt1 = topology.get_bolts().get(\"bolt1\");\r\n    Bolt bolt2 = topology.get_bolts().get(\"bolt2\");\r\n    assertEquals(Grouping.shuffle(new NullStruct()), bolt1.get_common().get_inputs().values().iterator().next());\r\n    assertEquals(Grouping.fields(Collections.singletonList(\"key\")), bolt2.get_common().get_inputs().values().iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testGlobalAggregate",
  "sourceCode" : "@Test\r\npublic void testGlobalAggregate() {\r\n    Stream<String> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0), 2);\r\n    stream.aggregate(new Count<>());\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(2, topology.get_bolts_size());\r\n    Bolt bolt1 = topology.get_bolts().get(\"bolt1\");\r\n    Bolt bolt2 = topology.get_bolts().get(\"bolt2\");\r\n    String spoutId = topology.get_spouts().keySet().iterator().next();\r\n    Map<GlobalStreamId, Grouping> expected1 = new HashMap<>();\r\n    expected1.put(new GlobalStreamId(spoutId, \"default\"), Grouping.shuffle(new NullStruct()));\r\n    Map<GlobalStreamId, Grouping> expected2 = new HashMap<>();\r\n    expected2.put(new GlobalStreamId(\"bolt1\", \"s1\"), Grouping.fields(Collections.emptyList()));\r\n    expected2.put(new GlobalStreamId(\"bolt1\", \"s1__punctuation\"), Grouping.all(new NullStruct()));\r\n    assertEquals(expected1, bolt1.get_common().get_inputs());\r\n    assertEquals(expected2, bolt2.get_common().get_inputs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testRepartition",
  "sourceCode" : "@Test\r\npublic void testRepartition() {\r\n    Stream<String> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0));\r\n    stream.repartition(3).filter(x -> true).repartition(2).filter(x -> true).aggregate(new Count<>());\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(1, topology.get_spouts_size());\r\n    SpoutSpec spout = topology.get_spouts().get(\"spout1\");\r\n    assertEquals(4, topology.get_bolts_size());\r\n    Bolt bolt1 = topology.get_bolts().get(\"bolt1\");\r\n    Bolt bolt2 = topology.get_bolts().get(\"bolt2\");\r\n    Bolt bolt3 = topology.get_bolts().get(\"bolt3\");\r\n    Bolt bolt4 = topology.get_bolts().get(\"bolt4\");\r\n    assertEquals(1, spout.get_common().get_parallelism_hint());\r\n    assertEquals(1, bolt1.get_common().get_parallelism_hint());\r\n    assertEquals(3, bolt2.get_common().get_parallelism_hint());\r\n    assertEquals(2, bolt3.get_common().get_parallelism_hint());\r\n    assertEquals(2, bolt4.get_common().get_parallelism_hint());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testBranchAndJoin",
  "sourceCode" : "@Test\r\npublic void testBranchAndJoin() {\r\n    TopologyContext mockContext = Mockito.mock(TopologyContext.class);\r\n    OutputCollector mockCollector = Mockito.mock(OutputCollector.class);\r\n    Stream<Integer> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0), 2);\r\n    Stream<Integer>[] streams = stream.branch(x -> x % 2 == 0, x -> x % 2 == 1);\r\n    PairStream<Integer, Pair<Integer, Integer>> joined = streams[0].mapToPair(x -> Pair.of(x, 1)).join(streams[1].mapToPair(x -> Pair.of(x, 1)));\r\n    assertTrue(joined.getNode() instanceof ProcessorNode);\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(2, topology.get_bolts_size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testMultiPartitionByKey",
  "sourceCode" : "@Test\r\npublic void testMultiPartitionByKey() {\r\n    TopologyContext mockContext = Mockito.mock(TopologyContext.class);\r\n    OutputCollector mockCollector = Mockito.mock(OutputCollector.class);\r\n    Stream<Integer> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0));\r\n    stream.mapToPair(x -> Pair.of(x, x)).window(TumblingWindows.of(BaseWindowedBolt.Count.of(10))).reduceByKey((x, y) -> x + y).reduceByKey((x, y) -> 0).print();\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(2, topology.get_bolts_size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testMultiPartitionByKeyWithRepartition",
  "sourceCode" : "@Test\r\npublic void testMultiPartitionByKeyWithRepartition() {\r\n    TopologyContext mockContext = Mockito.mock(TopologyContext.class);\r\n    OutputCollector mockCollector = Mockito.mock(OutputCollector.class);\r\n    Map<GlobalStreamId, Grouping> expected = new HashMap<>();\r\n    expected.put(new GlobalStreamId(\"bolt2\", \"s3\"), Grouping.fields(Collections.singletonList(\"key\")));\r\n    expected.put(new GlobalStreamId(\"bolt2\", \"s3__punctuation\"), Grouping.all(new NullStruct()));\r\n    Stream<Integer> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0));\r\n    stream.mapToPair(x -> Pair.of(x, x)).window(TumblingWindows.of(BaseWindowedBolt.Count.of(10))).reduceByKey((x, y) -> x + y).repartition(10).reduceByKey((x, y) -> 0).print();\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(3, topology.get_bolts_size());\r\n    assertEquals(expected, topology.get_bolts().get(\"bolt3\").get_common().get_inputs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\StreamBuilderTest.java",
  "methodName" : "testPartitionByKeySinglePartition",
  "sourceCode" : "@Test\r\npublic void testPartitionByKeySinglePartition() {\r\n    TopologyContext mockContext = Mockito.mock(TopologyContext.class);\r\n    OutputCollector mockCollector = Mockito.mock(OutputCollector.class);\r\n    Stream<Integer> stream = streamBuilder.newStream(newSpout(Utils.DEFAULT_STREAM_ID), new ValueMapper<>(0));\r\n    stream.mapToPair(x -> Pair.of(x, x)).reduceByKey((x, y) -> x + y).print();\r\n    StormTopology topology = streamBuilder.build();\r\n    assertEquals(1, topology.get_bolts_size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\streams\\WindowedProcessorBoltTest.java",
  "methodName" : "testEmit",
  "sourceCode" : "@Test\r\npublic void testEmit() throws Exception {\r\n    Window<?, ?> window = TumblingWindows.of(BaseWindowedBolt.Count.of(2));\r\n    setUpWindowedProcessorBolt(new AggregateProcessor<>(new Count<>()), window);\r\n    bolt.execute(getMockTupleWindow(mockTuple1, mockTuple2, mockTuple3));\r\n    ArgumentCaptor<Values> values = ArgumentCaptor.forClass(Values.class);\r\n    ArgumentCaptor<String> os = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(2)).emit(os.capture(), values.capture());\r\n    assertEquals(\"outputstream\", os.getAllValues().get(0));\r\n    assertEquals(new Values(3L), values.getAllValues().get(0));\r\n    assertEquals(\"outputstream__punctuation\", os.getAllValues().get(1));\r\n    assertEquals(new Values(WindowNode.PUNCTUATION), values.getAllValues().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "validPacemakerAuthTest",
  "sourceCode" : "@Test\r\npublic void validPacemakerAuthTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.PACEMAKER_AUTH_METHOD, \"NONE\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Config.PACEMAKER_AUTH_METHOD, \"DIGEST\");\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Config.PACEMAKER_AUTH_METHOD, \"KERBEROS\");\r\n    ConfigValidation.validateFields(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "invalidPacemakerAuthTest",
  "sourceCode" : "@Test\r\npublic void invalidPacemakerAuthTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.PACEMAKER_AUTH_METHOD, \"invalid\");\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "validConfigTest",
  "sourceCode" : "@Test\r\npublic void validConfigTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_MESSAGING_NETTY_SOCKET_BACKLOG, 5);\r\n    conf.put(Config.STORM_MESSAGING_NETTY_MIN_SLEEP_MS, 500);\r\n    conf.put(Config.STORM_MESSAGING_NETTY_AUTHENTICATION, true);\r\n    ConfigValidation.validateFields(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "invalidConfigTest",
  "sourceCode" : "@Test\r\npublic void invalidConfigTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_MESSAGING_NETTY_SOCKET_BACKLOG, 5);\r\n    conf.put(Config.STORM_MESSAGING_NETTY_MIN_SLEEP_MS, 500);\r\n    conf.put(Config.STORM_MESSAGING_NETTY_AUTHENTICATION, \"invalid\");\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testValidateTopologyBlobStoreMapWithBlobStore",
  "sourceCode" : "@Test\r\npublic void testValidateTopologyBlobStoreMapWithBlobStore() throws Throwable {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Map<String, Map<String, String>> topologyMap = new HashMap<>();\r\n    topologyMap.put(\"key1\", new HashMap<>());\r\n    topologyMap.put(\"key2\", new HashMap<>());\r\n    topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topologyMap);\r\n    Subject subject = ReqContext.context().subject();\r\n    BlobStore blobStoreMock = mock(BlobStore.class);\r\n    when(blobStoreMock.getBlobMeta(\"key1\", subject)).thenReturn(null);\r\n    when(blobStoreMock.getBlobMeta(\"key2\", subject)).thenThrow(new KeyNotFoundException());\r\n    assertThrows(InvalidTopologyException.class, () -> Utils.validateTopologyBlobStoreMap(topoConf, blobStoreMock));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testValidateTopologyBlobStoreMissingKey",
  "sourceCode" : "@Test\r\npublic void testValidateTopologyBlobStoreMissingKey() throws Throwable {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Map<String, Map<String, String>> topologyMap = new HashMap<>();\r\n    topologyMap.put(\"key1\", new HashMap<>());\r\n    topologyMap.put(\"key2\", new HashMap<>());\r\n    topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topologyMap);\r\n    NimbusBlobStore nimbusBlobStoreMock = mock(NimbusBlobStore.class);\r\n    when(nimbusBlobStoreMock.getBlobMeta(\"key1\")).thenReturn(null);\r\n    when(nimbusBlobStoreMock.getBlobMeta(\"key2\")).thenThrow(new KeyNotFoundException());\r\n    assertThrows(InvalidTopologyException.class, () -> Utils.validateTopologyBlobStoreMap(topoConf, nimbusBlobStoreMock));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testValidateTopologyBlobStoreMap",
  "sourceCode" : "@Test\r\npublic void testValidateTopologyBlobStoreMap() throws InvalidTopologyException, AuthorizationException, KeyNotFoundException {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Map<String, Map<String, Object>> topologyMap = new HashMap<>();\r\n    Map<String, Object> blobConf = new HashMap<>();\r\n    blobConf.put(\"uncompress\", false);\r\n    topologyMap.put(\"key1\", blobConf);\r\n    topologyMap.put(\"key2\", blobConf);\r\n    topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topologyMap);\r\n    NimbusBlobStore nimbusBlobStoreMock = mock(NimbusBlobStore.class);\r\n    when(nimbusBlobStoreMock.getBlobMeta(\"key1\")).thenReturn(null);\r\n    when(nimbusBlobStoreMock.getBlobMeta(\"key2\")).thenReturn(null);\r\n    Utils.validateTopologyBlobStoreMap(topoConf, nimbusBlobStoreMock);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testValidateTopologyBlobStoreMapInvalidOption",
  "sourceCode" : "@Test\r\npublic void testValidateTopologyBlobStoreMapInvalidOption() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    Map<String, Map<String, Object>> topologyMap = new HashMap<>();\r\n    Map<String, Object> blobConf = new HashMap<>();\r\n    blobConf.put(\"uncompress\", \"false\");\r\n    topologyMap.put(\"key1\", blobConf);\r\n    topologyMap.put(\"key2\", blobConf);\r\n    topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topologyMap);\r\n    NimbusBlobStore nimbusBlobStoreMock = mock(NimbusBlobStore.class);\r\n    assertThrows(InvalidTopologyException.class, () -> Utils.validateTopologyBlobStoreMap(topoConf, nimbusBlobStoreMock));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "defaultYamlTest",
  "sourceCode" : "@Test\r\npublic void defaultYamlTest() {\r\n    Map<String, Object> conf = Utils.readStormConfig();\r\n    ConfigValidation.validateFields(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testTopologyWorkersIsInteger",
  "sourceCode" : "@Test\r\npublic void testTopologyWorkersIsInteger() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_WORKERS, 42);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Config.TOPOLOGY_WORKERS, 3.14159);\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testTopologyStatsSampleRateIsFloat",
  "sourceCode" : "@Test\r\npublic void testTopologyStatsSampleRateIsFloat() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_STATS_SAMPLE_RATE, 0.5);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Config.TOPOLOGY_STATS_SAMPLE_RATE, 10);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(Config.TOPOLOGY_STATS_SAMPLE_RATE, Double.MAX_VALUE);\r\n    ConfigValidation.validateFields(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testWorkerChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testWorkerChildoptsIsStringOrStringList() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    passCases.add(null);\r\n    passCases.add(\"some string\");\r\n    String[] stuff = { \"some\", \"string\", \"list\" };\r\n    passCases.add(Arrays.asList(stuff));\r\n    failCases.add(42);\r\n    Integer[] wrongStuff = { 1, 2, 3 };\r\n    failCases.add(Arrays.asList(wrongStuff));\r\n    //worker.childopts validates\r\n    for (Object value : passCases) {\r\n        conf.put(Config.WORKER_CHILDOPTS, value);\r\n        ConfigValidation.validateFields(conf);\r\n    }\r\n    for (Object value : failCases) {\r\n        conf.put(Config.WORKER_CHILDOPTS, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf));\r\n    }\r\n    //topology.worker.childopts validates\r\n    conf.clear();\r\n    for (Object value : passCases) {\r\n        conf.put(Config.TOPOLOGY_WORKER_CHILDOPTS, value);\r\n        ConfigValidation.validateFields(conf);\r\n    }\r\n    for (Object value : failCases) {\r\n        conf.put(Config.TOPOLOGY_WORKER_CHILDOPTS, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testValidity",
  "sourceCode" : "@Test\r\npublic void testValidity() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_DEBUG, true);\r\n    conf.put(\"q\", \"asasdasd\");\r\n    conf.put(\"aaa\", Integer.valueOf(\"123\"));\r\n    conf.put(\"bbb\", Long.valueOf(\"456\"));\r\n    List<Object> testList = new ArrayList<>();\r\n    testList.add(1);\r\n    testList.add(2);\r\n    testList.add(Integer.valueOf(\"3\"));\r\n    testList.add(Long.valueOf(\"4\"));\r\n    testList.add(new Float(\"3\"));\r\n    testList.add(new Double(\"4\"));\r\n    testList.add(ImmutableList.of(\"asdf\", 3));\r\n    conf.put(\"eee\", testList);\r\n    assertTrue(Utils.isValidConf(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testNonValidConfigChar",
  "sourceCode" : "@Test\r\npublic void testNonValidConfigChar() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(\"q\", ImmutableList.of(\"asdf\", 'c'));\r\n    assertFalse(Utils.isValidConf(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testNonValidConfigRandomObject",
  "sourceCode" : "@Test\r\npublic void testNonValidConfigRandomObject() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(\"q\", ImmutableList.of(\"asdf\", new TestConfigValidate()));\r\n    assertFalse(Utils.isValidConf(conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testKryoRegValidator",
  "sourceCode" : "@Test\r\npublic void testKryoRegValidator() {\r\n    KryoRegValidator validator = new KryoRegValidator();\r\n    // fail cases\r\n    Object[] failCases = { ImmutableMap.of(\"f\", \"g\"), ImmutableList.of(1), Collections.singletonList(ImmutableMap.of(\"a\", 1)) };\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n    // pass cases\r\n    validator.validateField(\"test\", Arrays.asList(\"a\", \"b\", \"c\", ImmutableMap.of(\"d\", \"e\"), ImmutableMap.of(\"f\", \"g\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testPowerOf2Validator",
  "sourceCode" : "@Test\r\npublic void testPowerOf2Validator() {\r\n    PowerOf2Validator validator = new PowerOf2Validator();\r\n    Object[] failCases = { 42.42, 42, -33, 23423423423.0, -32, -1, -0.00001, 0, -0, \"Forty-two\" };\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n    Object[] passCases = { 64, 4294967296.0, 1, null };\r\n    for (Object value : passCases) {\r\n        validator.validateField(\"test\", value);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testPositiveNumberValidator",
  "sourceCode" : "@Test\r\npublic void testPositiveNumberValidator() {\r\n    PositiveNumberValidator validator = new PositiveNumberValidator();\r\n    Object[] passCases = { null, 1.0, 0.01, 1, 2147483647, 42 };\r\n    for (Object value : passCases) {\r\n        validator.validateField(\"test\", value);\r\n    }\r\n    Object[] failCases = { -1.0, -1, -0.01, 0.0, 0, \"43\", \"string\" };\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n    Object[] passCasesIncludeZero = { null, 1.0, 0.01, 0, 2147483647, 0.0 };\r\n    for (Object value : passCasesIncludeZero) {\r\n        validator.validateField(\"test\", true, value);\r\n    }\r\n    Object[] failCasesIncludeZero = { -1.0, -1, -0.01, \"43\", \"string\" };\r\n    for (Object value : failCasesIncludeZero) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", true, value));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testIntegerValidator",
  "sourceCode" : "@Test\r\npublic void testIntegerValidator() {\r\n    IntegerValidator validator = new IntegerValidator();\r\n    Object[] passCases = { null, 1000, Integer.MAX_VALUE };\r\n    for (Object value : passCases) {\r\n        validator.validateField(\"test\", value);\r\n    }\r\n    Object[] failCases = { 1.34, (long) Integer.MAX_VALUE + 1 };\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testLongValidator",
  "sourceCode" : "@Test\r\npublic void testLongValidator() {\r\n    LongValidator validator = new LongValidator();\r\n    Object[] passCases = { null, 1000, Integer.MAX_VALUE, Long.MAX_VALUE };\r\n    for (Object value : passCases) {\r\n        validator.validateField(\"test\", value);\r\n    }\r\n    Object[] failCases = { 1.34, BigInteger.valueOf(Long.MAX_VALUE).add(BigInteger.valueOf(1L)) };\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "NoDuplicateInListValidator",
  "sourceCode" : "@Test\r\npublic void NoDuplicateInListValidator() {\r\n    NoDuplicateInListValidator validator = new NoDuplicateInListValidator();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Object[] passCase1 = { 1000, 0, -1000 };\r\n    Object[] passCase2 = { \"one\", \"two\", \"three\" };\r\n    Object[] passCase3 = { false, true };\r\n    Object[] passCase4 = { false, true, 1000, 0, -1000, \"one\", \"two\", \"three\" };\r\n    Object[] passCase5 = { 1000.0, 0.0, -1000.0 };\r\n    passCases.add(Arrays.asList(passCase1));\r\n    passCases.add(Arrays.asList(passCase2));\r\n    passCases.add(Arrays.asList(passCase3));\r\n    passCases.add(Arrays.asList(passCase4));\r\n    passCases.add(Arrays.asList(passCase5));\r\n    passCases.add(null);\r\n    for (Object value : passCases) {\r\n        validator.validateField(\"test\", value);\r\n    }\r\n    Object[] failCase1 = { 1000, 0, 1000 };\r\n    Object[] failCase2 = { \"one\", \"one\", \"two\" };\r\n    Object[] failCase3 = { 5.0, 5.0, 6 };\r\n    failCases.add(Arrays.asList(failCase1));\r\n    failCases.add(Arrays.asList(failCase2));\r\n    failCases.add(Arrays.asList(failCase3));\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> validator.validateField(\"test\", value));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testListEntryTypeValidator",
  "sourceCode" : "@Test\r\npublic void testListEntryTypeValidator() {\r\n    Collection<Object> testCases1 = new LinkedList<>();\r\n    Collection<Object> testCases2 = new LinkedList<>();\r\n    Collection<Object> testCases3 = new LinkedList<>();\r\n    Object[] testCase1 = { \"one\", \"two\", \"three\" };\r\n    Object[] testCase2 = { \"three\" };\r\n    testCases1.add(Arrays.asList(testCase1));\r\n    testCases1.add(Arrays.asList(testCase2));\r\n    for (Object value : testCases1) {\r\n        ListEntryTypeValidator.validateField(\"test\", String.class, value);\r\n    }\r\n    for (Object value : testCases1) {\r\n        assertThrows(IllegalArgumentException.class, () -> ListEntryTypeValidator.validateField(\"test\", Number.class, value));\r\n    }\r\n    Object[] testCase3 = { 1000, 0, 1000 };\r\n    Object[] testCase4 = { 5 };\r\n    Object[] testCase5 = { 5.0, 5.0, 6 };\r\n    testCases2.add(Arrays.asList(testCase3));\r\n    testCases2.add(Arrays.asList(testCase4));\r\n    testCases2.add(Arrays.asList(testCase5));\r\n    for (Object value : testCases2) {\r\n        assertThrows(IllegalArgumentException.class, () -> ListEntryTypeValidator.validateField(\"test\", String.class, value));\r\n    }\r\n    for (Object value : testCases2) {\r\n        ListEntryTypeValidator.validateField(\"test\", Number.class, value);\r\n    }\r\n    Object[] testCase6 = { 1000, 0, 1000, \"5\" };\r\n    Object[] testCase7 = { \"4\", \"5\", 5 };\r\n    testCases3.add(Arrays.asList(testCase6));\r\n    testCases3.add(Arrays.asList(testCase7));\r\n    for (Object value : testCases3) {\r\n        assertThrows(IllegalArgumentException.class, () -> ListEntryTypeValidator.validateField(\"test\", String.class, value));\r\n    }\r\n    for (Object value : testCases1) {\r\n        assertThrows(IllegalArgumentException.class, () -> ListEntryTypeValidator.validateField(\"test\", Number.class, value));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testMapEntryTypeAnnotation",
  "sourceCode" : "@Test\r\npublic void testMapEntryTypeAnnotation() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Map<Object, Object> passCase1 = new HashMap<>();\r\n    passCase1.put(\"aaa\", 5);\r\n    passCase1.put(\"bbb\", 6);\r\n    passCase1.put(\"ccc\", 7);\r\n    passCases.add(passCase1);\r\n    passCases.add(null);\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    Map<Object, Object> failCase1 = new HashMap<>();\r\n    failCase1.put(\"aaa\", 5);\r\n    failCase1.put(5, 6);\r\n    failCase1.put(\"ccc\", 7);\r\n    Map<Object, Object> failCase2 = new HashMap<>();\r\n    failCase2.put(\"aaa\", \"str\");\r\n    failCase2.put(\"bbb\", 6);\r\n    failCase2.put(\"ccc\", 7);\r\n    failCases.add(failCase1);\r\n    failCases.add(failCase2);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testMapEntryCustomAnnotation",
  "sourceCode" : "@Test\r\npublic void testMapEntryCustomAnnotation() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Map<Object, Object> passCase1 = new HashMap<>();\r\n    passCase1.put(\"aaa\", 5);\r\n    passCase1.put(\"bbb\", 100);\r\n    passCase1.put(\"ccc\", Integer.MAX_VALUE);\r\n    passCases.add(passCase1);\r\n    passCases.add(null);\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_2, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    Map<Object, Object> failCase1 = new HashMap<>();\r\n    failCase1.put(\"aaa\", 5);\r\n    failCase1.put(5, 6);\r\n    failCase1.put(\"ccc\", 7);\r\n    Map<Object, Object> failCase2 = new HashMap<>();\r\n    failCase2.put(\"aaa\", \"str\");\r\n    failCase2.put(\"bbb\", 6);\r\n    failCase2.put(\"ccc\", 7);\r\n    Map<Object, Object> failCase3 = new HashMap<>();\r\n    failCase3.put(\"aaa\", -1);\r\n    failCase3.put(\"bbb\", 6);\r\n    failCase3.put(\"ccc\", 7);\r\n    Map<Object, Object> failCase4 = new HashMap<>();\r\n    failCase4.put(\"aaa\", 1);\r\n    failCase4.put(\"bbb\", 6);\r\n    failCase4.put(\"ccc\", 7.4);\r\n    failCases.add(failCase1);\r\n    failCases.add(failCase2);\r\n    failCases.add(failCase3);\r\n    failCases.add(failCase4);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_2, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testExactlyOneOfCustomAnnotation",
  "sourceCode" : "@Test\r\npublic void testExactlyOneOfCustomAnnotation() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    List<Object> passCaseListOfList = new ArrayList<>();\r\n    passCaseListOfList.add(Arrays.asList(\"comp1\", \"comp2\"));\r\n    passCaseListOfList.add(Arrays.asList(\"comp1\", \"comp3\"));\r\n    passCaseListOfList.add(Arrays.asList(\"comp2\", \"comp4\"));\r\n    passCaseListOfList.add(Arrays.asList(\"comp2\", \"comp5\"));\r\n    Map<Object, Object> passCaseMapOfMap = new HashMap<>();\r\n    passCaseMapOfMap.put(\"comp1\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 10 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp2\", \"comp3\") } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCaseMapOfMap.put(\"comp2\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 2 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp4\", \"comp5\") } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCases.add(passCaseMapOfMap);\r\n    passCaseMapOfMap = new HashMap<>();\r\n    passCaseMapOfMap.put(\"comp1\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp2\", \"comp3\") } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCaseMapOfMap.put(\"comp2\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 2 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp4\", \"comp5\") } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCases.add(passCaseMapOfMap);\r\n    passCaseMapOfMap = new HashMap<>();\r\n    passCaseMapOfMap.put(\"comp1\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, \"comp2\" } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCaseMapOfMap.put(\"comp2\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 2 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, \"comp4\" } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    passCases.add(passCaseMapOfMap);\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_9, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    List<Object> failCaseList = new ArrayList<>();\r\n    failCaseList.add(Arrays.asList(\"comp1\", Arrays.asList(\"comp2\", \"comp3\")));\r\n    failCaseList.add(Arrays.asList(\"comp3\", Arrays.asList(\"comp4\", \"comp5\")));\r\n    failCases.add(failCaseList);\r\n    Map<String, Object> failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"comp1\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 10 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(1, 2, 3) } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    failCaseMapOfMap.put(\"comp2\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 2 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp4\", \"comp5\") } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"comp1\", Stream.of(new Object[][] { { RasConstraintsTypeValidator.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, 10 }, { RasConstraintsTypeValidator.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, Arrays.asList(\"comp1\", 3) } }).collect(Collectors.toMap(data -> data[0], data -> data[1])));\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"comp1\", Arrays.asList(\"comp2\", \"comp3\"));\r\n    failCaseMapOfMap.put(\"comp2\", Arrays.asList(\"comp4\", \"comp5\"));\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"aaa\", \"str\");\r\n    failCaseMapOfMap.put(\"bbb\", 6);\r\n    failCaseMapOfMap.put(\"ccc\", 7);\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"aaa\", -1);\r\n    failCaseMapOfMap.put(\"bbb\", 6);\r\n    failCaseMapOfMap.put(\"ccc\", 7);\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"aaa\", 1);\r\n    failCaseMapOfMap.put(\"bbb\", 6);\r\n    failCaseMapOfMap.put(\"ccc\", 7.4);\r\n    failCases.add(failCaseMapOfMap);\r\n    failCaseMapOfMap = new HashMap<>();\r\n    failCaseMapOfMap.put(\"comp1\", \"comp2\");\r\n    failCaseMapOfMap.put(\"comp2\", \"comp4\");\r\n    failCases.add(failCaseMapOfMap);\r\n    failCases.add(null);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_9, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testListEntryTypeAnnotation",
  "sourceCode" : "@Test\r\npublic void testListEntryTypeAnnotation() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Object[] passCase1 = { 1, 5.0, -0.01, 0, Integer.MAX_VALUE, Double.MIN_VALUE };\r\n    Object[] passCase2 = { 1 };\r\n    passCases.add(Arrays.asList(passCase1));\r\n    passCases.add(Arrays.asList(passCase2));\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_3, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    Object[] failCase1 = { 1, 5.0, -0.01, 0, \"aaa\" };\r\n    Object[] failCase2 = { \"aaa\" };\r\n    failCases.add(failCase1);\r\n    failCases.add(failCase2);\r\n    failCases.add(1);\r\n    failCases.add(\"b\");\r\n    failCases.add(null);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_3, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "testListEntryCustomAnnotation",
  "sourceCode" : "@Test\r\npublic void testListEntryCustomAnnotation() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Object[] passCase1 = { 1, 5.0, 0.01, Double.MAX_VALUE };\r\n    Object[] passCase2 = { 1 };\r\n    passCases.add(Arrays.asList(passCase1));\r\n    passCases.add(Arrays.asList(passCase2));\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_4, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    Object[] failCase1 = { 1, 5.0, -0.01, 3.0 };\r\n    Object[] failCase2 = { 1, 5.0, -0.01, 1 };\r\n    Object[] failCase3 = { \"aaa\", \"bbb\", \"aaa\" };\r\n    Object[] failCase4 = { 1, 5.0, null, 1 };\r\n    Object[] failCase5 = { 1, 5.0, 0, 1 };\r\n    failCases.add(Arrays.asList(failCase1));\r\n    failCases.add(Arrays.asList(failCase2));\r\n    failCases.add(Arrays.asList(failCase3));\r\n    failCases.add(Arrays.asList(failCase4));\r\n    failCases.add(Arrays.asList(failCase5));\r\n    failCases.add(1);\r\n    failCases.add(\"b\");\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_4, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "TestAcceptedStrings",
  "sourceCode" : "@Test\r\npublic void TestAcceptedStrings() {\r\n    TestConfig config = new TestConfig();\r\n    String[] passCases = { \"aaa\", \"bbb\", \"ccc\" };\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_5, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    String[] failCases = { \"aa\", \"bb\", \"cc\", \"abc\", \"a\", \"b\", \"c\", \"\" };\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_5, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "TestImpersonationAclUserEntryValidator",
  "sourceCode" : "@Test\r\npublic void TestImpersonationAclUserEntryValidator() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Map<String, Map<String, List<String>>> passCase1 = new HashMap<>();\r\n    Map<String, List<String>> passCase1_hostsAndGroups = new HashMap<>();\r\n    String[] hosts = { \"host.1\", \"host.2\", \"host.3\" };\r\n    passCase1_hostsAndGroups.put(\"hosts\", Arrays.asList(hosts));\r\n    String[] groups = { \"group.1\", \"group.2\", \"group.3\" };\r\n    passCase1_hostsAndGroups.put(\"groups\", Arrays.asList(groups));\r\n    passCase1.put(\"jerry\", passCase1_hostsAndGroups);\r\n    passCases.add(passCase1);\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_6, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    Map<String, Map<String, List<String>>> failCase1 = new HashMap<>();\r\n    Map<String, List<String>> failCase1_hostsAndGroups = new HashMap<>();\r\n    failCase1_hostsAndGroups.put(\"hosts\", Arrays.asList(hosts));\r\n    failCase1.put(\"jerry\", failCase1_hostsAndGroups);\r\n    Map<String, Map<String, List<String>>> failCase2 = new HashMap<>();\r\n    Map<String, List<String>> failCase2_hostsAndGroups = new HashMap<>();\r\n    String[] failgroups = { \"group.1\", \"group.2\", \"group.3\" };\r\n    failCase2_hostsAndGroups.put(\"groups\", Arrays.asList(groups));\r\n    failCase2.put(\"jerry\", failCase2_hostsAndGroups);\r\n    failCases.add(failCase1);\r\n    failCases.add(failCase2);\r\n    failCases.add(\"stuff\");\r\n    failCases.add(5);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_6, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "TestResourceAwareSchedulerUserPool",
  "sourceCode" : "@Test\r\npublic void TestResourceAwareSchedulerUserPool() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Map<String, Map<String, Integer>> passCase1 = new HashMap<>();\r\n    passCase1.put(\"jerry\", new HashMap<>());\r\n    passCase1.put(\"bobby\", new HashMap<>());\r\n    passCase1.put(\"derek\", new HashMap<>());\r\n    passCase1.get(\"jerry\").put(\"cpu\", 10000);\r\n    passCase1.get(\"jerry\").put(\"memory\", 20148);\r\n    passCase1.get(\"bobby\").put(\"cpu\", 20000);\r\n    passCase1.get(\"bobby\").put(\"memory\", 40148);\r\n    passCase1.get(\"derek\").put(\"cpu\", 30000);\r\n    passCase1.get(\"derek\").put(\"memory\", 60148);\r\n    config.put(TestConfig.TEST_MAP_CONFIG_7, passCase1);\r\n    ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    Map<String, Map<String, Integer>> failCase1 = new HashMap<>();\r\n    failCase1.put(\"jerry\", new HashMap<>());\r\n    failCase1.put(\"bobby\", new HashMap<>());\r\n    failCase1.put(\"derek\", new HashMap<>());\r\n    failCase1.get(\"jerry\").put(\"cpu\", 10000);\r\n    failCase1.get(\"jerry\").put(\"memory\", 20148);\r\n    failCase1.get(\"bobby\").put(\"cpu\", 20000);\r\n    failCase1.get(\"bobby\").put(\"memory\", 40148);\r\n    //this will fail the test since user derek does not have an entry for memory\r\n    failCase1.get(\"derek\").put(\"cpu\", 30000);\r\n    Map<String, Map<String, Integer>> failCase2 = new HashMap<>();\r\n    //this will fail since jerry doesn't have either cpu or memory entries\r\n    failCase2.put(\"jerry\", new HashMap<>());\r\n    failCase2.put(\"bobby\", new HashMap<>());\r\n    failCase2.put(\"derek\", new HashMap<>());\r\n    failCase2.get(\"bobby\").put(\"cpu\", 20000);\r\n    failCase2.get(\"bobby\").put(\"memory\", 40148);\r\n    failCase2.get(\"derek\").put(\"cpu\", 30000);\r\n    failCase2.get(\"derek\").put(\"memory\", 60148);\r\n    failCases.add(failCase1);\r\n    failCases.add(failCase2);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_7, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestConfigValidate.java",
  "methodName" : "TestImplementsClassValidator",
  "sourceCode" : "@Test\r\npublic void TestImplementsClassValidator() {\r\n    TestConfig config = new TestConfig();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    passCases.add(\"org.apache.storm.networktopography.DefaultRackDNSToSwitchMapping\");\r\n    for (Object value : passCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_8, value);\r\n        ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class));\r\n    }\r\n    //will fail since org.apache.storm.nimbus.NimbusInfo doesn't implement or extend org.apache.storm.networktopography\r\n    // .DNSToSwitchMapping\r\n    failCases.add(\"org.apache.storm.nimbus.NimbusInfo\");\r\n    failCases.add(null);\r\n    for (Object value : failCases) {\r\n        config.put(TestConfig.TEST_MAP_CONFIG_8, value);\r\n        assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(config, Collections.singletonList(TestConfig.class)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormSubmitter.java",
  "methodName" : "invalidTopologyWithoutSpout",
  "sourceCode" : "@Test\r\npublic void invalidTopologyWithoutSpout() {\r\n    String expectedExceptionMsgFragment = \"does not have any spout\";\r\n    TopologyBuilder tb = new TopologyBuilder();\r\n    tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n    tb.setBolt(\"bolt11\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n    tb.setBolt(\"bolt12\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n    StormTopology topology = tb.createTopology();\r\n    Map<String, Object> topoConf = null;\r\n    SubmitOptions opts = new SubmitOptions(TopologyInitialStatus.INACTIVE);\r\n    try {\r\n        StormSubmitter.submitTopologyAs(\"test-topo-without-spout\", topoConf, topology, opts, null, \"none\");\r\n        fail(\"Topology without spout should fail in submission\");\r\n    } catch (InvalidTopologyException ex) {\r\n        if (!ex.getMessage().contains(expectedExceptionMsgFragment)) {\r\n            String err = String.format(\"Topology submit failure should contain string \\\"%s\\\", but is \\\"%s\\\"\", expectedExceptionMsgFragment, ex.getMessage());\r\n            fail(err);\r\n        }\r\n    } catch (Throwable ex) {\r\n        ex.printStackTrace();\r\n        fail(\"Unexpected exception submitting topology without spout: \" + ex);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormTimer.java",
  "methodName" : "testSchedule",
  "sourceCode" : "/**\r\n * Test {@link StormTimer#schedule(int, Runnable)} and {@link StormTimer#schedule(int, Runnable, boolean, int)}\r\n * for scheduling order under multithreaded environment.\r\n */\r\n@Test\r\npublic void testSchedule() {\r\n    StormTimer stormTimer = new StormTimer(\"testSchedule\", (x, y) -> {\r\n    });\r\n    int threadCnt = 100;\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.IMMEDIATE));\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.AFTER_1_SECOND));\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.IMMEDIATE_WITH_JITTER));\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.AFTER_MILLISECONDS_WITH_JITTER));\r\n    close(stormTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormTimer.java",
  "methodName" : "testScheduleMs",
  "sourceCode" : "/**\r\n * Test {@link StormTimer#scheduleMs(long, Runnable)} and\r\n * {@link StormTimer#scheduleMs(long, Runnable, boolean, int)} for scheduling order under multithreaded environment.\r\n */\r\n@Test\r\npublic void testScheduleMs() {\r\n    StormTimer stormTimer = new StormTimer(\"testScheduleMs\", (x, y) -> {\r\n    });\r\n    int threadCnt = 100;\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.AFTER_MILLISECONDS));\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.AFTER_MILLISECONDS_WITH_JITTER));\r\n    close(stormTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormTimer.java",
  "methodName" : "scheduleRecurring",
  "sourceCode" : "/**\r\n * Test {@link StormTimer#scheduleRecurring(int, int, Runnable)} for scheduling order under multithreaded environment.\r\n */\r\n@Test\r\npublic void scheduleRecurring() {\r\n    StormTimer stormTimer = new StormTimer(\"testScheduleMs\", (x, y) -> {\r\n    });\r\n    int threadCnt = 10;\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.RECURRING));\r\n    close(stormTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormTimer.java",
  "methodName" : "testScheduleRecurringMs",
  "sourceCode" : "/**\r\n * Test {@link StormTimer#scheduleRecurringMs(long, long, Runnable)} for scheduling order under multithreaded environment.\r\n */\r\n@Test\r\npublic void testScheduleRecurringMs() {\r\n    StormTimer stormTimer = new StormTimer(\"testScheduleRecurringMs\", (x, y) -> {\r\n    });\r\n    int threadCnt = 10;\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.RECURRING_MS));\r\n    close(stormTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\TestStormTimer.java",
  "methodName" : "testScheduleRecurringWithJitter",
  "sourceCode" : "/**\r\n * Test {@link StormTimer#scheduleRecurringWithJitter(int, int, int, Runnable)}\r\n * for scheduling order under multithreaded environment.\r\n */\r\n@Test\r\npublic void testScheduleRecurringWithJitter() {\r\n    StormTimer stormTimer = new StormTimer(\"testScheduleRecurringWithJitter\", (x, y) -> {\r\n    });\r\n    int threadCnt = 10;\r\n    Assertions.assertTrue(schedule(stormTimer, threadCnt, SCHEDULE_TYPE.RECURRING_WITH_JITTER));\r\n    close(stormTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testExecuteTuple",
  "sourceCode" : "@Test\r\npublic void testExecuteTuple() {\r\n    Mockito.when(mockWaterMarkEventGenerator.track(Mockito.any(), Mockito.anyLong())).thenReturn(true);\r\n    Mockito.when(mockTimestampExtractor.extractTimestamp(Mockito.any())).thenReturn(tupleTs);\r\n    Tuple mockTuple = Mockito.mock(Tuple.class);\r\n    executor.initState(null);\r\n    executor.waterMarkEventGenerator = mockWaterMarkEventGenerator;\r\n    executor.execute(mockTuple);\r\n    // should be ack-ed once\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockTuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testExecuteLatetuple",
  "sourceCode" : "@Test\r\npublic void testExecuteLatetuple() {\r\n    Mockito.when(mockWaterMarkEventGenerator.track(Mockito.any(), Mockito.anyLong())).thenReturn(false);\r\n    Mockito.when(mockTimestampExtractor.extractTimestamp(Mockito.any())).thenReturn(tupleTs);\r\n    Tuple mockTuple = Mockito.mock(Tuple.class);\r\n    executor.initState(null);\r\n    executor.waterMarkEventGenerator = mockWaterMarkEventGenerator;\r\n    executor.execute(mockTuple);\r\n    // ack-ed once\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockTuple);\r\n    // late tuple emitted\r\n    ArgumentCaptor<String> stringCaptor = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).emit(stringCaptor.capture(), anchorCaptor.capture(), valuesCaptor.capture());\r\n    assertEquals(LATE_STREAM, stringCaptor.getValue());\r\n    assertEquals(Collections.singletonList(mockTuple), anchorCaptor.getValue());\r\n    assertEquals(new Values(mockTuple), valuesCaptor.getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testActivation",
  "sourceCode" : "@Test\r\npublic void testActivation() {\r\n    Mockito.when(mockWaterMarkEventGenerator.track(Mockito.any(), Mockito.anyLong())).thenReturn(true);\r\n    Mockito.when(mockTimestampExtractor.extractTimestamp(Mockito.any())).thenReturn(tupleTs);\r\n    executor.initState(null);\r\n    executor.waterMarkEventGenerator = mockWaterMarkEventGenerator;\r\n    List<Tuple> mockTuples = getMockTuples(WINDOW_EVENT_COUNT);\r\n    mockTuples.forEach(t -> executor.execute(t));\r\n    // all tuples acked\r\n    Mockito.verify(mockOutputCollector, Mockito.times(WINDOW_EVENT_COUNT)).ack(tupleCaptor.capture());\r\n    assertArrayEquals(mockTuples.toArray(), tupleCaptor.getAllValues().toArray());\r\n    Mockito.doAnswer((Answer<Void>) invocation -> {\r\n        TupleWindow window = (TupleWindow) invocation.getArguments()[0];\r\n        assertEquals(WINDOW_EVENT_COUNT, window.get().size());\r\n        assertEquals(WINDOW_EVENT_COUNT, window.get().size());\r\n        assertEquals(WINDOW_EVENT_COUNT, window.get().size());\r\n        return null;\r\n    }).when(mockBolt).execute(Mockito.any());\r\n    // trigger the window\r\n    long activationTs = tupleTs + 1000;\r\n    executor.getWindowManager().add(new WaterMarkEvent<>(activationTs));\r\n    executor.prePrepare(0);\r\n    // partition ids\r\n    ArgumentCaptor<String> pkCatptor = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockPartitionState, Mockito.times(1)).put(pkCatptor.capture(), partitionValuesCaptor.capture());\r\n    assertEquals(PARTITION_KEY, pkCatptor.getValue());\r\n    List<Long> expectedPartitionIds = Collections.singletonList(0L);\r\n    assertThat(partitionValuesCaptor.getValue(), contains(expectedPartitionIds.toArray(new Long[0])));\r\n    // window partitions\r\n    Mockito.verify(mockWindowState, Mockito.times(1)).put(longCaptor.capture(), windowValuesCaptor.capture());\r\n    assertEquals((long) expectedPartitionIds.get(0), (long) longCaptor.getValue());\r\n    assertEquals(WINDOW_EVENT_COUNT, windowValuesCaptor.getValue().size());\r\n    List<Tuple> tuples = windowValuesCaptor.getValue().getEvents().stream().map(Event::get).collect(Collectors.toList());\r\n    assertArrayEquals(mockTuples.toArray(), tuples.toArray());\r\n    // window system state\r\n    ArgumentCaptor<String> keyCaptor = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockSystemState, Mockito.times(2)).put(keyCaptor.capture(), systemValuesCaptor.capture());\r\n    assertEquals(EVICTION_STATE_KEY, keyCaptor.getAllValues().get(0));\r\n    assertEquals(Optional.of(Pair.of((long) WINDOW_EVENT_COUNT, (long) WINDOW_EVENT_COUNT)), systemValuesCaptor.getAllValues().get(0));\r\n    assertEquals(TRIGGER_STATE_KEY, keyCaptor.getAllValues().get(1));\r\n    assertEquals(Optional.of(tupleTs), systemValuesCaptor.getAllValues().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testCacheEviction",
  "sourceCode" : "@Test\r\npublic void testCacheEviction() {\r\n    Mockito.when(mockWaterMarkEventGenerator.track(Mockito.any(), Mockito.anyLong())).thenReturn(true);\r\n    Mockito.when(mockTimestampExtractor.extractTimestamp(Mockito.any())).thenReturn(tupleTs);\r\n    executor.initState(null);\r\n    executor.waterMarkEventGenerator = mockWaterMarkEventGenerator;\r\n    int tupleCount = 20000;\r\n    List<Tuple> mockTuples = getMockTuples(tupleCount);\r\n    mockTuples.forEach(t -> executor.execute(t));\r\n    int numPartitions = tupleCount / WindowState.MAX_PARTITION_EVENTS;\r\n    int numEvictedPartitions = numPartitions - WindowState.MIN_PARTITIONS;\r\n    Mockito.verify(mockWindowState, Mockito.times(numEvictedPartitions)).put(longCaptor.capture(), windowValuesCaptor.capture());\r\n    // number of evicted events\r\n    assertEquals(numEvictedPartitions * WindowState.MAX_PARTITION_EVENTS, windowValuesCaptor.getAllValues().stream().mapToInt(x -> x.size()).sum());\r\n    Map<Long, WindowState.WindowPartition<Tuple>> partitionMap = new HashMap<>();\r\n    windowValuesCaptor.getAllValues().forEach(v -> partitionMap.put(v.getId(), v));\r\n    ArgumentCaptor<String> stringCaptor = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockPartitionState, Mockito.times(numPartitions)).put(stringCaptor.capture(), partitionValuesCaptor.capture());\r\n    // partition ids 0 .. 19\r\n    assertThat(partitionValuesCaptor.getAllValues().get(numPartitions - 1), contains(LongStream.range(0, numPartitions).boxed().collect(Collectors.toList()).toArray(new Long[0])));\r\n    Mockito.when(mockWindowState.get(Mockito.any(), Mockito.any())).then(invocation -> {\r\n        Long partition = invocation.getArgument(0);\r\n        WindowState.WindowPartition<Tuple> evicted = partitionMap.get(partition);\r\n        return evicted != null ? evicted : invocation.getArgument(1);\r\n    });\r\n    Mockito.doAnswer((Answer<Void>) invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        partitionMap.put((long) args[0], (WindowState.WindowPartition<Tuple>) args[1]);\r\n        return null;\r\n    }).when(mockWindowState).put(Mockito.any(), Mockito.any());\r\n    // trigger the window\r\n    long activationTs = tupleTs + 1000;\r\n    executor.getWindowManager().add(new WaterMarkEvent<>(activationTs));\r\n    Mockito.verify(mockBolt, Mockito.times(tupleCount / WINDOW_EVENT_COUNT)).execute(Mockito.any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testRollbackBeforeInit",
  "sourceCode" : "@Test\r\npublic void testRollbackBeforeInit() {\r\n    executor.preRollback();\r\n    Mockito.verify(mockBolt, Mockito.times(1)).preRollback();\r\n    // partition ids\r\n    Mockito.verify(mockPartitionState, Mockito.times(1)).rollback();\r\n    Mockito.verify(mockWindowState, Mockito.times(1)).rollback();\r\n    Mockito.verify(mockSystemState, Mockito.times(1)).rollback();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\PersistentWindowedBoltExecutorTest.java",
  "methodName" : "testRollbackAfterInit",
  "sourceCode" : "@Test\r\npublic void testRollbackAfterInit() {\r\n    executor.initState(null);\r\n    executor.prePrepare(0);\r\n    executor.preRollback();\r\n    Mockito.verify(mockBolt, Mockito.times(1)).preRollback();\r\n    Mockito.verify(mockPartitionState, Mockito.times(1)).rollback();\r\n    ArgumentCaptor<String> stringArgumentCaptor = ArgumentCaptor.forClass(String.class);\r\n    Mockito.verify(mockPartitionState, Mockito.times(2)).put(stringArgumentCaptor.capture(), partitionValuesCaptor.capture());\r\n    Mockito.verify(mockWindowState, Mockito.times(1)).rollback();\r\n    Mockito.verify(mockSystemState, Mockito.times(1)).rollback();\r\n    Mockito.verify(mockSystemState, Mockito.times(2)).iterator();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testBuildInvalid1",
  "sourceCode" : "@Test\r\npublic void testBuildInvalid1() {\r\n    assertThrows(IllegalArgumentException.class, () -> SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(0).build(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testBuildInvalid2",
  "sourceCode" : "@Test\r\npublic void testBuildInvalid2() {\r\n    assertThrows(IllegalArgumentException.class, () -> SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(-1).build(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testBuildInvalid3",
  "sourceCode" : "@Test\r\npublic void testBuildInvalid3() {\r\n    assertThrows(NullPointerException.class, () -> SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(1).build(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testBuildOk",
  "sourceCode" : "@Test\r\npublic void testBuildOk() {\r\n    SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(1).removalListener((key, val, removalCause) -> {\r\n    }).build(key -> key);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testGet",
  "sourceCode" : "@Test\r\npublic void testGet() {\r\n    List<Integer> removed = new ArrayList<>();\r\n    List<Integer> loaded = new ArrayList<>();\r\n    SimpleWindowPartitionCache<Integer, Integer> cache = SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(2).removalListener((key, val, removalCause) -> removed.add(key)).build(key -> {\r\n        loaded.add(key);\r\n        return key;\r\n    });\r\n    cache.get(1);\r\n    cache.get(2);\r\n    cache.get(3);\r\n    assertEquals(Arrays.asList(1, 2, 3), loaded);\r\n    // since 2 is the largest un-pinned entry before 3 is loaded\r\n    assertEquals(Collections.singletonList(2), removed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testGetNull",
  "sourceCode" : "@Test\r\npublic void testGetNull() {\r\n    assertThrows(NullPointerException.class, () -> {\r\n        SimpleWindowPartitionCache<Integer, Integer> cache = SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(2).build(key -> null);\r\n        cache.get(1);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testEvictNoRemovalListener",
  "sourceCode" : "@Test\r\npublic void testEvictNoRemovalListener() {\r\n    SimpleWindowPartitionCache<Integer, Integer> cache = SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(1).build(key -> key);\r\n    cache.get(1);\r\n    cache.get(2);\r\n    assertEquals(Collections.singletonMap(2, 2), cache.asMap());\r\n    cache.invalidate(2);\r\n    assertEquals(Collections.emptyMap(), cache.asMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testPinAndGet",
  "sourceCode" : "@Test\r\npublic void testPinAndGet() {\r\n    List<Integer> removed = new ArrayList<>();\r\n    List<Integer> loaded = new ArrayList<>();\r\n    SimpleWindowPartitionCache<Integer, Integer> cache = SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(1).removalListener((key, val, removalCause) -> removed.add(key)).build(key -> {\r\n        loaded.add(key);\r\n        return key;\r\n    });\r\n    cache.get(1);\r\n    cache.pinAndGet(2);\r\n    cache.get(3);\r\n    assertEquals(Arrays.asList(1, 2, 3), loaded);\r\n    assertEquals(Collections.singletonList(1), removed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testInvalidate",
  "sourceCode" : "@Test\r\npublic void testInvalidate() {\r\n    List<Integer> removed = new ArrayList<>();\r\n    List<Integer> loaded = new ArrayList<>();\r\n    SimpleWindowPartitionCache<Integer, Integer> cache = SimpleWindowPartitionCache.<Integer, Integer>newBuilder().maximumSize(1).removalListener((key, val, removalCause) -> removed.add(key)).build(key -> {\r\n        loaded.add(key);\r\n        return key;\r\n    });\r\n    cache.pinAndGet(1);\r\n    cache.invalidate(1);\r\n    assertEquals(Collections.singletonList(1), loaded);\r\n    assertEquals(Collections.emptyList(), removed);\r\n    assertEquals(cache.asMap(), Collections.singletonMap(1, 1));\r\n    cache.unpin(1);\r\n    cache.invalidate(1);\r\n    assertTrue(cache.asMap().isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testConcurrentGet",
  "sourceCode" : "@Timeout(10000)\r\n@Test\r\npublic void testConcurrentGet() throws Exception {\r\n    List<Integer> loaded = new ArrayList<>();\r\n    SimpleWindowPartitionCache<Integer, Object> cache = SimpleWindowPartitionCache.<Integer, Object>newBuilder().maximumSize(1).build(key -> {\r\n        Utils.sleep(1000);\r\n        loaded.add(key);\r\n        return new Object();\r\n    });\r\n    FutureTask<Object> ft1 = new FutureTask<>(() -> cache.pinAndGet(1));\r\n    FutureTask<Object> ft2 = new FutureTask<>(() -> cache.pinAndGet(1));\r\n    Thread t1 = new Thread(ft1);\r\n    Thread t2 = new Thread(ft2);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join();\r\n    t2.join();\r\n    assertEquals(Collections.singletonList(1), loaded);\r\n    assertEquals(ft1.get(), ft2.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testConcurrentUnpin",
  "sourceCode" : "@Test\r\npublic void testConcurrentUnpin() throws Exception {\r\n    SimpleWindowPartitionCache<Integer, Object> cache = SimpleWindowPartitionCache.<Integer, Object>newBuilder().maximumSize(1).build(key -> new Object());\r\n    cache.pinAndGet(1);\r\n    FutureTask<Boolean> ft1 = new FutureTask<>(() -> cache.unpin(1));\r\n    FutureTask<Boolean> ft2 = new FutureTask<>(() -> cache.unpin(1));\r\n    Thread t1 = new Thread(ft1);\r\n    Thread t2 = new Thread(ft2);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join();\r\n    t2.join();\r\n    assertTrue(ft1.get() || ft2.get());\r\n    assertFalse(ft1.get() && ft2.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\SimpleWindowPartitionCacheTest.java",
  "methodName" : "testEviction",
  "sourceCode" : "@Test\r\npublic void testEviction() {\r\n    List<Integer> removed = new ArrayList<>();\r\n    SimpleWindowPartitionCache<Integer, Object> cache = SimpleWindowPartitionCache.<Integer, Object>newBuilder().maximumSize(1).removalListener((key, val, removalCause) -> removed.add(key)).build(key -> new Object());\r\n    cache.get(0);\r\n    cache.pinAndGet(1);\r\n    assertEquals(Collections.singletonList(0), removed);\r\n    cache.get(2);\r\n    assertEquals(Collections.singletonList(0), removed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testHandleTupleBeforeInit",
  "sourceCode" : "@Test\r\npublic void testHandleTupleBeforeInit() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    executor.execute(mockTuple);\r\n    Mockito.verify(mockBolt, Mockito.times(0)).execute(Mockito.any(Tuple.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testHandleTuple",
  "sourceCode" : "@Test\r\npublic void testHandleTuple() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(INITSTATE);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(0));\r\n    Mockito.doNothing().when(mockOutputCollector).ack(mockCheckpointTuple);\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockBolt, Mockito.times(1)).execute(mockTuple);\r\n    Mockito.verify(mockBolt, Mockito.times(1)).initState(Mockito.any(KeyValueState.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testRollback",
  "sourceCode" : "@Test\r\npublic void testRollback() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(ROLLBACK);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(0));\r\n    Mockito.doNothing().when(mockOutputCollector).ack(mockCheckpointTuple);\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockState, Mockito.times(1)).rollback();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testCommit",
  "sourceCode" : "@Test\r\npublic void testCommit() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(COMMIT);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(0));\r\n    Mockito.doNothing().when(mockOutputCollector).ack(mockCheckpointTuple);\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockBolt, Mockito.times(1)).preCommit(0L);\r\n    Mockito.verify(mockState, Mockito.times(1)).commit(0L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testPrepareAndRollbackBeforeInitstate",
  "sourceCode" : "@Test\r\npublic void testPrepareAndRollbackBeforeInitstate() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(PREPARE);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).fail(mockCheckpointTuple);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(ROLLBACK);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    Mockito.doNothing().when(mockOutputCollector).ack(mockCheckpointTuple);\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockState, Mockito.times(1)).rollback();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testCommitBeforeInitstate",
  "sourceCode" : "@Test\r\npublic void testCommitBeforeInitstate() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(COMMIT);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockCheckpointTuple);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(ROLLBACK);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    executor.execute(mockCheckpointTuple);\r\n    Mockito.verify(mockState, Mockito.times(1)).rollback();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulBoltExecutorTest.java",
  "methodName" : "testPrepareAndCommit",
  "sourceCode" : "@Test\r\npublic void testPrepareAndCommit() {\r\n    Mockito.when(mockTuple.getSourceStreamId()).thenReturn(\"default\");\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(INITSTATE);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(0));\r\n    executor.execute(mockCheckpointTuple);\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getSourceStreamId()).thenReturn(CheckpointSpout.CHECKPOINT_STREAM_ID);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(PREPARE);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    executor.execute(mockCheckpointTuple);\r\n    executor.execute(mockTuple);\r\n    Mockito.when(mockCheckpointTuple.getValueByField(CHECKPOINT_FIELD_ACTION)).thenReturn(COMMIT);\r\n    Mockito.when(mockCheckpointTuple.getLongByField(CHECKPOINT_FIELD_TXID)).thenReturn(Long.valueOf(100));\r\n    executor.execute(mockCheckpointTuple);\r\n    mockOutputCollector.ack(mockTuple);\r\n    Mockito.verify(mockState, Mockito.times(1)).commit(100L);\r\n    Mockito.verify(mockBolt, Mockito.times(2)).execute(mockTuple);\r\n    Mockito.verify(mockOutputCollector, Mockito.times(1)).ack(mockTuple);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulWindowedBoltExecutorTest.java",
  "methodName" : "testPrepare",
  "sourceCode" : "@Test\r\npublic void testPrepare() {\r\n    assertThrows(IllegalArgumentException.class, () -> executor.prepare(mockStormConf, mockTopologyContext, mockOutputCollector));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulWindowedBoltExecutorTest.java",
  "methodName" : "testPrepareWithMsgid",
  "sourceCode" : "@Test\r\npublic void testPrepareWithMsgid() {\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_MESSAGE_ID_FIELD_NAME, \"msgid\");\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_COUNT, 5);\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT, 5);\r\n    executor.prepare(mockStormConf, mockTopologyContext, mockOutputCollector);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulWindowedBoltExecutorTest.java",
  "methodName" : "testExecute",
  "sourceCode" : "@Test\r\npublic void testExecute() throws Exception {\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_MESSAGE_ID_FIELD_NAME, \"msgid\");\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_COUNT, 5);\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT, 5);\r\n    KeyValueState<TaskStream, WindowState> mockState;\r\n    mockState = Mockito.mock(KeyValueState.class);\r\n    executor.prepare(mockStormConf, mockTopologyContext, mockOutputCollector, mockState);\r\n    executor.initState(null);\r\n    List<Tuple> tuples = getMockTuples(5);\r\n    for (Tuple tuple : tuples) {\r\n        executor.execute(tuple);\r\n    }\r\n    Mockito.verify(mockBolt, Mockito.times(1)).execute(getTupleWindow(tuples));\r\n    WindowState expectedState = new WindowState(Long.MIN_VALUE, 4);\r\n    Mockito.verify(mockState, Mockito.times(1)).put(Mockito.any(TaskStream.class), Mockito.eq(expectedState));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\StatefulWindowedBoltExecutorTest.java",
  "methodName" : "testRecovery",
  "sourceCode" : "@Test\r\npublic void testRecovery() {\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_MESSAGE_ID_FIELD_NAME, \"msgid\");\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_COUNT, 5);\r\n    mockStormConf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT, 5);\r\n    KeyValueState<TaskStream, WindowState> mockState;\r\n    mockState = Mockito.mock(KeyValueState.class);\r\n    Map<GlobalStreamId, Grouping> mockMap = Mockito.mock(Map.class);\r\n    Mockito.when(mockTopologyContext.getThisSources()).thenReturn(mockMap);\r\n    Mockito.when(mockTopologyContext.getComponentTasks(Mockito.anyString())).thenReturn(Collections.singletonList(1));\r\n    Mockito.when(mockMap.keySet()).thenReturn(Collections.singleton(new GlobalStreamId(\"a\", \"s\")));\r\n    WindowState mockWindowState = new WindowState(4, 4);\r\n    Mockito.when(mockState.get(Mockito.any(TaskStream.class))).thenReturn(mockWindowState);\r\n    executor.prepare(mockStormConf, mockTopologyContext, mockOutputCollector, mockState);\r\n    executor.initState(null);\r\n    List<Tuple> tuples = getMockTuples(10);\r\n    for (Tuple tuple : tuples) {\r\n        executor.execute(tuple);\r\n    }\r\n    WindowState expectedState = new WindowState(4, 9);\r\n    Mockito.verify(mockState, Mockito.times(1)).put(Mockito.any(TaskStream.class), Mockito.eq(expectedState));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\TopologyBuilderTest.java",
  "methodName" : "testSetRichBolt",
  "sourceCode" : "@Test\r\npublic void testSetRichBolt() {\r\n    assertThrows(IllegalArgumentException.class, () -> builder.setBolt(\"bolt\", mock(IRichBolt.class), 0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\TopologyBuilderTest.java",
  "methodName" : "testSetBasicBolt",
  "sourceCode" : "@Test\r\npublic void testSetBasicBolt() {\r\n    assertThrows(IllegalArgumentException.class, () -> builder.setBolt(\"bolt\", mock(IBasicBolt.class), 0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\TopologyBuilderTest.java",
  "methodName" : "testSetSpout",
  "sourceCode" : "@Test\r\npublic void testSetSpout() {\r\n    assertThrows(IllegalArgumentException.class, () -> builder.setSpout(\"spout\", mock(IRichSpout.class), 0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\TopologyBuilderTest.java",
  "methodName" : "testAddWorkerHook",
  "sourceCode" : "@Test\r\npublic void testAddWorkerHook() {\r\n    assertThrows(IllegalArgumentException.class, () -> builder.addWorkerHook(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\TopologyBuilderTest.java",
  "methodName" : "testStatefulTopology",
  "sourceCode" : "@Test\r\npublic void testStatefulTopology() {\r\n    builder.setSpout(\"spout1\", makeDummySpout());\r\n    builder.setSpout(\"spout2\", makeDummySpout());\r\n    builder.setBolt(\"bolt1\", makeDummyStatefulBolt(), 1).shuffleGrouping(\"spout1\").shuffleGrouping(\"spout2\");\r\n    builder.setBolt(\"bolt2\", makeDummyStatefulBolt(), 1).shuffleGrouping(\"spout1\");\r\n    builder.setBolt(\"bolt3\", makeDummyStatefulBolt(), 1).shuffleGrouping(\"bolt1\").shuffleGrouping(\"bolt2\");\r\n    StormTopology topology = builder.createTopology();\r\n    assertNotNull(topology);\r\n    Set<String> spouts = topology.get_spouts().keySet();\r\n    // checkpoint spout should 've been added\r\n    assertEquals(ImmutableSet.of(\"spout1\", \"spout2\", \"$checkpointspout\"), spouts);\r\n    // bolt1, bolt2 should also receive from checkpoint spout\r\n    assertEquals(ImmutableSet.of(new GlobalStreamId(\"spout1\", \"default\"), new GlobalStreamId(\"spout2\", \"default\"), new GlobalStreamId(\"$checkpointspout\", \"$checkpoint\")), topology.get_bolts().get(\"bolt1\").get_common().get_inputs().keySet());\r\n    assertEquals(ImmutableSet.of(new GlobalStreamId(\"spout1\", \"default\"), new GlobalStreamId(\"$checkpointspout\", \"$checkpoint\")), topology.get_bolts().get(\"bolt2\").get_common().get_inputs().keySet());\r\n    // bolt3 should also receive from checkpoint streams of bolt1, bolt2\r\n    assertEquals(ImmutableSet.of(new GlobalStreamId(\"bolt1\", \"default\"), new GlobalStreamId(\"bolt1\", \"$checkpoint\"), new GlobalStreamId(\"bolt2\", \"default\"), new GlobalStreamId(\"bolt2\", \"$checkpoint\")), topology.get_bolts().get(\"bolt3\").get_common().get_inputs().keySet());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testExecuteWithoutTs",
  "sourceCode" : "@Test\r\npublic void testExecuteWithoutTs() {\r\n    assertThrows(IllegalArgumentException.class, () -> executor.execute(getTuple(\"s1\", new Fields(\"a\"), new Values(1), \"s1Src\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testExecuteWithTs",
  "sourceCode" : "@Test\r\npublic void testExecuteWithTs() {\r\n    long[] timestamps = { 603, 605, 607, 618, 626, 636 };\r\n    for (long ts : timestamps) {\r\n        executor.execute(getTuple(\"s1\", new Fields(\"ts\"), new Values(ts), \"s1Src\"));\r\n    }\r\n    //Thread.sleep(120);\r\n    executor.waterMarkEventGenerator.run();\r\n    //System.out.println(testWindowedBolt.tupleWindows);\r\n    assertEquals(3, testWindowedBolt.tupleWindows.size());\r\n    TupleWindow first = testWindowedBolt.tupleWindows.get(0);\r\n    assertArrayEquals(new long[] { 603, 605, 607 }, new long[] { (long) first.get().get(0).getValue(0), (long) first.get().get(1).getValue(0), (long) first.get().get(2).getValue(0) });\r\n    TupleWindow second = testWindowedBolt.tupleWindows.get(1);\r\n    assertArrayEquals(new long[] { 603, 605, 607, 618 }, new long[] { (long) second.get().get(0).getValue(0), (long) second.get().get(1).getValue(0), (long) second.get().get(2).getValue(0), (long) second.get().get(3).getValue(0) });\r\n    TupleWindow third = testWindowedBolt.tupleWindows.get(2);\r\n    assertArrayEquals(new long[] { 618, 626 }, new long[] { (long) third.get().get(0).getValue(0), (long) third.get().get(1).getValue(0) });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testPrepareLateTupleStreamWithoutTs",
  "sourceCode" : "@Test\r\npublic void testPrepareLateTupleStreamWithoutTs() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 100000);\r\n    conf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS, 20);\r\n    conf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS, 10);\r\n    conf.put(Config.TOPOLOGY_BOLTS_LATE_TUPLE_STREAM, \"$late\");\r\n    conf.put(Config.TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_MAX_LAG_MS, 5);\r\n    conf.put(Config.TOPOLOGY_BOLTS_WATERMARK_EVENT_INTERVAL_MS, 10);\r\n    testWindowedBolt = new TestWindowedBolt();\r\n    executor = new WindowedBoltExecutor(testWindowedBolt);\r\n    TopologyContext context = getTopologyContext();\r\n    // emulate the call of withLateTupleStream method\r\n    Mockito.when(context.getThisStreams()).thenReturn(new HashSet<>(Arrays.asList(\"default\", \"$late\")));\r\n    try {\r\n        executor.prepare(conf, context, getOutputCollector());\r\n        fail();\r\n    } catch (IllegalArgumentException e) {\r\n        assertThat(e.getMessage(), is(\"Late tuple stream can be defined only when specifying a timestamp field\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testPrepareLateTupleStreamWithoutBuilder",
  "sourceCode" : "@Test\r\npublic void testPrepareLateTupleStreamWithoutBuilder() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 100000);\r\n    conf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS, 20);\r\n    conf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS, 10);\r\n    conf.put(Config.TOPOLOGY_BOLTS_LATE_TUPLE_STREAM, \"$late\");\r\n    conf.put(Config.TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_MAX_LAG_MS, 5);\r\n    conf.put(Config.TOPOLOGY_BOLTS_WATERMARK_EVENT_INTERVAL_MS, 10);\r\n    testWindowedBolt = new TestWindowedBolt();\r\n    testWindowedBolt.withTimestampField(\"ts\");\r\n    executor = new WindowedBoltExecutor(testWindowedBolt);\r\n    TopologyContext context = getTopologyContext();\r\n    try {\r\n        executor.prepare(conf, context, getOutputCollector());\r\n        fail();\r\n    } catch (IllegalArgumentException e) {\r\n        assertThat(e.getMessage(), is(\"Stream for late tuples must be defined with the builder method withLateTupleStream\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testExecuteWithLateTupleStream",
  "sourceCode" : "@Test\r\npublic void testExecuteWithLateTupleStream() {\r\n    testWindowedBolt = new TestWindowedBolt();\r\n    testWindowedBolt.withTimestampField(\"ts\");\r\n    executor = new WindowedBoltExecutor(testWindowedBolt);\r\n    TopologyContext context = getTopologyContext();\r\n    Mockito.when(context.getThisStreams()).thenReturn(new HashSet<>(Arrays.asList(\"default\", \"$late\")));\r\n    OutputCollector outputCollector = Mockito.mock(OutputCollector.class);\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 100000);\r\n    conf.put(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS, 20);\r\n    conf.put(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS, 10);\r\n    conf.put(Config.TOPOLOGY_BOLTS_LATE_TUPLE_STREAM, \"$late\");\r\n    conf.put(Config.TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_MAX_LAG_MS, 5);\r\n    //Trigger manually to avoid timing issues\r\n    conf.put(Config.TOPOLOGY_BOLTS_WATERMARK_EVENT_INTERVAL_MS, 1_000_000);\r\n    executor.prepare(conf, context, outputCollector);\r\n    long[] timestamps = { 603, 605, 607, 618, 626, 636, 600 };\r\n    List<Tuple> tuples = new ArrayList<>(timestamps.length);\r\n    for (long ts : timestamps) {\r\n        Tuple tuple = getTuple(\"s1\", new Fields(\"ts\"), new Values(ts), \"s1Src\");\r\n        tuples.add(tuple);\r\n        executor.execute(tuple);\r\n        //Update the watermark to this timestamp\r\n        executor.waterMarkEventGenerator.run();\r\n    }\r\n    System.out.println(testWindowedBolt.tupleWindows);\r\n    Tuple tuple = tuples.get(tuples.size() - 1);\r\n    Mockito.verify(outputCollector).emit(\"$late\", Collections.singletonList(tuple), new Values(tuple));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\topology\\WindowedBoltExecutorTest.java",
  "methodName" : "testEmptyConfigOnWrappedBolt",
  "sourceCode" : "@Test\r\npublic void testEmptyConfigOnWrappedBolt() {\r\n    IWindowedBolt wrappedBolt = Mockito.mock(IWindowedBolt.class);\r\n    Mockito.when(wrappedBolt.getComponentConfiguration()).thenReturn(null);\r\n    executor = new WindowedBoltExecutor(wrappedBolt);\r\n    assertTrue(executor.getComponentConfiguration().isEmpty(), \"Configuration is not empty\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\trident\\TestTridentTopology.java",
  "methodName" : "testGenBoltId",
  "sourceCode" : "@Test\r\npublic void testGenBoltId() {\r\n    Set<String> pre = null;\r\n    for (int i = 0; i < 100; i++) {\r\n        StormTopology topology = buildTopology();\r\n        Map<String, Bolt> cur = topology.get_bolts();\r\n        System.out.println(cur.keySet());\r\n        if (pre != null) {\r\n            assertEquals(pre, cur.keySet(), \"bolt id not consistent with group name\");\r\n        }\r\n        pre = cur.keySet();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\trident\\TridentWindowingTest.java",
  "methodName" : "testWindowStrategyInstances",
  "sourceCode" : "@Test\r\npublic void testWindowStrategyInstances() {\r\n    WindowStrategy<Object> tumblingCountStrategy = TumblingCountWindow.of(10).getWindowStrategy();\r\n    assertTrue(tumblingCountStrategy instanceof TumblingCountWindowStrategy);\r\n    WindowStrategy<Object> slidingCountStrategy = SlidingCountWindow.of(100, 10).getWindowStrategy();\r\n    assertTrue(slidingCountStrategy instanceof SlidingCountWindowStrategy);\r\n    WindowStrategy<Object> tumblingDurationStrategy = TumblingDurationWindow.of(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS)).getWindowStrategy();\r\n    assertTrue(tumblingDurationStrategy instanceof TumblingDurationWindowStrategy);\r\n    WindowStrategy<Object> slidingDurationStrategy = SlidingDurationWindow.of(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS), new BaseWindowedBolt.Duration(2, TimeUnit.SECONDS)).getWindowStrategy();\r\n    assertTrue(slidingDurationStrategy instanceof SlidingDurationWindowStrategy);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\trident\\TridentWindowingTest.java",
  "methodName" : "testWindowConfig",
  "sourceCode" : "@Test\r\npublic void testWindowConfig() {\r\n    int windowLength = 9;\r\n    TumblingCountWindow tumblingCountWindow = TumblingCountWindow.of(windowLength);\r\n    assertTrue(tumblingCountWindow.getWindowLength() == windowLength);\r\n    assertTrue(tumblingCountWindow.getSlidingLength() == windowLength);\r\n    windowLength = 10;\r\n    int slidingLength = 2;\r\n    SlidingCountWindow slidingCountWindow = SlidingCountWindow.of(10, 2);\r\n    assertTrue(slidingCountWindow.getWindowLength() == windowLength);\r\n    assertTrue(slidingCountWindow.getSlidingLength() == slidingLength);\r\n    windowLength = 20;\r\n    TumblingDurationWindow tumblingDurationWindow = TumblingDurationWindow.of(new BaseWindowedBolt.Duration(windowLength, TimeUnit.SECONDS));\r\n    assertTrue(tumblingDurationWindow.getWindowLength() == windowLength * 1000);\r\n    assertTrue(tumblingDurationWindow.getSlidingLength() == windowLength * 1000);\r\n    windowLength = 50;\r\n    slidingLength = 10;\r\n    SlidingDurationWindow slidingDurationWindow = SlidingDurationWindow.of(new BaseWindowedBolt.Duration(windowLength, TimeUnit.SECONDS), new BaseWindowedBolt.Duration(slidingLength, TimeUnit.SECONDS));\r\n    assertTrue(slidingDurationWindow.getWindowLength() == windowLength * 1000);\r\n    assertTrue(slidingDurationWindow.getSlidingLength() == slidingLength * 1000);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\trident\\TridentWindowingTest.java",
  "methodName" : "testInMemoryWindowStore",
  "sourceCode" : "@Test\r\npublic void testInMemoryWindowStore() {\r\n    InMemoryWindowsStore store = new InMemoryWindowsStore();\r\n    String keyPrefix = \"key\";\r\n    String valuePrefix = \"valuePrefix\";\r\n    int ct = 10;\r\n    for (int i = 0; i < ct; i++) {\r\n        store.put(keyPrefix + i, valuePrefix + i);\r\n    }\r\n    for (int i = 0; i < ct; i++) {\r\n        assertTrue((valuePrefix + i).equals(store.get(keyPrefix + i)));\r\n    }\r\n    store.remove(keyPrefix + 1);\r\n    assertNull(store.get(keyPrefix + 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "fieldsConstructorDoesNotThrowWithValidArgsTest",
  "sourceCode" : "@Test\r\npublic void fieldsConstructorDoesNotThrowWithValidArgsTest() {\r\n    assertEquals(new Fields(\"foo\", \"bar\").size(), 2);\r\n    assertEquals(new Fields(\"foo\", \"bar\").size(), 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "duplicateFieldsNotAllowedWhenConstructingWithVarArgsTest",
  "sourceCode" : "@Test\r\npublic void duplicateFieldsNotAllowedWhenConstructingWithVarArgsTest() {\r\n    assertThrows(IllegalArgumentException.class, () -> new Fields(\"foo\", \"bar\", \"foo\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "duplicateFieldsNotAllowedTestWhenConstructingFromListTest",
  "sourceCode" : "@Test\r\npublic void duplicateFieldsNotAllowedTestWhenConstructingFromListTest() {\r\n    assertThrows(IllegalArgumentException.class, () -> new Fields(\"foo\", \"bar\", \"foo\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "getDoesNotThrowWithValidIndexTest",
  "sourceCode" : "@Test\r\npublic void getDoesNotThrowWithValidIndexTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    assertEquals(fields.get(0), \"foo\");\r\n    assertEquals(fields.get(1), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "getThrowsWhenOutOfBoundsTest",
  "sourceCode" : "@Test\r\npublic void getThrowsWhenOutOfBoundsTest() {\r\n    assertThrows(IndexOutOfBoundsException.class, () -> {\r\n        Fields fields = new Fields(\"foo\", \"bar\");\r\n        fields.get(2);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "fieldIndexTest",
  "sourceCode" : "@Test\r\npublic void fieldIndexTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    assertEquals(fields.fieldIndex(\"foo\"), 0);\r\n    assertEquals(fields.fieldIndex(\"bar\"), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "fieldIndexThrowsWhenOutOfBoundsTest",
  "sourceCode" : "@Test\r\npublic void fieldIndexThrowsWhenOutOfBoundsTest() {\r\n    assertThrows(IllegalArgumentException.class, () -> new Fields(\"foo\").fieldIndex(\"baz\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "containsTest",
  "sourceCode" : "@Test\r\npublic void containsTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    assertTrue(fields.contains(\"foo\"));\r\n    assertTrue(fields.contains(\"bar\"));\r\n    assertFalse(fields.contains(\"baz\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "toListTest",
  "sourceCode" : "@Test\r\npublic void toListTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    List<String> fieldList = fields.toList();\r\n    assertEquals(fieldList.size(), 2);\r\n    assertEquals(fieldList.get(0), \"foo\");\r\n    assertEquals(fieldList.get(1), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "toIteratorTest",
  "sourceCode" : "@Test\r\npublic void toIteratorTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    Iterator<String> fieldIter = fields.iterator();\r\n    assertTrue(fieldIter.hasNext(), \"First item is foo\");\r\n    assertEquals(fieldIter.next(), \"foo\");\r\n    assertTrue(fieldIter.hasNext(), \"Second item is bar\");\r\n    assertEquals(fieldIter.next(), \"bar\");\r\n    assertFalse(fieldIter.hasNext(), \"At end. hasNext should return false\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "selectTest",
  "sourceCode" : "@Test\r\npublic void selectTest() {\r\n    Fields fields = new Fields(\"foo\", \"bar\");\r\n    List<Object> second = Arrays.asList(new Object[] { \"b\" });\r\n    List<Object> tuple = Arrays.asList(new Object[] { \"a\", \"b\", \"c\" });\r\n    List<Object> pickSecond = fields.select(new Fields(\"bar\"), tuple);\r\n    assertEquals(pickSecond, second);\r\n    List<Object> secondAndFirst = Arrays.asList(new Object[] { \"b\", \"a\" });\r\n    List<Object> pickSecondAndFirst = fields.select(new Fields(\"bar\", \"foo\"), tuple);\r\n    assertEquals(pickSecondAndFirst, secondAndFirst);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\FieldsTest.java",
  "methodName" : "selectingUnknownFieldThrowsTest",
  "sourceCode" : "@Test\r\npublic void selectingUnknownFieldThrowsTest() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Fields fields = new Fields(\"foo\", \"bar\");\r\n        fields.select(new Fields(\"bar\", \"baz\"), Arrays.asList(new Object[] { \"a\", \"b\", \"c\" }));\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\ValuesTest.java",
  "methodName" : "testNoArgsToValues",
  "sourceCode" : "@Test\r\npublic void testNoArgsToValues() {\r\n    Values vals = new Values();\r\n    Assertions.assertEquals(0, vals.size(), \"Failed to add null to Values\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\ValuesTest.java",
  "methodName" : "testNullArgsToValues",
  "sourceCode" : "@Test\r\npublic void testNullArgsToValues() {\r\n    Values vals = new Values(null);\r\n    Assertions.assertEquals(1, vals.size(), \"Failed to add null to Values\");\r\n    Assertions.assertNull(vals.get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\ValuesTest.java",
  "methodName" : "testNonNullArgsToValues",
  "sourceCode" : "@Test\r\npublic void testNonNullArgsToValues() {\r\n    Values vals = new Values(\"A\", \"B\");\r\n    Assertions.assertEquals(2, vals.size(), \"Failed to Add values to Values\");\r\n    Assertions.assertEquals(vals.get(0), \"A\");\r\n    Assertions.assertEquals(vals.get(1), \"B\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\tuple\\ValuesTest.java",
  "methodName" : "testNullAsArgsToValues",
  "sourceCode" : "@Test\r\npublic void testNullAsArgsToValues() {\r\n    Values vals = new Values(null, \"A\");\r\n    Assertions.assertEquals(2, vals.size(), \"Failed to Add values to Values\");\r\n    Assertions.assertNull(vals.get(0));\r\n    Assertions.assertEquals(vals.get(1), \"A\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nullKeySupported",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nullKeySupported() {\r\n    String key = null;\r\n    List<String> value = Collections.singletonList(\"test\");\r\n    Map<String, Object> map = mockMap(key, value);\r\n    assertEquals(value, ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nullKeyNotSupported",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nullKeyNotSupported() {\r\n    assertThrows(NullPointerException.class, () -> {\r\n        String key = null;\r\n        Map<String, Object> map = new Hashtable<>();\r\n        ConfigUtils.getValueAsList(key, map);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nullConfig",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nullConfig() {\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigUtils.getValueAsList(Config.WORKER_CHILDOPTS, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nullValue",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nullValue() {\r\n    String key = Config.WORKER_CHILDOPTS;\r\n    Map<String, Object> map = mockMap(key, null);\r\n    assertNull(ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nonStringValue",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nonStringValue() {\r\n    String key = Config.WORKER_CHILDOPTS;\r\n    List<String> expectedValue = Collections.singletonList(\"1\");\r\n    Map<String, Object> map = mockMap(key, 1);\r\n    assertEquals(expectedValue, ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_spaceSeparatedString",
  "sourceCode" : "@Test\r\npublic void getValueAsList_spaceSeparatedString() {\r\n    String key = Config.WORKER_CHILDOPTS;\r\n    String value = \"-Xms1024m -Xmx1024m\";\r\n    List<String> expectedValue = Arrays.asList(\"-Xms1024m\", \"-Xmx1024m\");\r\n    Map<String, Object> map = mockMap(key, value);\r\n    assertEquals(expectedValue, ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_stringList",
  "sourceCode" : "@Test\r\npublic void getValueAsList_stringList() {\r\n    String key = Config.WORKER_CHILDOPTS;\r\n    List<String> values = Arrays.asList(\"-Xms1024m\", \"-Xmx1024m\");\r\n    Map<String, Object> map = mockMap(key, values);\r\n    assertEquals(values, ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getValueAsList_nonStringList",
  "sourceCode" : "@Test\r\npublic void getValueAsList_nonStringList() {\r\n    String key = Config.WORKER_CHILDOPTS;\r\n    List<Object> values = Arrays.asList(1, 2);\r\n    List<String> expectedValue = Arrays.asList(\"1\", \"2\");\r\n    Map<String, Object> map = mockMap(key, values);\r\n    assertEquals(expectedValue, ConfigUtils.getValueAsList(key, map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getBlobstoreHDFSPrincipal",
  "sourceCode" : "@Deprecated\r\n@Test\r\npublic void getBlobstoreHDFSPrincipal() throws UnknownHostException {\r\n    Map<String, Object> conf = mockMap(Config.BLOBSTORE_HDFS_PRINCIPAL, \"primary/_HOST@EXAMPLE.COM\");\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), \"primary/\" + Utils.localHostname() + \"@EXAMPLE.COM\");\r\n    String principal = \"primary/_HOST_HOST@EXAMPLE.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n    principal = \"primary/_HOST2@EXAMPLE.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n    principal = \"_HOST/instance@EXAMPLE.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n    principal = \"primary/instance@_HOST.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n    principal = \"_HOST@EXAMPLE.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n    principal = \"primary/instance@EXAMPLE.COM\";\r\n    conf.put(Config.BLOBSTORE_HDFS_PRINCIPAL, principal);\r\n    assertEquals(Config.getBlobstoreHDFSPrincipal(conf), principal);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ConfigUtilsTest.java",
  "methodName" : "getHfdsPrincipal",
  "sourceCode" : "@Test\r\npublic void getHfdsPrincipal() throws UnknownHostException {\r\n    Map<String, Object> conf = mockMap(Config.STORM_HDFS_LOGIN_PRINCIPAL, \"primary/_HOST@EXAMPLE.COM\");\r\n    assertEquals(Config.getHdfsPrincipal(conf), \"primary/\" + Utils.localHostname() + \"@EXAMPLE.COM\");\r\n    String principal = \"primary/_HOST_HOST@EXAMPLE.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n    principal = \"primary/_HOST2@EXAMPLE.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n    principal = \"_HOST/instance@EXAMPLE.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n    principal = \"primary/instance@_HOST.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n    principal = \"_HOST@EXAMPLE.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n    principal = \"primary/instance@EXAMPLE.COM\";\r\n    conf.put(Config.STORM_HDFS_LOGIN_PRINCIPAL, principal);\r\n    assertEquals(Config.getHdfsPrincipal(conf), principal);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\CuratorUtilsTest.java",
  "methodName" : "newCuratorUsesExponentialBackoffTest",
  "sourceCode" : "@Test\r\npublic void newCuratorUsesExponentialBackoffTest() {\r\n    final int expectedInterval = 2400;\r\n    final int expectedRetries = 10;\r\n    final int expectedCeiling = 3000;\r\n    Map<String, Object> config = Utils.readDefaultConfig();\r\n    config.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL, expectedInterval);\r\n    config.put(Config.STORM_ZOOKEEPER_RETRY_TIMES, expectedRetries);\r\n    config.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL_CEILING, expectedCeiling);\r\n    CuratorFramework curator = CuratorUtils.newCurator(config, Collections.singletonList(\"bogus_server\"), 42, \"\", DaemonType.WORKER.getDefaultZkAcls(config));\r\n    StormBoundedExponentialBackoffRetry policy = (StormBoundedExponentialBackoffRetry) curator.getZookeeperClient().getRetryPolicy();\r\n    assertEquals(policy.getBaseSleepTimeMs(), expectedInterval);\r\n    assertEquals(policy.getN(), expectedRetries);\r\n    assertEquals(policy.getSleepTimeMs(10, 0), expectedCeiling);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\CuratorUtilsTest.java",
  "methodName" : "givenNoExhibitorServersBuilderUsesFixedProviderTest",
  "sourceCode" : "@Test\r\npublic void givenNoExhibitorServersBuilderUsesFixedProviderTest() {\r\n    CuratorFrameworkFactory.Builder builder = setupBuilder(false);\r\n    assertEquals(builder.getEnsembleProvider().getConnectionString(), \"zk_connection_string\");\r\n    assertEquals(builder.getEnsembleProvider().getClass(), FixedEnsembleProvider.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\CuratorUtilsTest.java",
  "methodName" : "givenSchemeAndPayloadBuilderUsesAuthTest",
  "sourceCode" : "@Test\r\npublic void givenSchemeAndPayloadBuilderUsesAuthTest() {\r\n    CuratorFrameworkFactory.Builder builder = setupBuilder(true);\r\n    List<AuthInfo> authInfos = builder.getAuthInfos();\r\n    AuthInfo authInfo = authInfos.get(0);\r\n    assertEquals(authInfo.getScheme(), \"scheme\");\r\n    assertArrayEquals(authInfo.getAuth(), \"abc\".getBytes());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\CuratorUtilsTest.java",
  "methodName" : "testSecureZKConfiguration",
  "sourceCode" : "@Test\r\npublic void testSecureZKConfiguration() throws Exception {\r\n    LOG.info(\"Entered to the testSecureZKConfiguration test case.\");\r\n    Map<String, Object> conf = setUpSecureConfig(\"test/resources/ssl/\");\r\n    conf.put(Config.STORM_ZOOKEEPER_CONNECTION_TIMEOUT, 0);\r\n    conf.put(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT, 0);\r\n    conf.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL, 0);\r\n    conf.put(Config.STORM_ZOOKEEPER_RETRY_INTERVAL_CEILING, 0);\r\n    conf.put(Config.STORM_ZOOKEEPER_RETRY_TIMES, 0);\r\n    conf.put(Config.ZK_SSL_ENABLE, true);\r\n    String zkStr = this.server.getConnectString();\r\n    CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder();\r\n    CuratorUtils.testSetupBuilder(builder, zkStr, conf, null);\r\n    CuratorFramework curatorFramework = builder.build();\r\n    curatorFramework.start();\r\n    ZooKeeper zk = curatorFramework.getZookeeperClient().getZooKeeper();\r\n    validateSSLConfiguration(ObjectReader.getString(conf.get(Config.STORM_ZOOKEEPER_SSL_KEYSTORE_PATH)), ObjectReader.getString(conf.get(Config.STORM_ZOOKEEPER_SSL_KEYSTORE_PASSWORD)), ObjectReader.getString(conf.get(Config.STORM_ZOOKEEPER_SSL_TRUSTSTORE_PATH)), ObjectReader.getString(conf.get(Config.STORM_ZOOKEEPER_SSL_TRUSTSTORE_PASSWORD)), ObjectReader.getBoolean(conf.get(Config.STORM_ZOOKEEPER_SSL_HOSTNAME_VERIFICATION), true), zk);\r\n    this.server.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\CuratorUtilsTest.java",
  "methodName" : "testTruststoreKeystoreConfiguration",
  "sourceCode" : "@Test\r\npublic void testTruststoreKeystoreConfiguration() {\r\n    LOG.info(\"Entered to the testTruststoreKeystoreConfiguration test case.\");\r\n    /*\r\n      By default the truststore/keystore configurations are not set, hence the values are null.\r\n      Validate that the null values are converted into empty strings by the class.\r\n     */\r\n    Map<String, Object> conf = new HashMap<>();\r\n    CuratorUtils.SslConf zkSslConf = CuratorUtils.getSslConf(conf);\r\n    assertEquals(\"\", zkSslConf.getKeystoreLocation(), \"Validate that null value is converted to empty string.\");\r\n    assertEquals(\"\", zkSslConf.getKeystorePassword(), \"Validate that null value is converted to empty string.\");\r\n    assertEquals(\"\", zkSslConf.getTruststoreLocation(), \"Validate that null value is converted to empty string.\");\r\n    assertEquals(\"\", zkSslConf.getTruststorePassword(), \"Validate that null value is converted to empty string.\");\r\n    assertEquals(true, zkSslConf.getHostnameVerification(), \"Validate that null value is converted to false.\");\r\n    //Validate that non-null values will remain intact\r\n    conf.put(Config.STORM_ZOOKEEPER_SSL_KEYSTORE_PATH, \"/keystore.jks\");\r\n    conf.put(Config.STORM_ZOOKEEPER_SSL_KEYSTORE_PASSWORD, \"keystorePassword\");\r\n    conf.put(Config.STORM_ZOOKEEPER_SSL_TRUSTSTORE_PATH, \"/truststore.jks\");\r\n    conf.put(Config.STORM_ZOOKEEPER_SSL_TRUSTSTORE_PASSWORD, \"truststorePassword\");\r\n    conf.put(Config.STORM_ZOOKEEPER_SSL_HOSTNAME_VERIFICATION, false);\r\n    zkSslConf = CuratorUtils.getSslConf(conf);\r\n    assertEquals(\"/keystore.jks\", zkSslConf.getKeystoreLocation(), \"Validate that non-null value kept intact.\");\r\n    assertEquals(\"keystorePassword\", zkSslConf.getKeystorePassword(), \"Validate that non-null value kept intact.\");\r\n    assertEquals(\"/truststore.jks\", zkSslConf.getTruststoreLocation(), \"Validate that non-null value kept intact.\");\r\n    assertEquals(\"truststorePassword\", zkSslConf.getTruststorePassword(), \"Validate that non-null value kept intact.\");\r\n    assertEquals(false, zkSslConf.getHostnameVerification(), \"Validate that non-null value kept intact.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\DefaultShellLogHandlerTest.java",
  "methodName" : "setUpContext_allNull",
  "sourceCode" : "/**\r\n * It's fine to pass only null arguments to setUpContext.\r\n */\r\n@Test\r\npublic void setUpContext_allNull() {\r\n    ShellMsg msg = mockMsg();\r\n    logHandler.setUpContext(null, null, null);\r\n    logHandler.log(msg);\r\n    verify(msg).getMsg();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\DefaultShellLogHandlerTest.java",
  "methodName" : "setUpContext_optional",
  "sourceCode" : "/**\r\n * Calling setUpContext is optional.\r\n */\r\n@Test\r\npublic void setUpContext_optional() {\r\n    ShellMsg msg = mockMsg();\r\n    logHandler.log(msg);\r\n    verify(msg).getMsg();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\DefaultShellLogHandlerTest.java",
  "methodName" : "handleLog_nullShellMsg",
  "sourceCode" : "/**\r\n * A null {@link ShellMsg} will throw IllegalArgumentException.\r\n */\r\n@Test\r\npublic void handleLog_nullShellMsg() {\r\n    assertThrows(IllegalArgumentException.class, () -> logHandler.log(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\DefaultShellLogHandlerTest.java",
  "methodName" : "handleLog_nullProcess",
  "sourceCode" : "/**\r\n * A null {@link ShellProcess} will not throw an exception.\r\n */\r\n@Test\r\npublic void handleLog_nullProcess() {\r\n    ShellMsg msg = mockMsg();\r\n    ShellProcess process = mockProcess();\r\n    logHandler.setUpContext(DefaultShellLogHandlerTest.class, process, null);\r\n    logHandler.log(msg);\r\n    verify(msg).getMsg();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\DefaultShellLogHandlerTest.java",
  "methodName" : "handleLog_valid",
  "sourceCode" : "/**\r\n * If both {@link ShellMsg} and {@link ShellProcess} are provided, both\r\n * will be used to build the log message.\r\n */\r\n@Test\r\npublic void handleLog_valid() {\r\n    ShellMsg msg = mockMsg();\r\n    ShellProcess process = mockProcess();\r\n    logHandler.setUpContext(DefaultShellLogHandlerTest.class, process, null);\r\n    logHandler.log(msg);\r\n    verify(msg).getMsg();\r\n    verify(process).getProcessInfoString();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\JCQueueBackpressureTest.java",
  "methodName" : "testNoReOrderingUnderBackPressure",
  "sourceCode" : "@Test\r\npublic void testNoReOrderingUnderBackPressure() {\r\n    final int MESSAGES = 100;\r\n    final int CAPACITY = 64;\r\n    final JCQueue queue = createQueue(\"testBackPressure\", CAPACITY);\r\n    for (int i = 0; i < MESSAGES; i++) {\r\n        if (!queue.tryPublish(i)) {\r\n            assertTrue(queue.tryPublishToOverflow(i));\r\n        }\r\n    }\r\n    TestConsumer consumer = new TestConsumer();\r\n    queue.consume(consumer);\r\n    assertEquals(MESSAGES, consumer.lastMsg);\r\n    queue.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\JCQueueBackpressureTest.java",
  "methodName" : "testBasicBackPressure",
  "sourceCode" : "// check that tryPublish() & tryOverflowPublish() work as expected\r\n@Test\r\npublic void testBasicBackPressure() {\r\n    final int MESSAGES = 100;\r\n    final int CAPACITY = 64;\r\n    final JCQueue queue = createQueue(\"testBackPressure\", CAPACITY);\r\n    // pump more msgs than Q size & verify msg count is as expexted\r\n    for (int i = 0; i < MESSAGES; i++) {\r\n        if (i >= CAPACITY) {\r\n            assertFalse(queue.tryPublish(i));\r\n        } else {\r\n            assertTrue(queue.tryPublish(i));\r\n        }\r\n    }\r\n    assertEquals(CAPACITY, queue.size());\r\n    assertEquals(0, queue.getOverflowCount());\r\n    // drain 1 element and ensure BP is relieved (i.e tryPublish() succeeds)\r\n    final MutableLong consumeCount = new MutableLong(0);\r\n    queue.consume(new TestConsumer(), () -> consumeCount.increment() <= 1);\r\n    assertEquals(CAPACITY - 1, queue.size());\r\n    assertTrue(queue.tryPublish(0));\r\n    queue.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\JCQueueTest.java",
  "methodName" : "testFirstMessageFirst",
  "sourceCode" : "@Test\r\npublic void testFirstMessageFirst() {\r\n    Assertions.assertTimeoutPreemptively(Duration.ofSeconds(10), () -> {\r\n        JCQueue queue = createQueue(\"firstMessageOrder\", 16);\r\n        queue.publish(\"FIRST\");\r\n        Runnable producer = new IncProducer(queue, 100, 1);\r\n        final AtomicReference<Object> result = new AtomicReference<>();\r\n        Runnable consumer = new ConsumerThd(queue, new JCQueue.Consumer() {\r\n\r\n            private boolean head = true;\r\n\r\n            @Override\r\n            public void accept(Object event) {\r\n                if (head) {\r\n                    head = false;\r\n                    result.set(event);\r\n                }\r\n            }\r\n\r\n            @Override\r\n            public void flush() {\r\n            }\r\n        });\r\n        run(producer, consumer, queue);\r\n        assertEquals(\"FIRST\", result.get(), \"We expect to receive first published message first, but received \" + result.get());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\JCQueueTest.java",
  "methodName" : "testInOrder",
  "sourceCode" : "@Test\r\npublic void testInOrder() {\r\n    Assertions.assertTimeoutPreemptively(Duration.ofSeconds(10), () -> {\r\n        final AtomicBoolean allInOrder = new AtomicBoolean(true);\r\n        JCQueue queue = createQueue(\"consumerHang\", 1024);\r\n        Runnable producer = new IncProducer(queue, 1024 * 1024, 100);\r\n        Runnable consumer = new ConsumerThd(queue, new JCQueue.Consumer() {\r\n\r\n            long _expected = 0;\r\n\r\n            @Override\r\n            public void accept(Object obj) {\r\n                if (_expected != ((Number) obj).longValue()) {\r\n                    allInOrder.set(false);\r\n                    System.out.println(\"Expected \" + _expected + \" but got \" + obj);\r\n                }\r\n                _expected++;\r\n            }\r\n\r\n            @Override\r\n            public void flush() {\r\n            }\r\n        });\r\n        run(producer, consumer, queue, 1000, 1);\r\n        assertTrue(allInOrder.get(), \"Messages delivered out of order\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\JCQueueTest.java",
  "methodName" : "testInOrderBatch",
  "sourceCode" : "@Test\r\npublic void testInOrderBatch() {\r\n    Assertions.assertTimeoutPreemptively(Duration.ofSeconds(10), () -> {\r\n        final AtomicBoolean allInOrder = new AtomicBoolean(true);\r\n        JCQueue queue = createQueue(\"consumerHang\", 10, 1024);\r\n        Runnable producer = new IncProducer(queue, 1024 * 1024, 100);\r\n        Runnable consumer = new ConsumerThd(queue, new JCQueue.Consumer() {\r\n\r\n            long _expected = 0;\r\n\r\n            @Override\r\n            public void accept(Object obj) {\r\n                if (_expected != ((Number) obj).longValue()) {\r\n                    allInOrder.set(false);\r\n                    System.out.println(\"Expected \" + _expected + \" but got \" + obj);\r\n                }\r\n                _expected++;\r\n            }\r\n\r\n            @Override\r\n            public void flush() {\r\n            }\r\n        });\r\n        run(producer, consumer, queue, 1000, 1);\r\n        assertTrue(allInOrder.get(), \"Messages delivered out of order\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SecurityUtilsTest.java",
  "methodName" : "testInferKeyStoreTypeFromPath_Pkcs12Extension",
  "sourceCode" : "@Test\r\npublic void testInferKeyStoreTypeFromPath_Pkcs12Extension() {\r\n    Assertions.assertEquals(\"PKCS12\", SecurityUtils.inferKeyStoreTypeFromPath(\"keystore.p12\"));\r\n    Assertions.assertEquals(\"PKCS12\", SecurityUtils.inferKeyStoreTypeFromPath(\"mykeys.P12\"));\r\n    Assertions.assertEquals(\"PKCS12\", SecurityUtils.inferKeyStoreTypeFromPath(\"path/to/keystore.pkcs12\"));\r\n    Assertions.assertEquals(\"PKCS12\", SecurityUtils.inferKeyStoreTypeFromPath(\"mykeys.pKCS12\"));\r\n    Assertions.assertEquals(\"PKCS12\", SecurityUtils.inferKeyStoreTypeFromPath(\"another/path/to/keystore.pfx\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SecurityUtilsTest.java",
  "methodName" : "testInferKeyStoreTypeFromPath_JksExtension",
  "sourceCode" : "@Test\r\npublic void testInferKeyStoreTypeFromPath_JksExtension() {\r\n    Assertions.assertEquals(\"JKS\", SecurityUtils.inferKeyStoreTypeFromPath(\"keystore.jks\"));\r\n    Assertions.assertEquals(\"JKS\", SecurityUtils.inferKeyStoreTypeFromPath(\"mykeys.JKS\"));\r\n    Assertions.assertEquals(\"JKS\", SecurityUtils.inferKeyStoreTypeFromPath(\"path/to/keystore.jKs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SecurityUtilsTest.java",
  "methodName" : "testInferKeyStoreTypeFromPath_UnsupportedExtension",
  "sourceCode" : "@Test\r\npublic void testInferKeyStoreTypeFromPath_UnsupportedExtension() {\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(\"keystore.pem\"));\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(\"certificate.crt\"));\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(\"path/to/keystore.txt\"));\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(\"another/path/to/keystore.pem\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SecurityUtilsTest.java",
  "methodName" : "testInferKeyStoreTypeFromPath_EmptyOrNullPath",
  "sourceCode" : "@Test\r\npublic void testInferKeyStoreTypeFromPath_EmptyOrNullPath() {\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(\"\"));\r\n    Assertions.assertNull(SecurityUtils.inferKeyStoreTypeFromPath(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellBoltMessageQueueTest.java",
  "methodName" : "testPollTaskIdsFirst",
  "sourceCode" : "@Test\r\npublic void testPollTaskIdsFirst() throws InterruptedException {\r\n    ShellBoltMessageQueue queue = new ShellBoltMessageQueue();\r\n    // put bolt message first, then put task ids\r\n    queue.putBoltMsg(new BoltMsg());\r\n    ArrayList<Integer> taskIds = Lists.newArrayList(1, 2, 3);\r\n    queue.putTaskIds(taskIds);\r\n    Object msg = queue.poll(10, TimeUnit.SECONDS);\r\n    // task ids should be pulled first\r\n    assertTrue(msg instanceof List<?>);\r\n    assertEquals(msg, taskIds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellBoltMessageQueueTest.java",
  "methodName" : "testPollWhileThereAreNoDataAvailable",
  "sourceCode" : "@Test\r\npublic void testPollWhileThereAreNoDataAvailable() throws InterruptedException {\r\n    ShellBoltMessageQueue queue = new ShellBoltMessageQueue();\r\n    long start = System.currentTimeMillis();\r\n    Object msg = queue.poll(1, TimeUnit.SECONDS);\r\n    long finish = System.currentTimeMillis();\r\n    long waitDuration = finish - start;\r\n    assertNull(msg);\r\n    assertTrue(waitDuration >= 1000, \"wait duration should be equal or greater than 1000, current: \" + waitDuration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellBoltMessageQueueTest.java",
  "methodName" : "testPollShouldReturnASAPWhenDataAvailable",
  "sourceCode" : "@Test\r\npublic void testPollShouldReturnASAPWhenDataAvailable() throws InterruptedException {\r\n    final ShellBoltMessageQueue queue = new ShellBoltMessageQueue();\r\n    final List<Integer> taskIds = Lists.newArrayList(1, 2, 3);\r\n    Thread t = new Thread(() -> {\r\n        try {\r\n            Thread.sleep(1000);\r\n        } catch (InterruptedException e) {\r\n            // NOOP\r\n        }\r\n        queue.putTaskIds(taskIds);\r\n    });\r\n    t.start();\r\n    long start = System.currentTimeMillis();\r\n    Object msg = queue.poll(10, TimeUnit.SECONDS);\r\n    long finish = System.currentTimeMillis();\r\n    assertEquals(msg, taskIds);\r\n    assertTrue(finish - start < (10 * 1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellUtilsTest.java",
  "methodName" : "getLogHandler_nullConf",
  "sourceCode" : "/**\r\n * A null config will throw IllegalArgumentException.\r\n */\r\n@Test\r\npublic void getLogHandler_nullConf() {\r\n    assertThrows(IllegalArgumentException.class, () -> ShellUtils.getLogHandler(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellUtilsTest.java",
  "methodName" : "getLogHandler_notConfigured",
  "sourceCode" : "/**\r\n * If a log handler is not configured, {@link DefaultShellLogHandler}\r\n * will be returned.\r\n */\r\n@Test\r\npublic void getLogHandler_notConfigured() {\r\n    ShellLogHandler logHandler = ShellUtils.getLogHandler(new HashMap<>());\r\n    assertSame(logHandler.getClass(), DefaultShellLogHandler.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellUtilsTest.java",
  "methodName" : "getLogHandler_notFound",
  "sourceCode" : "/**\r\n * If a log handler cannot be found, a {@link RuntimeException} will be\r\n * thrown with {@link ClassNotFoundException} as the cause.\r\n */\r\n@Test\r\npublic void getLogHandler_notFound() {\r\n    try {\r\n        configureLogHandler(\"class.not.Found\");\r\n    } catch (RuntimeException e) {\r\n        assert (e.getCause().getClass() == ClassNotFoundException.class);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellUtilsTest.java",
  "methodName" : "getLogHandler_notAShellLogHandler",
  "sourceCode" : "/**\r\n * If a log handler is not an instance of {@link ShellLogHandler}, a\r\n * {@link RuntimeException} will be thrown with {@link ClassCastException}\r\n * as the cause.\r\n */\r\n@Test\r\npublic void getLogHandler_notAShellLogHandler() {\r\n    try {\r\n        configureLogHandler(\"java.lang.String\");\r\n    } catch (RuntimeException e) {\r\n        assert (e.getCause().getClass() == ClassCastException.class);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ShellUtilsTest.java",
  "methodName" : "getLogHandler_customHandler",
  "sourceCode" : "/**\r\n * If a log handler is correctly configured, it will be returned.\r\n */\r\n@Test\r\npublic void getLogHandler_customHandler() {\r\n    Map<String, Object> conf = configureLogHandler(\"org.apache.storm.utils.ShellUtilsTest$CustomShellLogHandler\");\r\n    ShellLogHandler logHandler = ShellUtils.getLogHandler(conf);\r\n    assertSame(logHandler.getClass(), CustomShellLogHandler.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm2x",
  "sourceCode" : "@Test\r\npublic void testParseStorm2x() {\r\n    SimpleVersion version = new SimpleVersion(\"2.1.2\");\r\n    assertEquals(2, version.getMajor());\r\n    assertEquals(1, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm2xSnapshot",
  "sourceCode" : "@Test\r\npublic void testParseStorm2xSnapshot() {\r\n    SimpleVersion version = new SimpleVersion(\"2.1.2-SNAPSHOT\");\r\n    assertEquals(2, version.getMajor());\r\n    assertEquals(1, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm1x",
  "sourceCode" : "@Test\r\npublic void testParseStorm1x() {\r\n    SimpleVersion version = new SimpleVersion(\"1.0.4\");\r\n    assertEquals(1, version.getMajor());\r\n    assertEquals(0, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm1xSnapshot",
  "sourceCode" : "@Test\r\npublic void testParseStorm1xSnapshot() {\r\n    SimpleVersion version = new SimpleVersion(\"1.0.4-SNAPSHOT\");\r\n    assertEquals(1, version.getMajor());\r\n    assertEquals(0, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm0x",
  "sourceCode" : "@Test\r\npublic void testParseStorm0x() {\r\n    SimpleVersion version = new SimpleVersion(\"0.10.3\");\r\n    assertEquals(0, version.getMajor());\r\n    assertEquals(10, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\SimpleVersionTest.java",
  "methodName" : "testParseStorm0xSnapshot",
  "sourceCode" : "@Test\r\npublic void testParseStorm0xSnapshot() {\r\n    SimpleVersion version = new SimpleVersion(\"0.10.3-SNAPSHOT\");\r\n    assertEquals(0, version.getMajor());\r\n    assertEquals(10, version.getMinor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\StormBoundedExponentialBackoffRetryTest.java",
  "methodName" : "testExponentialSleepLargeRetries",
  "sourceCode" : "@Test\r\npublic void testExponentialSleepLargeRetries() {\r\n    int baseSleepMs = 10;\r\n    int maxSleepMs = 1000;\r\n    int maxRetries = 900;\r\n    validateSleepTimes(baseSleepMs, maxSleepMs, maxRetries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\StormBoundedExponentialBackoffRetryTest.java",
  "methodName" : "testExponentialSleep",
  "sourceCode" : "@Test\r\npublic void testExponentialSleep() {\r\n    int baseSleepMs = 10;\r\n    int maxSleepMs = 100;\r\n    int maxRetries = 40;\r\n    validateSleepTimes(baseSleepMs, maxSleepMs, maxRetries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\StormBoundedExponentialBackoffRetryTest.java",
  "methodName" : "testExponentialSleepSmallMaxRetries",
  "sourceCode" : "@Test\r\npublic void testExponentialSleepSmallMaxRetries() {\r\n    int baseSleepMs = 1000;\r\n    int maxSleepMs = 5000;\r\n    int maxRetries = 10;\r\n    validateSleepTimes(baseSleepMs, maxSleepMs, maxRetries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\StormBoundedExponentialBackoffRetryTest.java",
  "methodName" : "testExponentialSleepZeroMaxTries",
  "sourceCode" : "@Test\r\npublic void testExponentialSleepZeroMaxTries() {\r\n    int baseSleepMs = 10;\r\n    int maxSleepMs = 100;\r\n    int maxRetries = 0;\r\n    validateSleepTimes(baseSleepMs, maxSleepMs, maxRetries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\StormBoundedExponentialBackoffRetryTest.java",
  "methodName" : "testExponentialSleepSmallMaxTries",
  "sourceCode" : "@Test\r\npublic void testExponentialSleepSmallMaxTries() {\r\n    int baseSleepMs = 10;\r\n    int maxSleepMs = 100;\r\n    int maxRetries = 10;\r\n    validateSleepTimes(baseSleepMs, maxSleepMs, maxRetries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testIsWorkerHook",
  "sourceCode" : "@Test\r\npublic void testIsWorkerHook() {\r\n    assertFalse(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.BOLTS));\r\n    assertFalse(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.SPOUTS));\r\n    assertFalse(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.STATE_SPOUTS));\r\n    assertFalse(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.DEPENDENCY_JARS));\r\n    assertFalse(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.DEPENDENCY_ARTIFACTS));\r\n    assertTrue(ThriftTopologyUtils.isWorkerHook(StormTopology._Fields.WORKER_HOOKS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testIsDependencies",
  "sourceCode" : "@Test\r\npublic void testIsDependencies() {\r\n    assertFalse(ThriftTopologyUtils.isDependencies(StormTopology._Fields.BOLTS));\r\n    assertFalse(ThriftTopologyUtils.isDependencies(StormTopology._Fields.SPOUTS));\r\n    assertFalse(ThriftTopologyUtils.isDependencies(StormTopology._Fields.STATE_SPOUTS));\r\n    assertFalse(ThriftTopologyUtils.isDependencies(StormTopology._Fields.WORKER_HOOKS));\r\n    assertTrue(ThriftTopologyUtils.isDependencies(StormTopology._Fields.DEPENDENCY_JARS));\r\n    assertTrue(ThriftTopologyUtils.isDependencies(StormTopology._Fields.DEPENDENCY_ARTIFACTS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testGetComponentIdsWithWorkerHook",
  "sourceCode" : "@Test\r\npublic void testGetComponentIdsWithWorkerHook() {\r\n    StormTopology stormTopology = genereateStormTopology(true);\r\n    Set<String> componentIds = ThriftTopologyUtils.getComponentIds(stormTopology);\r\n    assertEquals(ImmutableSet.of(\"bolt-1\", \"spout-1\"), componentIds, \"We expect to get the IDs of the components sans the Worker Hook\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testGetComponentIdsWithoutWorkerHook",
  "sourceCode" : "@Test\r\npublic void testGetComponentIdsWithoutWorkerHook() {\r\n    StormTopology stormTopology = genereateStormTopology(false);\r\n    Set<String> componentIds = ThriftTopologyUtils.getComponentIds(stormTopology);\r\n    assertEquals(ImmutableSet.of(\"bolt-1\", \"spout-1\"), componentIds, \"We expect to get the IDs of the components sans the Worker Hook\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testGetComponentCommonWithWorkerHook",
  "sourceCode" : "@Test\r\npublic void testGetComponentCommonWithWorkerHook() {\r\n    StormTopology stormTopology = genereateStormTopology(true);\r\n    ComponentCommon componentCommon = ThriftTopologyUtils.getComponentCommon(stormTopology, \"bolt-1\");\r\n    assertEquals(new Bolt().get_common(), componentCommon, \"We expect to get bolt-1's common\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\ThriftTopologyUtilsTest.java",
  "methodName" : "testGetComponentCommonWithoutWorkerHook",
  "sourceCode" : "@Test\r\npublic void testGetComponentCommonWithoutWorkerHook() {\r\n    StormTopology stormTopology = genereateStormTopology(false);\r\n    ComponentCommon componentCommon = ThriftTopologyUtils.getComponentCommon(stormTopology, \"bolt-1\");\r\n    assertEquals(new Bolt().get_common(), componentCommon, \"We expect to get bolt-1's common\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "secsToMillisLongTest",
  "sourceCode" : "@Test\r\npublic void secsToMillisLongTest() {\r\n    assertEquals(Time.secsToMillisLong(0), 0);\r\n    assertEquals(Time.secsToMillisLong(0.002), 2);\r\n    assertEquals(Time.secsToMillisLong(1), 1000);\r\n    assertEquals(Time.secsToMillisLong(1.08), 1080);\r\n    assertEquals(Time.secsToMillisLong(10), 10000);\r\n    assertEquals(Time.secsToMillisLong(10.1), 10100);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "ifNotSimulatingAdvanceTimeThrowsTest",
  "sourceCode" : "@Test\r\npublic void ifNotSimulatingAdvanceTimeThrowsTest() {\r\n    assertThrows(IllegalStateException.class, () -> Time.advanceTime(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "isSimulatingReturnsTrueDuringSimulationTest",
  "sourceCode" : "@Test\r\npublic void isSimulatingReturnsTrueDuringSimulationTest() {\r\n    assertFalse(Time.isSimulating());\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        assertTrue(Time.isSimulating());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "shouldNotAdvanceTimeTest",
  "sourceCode" : "@Test\r\npublic void shouldNotAdvanceTimeTest() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long current = Time.currentTimeMillis();\r\n        Time.advanceTime(0);\r\n        assertEquals(Time.deltaMs(current), 0);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "shouldAdvanceForwardTest",
  "sourceCode" : "@Test\r\npublic void shouldAdvanceForwardTest() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        long current = Time.currentTimeMillis();\r\n        Time.advanceTime(1000);\r\n        assertEquals(Time.deltaMs(current), 1000);\r\n        Time.advanceTime(500);\r\n        assertEquals(Time.deltaMs(current), 1500);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "shouldThrowIfAttemptToAdvanceBackwardsTest",
  "sourceCode" : "@Test\r\npublic void shouldThrowIfAttemptToAdvanceBackwardsTest() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        try (SimulatedTime t = new SimulatedTime()) {\r\n            Time.advanceTime(-1500);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "deltaSecsConvertsToSecondsTest",
  "sourceCode" : "@Test\r\npublic void deltaSecsConvertsToSecondsTest() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        int current = Time.currentTimeSecs();\r\n        Time.advanceTime(1000);\r\n        assertEquals(Time.deltaSecs(current), 1);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\TimeTest.java",
  "methodName" : "deltaSecsTruncatesFractionalSecondsTest",
  "sourceCode" : "@Test\r\npublic void deltaSecsTruncatesFractionalSecondsTest() {\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        int current = Time.currentTimeSecs();\r\n        Time.advanceTime(1500);\r\n        assertEquals(Time.deltaSecs(current), 1, 0);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "isZkAuthenticationConfiguredTopologyTest",
  "sourceCode" : "@Test\r\npublic void isZkAuthenticationConfiguredTopologyTest() {\r\n    assertFalse(Utils.isZkAuthenticationConfiguredTopology(null), \"Returns null if given null config\");\r\n    assertFalse(Utils.isZkAuthenticationConfiguredTopology(emptyMockMap()), \"Returns false if scheme key is missing\");\r\n    assertFalse(Utils.isZkAuthenticationConfiguredTopology(topologyMockMap(null)), \"Returns false if scheme value is null\");\r\n    assertTrue(Utils.isZkAuthenticationConfiguredTopology(topologyMockMap(\"foobar\")), \"Returns true if scheme value is string\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestK",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestK() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx1024k results in 1 MB\", \"Xmx1024k\", 1.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx1024K results in 1 MB\", \"Xmx1024K\", 1.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"-Xmx1024k results in 1 MB\", \"-Xmx1024k\", 1.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestM",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestM() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx100M results in 100 MB\", \"Xmx100m\", 100.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx100m results in 100 MB\", \"Xmx100M\", 100.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"-Xmx100M results in 100 MB\", \"-Xmx100m\", 100.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"-Xmx2048M results in 2048 MB\", \"-Xmx2048m\", 2048.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestG",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestG() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx1g results in 1024 MB\", \"Xmx1g\", 1024.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Xmx1G results in 1024 MB\", \"Xmx1G\", 1024.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"-Xmx1g results in 1024 MB\", \"-Xmx1g\", 1024.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"-Xmx2g results in 2048 MB\", \"-Xmx2g\", 2048.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestNoMatch",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestNoMatch() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Unmatched unit results in default\", \"Xmx1t\", 123.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Unmatched option results in default\", \"Xms1g\", 123.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestNulls",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestNulls() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Null value results in default\", (String) null, 123.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Null list results in default\", (List<String>) null, 123.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestExtraChars",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestExtraChars() {\r\n    doParseJvmHeapMemByChildOptsTest(\"Leading characters are ignored\", \"---Xmx1g\", 1024.0);\r\n    doParseJvmHeapMemByChildOptsTest(\"Trailing characters are ignored\", \"Xmx1ggggg\", 1024.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "parseJvmHeapMemByChildOptsTestFirstMatch",
  "sourceCode" : "@Test\r\npublic void parseJvmHeapMemByChildOptsTestFirstMatch() {\r\n    doParseJvmHeapMemByChildOptsTest(\"First valid match is used\", Arrays.asList(null, \"Xmx1t\", \"Xmx1g\", \"Xms1024k Xmx1024k\", \"Xmx100m\"), 1024.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "isZkAuthenticationConfiguredStormServerTest",
  "sourceCode" : "@Test\r\npublic void isZkAuthenticationConfiguredStormServerTest() {\r\n    assertFalse(Utils.isZkAuthenticationConfiguredStormServer(null), \"Returns false if given null config\");\r\n    assertFalse(Utils.isZkAuthenticationConfiguredStormServer(emptyMockMap()), \"Returns false if scheme key is missing\");\r\n    assertFalse(Utils.isZkAuthenticationConfiguredStormServer(serverMockMap(null)), \"Returns false if scheme value is null\");\r\n    assertTrue(Utils.isZkAuthenticationConfiguredStormServer(serverMockMap(\"foobar\")), \"Returns true if scheme value is string\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "isZkAuthenticationConfiguredStormServerWithPropertyTest",
  "sourceCode" : "@Test\r\npublic void isZkAuthenticationConfiguredStormServerWithPropertyTest() {\r\n    String key = \"java.security.auth.login.config\";\r\n    String oldValue = System.getProperty(key);\r\n    try {\r\n        System.setProperty(\"java.security.auth.login.config\", \"anything\");\r\n        assertTrue(Utils.isZkAuthenticationConfiguredStormServer(emptyMockMap()));\r\n    } finally {\r\n        // reset property\r\n        if (oldValue == null) {\r\n            System.clearProperty(key);\r\n        } else {\r\n            System.setProperty(key, oldValue);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfEmpty",
  "sourceCode" : "@Test\r\npublic void testIsValidConfEmpty() {\r\n    Map<String, Object> map0 = ImmutableMap.of();\r\n    assertTrue(Utils.isValidConf(map0, map0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfIdentical",
  "sourceCode" : "@Test\r\npublic void testIsValidConfIdentical() {\r\n    Map<String, Object> map1 = ImmutableMap.of(\"k0\", ImmutableList.of(1L, 2L), \"k1\", ImmutableSet.of('s', 'f'), \"k2\", \"as\");\r\n    assertTrue(Utils.isValidConf(map1, map1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfEqual",
  "sourceCode" : "@Test\r\npublic void testIsValidConfEqual() {\r\n    Map<String, Object> map1 = ImmutableMap.of(\"k0\", ImmutableList.of(1L, 2L), \"k1\", ImmutableSet.of('s', 'f'), \"k2\", \"as\");\r\n    Map<String, Object> map2 = ImmutableMap.of(\"k0\", ImmutableList.of(1L, 2L), \"k1\", ImmutableSet.of('s', 'f'), \"k2\", \"as\");\r\n    // test deep equal\r\n    assertTrue(Utils.isValidConf(map1, map2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfNotEqual",
  "sourceCode" : "@Test\r\npublic void testIsValidConfNotEqual() {\r\n    Map<String, Object> map1 = ImmutableMap.of(\"k0\", ImmutableList.of(1L, 2L), \"k1\", ImmutableSet.of('s', 'f'), \"k2\", \"as\");\r\n    Map<String, Object> map3 = ImmutableMap.of(\"k0\", ImmutableList.of(1L, 2L), \"k1\", ImmutableSet.of('s', 't'), \"k2\", \"as\");\r\n    assertFalse(Utils.isValidConf(map1, map3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfPrimitiveNotEqual",
  "sourceCode" : "@Test\r\npublic void testIsValidConfPrimitiveNotEqual() {\r\n    Map<String, Object> map4 = ImmutableMap.of(\"k0\", 2L);\r\n    Map<String, Object> map5 = ImmutableMap.of(\"k0\", 3L);\r\n    assertFalse(Utils.isValidConf(map4, map5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testIsValidConfEmptyNotEqual",
  "sourceCode" : "@Test\r\npublic void testIsValidConfEmptyNotEqual() {\r\n    Map<String, Object> map0 = ImmutableMap.of();\r\n    Map<String, Object> map5 = ImmutableMap.of(\"k0\", 3L);\r\n    assertFalse(Utils.isValidConf(map0, map5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "checkVersionInfo",
  "sourceCode" : "@Test\r\npublic void checkVersionInfo() {\r\n    Map<String, String> versions = new HashMap<>();\r\n    String key = VersionInfo.getVersion();\r\n    assertNotEquals(\"Unknown\", key, \"Looks like we don't know what version of storm we are\");\r\n    versions.put(key, System.getProperty(\"java.class.path\"));\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.SUPERVISOR_WORKER_VERSION_CLASSPATH_MAP, versions);\r\n    NavigableMap<String, IVersionInfo> alternativeVersions = Utils.getAlternativeVersionsMap(conf);\r\n    assertEquals(1, alternativeVersions.size());\r\n    IVersionInfo found = alternativeVersions.get(key);\r\n    assertNotNull(found);\r\n    assertEquals(key, found.getVersion());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testFindComponentCycles",
  "sourceCode" : "@Test\r\npublic void testFindComponentCycles() {\r\n    class CycleDetectionScenario {\r\n\r\n        final String testName;\r\n\r\n        final String testDescription;\r\n\r\n        final StormTopology topology;\r\n\r\n        final int expectedCycles;\r\n\r\n        CycleDetectionScenario() {\r\n            testName = \"dummy\";\r\n            testDescription = \"dummy test\";\r\n            topology = null;\r\n            expectedCycles = 0;\r\n        }\r\n\r\n        CycleDetectionScenario(String testName, String testDescription, StormTopology topology, int expectedCycles) {\r\n            this.testName = testName.replace(' ', '-');\r\n            this.testDescription = testDescription;\r\n            this.topology = topology;\r\n            this.expectedCycles = expectedCycles;\r\n        }\r\n\r\n        public List<CycleDetectionScenario> createTestScenarios() {\r\n            List<CycleDetectionScenario> ret = new ArrayList<>();\r\n            int testNo = 0;\r\n            CycleDetectionScenario s;\r\n            TopologyBuilder tb;\r\n            // Base case\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt11\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt12\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt21\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt22\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                s = new CycleDetectionScenario(String.format(\"(%d) Base\", testNo), \"Three level component hierarchy with no loops\", tb.createTopology(), 0);\r\n                ret.add(s);\r\n            }\r\n            // single loop with one bolt\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt11\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt12\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt21\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt22\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                // loop bolt 3  (also connect bolt3 to spout 1)\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt3\");\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) One Loop\", testNo), \"Three level component hierarchy with 1 cycle in bolt3\", tb.createTopology(), 1));\r\n            }\r\n            // single loop with three bolts\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt11\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt12\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt21\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt22\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                // loop bolt 3 -> 4 -> 5 -> 3 (also connect bolt3 to spout1)\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt5\");\r\n                tb.setBolt(\"bolt4\", new TestWordCounter(), 10).shuffleGrouping(\"bolt3\");\r\n                tb.setBolt(\"bolt5\", new TestWordCounter(), 10).shuffleGrouping(\"bolt4\");\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) One Loop\", testNo), \"Four level component hierarchy with 1 cycle in bolt3,bolt4,bolt5\", tb.createTopology(), 1));\r\n            }\r\n            // two loops with three bolts, and one bolt\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\");\r\n                tb.setBolt(\"bolt11\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt12\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt21\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt22\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\");\r\n                // loop bolt 3 -> 4 -> 5 -> 3 (also connect bolt3 to spout1)\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt5\");\r\n                tb.setBolt(\"bolt4\", new TestWordCounter(), 10).shuffleGrouping(\"bolt3\");\r\n                tb.setBolt(\"bolt5\", new TestWordCounter(), 10).shuffleGrouping(\"bolt4\");\r\n                // loop bolt 6  (also connect bolt6 to spout 1)\r\n                tb.setBolt(\"bolt6\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt6\");\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) Two Loops\", testNo), \"Four level component hierarchy with 2 cycles in bolt3,bolt4,bolt5 and bolt6\", tb.createTopology(), 2));\r\n            }\r\n            // complex cycle\r\n            {\r\n                // (S1 -> B1 -> B2 -> B3 -> B4 <- S2), (B4 -> B3), (B4 -> B1)\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setSpout(\"spout2\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt4\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\").shuffleGrouping(\"bolt4\");\r\n                tb.setBolt(\"bolt4\", new TestWordCounter(), 10).shuffleGrouping(\"bolt3\").shuffleGrouping(\"spout2\");\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) Complex Loops#1\", testNo), \"Complex cycle (S1 -> B1 -> B2 -> B3 -> B4 <- S2), (B4 -> B3), (B4 -> B1)\", tb.createTopology(), 1));\r\n            }\r\n            // another complex\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setSpout(\"spout1\", new TestWordSpout(), 10);\r\n                tb.setSpout(\"spout2\", new TestWordSpout(), 10);\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"spout1\").shuffleGrouping(\"bolt4\").shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\").shuffleGrouping(\"bolt4\");\r\n                tb.setBolt(\"bolt4\", new TestWordCounter(), 10).shuffleGrouping(\"spout2\");\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) Complex Loops#2\", testNo), \"Complex cycle 2 (S1 -> B1 <-> B2 -> B3 ), (S2 -> B4 -> B3), (B4 -> B1)\", tb.createTopology(), 1));\r\n            }\r\n            // no spouts but with loops; the loops wont be detected\r\n            {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                tb.setBolt(\"bolt1\", new TestWordCounter(), 10).shuffleGrouping(\"bolt4\").shuffleGrouping(\"bolt2\");\r\n                tb.setBolt(\"bolt2\", new TestWordCounter(), 10).shuffleGrouping(\"bolt1\");\r\n                tb.setBolt(\"bolt3\", new TestWordCounter(), 10).shuffleGrouping(\"bolt2\").shuffleGrouping(\"bolt4\");\r\n                tb.setBolt(\"bolt4\", new TestWordCounter(), 10);\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) No spout complex loops\", testNo), \"No Spouts, but with cycles (B1 <-> B2 -> B3 ), (B4 -> B3), (B4 -> B1)\", tb.createTopology(), 0));\r\n            }\r\n            // now some randomly generated topologies\r\n            int maxSpouts = 10;\r\n            int maxBolts = 30;\r\n            int randomTopoCnt = 100;\r\n            for (int iRandTest = 0; iRandTest < randomTopoCnt; iRandTest++) {\r\n                testNo++;\r\n                tb = new TopologyBuilder();\r\n                // topology component and connection counts\r\n                int spoutCnt = ThreadLocalRandom.current().nextInt(0, maxSpouts) + 1;\r\n                int boltCnt = ThreadLocalRandom.current().nextInt(0, maxBolts) + 1;\r\n                int spoutToBoltConnectionCnt = ThreadLocalRandom.current().nextInt(spoutCnt * boltCnt) + 1;\r\n                int boltToBoltConnectionCnt = ThreadLocalRandom.current().nextInt(boltCnt * boltCnt) + 1;\r\n                Map<Integer, BoltDeclarer> boltDeclarers = new HashMap<>();\r\n                for (int iSpout = 0; iSpout < spoutCnt; iSpout++) {\r\n                    tb.setSpout(\"spout\" + iSpout, new TestWordSpout(), 10);\r\n                }\r\n                for (int iBolt = 0; iBolt < boltCnt; iBolt++) {\r\n                    boltDeclarers.put(iBolt, tb.setBolt(\"bolt\" + iBolt, new TestWordCounter(), 10));\r\n                }\r\n                // spout to bolt connections\r\n                for (int i = 0; i < spoutToBoltConnectionCnt; i++) {\r\n                    int iSpout = ThreadLocalRandom.current().nextInt(0, spoutCnt);\r\n                    int iBolt = ThreadLocalRandom.current().nextInt(0, boltCnt);\r\n                    boltDeclarers.get(iBolt).shuffleGrouping(\"spout\" + iSpout);\r\n                }\r\n                // bolt to bolt connections\r\n                for (int i = 0; i < boltToBoltConnectionCnt; i++) {\r\n                    int iBolt1 = ThreadLocalRandom.current().nextInt(0, boltCnt);\r\n                    int iBolt2 = ThreadLocalRandom.current().nextInt(0, boltCnt);\r\n                    boltDeclarers.get(iBolt2).shuffleGrouping(\"bolt\" + iBolt1);\r\n                }\r\n                ret.add(new CycleDetectionScenario(String.format(\"(%d) Random Topo#%d\", testNo, iRandTest), String.format(\"Random topology #%d, spouts=%d, bolts=%d, connections: fromSpouts=%d/fromBolts=%d\", iRandTest, spoutCnt, boltCnt, spoutToBoltConnectionCnt, boltToBoltConnectionCnt), tb.createTopology(), -1));\r\n            }\r\n            return ret;\r\n        }\r\n    }\r\n    List<String> testFailures = new ArrayList<>();\r\n    new CycleDetectionScenario().createTestScenarios().forEach(x -> {\r\n        LOG.info(\"==================== Running Test Scenario: {} =======================\", x.testName);\r\n        LOG.info(\"{}: {}\", x.testName, x.testDescription);\r\n        List<List<String>> loops = Utils.findComponentCycles(x.topology, x.testName);\r\n        if (x.expectedCycles >= 0) {\r\n            if (!loops.isEmpty()) {\r\n                LOG.info(\"{} detected loops are \\\"{}\\\"\", x.testName, loops.stream().map(y -> String.join(\",\", y)).collect(Collectors.joining(\" ; \")));\r\n            }\r\n            if (loops.size() != x.expectedCycles) {\r\n                testFailures.add(String.format(\"Test \\\"%s\\\" failed, detected cycles=%d does not match expected=%d for \\\"%s\\\"\", x.testName, loops.size(), x.expectedCycles, x.testDescription));\r\n                if (!loops.isEmpty()) {\r\n                    testFailures.add(String.format(\"\\t\\tdetected loops are \\\"%s\\\"\", loops.stream().map(y -> String.join(\",\", y)).collect(Collectors.joining(\" ; \"))));\r\n                }\r\n            }\r\n        } else {\r\n            // these are random topologies, with indeterminate number of loops\r\n            LOG.info(\"{} detected loop count is \\\"{}\\\"\", x.testName, loops.size());\r\n        }\r\n    });\r\n    if (!testFailures.isEmpty()) {\r\n        fail(String.join(\"\\n\", testFailures));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\utils\\UtilsTest.java",
  "methodName" : "testHandleUncaughtExceptionSwallowsCausedAndDerivedExceptions",
  "sourceCode" : "@Test\r\npublic void testHandleUncaughtExceptionSwallowsCausedAndDerivedExceptions() {\r\n    Set<Class<?>> allowedExceptions = new HashSet<>(Arrays.asList(new Class<?>[] { IOException.class }));\r\n    try {\r\n        handleUncaughtException(new IOException(), allowedExceptions, false);\r\n    } catch (Throwable unexpected) {\r\n        fail(\"Should have swallowed IOException!\", unexpected);\r\n    }\r\n    try {\r\n        handleUncaughtException(new SocketException(), allowedExceptions, false);\r\n    } catch (Throwable unexpected) {\r\n        fail(\"Should have swallowed Throwable derived from IOException!\", unexpected);\r\n    }\r\n    try {\r\n        handleUncaughtException(new TTransportException(new IOException()), allowedExceptions, false);\r\n    } catch (Throwable unexpected) {\r\n        fail(\"Should have swallowed Throwable caused by an IOException!\", unexpected);\r\n    }\r\n    try {\r\n        handleUncaughtException(new TTransportException(new SocketException()), allowedExceptions, false);\r\n    } catch (Throwable unexpected) {\r\n        fail(\"Should have swallowed Throwable caused by a Throwable derived from IOException!\", unexpected);\r\n    }\r\n    Throwable t = new NullPointerException();\r\n    String expectationMessage = \"Should have thrown an Error() with a cause of NullPointerException\";\r\n    try {\r\n        handleUncaughtException(t, allowedExceptions, false);\r\n        fail(expectationMessage);\r\n    } catch (Error expected) {\r\n        assertEquals(expected.getCause(), t, expectationMessage);\r\n    } catch (Throwable unexpected) {\r\n        fail(expectationMessage, unexpected);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\persistence\\WindowStateTest.java",
  "methodName" : "testAdd",
  "sourceCode" : "@Test\r\npublic void testAdd() throws Exception {\r\n    Mockito.when(partitionIdsState.get(Mockito.any(), Mockito.any())).then(returnsArgAt(1));\r\n    Mockito.when(windowState.get(Mockito.any(), Mockito.any())).then(returnsArgAt(1));\r\n    WindowState<Integer> ws = getWindowState(10 * WindowState.MAX_PARTITION_EVENTS);\r\n    long partitions = 15;\r\n    long numEvents = partitions * WindowState.MAX_PARTITION_EVENTS;\r\n    for (int i = 0; i < numEvents; i++) {\r\n        ws.add(getEvent(i));\r\n    }\r\n    // 5 partitions evicted to window state\r\n    Mockito.verify(windowState, Mockito.times(5)).put(longCaptor.capture(), windowValuesCaptor.capture());\r\n    assertEquals(5, longCaptor.getAllValues().size());\r\n    // each evicted partition has MAX_EVENTS_PER_PARTITION\r\n    windowValuesCaptor.getAllValues().forEach(wp -> assertEquals(WindowState.MAX_PARTITION_EVENTS, wp.size()));\r\n    // last partition is not evicted\r\n    assertFalse(longCaptor.getAllValues().contains(partitions - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\persistence\\WindowStateTest.java",
  "methodName" : "testIterator",
  "sourceCode" : "@Test\r\npublic void testIterator() {\r\n    Map<Long, WindowState.WindowPartition<Event<Tuple>>> partitionMap = new HashMap<>();\r\n    Mockito.when(partitionIdsState.get(Mockito.any(), Mockito.any())).then(returnsArgAt(1));\r\n    Mockito.when(windowState.get(Mockito.any(), Mockito.any())).then(invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        WindowState.WindowPartition<Event<Tuple>> evicted = partitionMap.get(args[0]);\r\n        return evicted != null ? evicted : args[1];\r\n    });\r\n    Mockito.doAnswer((Answer<Void>) invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        partitionMap.put((long) args[0], (WindowState.WindowPartition<Event<Tuple>>) args[1]);\r\n        return null;\r\n    }).when(windowState).put(Mockito.any(), Mockito.any());\r\n    Mockito.doAnswer((Answer<Void>) invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        partitionMap.remove(args[0]);\r\n        return null;\r\n    }).when(windowState).delete(Mockito.anyLong());\r\n    WindowState<Integer> ws = getWindowState(10 * WindowState.MAX_PARTITION_EVENTS);\r\n    long partitions = 15;\r\n    long numEvents = partitions * WindowState.MAX_PARTITION_EVENTS;\r\n    List<Event<Integer>> expected = new ArrayList<>();\r\n    for (int i = 0; i < numEvents; i++) {\r\n        Event<Integer> event = getEvent(i);\r\n        expected.add(event);\r\n        ws.add(event);\r\n    }\r\n    assertEquals(5, partitionMap.size());\r\n    Iterator<Event<Integer>> it = ws.iterator();\r\n    List<Event<Integer>> actual = new ArrayList<>();\r\n    it.forEachRemaining(actual::add);\r\n    assertEquals(expected, actual);\r\n    // iterate again\r\n    it = ws.iterator();\r\n    actual.clear();\r\n    it.forEachRemaining(actual::add);\r\n    assertEquals(expected, actual);\r\n    // remove\r\n    it = ws.iterator();\r\n    while (it.hasNext()) {\r\n        it.next();\r\n        it.remove();\r\n    }\r\n    it = ws.iterator();\r\n    actual.clear();\r\n    it.forEachRemaining(actual::add);\r\n    assertEquals(Collections.emptyList(), actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\persistence\\WindowStateTest.java",
  "methodName" : "testIteratorPartitionNotEvicted",
  "sourceCode" : "@Test\r\npublic void testIteratorPartitionNotEvicted() {\r\n    Map<Long, WindowState.WindowPartition<Event<Tuple>>> partitionMap = new HashMap<>();\r\n    Mockito.when(partitionIdsState.get(Mockito.any(), Mockito.any())).then(returnsArgAt(1));\r\n    Mockito.when(windowState.get(Mockito.any(), Mockito.any())).then(invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        WindowState.WindowPartition<Event<Tuple>> evicted = partitionMap.get(args[0]);\r\n        return evicted != null ? evicted : args[1];\r\n    });\r\n    Mockito.doAnswer((Answer<Void>) invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        partitionMap.put((long) args[0], (WindowState.WindowPartition<Event<Tuple>>) args[1]);\r\n        return null;\r\n    }).when(windowState).put(Mockito.any(), Mockito.any());\r\n    WindowState<Integer> ws = getWindowState(10 * WindowState.MAX_PARTITION_EVENTS);\r\n    long partitions = 10;\r\n    long numEvents = partitions * WindowState.MAX_PARTITION_EVENTS;\r\n    List<Event<Integer>> expected = new ArrayList<>();\r\n    for (int i = 0; i < numEvents; i++) {\r\n        Event<Integer> event = getEvent(i);\r\n        expected.add(event);\r\n        ws.add(event);\r\n    }\r\n    // Stop iterating in the middle of the 10th partition\r\n    Iterator<Event<Integer>> it = ws.iterator();\r\n    for (int i = 0; i < 9500; i++) {\r\n        it.next();\r\n    }\r\n    for (int i = 0; i < numEvents; i++) {\r\n        Event<Integer> event = getEvent(i);\r\n        expected.add(event);\r\n        ws.add(event);\r\n    }\r\n    // 10th partition should not have been evicted\r\n    assertFalse(partitionMap.containsKey(9L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WaterMarkEventGeneratorTest.java",
  "methodName" : "testTrackSingleStream",
  "sourceCode" : "@Test\r\npublic void testTrackSingleStream() {\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 100);\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 110);\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.get(0).isWatermark());\r\n    assertEquals(105, eventList.get(0).getTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WaterMarkEventGeneratorTest.java",
  "methodName" : "testTrackSingleStreamOutOfOrder",
  "sourceCode" : "@Test\r\npublic void testTrackSingleStreamOutOfOrder() {\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 100);\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 110);\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 104);\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.get(0).isWatermark());\r\n    assertEquals(105, eventList.get(0).getTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WaterMarkEventGeneratorTest.java",
  "methodName" : "testTrackTwoStreams",
  "sourceCode" : "@Test\r\npublic void testTrackTwoStreams() {\r\n    Set<GlobalStreamId> streams = new HashSet<>();\r\n    streams.add(streamId(\"s1\"));\r\n    streams.add(streamId(\"s2\"));\r\n    waterMarkEventGenerator = new WaterMarkEventGenerator<>(windowManager, 100000, 5, streams);\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 100);\r\n    waterMarkEventGenerator.track(streamId(\"s1\"), 110);\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.isEmpty());\r\n    waterMarkEventGenerator.track(streamId(\"s2\"), 95);\r\n    waterMarkEventGenerator.track(streamId(\"s2\"), 98);\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.get(0).isWatermark());\r\n    assertEquals(93, eventList.get(0).getTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WaterMarkEventGeneratorTest.java",
  "methodName" : "testNoEvents",
  "sourceCode" : "@Test\r\npublic void testNoEvents() {\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WaterMarkEventGeneratorTest.java",
  "methodName" : "testLateEvent",
  "sourceCode" : "@Test\r\npublic void testLateEvent() {\r\n    assertTrue(waterMarkEventGenerator.track(streamId(\"s1\"), 100));\r\n    assertTrue(waterMarkEventGenerator.track(streamId(\"s1\"), 110));\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.get(0).isWatermark());\r\n    assertEquals(105, eventList.get(0).getTimestamp());\r\n    eventList.clear();\r\n    assertTrue(waterMarkEventGenerator.track(streamId(\"s1\"), 105));\r\n    assertTrue(waterMarkEventGenerator.track(streamId(\"s1\"), 106));\r\n    assertTrue(waterMarkEventGenerator.track(streamId(\"s1\"), 115));\r\n    assertFalse(waterMarkEventGenerator.track(streamId(\"s1\"), 104));\r\n    waterMarkEventGenerator.run();\r\n    assertTrue(eventList.get(0).isWatermark());\r\n    assertEquals(110, eventList.get(0).getTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testCountBasedWindow",
  "sourceCode" : "@Test\r\npublic void testCountBasedWindow() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new CountEvictionPolicy<>(5);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new CountTriggerPolicy<>(2, windowManager, evictionPolicy);\r\n    triggerPolicy.start();\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1);\r\n    windowManager.add(2);\r\n    // nothing expired yet\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    assertEquals(seq(1, 2), listener.onActivationEvents);\r\n    assertEquals(seq(1, 2), listener.onActivationNewEvents);\r\n    assertTrue(listener.onActivationExpiredEvents.isEmpty());\r\n    windowManager.add(3);\r\n    windowManager.add(4);\r\n    // nothing expired yet\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    assertEquals(seq(1, 4), listener.onActivationEvents);\r\n    assertEquals(seq(3, 4), listener.onActivationNewEvents);\r\n    assertTrue(listener.onActivationExpiredEvents.isEmpty());\r\n    windowManager.add(5);\r\n    windowManager.add(6);\r\n    // 1 expired\r\n    assertEquals(seq(1), listener.onExpiryEvents);\r\n    assertEquals(seq(2, 6), listener.onActivationEvents);\r\n    assertEquals(seq(5, 6), listener.onActivationNewEvents);\r\n    assertEquals(seq(1), listener.onActivationExpiredEvents);\r\n    listener.clear();\r\n    windowManager.add(7);\r\n    // nothing expires until threshold is hit\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    windowManager.add(8);\r\n    // 1 expired\r\n    assertEquals(seq(2, 3), listener.onExpiryEvents);\r\n    assertEquals(seq(4, 8), listener.onActivationEvents);\r\n    assertEquals(seq(7, 8), listener.onActivationNewEvents);\r\n    assertEquals(seq(2, 3), listener.onActivationExpiredEvents);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testExpireThreshold",
  "sourceCode" : "@Test\r\npublic void testExpireThreshold() {\r\n    int threshold = WindowManager.EXPIRE_EVENTS_THRESHOLD;\r\n    int windowLength = 5;\r\n    windowManager.setEvictionPolicy(new CountEvictionPolicy<>(5));\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new TimeTriggerPolicy<>(new Duration(1, TimeUnit.HOURS).value, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    for (int i : seq(1, 5)) {\r\n        windowManager.add(i);\r\n    }\r\n    // nothing expired yet\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    for (int i : seq(6, 10)) {\r\n        windowManager.add(i);\r\n    }\r\n    for (int i : seq(11, threshold)) {\r\n        windowManager.add(i);\r\n    }\r\n    // window should be compacted and events should be expired.\r\n    assertEquals(seq(1, threshold - windowLength), listener.onExpiryEvents);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testExpireThresholdWithWatermarkCountEvictionPolicy",
  "sourceCode" : "@Test\r\npublic void testExpireThresholdWithWatermarkCountEvictionPolicy() {\r\n    int windowLength = WindowManager.EXPIRE_EVENTS_THRESHOLD;\r\n    EvictionPolicy<Integer, ?> watermarkCountEvictionPolicy = new WatermarkCountEvictionPolicy<>(windowLength);\r\n    testEvictBeforeWatermarkForWatermarkEvictionPolicy(watermarkCountEvictionPolicy, windowLength);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testExpireThresholdWithWatermarkTimeEvictionPolicy",
  "sourceCode" : "@Test\r\npublic void testExpireThresholdWithWatermarkTimeEvictionPolicy() {\r\n    int windowLength = WindowManager.EXPIRE_EVENTS_THRESHOLD;\r\n    EvictionPolicy<Integer, ?> watermarkTimeEvictionPolicy = new WatermarkTimeEvictionPolicy<>(windowLength);\r\n    testEvictBeforeWatermarkForWatermarkEvictionPolicy(watermarkTimeEvictionPolicy, windowLength);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testTimeBasedWindow",
  "sourceCode" : "@Test\r\npublic void testTimeBasedWindow() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new TimeEvictionPolicy<>(new Duration(1, TimeUnit.SECONDS).value);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    /*\r\n         * Don't wait for Timetrigger to fire since this could lead to timing issues in unit tests.\r\n         * Set it to a large value and trigger manually.\r\n         */\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new TimeTriggerPolicy<>(new Duration(1, TimeUnit.DAYS).value, windowManager, evictionPolicy);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    long now = System.currentTimeMillis();\r\n    // add with past ts\r\n    for (int i : seq(1, 50)) {\r\n        windowManager.add(i, now - 1000);\r\n    }\r\n    // add with current ts\r\n    for (int i : seq(51, WindowManager.EXPIRE_EVENTS_THRESHOLD)) {\r\n        windowManager.add(i, now);\r\n    }\r\n    // first 50 should have expired due to expire events threshold\r\n    assertEquals(50, listener.onExpiryEvents.size());\r\n    // add more events with past ts\r\n    for (int i : seq(WindowManager.EXPIRE_EVENTS_THRESHOLD + 1, WindowManager.EXPIRE_EVENTS_THRESHOLD + 100)) {\r\n        windowManager.add(i, now - 1000);\r\n    }\r\n    // simulate the time trigger by setting the reference time and invoking onTrigger() manually\r\n    evictionPolicy.setContext(new DefaultEvictionContext(now + 100));\r\n    windowManager.onTrigger();\r\n    // 100 events with past ts should expire\r\n    assertEquals(100, listener.onExpiryEvents.size());\r\n    assertEquals(seq(WindowManager.EXPIRE_EVENTS_THRESHOLD + 1, WindowManager.EXPIRE_EVENTS_THRESHOLD + 100), listener.onExpiryEvents);\r\n    List<Integer> activationsEvents = seq(51, WindowManager.EXPIRE_EVENTS_THRESHOLD);\r\n    assertEquals(seq(51, WindowManager.EXPIRE_EVENTS_THRESHOLD), listener.onActivationEvents);\r\n    assertEquals(seq(51, WindowManager.EXPIRE_EVENTS_THRESHOLD), listener.onActivationNewEvents);\r\n    // activation expired list should contain even the ones expired due to EXPIRE_EVENTS_THRESHOLD\r\n    List<Integer> expiredList = seq(1, 50);\r\n    expiredList.addAll(seq(WindowManager.EXPIRE_EVENTS_THRESHOLD + 1, WindowManager.EXPIRE_EVENTS_THRESHOLD + 100));\r\n    assertEquals(expiredList, listener.onActivationExpiredEvents);\r\n    listener.clear();\r\n    // add more events with current ts\r\n    List<Integer> newEvents = seq(WindowManager.EXPIRE_EVENTS_THRESHOLD + 101, WindowManager.EXPIRE_EVENTS_THRESHOLD + 200);\r\n    for (int i : newEvents) {\r\n        windowManager.add(i, now);\r\n    }\r\n    activationsEvents.addAll(newEvents);\r\n    // simulate the time trigger by setting the reference time and invoking onTrigger() manually\r\n    evictionPolicy.setContext(new DefaultEvictionContext(now + 200));\r\n    windowManager.onTrigger();\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    assertEquals(activationsEvents, listener.onActivationEvents);\r\n    assertEquals(newEvents, listener.onActivationNewEvents);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testTimeBasedWindowExpiry",
  "sourceCode" : "@Test\r\npublic void testTimeBasedWindowExpiry() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new TimeEvictionPolicy<>(new Duration(100, TimeUnit.MILLISECONDS).value);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    /*\r\n         * Don't wait for Timetrigger to fire since this could lead to timing issues in unit tests.\r\n         * Set it to a large value and trigger manually.\r\n         */\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new TimeTriggerPolicy<>(new Duration(1, TimeUnit.DAYS).value, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    long now = System.currentTimeMillis();\r\n    // add 10 events\r\n    for (int i : seq(1, 10)) {\r\n        windowManager.add(i);\r\n    }\r\n    // simulate the time trigger by setting the reference time and invoking onTrigger() manually\r\n    evictionPolicy.setContext(new DefaultEvictionContext(now + 60));\r\n    windowManager.onTrigger();\r\n    assertEquals(seq(1, 10), listener.onActivationEvents);\r\n    assertTrue(listener.onActivationExpiredEvents.isEmpty());\r\n    listener.clear();\r\n    // wait so all events expire\r\n    evictionPolicy.setContext(new DefaultEvictionContext(now + 120));\r\n    windowManager.onTrigger();\r\n    assertEquals(seq(1, 10), listener.onExpiryEvents);\r\n    assertTrue(listener.onActivationEvents.isEmpty());\r\n    listener.clear();\r\n    evictionPolicy.setContext(new DefaultEvictionContext(now + 180));\r\n    windowManager.onTrigger();\r\n    assertTrue(listener.onActivationExpiredEvents.isEmpty());\r\n    assertTrue(listener.onActivationEvents.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testTumblingWindow",
  "sourceCode" : "@Test\r\npublic void testTumblingWindow() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new CountEvictionPolicy<>(3);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new CountTriggerPolicy<>(3, windowManager, evictionPolicy);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1);\r\n    windowManager.add(2);\r\n    // nothing expired yet\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    windowManager.add(3);\r\n    assertTrue(listener.onExpiryEvents.isEmpty());\r\n    assertEquals(seq(1, 3), listener.onActivationEvents);\r\n    assertTrue(listener.onActivationExpiredEvents.isEmpty());\r\n    assertEquals(seq(1, 3), listener.onActivationNewEvents);\r\n    listener.clear();\r\n    windowManager.add(4);\r\n    windowManager.add(5);\r\n    windowManager.add(6);\r\n    assertEquals(seq(1, 3), listener.onExpiryEvents);\r\n    assertEquals(seq(4, 6), listener.onActivationEvents);\r\n    assertEquals(seq(1, 3), listener.onActivationExpiredEvents);\r\n    assertEquals(seq(4, 6), listener.onActivationNewEvents);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testEventTimeBasedWindow",
  "sourceCode" : "@Test\r\npublic void testEventTimeBasedWindow() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkTimeEvictionPolicy<>(20);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkTimeTriggerPolicy<>(10, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 603);\r\n    windowManager.add(2, 605);\r\n    windowManager.add(3, 607);\r\n    // This should trigger the scan to find\r\n    // the next aligned window end ts, but not produce any activations\r\n    windowManager.add(new WaterMarkEvent<>(609));\r\n    assertEquals(Collections.emptyList(), listener.allOnActivationEvents);\r\n    windowManager.add(4, 618);\r\n    windowManager.add(5, 626);\r\n    windowManager.add(6, 636);\r\n    // send a watermark event, which should trigger three windows.\r\n    windowManager.add(new WaterMarkEvent<>(631));\r\n    //        System.out.println(listener.allOnActivationEvents);\r\n    assertEquals(3, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 3), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(1, 4), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(4, 5), listener.allOnActivationEvents.get(2));\r\n    assertEquals(Collections.emptyList(), listener.allOnActivationExpiredEvents.get(0));\r\n    assertEquals(Collections.emptyList(), listener.allOnActivationExpiredEvents.get(1));\r\n    assertEquals(seq(1, 3), listener.allOnActivationExpiredEvents.get(2));\r\n    assertEquals(seq(1, 3), listener.allOnActivationNewEvents.get(0));\r\n    assertEquals(seq(4, 4), listener.allOnActivationNewEvents.get(1));\r\n    assertEquals(seq(5, 5), listener.allOnActivationNewEvents.get(2));\r\n    assertEquals(seq(1, 3), listener.allOnExpiryEvents.get(0));\r\n    // add more events with a gap in ts\r\n    windowManager.add(7, 825);\r\n    windowManager.add(8, 826);\r\n    windowManager.add(9, 827);\r\n    windowManager.add(10, 839);\r\n    listener.clear();\r\n    windowManager.add(new WaterMarkEvent<>(834));\r\n    assertEquals(3, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(5, 6), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(6, 6), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(7, 9), listener.allOnActivationEvents.get(2));\r\n    assertEquals(seq(4, 4), listener.allOnActivationExpiredEvents.get(0));\r\n    assertEquals(seq(5, 5), listener.allOnActivationExpiredEvents.get(1));\r\n    assertEquals(Collections.emptyList(), listener.allOnActivationExpiredEvents.get(2));\r\n    assertEquals(seq(6, 6), listener.allOnActivationNewEvents.get(0));\r\n    assertEquals(Collections.emptyList(), listener.allOnActivationNewEvents.get(1));\r\n    assertEquals(seq(7, 9), listener.allOnActivationNewEvents.get(2));\r\n    assertEquals(seq(4, 4), listener.allOnExpiryEvents.get(0));\r\n    assertEquals(seq(5, 5), listener.allOnExpiryEvents.get(1));\r\n    assertEquals(seq(6, 6), listener.allOnExpiryEvents.get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testCountBasedWindowWithEventTs",
  "sourceCode" : "@Test\r\npublic void testCountBasedWindowWithEventTs() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkCountEvictionPolicy<>(3);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkTimeTriggerPolicy<>(10, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 603);\r\n    windowManager.add(2, 605);\r\n    windowManager.add(3, 607);\r\n    windowManager.add(4, 618);\r\n    windowManager.add(5, 626);\r\n    windowManager.add(6, 636);\r\n    // send a watermark event, which should trigger three windows.\r\n    windowManager.add(new WaterMarkEvent<>(631));\r\n    assertEquals(3, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 3), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(2, 4), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(3, 5), listener.allOnActivationEvents.get(2));\r\n    // add more events with a gap in ts\r\n    windowManager.add(7, 665);\r\n    windowManager.add(8, 666);\r\n    windowManager.add(9, 667);\r\n    windowManager.add(10, 679);\r\n    listener.clear();\r\n    windowManager.add(new WaterMarkEvent<>(674));\r\n    //        System.out.println(listener.allOnActivationEvents);\r\n    assertEquals(4, listener.allOnActivationEvents.size());\r\n    // same set of events part of three windows\r\n    assertEquals(seq(4, 6), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(4, 6), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(4, 6), listener.allOnActivationEvents.get(2));\r\n    assertEquals(seq(7, 9), listener.allOnActivationEvents.get(3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testCountBasedTriggerWithEventTs",
  "sourceCode" : "@Test\r\npublic void testCountBasedTriggerWithEventTs() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkTimeEvictionPolicy<>(20);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkCountTriggerPolicy<>(3, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 603);\r\n    windowManager.add(2, 605);\r\n    windowManager.add(3, 607);\r\n    windowManager.add(4, 618);\r\n    windowManager.add(5, 625);\r\n    windowManager.add(6, 626);\r\n    windowManager.add(7, 629);\r\n    windowManager.add(8, 636);\r\n    // send a watermark event, which should trigger three windows.\r\n    windowManager.add(new WaterMarkEvent<>(631));\r\n    //        System.out.println(listener.allOnActivationEvents);\r\n    assertEquals(2, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 3), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(3, 6), listener.allOnActivationEvents.get(1));\r\n    // add more events with a gap in ts\r\n    windowManager.add(9, 665);\r\n    windowManager.add(10, 666);\r\n    windowManager.add(11, 667);\r\n    windowManager.add(12, 669);\r\n    windowManager.add(12, 679);\r\n    listener.clear();\r\n    windowManager.add(new WaterMarkEvent<>(674));\r\n    //        System.out.println(listener.allOnActivationEvents);\r\n    assertEquals(2, listener.allOnActivationEvents.size());\r\n    // same set of events part of three windows\r\n    assertEquals(seq(9), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(9, 12), listener.allOnActivationEvents.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testCountBasedTumblingWithSameEventTs",
  "sourceCode" : "@Test\r\npublic void testCountBasedTumblingWithSameEventTs() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkCountEvictionPolicy<>(2);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkCountTriggerPolicy<>(2, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 10);\r\n    windowManager.add(2, 10);\r\n    windowManager.add(3, 11);\r\n    windowManager.add(4, 12);\r\n    windowManager.add(5, 12);\r\n    windowManager.add(6, 12);\r\n    windowManager.add(7, 12);\r\n    windowManager.add(8, 13);\r\n    windowManager.add(9, 14);\r\n    windowManager.add(10, 15);\r\n    windowManager.add(new WaterMarkEvent<>(20));\r\n    assertEquals(5, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 2), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(3, 4), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(5, 6), listener.allOnActivationEvents.get(2));\r\n    assertEquals(seq(7, 8), listener.allOnActivationEvents.get(3));\r\n    assertEquals(seq(9, 10), listener.allOnActivationEvents.get(4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testCountBasedSlidingWithSameEventTs",
  "sourceCode" : "@Test\r\npublic void testCountBasedSlidingWithSameEventTs() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkCountEvictionPolicy<>(5);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkCountTriggerPolicy<>(2, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 10);\r\n    windowManager.add(2, 10);\r\n    windowManager.add(3, 11);\r\n    windowManager.add(4, 12);\r\n    windowManager.add(5, 12);\r\n    windowManager.add(6, 12);\r\n    windowManager.add(7, 12);\r\n    windowManager.add(8, 13);\r\n    windowManager.add(9, 14);\r\n    windowManager.add(10, 15);\r\n    windowManager.add(new WaterMarkEvent<>(20));\r\n    assertEquals(5, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 2), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(1, 4), listener.allOnActivationEvents.get(1));\r\n    assertEquals(seq(2, 6), listener.allOnActivationEvents.get(2));\r\n    assertEquals(seq(4, 8), listener.allOnActivationEvents.get(3));\r\n    assertEquals(seq(6, 10), listener.allOnActivationEvents.get(4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testEventTimeLag",
  "sourceCode" : "@Test\r\npublic void testEventTimeLag() {\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkTimeEvictionPolicy<>(20, 5);\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkTimeTriggerPolicy<>(10, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 603);\r\n    windowManager.add(2, 605);\r\n    windowManager.add(3, 607);\r\n    windowManager.add(4, 618);\r\n    windowManager.add(5, 626);\r\n    windowManager.add(6, 632);\r\n    windowManager.add(7, 629);\r\n    windowManager.add(8, 636);\r\n    // send a watermark event, which should trigger three windows.\r\n    windowManager.add(new WaterMarkEvent<>(631));\r\n    //        System.out.println(listener.allOnActivationEvents);\r\n    assertEquals(3, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 3), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(1, 4), listener.allOnActivationEvents.get(1));\r\n    // out of order events should be processed upto the lag\r\n    assertEquals(Arrays.asList(4, 5, 7), listener.allOnActivationEvents.get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-client\\test\\jvm\\org\\apache\\storm\\windowing\\WindowManagerTest.java",
  "methodName" : "testScanStop",
  "sourceCode" : "@Test\r\npublic void testScanStop() {\r\n    final Set<Integer> eventsScanned = new HashSet<>();\r\n    EvictionPolicy<Integer, ?> evictionPolicy = new WatermarkTimeEvictionPolicy<Integer>(20, 5) {\r\n\r\n        @Override\r\n        public Action evict(Event<Integer> event) {\r\n            eventsScanned.add(event.get());\r\n            return super.evict(event);\r\n        }\r\n    };\r\n    windowManager.setEvictionPolicy(evictionPolicy);\r\n    TriggerPolicy<Integer, ?> triggerPolicy = new WatermarkTimeTriggerPolicy<>(10, windowManager, evictionPolicy, windowManager);\r\n    triggerPolicy.start();\r\n    windowManager.setTriggerPolicy(triggerPolicy);\r\n    windowManager.add(1, 603);\r\n    windowManager.add(2, 605);\r\n    windowManager.add(3, 607);\r\n    windowManager.add(4, 618);\r\n    windowManager.add(5, 626);\r\n    windowManager.add(6, 629);\r\n    windowManager.add(7, 636);\r\n    windowManager.add(8, 637);\r\n    windowManager.add(9, 638);\r\n    windowManager.add(10, 639);\r\n    // send a watermark event, which should trigger three windows.\r\n    windowManager.add(new WaterMarkEvent<>(631));\r\n    assertEquals(3, listener.allOnActivationEvents.size());\r\n    assertEquals(seq(1, 3), listener.allOnActivationEvents.get(0));\r\n    assertEquals(seq(1, 4), listener.allOnActivationEvents.get(1));\r\n    // out of order events should be processed upto the lag\r\n    assertEquals(Arrays.asList(4, 5, 6), listener.allOnActivationEvents.get(2));\r\n    // events 8, 9, 10 should not be scanned at all since TimeEvictionPolicy lag 5s should break\r\n    // the WindowManager scan loop early.\r\n    assertEquals(new HashSet<>(seq(1, 7)), eventsScanned);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\RebalanceTest.java",
  "methodName" : "testParser",
  "sourceCode" : "@Test\r\npublic void testParser() {\r\n    Rebalance.ExecutorParser executorParser = new Rebalance.ExecutorParser();\r\n    Map<String, Integer> componentParallelism = (Map<String, Integer>) executorParser.parse(\"comp1=3\");\r\n    assertEquals(3, (int) componentParallelism.get(\"comp1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\RebalanceTest.java",
  "methodName" : "testException",
  "sourceCode" : "@Test\r\npublic void testException() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Rebalance.ExecutorParser executorParser = new Rebalance.ExecutorParser();\r\n        executorParser.parse(\"comp1 3\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\SetLogLevelTest.java",
  "methodName" : "testUpdateLogLevelParser",
  "sourceCode" : "@Test\r\npublic void testUpdateLogLevelParser() {\r\n    SetLogLevel.LogLevelsParser logLevelsParser = new SetLogLevel.LogLevelsParser(LogLevelAction.UPDATE);\r\n    LogLevel logLevel = ((Map<String, LogLevel>) logLevelsParser.parse(\"com.foo.one=warn\")).get(\"com.foo.one\");\r\n    assertEquals(0, logLevel.get_reset_log_level_timeout_secs());\r\n    assertEquals(\"WARN\", logLevel.get_target_log_level());\r\n    logLevel = ((Map<String, LogLevel>) logLevelsParser.parse(\"com.foo.two=DEBUG:10\")).get(\"com.foo.two\");\r\n    assertEquals(10, logLevel.get_reset_log_level_timeout_secs());\r\n    assertEquals(\"DEBUG\", logLevel.get_target_log_level());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\SetLogLevelTest.java",
  "methodName" : "testInvalidTimeout",
  "sourceCode" : "@Test\r\npublic void testInvalidTimeout() {\r\n    SetLogLevel.LogLevelsParser logLevelsParser = new SetLogLevel.LogLevelsParser(LogLevelAction.UPDATE);\r\n    assertThrows(NumberFormatException.class, () -> logLevelsParser.parse(\"com.foo.bar=warn:NaN\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\SetLogLevelTest.java",
  "methodName" : "testInvalidLogLevel",
  "sourceCode" : "@Test\r\npublic void testInvalidLogLevel() {\r\n    SetLogLevel.LogLevelsParser logLevelsParser = new SetLogLevel.LogLevelsParser(LogLevelAction.UPDATE);\r\n    assertThrows(IllegalArgumentException.class, () -> logLevelsParser.parse(\"com.foo.bar=CRITICAL\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\TestCLI.java",
  "methodName" : "testSimple",
  "sourceCode" : "@Test\r\npublic void testSimple() throws Exception {\r\n    Map<String, Object> values = CLI.opt(\"a\", \"aa\", null).opt(\"b\", \"bb\", 1, CLI.AS_INT).opt(\"c\", \"cc\", 1, CLI.AS_INT, CLI.FIRST_WINS).opt(\"d\", \"dd\", null, CLI.AS_STRING, CLI.INTO_LIST).opt(\"e\", \"ee\", null, CLI.AS_INT).opt(\"f\", \"ff\", null, new PairParse(), CLI.INTO_MAP).arg(\"A\").arg(\"B\", CLI.AS_INT).parse(\"-a100\", \"--aa\", \"200\", \"-c2\", \"-b\", \"50\", \"--cc\", \"100\", \"A-VALUE\", \"1\", \"2\", \"3\", \"-b40\", \"-d1\", \"-d2\", \"-d3\", \"-f\", \"key1=value1\", \"-f\", \"key2=value2\");\r\n    assertEquals(8, values.size());\r\n    assertEquals(\"200\", values.get(\"a\"));\r\n    assertEquals((Integer) 40, (Integer) values.get(\"b\"));\r\n    assertEquals((Integer) 2, (Integer) values.get(\"c\"));\r\n    assertNull(values.get(\"e\"));\r\n    List<String> d = (List<String>) values.get(\"d\");\r\n    assertEquals(3, d.size());\r\n    assertEquals(\"1\", d.get(0));\r\n    assertEquals(\"2\", d.get(1));\r\n    assertEquals(\"3\", d.get(2));\r\n    List<String> A = (List<String>) values.get(\"A\");\r\n    assertEquals(1, A.size());\r\n    assertEquals(\"A-VALUE\", A.get(0));\r\n    List<Integer> B = (List<Integer>) values.get(\"B\");\r\n    assertEquals(3, B.size());\r\n    assertEquals((Integer) 1, B.get(0));\r\n    assertEquals((Integer) 2, B.get(1));\r\n    assertEquals((Integer) 3, B.get(2));\r\n    Map<String, String> f = (Map<String, String>) values.get(\"f\");\r\n    assertEquals(2, f.size());\r\n    assertEquals(\"value1\", f.get(\"key1\"));\r\n    assertEquals(\"value2\", f.get(\"key2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\TestCLI.java",
  "methodName" : "testOptional",
  "sourceCode" : "@Test\r\npublic void testOptional() throws Exception {\r\n    Map<String, Object> values = CLI.optionalArg(\"A\", CLI.LAST_WINS).parse(\"TEST\");\r\n    assertEquals(1, values.size());\r\n    assertEquals(\"TEST\", values.get(\"A\"));\r\n    values = CLI.optionalArg(\"A\", CLI.LAST_WINS).parse();\r\n    assertEquals(1, values.size());\r\n    assertNull(values.get(\"A\"));\r\n    values = CLI.optionalArg(\"A\", CLI.LAST_WINS).parse(\"THIS\", \"IS\", \"A\", \"TEST\");\r\n    assertEquals(1, values.size());\r\n    assertEquals(\"TEST\", values.get(\"A\"));\r\n    values = CLI.arg(\"A\", CLI.LAST_WINS).optionalArg(\"B\", CLI.LAST_WINS).parse(\"THIS\", \"IS\", \"A\", \"TEST\");\r\n    assertEquals(2, values.size());\r\n    assertEquals(\"THIS\", values.get(\"A\"));\r\n    assertEquals(\"TEST\", values.get(\"B\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\command\\TestCLI.java",
  "methodName" : "argAfterOptional",
  "sourceCode" : "@Test\r\npublic void argAfterOptional() {\r\n    try {\r\n        CLI.optionalArg(\"A\", CLI.LAST_WINS).arg(\"B\");\r\n        fail(\"Expected an exception to be thrown by now\");\r\n    } catch (IllegalStateException is) {\r\n        //Expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testSimulatedTime",
  "sourceCode" : "@Test\r\npublic void testSimulatedTime() throws Exception {\r\n    assertThat(Time.isSimulating(), is(false));\r\n    try (SimulatedTime time = new SimulatedTime()) {\r\n        assertThat(Time.isSimulating(), is(true));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testWithLocalCluster",
  "sourceCode" : "@Test\r\npublic void testWithLocalCluster() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSupervisors(2).withPortsPerSupervisor(5).build()) {\r\n        assertThat(cluster, notNullValue());\r\n        assertThat(cluster.getNimbus(), notNullValue());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testWithSimulatedTimeLocalCluster",
  "sourceCode" : "@Test\r\npublic void testWithSimulatedTimeLocalCluster() throws Exception {\r\n    assertThat(Time.isSimulating(), is(false));\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSupervisors(2).withPortsPerSupervisor(5).withSimulatedTime().build()) {\r\n        assertThat(cluster, notNullValue());\r\n        assertThat(cluster.getNimbus(), notNullValue());\r\n        assertThat(Time.isSimulating(), is(true));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testWithTrackedCluster",
  "sourceCode" : "@Test\r\npublic void testWithTrackedCluster() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withTracked().build()) {\r\n        AckTrackingFeeder feeder = new AckTrackingFeeder(\"num\");\r\n        Map<String, Thrift.SpoutDetails> spoutMap = new HashMap<>();\r\n        spoutMap.put(\"1\", Thrift.prepareSpoutDetails(feeder.getSpout()));\r\n        Map<String, Thrift.BoltDetails> boltMap = new HashMap<>();\r\n        boltMap.put(\"2\", Thrift.prepareBoltDetails(Collections.singletonMap(Utils.getGlobalStreamId(\"1\", null), Thrift.prepareShuffleGrouping()), new IdentityBolt()));\r\n        boltMap.put(\"3\", Thrift.prepareBoltDetails(Collections.singletonMap(Utils.getGlobalStreamId(\"1\", null), Thrift.prepareShuffleGrouping()), new IdentityBolt()));\r\n        Map<GlobalStreamId, Grouping> aggregatorInputs = new HashMap<>();\r\n        aggregatorInputs.put(Utils.getGlobalStreamId(\"2\", null), Thrift.prepareShuffleGrouping());\r\n        aggregatorInputs.put(Utils.getGlobalStreamId(\"3\", null), Thrift.prepareShuffleGrouping());\r\n        boltMap.put(\"4\", Thrift.prepareBoltDetails(aggregatorInputs, new AggBolt(4)));\r\n        TrackedTopology tracked = new TrackedTopology(Thrift.buildTopology(spoutMap, boltMap), cluster);\r\n        ;\r\n        cluster.submitTopology(\"test-acking2\", new Config(), tracked);\r\n        cluster.advanceClusterTime(11);\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder.assertNumAcks(0);\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder.assertNumAcks(2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testAdvanceClusterTime",
  "sourceCode" : "@Test\r\npublic void testAdvanceClusterTime() throws Exception {\r\n    Config daemonConf = new Config();\r\n    daemonConf.put(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, true);\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withDaemonConf(daemonConf).withSimulatedTime().build()) {\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        Map<String, Thrift.SpoutDetails> spoutMap = new HashMap<>();\r\n        spoutMap.put(\"1\", Thrift.prepareSpoutDetails(feeder));\r\n        Map<String, Thrift.BoltDetails> boltMap = new HashMap<>();\r\n        boltMap.put(\"2\", Thrift.prepareBoltDetails(Collections.singletonMap(Utils.getGlobalStreamId(\"1\", null), Thrift.prepareShuffleGrouping()), new AckEveryOtherBolt()));\r\n        StormTopology topology = Thrift.buildTopology(spoutMap, boltMap);\r\n        Config stormConf = new Config();\r\n        stormConf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 10);\r\n        cluster.submitTopology(\"timeout-tester\", stormConf, topology);\r\n        feeder.feed(new Values(\"a\"), 1);\r\n        feeder.feed(new Values(\"b\"), 2);\r\n        feeder.feed(new Values(\"c\"), 3);\r\n        cluster.advanceClusterTime(9);\r\n        assertAcked(tracker, 1, 3);\r\n        assertThat(tracker.isFailed(2), is(false));\r\n        cluster.advanceClusterTime(12);\r\n        assertFailed(tracker, 2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testDisableTupleTimeout",
  "sourceCode" : "@Test\r\npublic void testDisableTupleTimeout() throws Exception {\r\n    Config daemonConf = new Config();\r\n    daemonConf.put(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, false);\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withDaemonConf(daemonConf).withSimulatedTime().build()) {\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        Map<String, Thrift.SpoutDetails> spoutMap = new HashMap<>();\r\n        spoutMap.put(\"1\", Thrift.prepareSpoutDetails(feeder));\r\n        Map<String, Thrift.BoltDetails> boltMap = new HashMap<>();\r\n        boltMap.put(\"2\", Thrift.prepareBoltDetails(Collections.singletonMap(Utils.getGlobalStreamId(\"1\", null), Thrift.prepareShuffleGrouping()), new AckEveryOtherBolt()));\r\n        StormTopology topology = Thrift.buildTopology(spoutMap, boltMap);\r\n        Config stormConf = new Config();\r\n        stormConf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 10);\r\n        stormConf.put(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, false);\r\n        cluster.submitTopology(\"disable-timeout-tester\", stormConf, topology);\r\n        feeder.feed(new Values(\"a\"), 1);\r\n        feeder.feed(new Values(\"b\"), 2);\r\n        feeder.feed(new Values(\"c\"), 3);\r\n        cluster.advanceClusterTime(9);\r\n        assertAcked(tracker, 1, 3);\r\n        assertThat(tracker.isFailed(2), is(false));\r\n        cluster.advanceClusterTime(12);\r\n        assertThat(tracker.isFailed(2), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testTestTuple",
  "sourceCode" : "@Test\r\npublic void testTestTuple() throws Exception {\r\n    Tuple tuple = Testing.testTuple(new Values(\"james\", \"bond\"));\r\n    assertThat(tuple.getValues(), is(new Values(\"james\", \"bond\")));\r\n    assertThat(tuple.getSourceStreamId(), is(Utils.DEFAULT_STREAM_ID));\r\n    assertThat(tuple.getFields().toList(), is(Arrays.asList(\"field1\", \"field2\")));\r\n    assertThat(tuple.getSourceComponent(), is(\"component\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TestingTest.java",
  "methodName" : "testTestTupleWithMkTupleParam",
  "sourceCode" : "@Test\r\npublic void testTestTupleWithMkTupleParam() throws Exception {\r\n    MkTupleParam mkTupleParam = new MkTupleParam();\r\n    mkTupleParam.setStream(\"test-stream\");\r\n    mkTupleParam.setComponent(\"test-component\");\r\n    mkTupleParam.setFields(\"fname\", \"lname\");\r\n    Tuple tuple = Testing.testTuple(new Values(\"james\", \"bond\"), mkTupleParam);\r\n    assertThat(tuple.getValues(), is(new Values(\"james\", \"bond\")));\r\n    assertThat(tuple.getSourceStreamId(), is(\"test-stream\"));\r\n    assertThat(tuple.getFields().toList(), is(Arrays.asList(\"fname\", \"lname\")));\r\n    assertThat(tuple.getSourceComponent(), is(\"test-component\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testBasicTopology",
  "sourceCode" : "@ParameterizedTest\r\n@ValueSource(strings = { \"true\", \"false\" })\r\npublic void testBasicTopology(boolean useLocalMessaging) throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withSupervisors(4).withDaemonConf(Collections.singletonMap(Config.STORM_LOCAL_MODE_ZMQ, !useLocalMessaging)).build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestWordSpout(true), 3);\r\n        builder.setBolt(\"2\", new TestWordCounter(), 4).fieldsGrouping(\"1\", new Fields(\"word\"));\r\n        builder.setBolt(\"3\", new TestGlobalCount()).globalGrouping(\"1\");\r\n        builder.setBolt(\"4\", new TestAggregatesCounter()).globalGrouping(\"2\");\r\n        StormTopology topology = builder.createTopology();\r\n        Map<String, Object> stormConf = new HashMap<>();\r\n        stormConf.put(Config.TOPOLOGY_WORKERS, 2);\r\n        stormConf.put(Config.TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE, true);\r\n        List<FixedTuple> testTuples = Stream.of(\"nathan\", \"bob\", \"joey\", \"nathan\").map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        completeTopologyParams.setStormConf(stormConf);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        assertThat(Testing.readTuples(results, \"1\"), containsInAnyOrder(new Values(\"nathan\"), new Values(\"nathan\"), new Values(\"bob\"), new Values(\"joey\")));\r\n        assertThat(Testing.readTuples(results, \"2\"), containsInAnyOrder(new Values(\"nathan\", 1), new Values(\"nathan\", 2), new Values(\"bob\", 1), new Values(\"joey\", 1)));\r\n        assertThat(Testing.readTuples(results, \"3\"), contains(new Values(1), new Values(2), new Values(3), new Values(4)));\r\n        assertThat(Testing.readTuples(results, \"4\"), contains(new Values(1), new Values(2), new Values(3), new Values(4)));\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testMultiTasksPerCluster",
  "sourceCode" : "@Test\r\npublic void testMultiTasksPerCluster() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withSupervisors(4).build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestWordSpout(true));\r\n        builder.setBolt(\"2\", new EmitTaskIdBolt(), 3).allGrouping(\"1\").addConfigurations(Collections.singletonMap(Config.TOPOLOGY_TASKS, 6));\r\n        StormTopology topology = builder.createTopology();\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", Collections.singletonList(new FixedTuple(new Values(\"a\")))));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        assertThat(Testing.readTuples(results, \"2\"), containsInAnyOrder(new Values(0), new Values(1), new Values(2), new Values(3), new Values(4), new Values(5)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testTimeout",
  "sourceCode" : "@Test\r\npublic void testTimeout() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withSupervisors(4).withDaemonConf(Collections.singletonMap(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, true)).build()) {\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder);\r\n        builder.setBolt(\"2\", new AckEveryOtherBolt()).globalGrouping(\"1\");\r\n        StormTopology topology = builder.createTopology();\r\n        cluster.submitTopology(\"timeout-tester\", Collections.singletonMap(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 10), topology);\r\n        cluster.advanceClusterTime(11);\r\n        feeder.feed(new Values(\"a\"), 1);\r\n        feeder.feed(new Values(\"b\"), 2);\r\n        feeder.feed(new Values(\"c\"), 3);\r\n        cluster.advanceClusterTime(9);\r\n        assertAcked(tracker, 1, 3);\r\n        assertThat(tracker.isFailed(2), is(false));\r\n        cluster.advanceClusterTime(12);\r\n        assertFailed(tracker, 2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testResetTimeout",
  "sourceCode" : "@Test\r\npublic void testResetTimeout() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withDaemonConf(Collections.singletonMap(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, true)).build()) {\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder);\r\n        builder.setBolt(\"2\", new ResetTimeoutBolt()).globalGrouping(\"1\");\r\n        StormTopology topology = builder.createTopology();\r\n        cluster.submitTopology(\"reset-timeout-tester\", Collections.singletonMap(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 10), topology);\r\n        //The first tuple wil be used to check timeout reset\r\n        feeder.feed(new Values(\"a\"), 1);\r\n        //The second tuple is used to wait for the spout to rotate its pending map\r\n        feeder.feed(new Values(\"b\"), 2);\r\n        cluster.advanceClusterTime(9);\r\n        //The other tuples are used to reset the first tuple's timeout,\r\n        //and to wait for the message to get through to the spout (acks use the same path as timeout resets)\r\n        feeder.feed(new Values(\"c\"), 3);\r\n        assertAcked(tracker, 3);\r\n        cluster.advanceClusterTime(9);\r\n        feeder.feed(new Values(\"d\"), 4);\r\n        assertAcked(tracker, 4);\r\n        cluster.advanceClusterTime(2);\r\n        //The time is now twice the message timeout, the second tuple should expire since it was not acked\r\n        //Waiting for this also ensures that the first tuple gets failed if reset-timeout doesn't work\r\n        assertFailed(tracker, 2);\r\n        //Put in a tuple to cause the first tuple to be acked\r\n        feeder.feed(new Values(\"e\"), 5);\r\n        assertAcked(tracker, 5);\r\n        //The first tuple should be acked, and should not have failed\r\n        assertThat(tracker.isFailed(1), is(false));\r\n        assertAcked(tracker, 1);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testValidateTopologystructure",
  "sourceCode" : "@Test\r\npublic void testValidateTopologystructure() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withDaemonConf(Collections.singletonMap(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, true)).build()) {\r\n        assertThat(tryCompleteWordCountTopology(cluster, mkValidateTopology()), is(false));\r\n        assertThat(tryCompleteWordCountTopology(cluster, mkInvalidateTopology1()), is(true));\r\n        assertThat(tryCompleteWordCountTopology(cluster, mkInvalidateTopology2()), is(true));\r\n        assertThat(tryCompleteWordCountTopology(cluster, mkInvalidateTopology3()), is(true));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testSystemStream",
  "sourceCode" : "@Test\r\npublic void testSystemStream() throws Exception {\r\n    //this test works because mocking a spout splits up the tuples evenly among the tasks\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestWordSpout(true), 3);\r\n        builder.setBolt(\"2\", new IdentityBolt(), 1).fieldsGrouping(\"1\", new Fields(\"word\")).globalGrouping(\"1\", \"__system\");\r\n        StormTopology topology = builder.createTopology();\r\n        Map<String, Object> stormConf = new HashMap<>();\r\n        stormConf.put(Config.TOPOLOGY_WORKERS, 2);\r\n        List<FixedTuple> testTuples = Stream.of(\"a\", \"b\", \"c\").map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        completeTopologyParams.setStormConf(stormConf);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        assertThat(Testing.readTuples(results, \"2\"), containsInAnyOrder(new Values(\"a\"), new Values(\"b\"), new Values(\"c\")));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testAcking",
  "sourceCode" : "@Test\r\npublic void testAcking() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withTracked().build()) {\r\n        AckTrackingFeeder feeder1 = new AckTrackingFeeder(\"num\");\r\n        AckTrackingFeeder feeder2 = new AckTrackingFeeder(\"num\");\r\n        AckTrackingFeeder feeder3 = new AckTrackingFeeder(\"num\");\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder1.getSpout());\r\n        builder.setSpout(\"2\", feeder2.getSpout());\r\n        builder.setSpout(\"3\", feeder3.getSpout());\r\n        builder.setBolt(\"4\", new BranchingBolt(2)).shuffleGrouping(\"1\");\r\n        builder.setBolt(\"5\", new BranchingBolt(4)).shuffleGrouping(\"2\");\r\n        builder.setBolt(\"6\", new BranchingBolt(1)).shuffleGrouping(\"3\");\r\n        builder.setBolt(\"7\", new AggBolt(3)).shuffleGrouping(\"4\").shuffleGrouping(\"5\").shuffleGrouping(\"6\");\r\n        builder.setBolt(\"8\", new BranchingBolt(2)).shuffleGrouping(\"7\");\r\n        builder.setBolt(\"9\", new AckBolt()).shuffleGrouping(\"8\");\r\n        TrackedTopology tracked = new TrackedTopology(builder.createTopology(), cluster);\r\n        cluster.submitTopology(\"acking-test1\", Collections.emptyMap(), tracked);\r\n        cluster.advanceClusterTime(11);\r\n        feeder1.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.assertNumAcks(0);\r\n        feeder2.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.assertNumAcks(1);\r\n        feeder2.assertNumAcks(1);\r\n        feeder1.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.assertNumAcks(0);\r\n        feeder1.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.assertNumAcks(1);\r\n        feeder3.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.assertNumAcks(0);\r\n        feeder3.assertNumAcks(0);\r\n        feeder2.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder1.feed(new Values(1));\r\n        feeder2.feed(new Values(1));\r\n        feeder3.feed(new Values(1));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testAckBranching",
  "sourceCode" : "@Test\r\npublic void testAckBranching() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withTracked().build()) {\r\n        AckTrackingFeeder feeder = new AckTrackingFeeder(\"num\");\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder.getSpout());\r\n        builder.setBolt(\"2\", new IdentityBolt()).shuffleGrouping(\"1\");\r\n        builder.setBolt(\"3\", new IdentityBolt()).shuffleGrouping(\"1\");\r\n        builder.setBolt(\"4\", new AggBolt(4)).shuffleGrouping(\"2\").shuffleGrouping(\"3\");\r\n        TrackedTopology tracked = new TrackedTopology(builder.createTopology(), cluster);\r\n        cluster.submitTopology(\"test-acking2\", Collections.emptyMap(), tracked);\r\n        cluster.advanceClusterTime(11);\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder.assertNumAcks(0);\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder.assertNumAcks(2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testSubmitInactiveTopology",
  "sourceCode" : "@Test\r\npublic void testSubmitInactiveTopology() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withDaemonConf(Collections.singletonMap(Config.TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS, true)).build()) {\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder);\r\n        builder.setSpout(\"2\", new OpenTrackedSpout());\r\n        builder.setBolt(\"3\", new PrepareTrackedBolt()).globalGrouping(\"1\");\r\n        boltPrepared = false;\r\n        spoutOpened = false;\r\n        StormTopology topology = builder.createTopology();\r\n        cluster.submitTopologyWithOpts(\"test\", Collections.singletonMap(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 10), topology, new SubmitOptions(TopologyInitialStatus.INACTIVE));\r\n        cluster.advanceClusterTime(11);\r\n        feeder.feed(new Values(\"a\"), 1);\r\n        cluster.advanceClusterTime(9);\r\n        assertThat(boltPrepared, is(false));\r\n        assertThat(spoutOpened, is(false));\r\n        cluster.getNimbus().activate(\"test\");\r\n        cluster.advanceClusterTime(12);\r\n        assertAcked(tracker, 1);\r\n        assertThat(boltPrepared, is(true));\r\n        assertThat(spoutOpened, is(true));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testAckingSelfAnchor",
  "sourceCode" : "@Test\r\npublic void testAckingSelfAnchor() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withTracked().build()) {\r\n        AckTrackingFeeder feeder = new AckTrackingFeeder(\"num\");\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", feeder.getSpout());\r\n        builder.setBolt(\"2\", new DupAnchorBolt()).shuffleGrouping(\"1\");\r\n        builder.setBolt(\"3\", new AckBolt()).shuffleGrouping(\"2\");\r\n        TrackedTopology tracked = new TrackedTopology(builder.createTopology(), cluster);\r\n        cluster.submitTopology(\"test\", Collections.emptyMap(), tracked);\r\n        cluster.advanceClusterTime(11);\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 1);\r\n        feeder.assertNumAcks(1);\r\n        feeder.feed(new Values(1));\r\n        feeder.feed(new Values(1));\r\n        feeder.feed(new Values(1));\r\n        Testing.trackedWait(tracked, 3);\r\n        feeder.assertNumAcks(3);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testKryoDecoratorsConfig",
  "sourceCode" : "@Test\r\npublic void testKryoDecoratorsConfig() throws Exception {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    daemonConf.put(Config.TOPOLOGY_SKIP_MISSING_KRYO_REGISTRATIONS, true);\r\n    daemonConf.put(Config.TOPOLOGY_KRYO_DECORATORS, \"this-is-overridden\");\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withDaemonConf(daemonConf).build()) {\r\n        TopologyBuilder topologyBuilder = new TopologyBuilder();\r\n        topologyBuilder.setSpout(\"1\", new TestPlannerSpout(new Fields(\"conf\")));\r\n        topologyBuilder.setBolt(\"2\", new TestConfBolt(Collections.singletonMap(Config.TOPOLOGY_KRYO_DECORATORS, Arrays.asList(\"one\", \"two\")))).shuffleGrouping(\"1\");\r\n        List<FixedTuple> testTuples = Collections.singletonList(new Values(Config.TOPOLOGY_KRYO_DECORATORS)).stream().map(FixedTuple::new).collect(Collectors.toList());\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        completeTopologyParams.setStormConf(Collections.singletonMap(Config.TOPOLOGY_KRYO_DECORATORS, Arrays.asList(\"one\", \"three\")));\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topologyBuilder.createTopology(), completeTopologyParams);\r\n        List<Object> concatValues = Testing.readTuples(results, \"2\").stream().flatMap(Collection::stream).collect(Collectors.toList());\r\n        assertThat(concatValues.get(0), is(Config.TOPOLOGY_KRYO_DECORATORS));\r\n        assertThat(concatValues.get(1), is(Arrays.asList(\"one\", \"two\", \"three\")));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testComponentSpecificConfig",
  "sourceCode" : "@Test\r\npublic void testComponentSpecificConfig() throws Exception {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    daemonConf.put(Config.TOPOLOGY_SKIP_MISSING_KRYO_REGISTRATIONS, true);\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withDaemonConf(daemonConf).build()) {\r\n        TopologyBuilder topologyBuilder = new TopologyBuilder();\r\n        topologyBuilder.setSpout(\"1\", new TestPlannerSpout(new Fields(\"conf\")));\r\n        Map<String, Object> componentConf = new HashMap<>();\r\n        componentConf.put(\"fake.config\", 123);\r\n        componentConf.put(Config.TOPOLOGY_MAX_TASK_PARALLELISM, 20);\r\n        componentConf.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 30);\r\n        componentConf.put(Config.TOPOLOGY_KRYO_REGISTER, Arrays.asList(Collections.singletonMap(\"fake.type\", \"bad.serializer\"), Collections.singletonMap(\"fake.type2\", \"a.serializer\")));\r\n        topologyBuilder.setBolt(\"2\", new TestConfBolt(componentConf)).shuffleGrouping(\"1\").setMaxTaskParallelism(2).addConfiguration(\"fake.config2\", 987);\r\n        List<FixedTuple> testTuples = Stream.of(\"fake.config\", Config.TOPOLOGY_MAX_TASK_PARALLELISM, Config.TOPOLOGY_MAX_SPOUT_PENDING, \"fake.config2\", Config.TOPOLOGY_KRYO_REGISTER).map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        Map<String, String> kryoRegister = new HashMap<>();\r\n        kryoRegister.put(\"fake.type\", \"good.serializer\");\r\n        kryoRegister.put(\"fake.type3\", \"a.serializer3\");\r\n        Map<String, Object> stormConf = new HashMap<>();\r\n        stormConf.put(Config.TOPOLOGY_KRYO_REGISTER, Collections.singletonList(kryoRegister));\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        completeTopologyParams.setStormConf(stormConf);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topologyBuilder.createTopology(), completeTopologyParams);\r\n        Map<String, Object> expectedValues = new HashMap<>();\r\n        expectedValues.put(\"fake.config\", 123L);\r\n        expectedValues.put(\"fake.config2\", 987L);\r\n        expectedValues.put(Config.TOPOLOGY_MAX_TASK_PARALLELISM, 2L);\r\n        expectedValues.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 30L);\r\n        Map<String, String> expectedKryoRegister = new HashMap<>(kryoRegister);\r\n        expectedKryoRegister.put(\"fake.type2\", \"a.serializer\");\r\n        expectedValues.put(Config.TOPOLOGY_KRYO_REGISTER, expectedKryoRegister);\r\n        List<Object> concatValues = Testing.readTuples(results, \"2\").stream().flatMap(Collection::stream).collect(Collectors.toList());\r\n        assertThat(listToMap(concatValues), is(expectedValues));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testHooks",
  "sourceCode" : "@Test\r\npublic void testHooks() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestPlannerSpout(new Fields(\"conf\")));\r\n        builder.setBolt(\"2\", new HooksBolt()).shuffleGrouping(\"1\");\r\n        StormTopology topology = builder.createTopology();\r\n        List<FixedTuple> testTuples = Stream.of(1, 1, 1, 1).map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        List<List<Object>> expectedTuples = Arrays.asList(Arrays.asList(0, 0, 0, 0), Arrays.asList(2, 1, 0, 1), Arrays.asList(4, 1, 1, 2), Arrays.asList(6, 2, 1, 3));\r\n        assertThat(Testing.readTuples(results, \"2\"), is(expectedTuples));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\integration\\TopologyIntegrationTest.java",
  "methodName" : "testUserResourcesAreVisibleToTasks",
  "sourceCode" : "@Test\r\npublic void testUserResourcesAreVisibleToTasks() throws Exception {\r\n    try (LocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().build()) {\r\n        Map<String, String> resourceMap = new HashMap<>(1);\r\n        resourceMap.put(\"resource-key1\", \"resource-value1\");\r\n        List<FixedTuple> testTuples = resourceMap.keySet().stream().map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.addWorkerHook(new ResourceInitializingWorkerHook(resourceMap));\r\n        builder.setSpout(\"1\", new FixedTupleSpout(testTuples, new Fields(\"key\")));\r\n        builder.setBolt(\"2\", new ResourceForwardingBolt()).shuffleGrouping(\"1\");\r\n        StormTopology topology = builder.createTopology();\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        List<Object> expectedTuple = Arrays.asList(\"resource-key1\", new TestUserResource(\"resource-key1\", \"resource-value1\"));\r\n        assertThat(Testing.readTuples(results, \"2\"), hasItem(expectedTuple));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() throws Exception {\r\n    doTestBasic(basicConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testBasicWithSasl",
  "sourceCode" : "@Test\r\npublic void testBasicWithSasl() throws Exception {\r\n    doTestBasic(withSaslConf(basicConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testLoad",
  "sourceCode" : "@Test\r\npublic void testLoad() throws Exception {\r\n    doTestLoad(basicConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testLoadWithSasl",
  "sourceCode" : "@Test\r\npublic void testLoadWithSasl() throws Exception {\r\n    doTestLoad(withSaslConf(basicConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testLargeMessage",
  "sourceCode" : "@Test\r\npublic void testLargeMessage() throws Exception {\r\n    doTestLargeMessage(largeMessageConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testLargeMessageWithSasl",
  "sourceCode" : "@Test\r\npublic void testLargeMessageWithSasl() throws Exception {\r\n    doTestLargeMessage(withSaslConf(largeMessageConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testServerDelayed",
  "sourceCode" : "@Test\r\npublic void testServerDelayed() throws Exception {\r\n    doTestServerDelayed(basicConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testServerDelayedWithSasl",
  "sourceCode" : "@Test\r\npublic void testServerDelayedWithSasl() throws Exception {\r\n    doTestServerDelayed(withSaslConf(basicConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testBatch",
  "sourceCode" : "@Test\r\npublic void testBatch() throws Exception {\r\n    doTestBatch(batchConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testBatchWithSasl",
  "sourceCode" : "@Test\r\npublic void testBatchWithSasl() throws Exception {\r\n    doTestBatch(withSaslConf(batchConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testServerAlwaysReconnects",
  "sourceCode" : "@Test\r\npublic void testServerAlwaysReconnects() throws Exception {\r\n    doTestServerAlwaysReconnects(basicConf());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testServerAlwaysReconnectsWithSasl",
  "sourceCode" : "@Test\r\npublic void testServerAlwaysReconnectsWithSasl() throws Exception {\r\n    doTestServerAlwaysReconnects(withSaslConf(basicConf()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\netty\\NettyTest.java",
  "methodName" : "testRebind",
  "sourceCode" : "@Test\r\npublic void testRebind() throws Exception {\r\n    for (int i = 0; i < 10; ++i) {\r\n        final long startTime = System.nanoTime();\r\n        LOG.info(\"Binding to port 6700 iter: \" + (i + 1));\r\n        connectToFixedPort(basicConf(), 6700);\r\n        final long endTime = System.nanoTime();\r\n        LOG.info(\"Expected time taken should be less than 5 sec, actual time is: \" + (endTime - startTime) / 1_000_000 + \" ms\");\r\n        assertThat((endTime - startTime) / 1_000_000, lessThan(5_000L));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\messaging\\NettyIntegrationTest.java",
  "methodName" : "testIntegration",
  "sourceCode" : "@Test\r\npublic void testIntegration() throws Exception {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    daemonConf.put(Config.STORM_LOCAL_MODE_ZMQ, true);\r\n    daemonConf.put(Config.STORM_MESSAGING_TRANSPORT, \"org.apache.storm.messaging.netty.Context\");\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_AUTHENTICATION, false);\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_BUFFER_SIZE, 1024000);\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_MIN_SLEEP_MS, 1000);\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_MAX_SLEEP_MS, 5000);\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_CLIENT_WORKER_THREADS, 1);\r\n    daemonConf.put(Config.STORM_MESSAGING_NETTY_SERVER_WORKER_THREADS, 1);\r\n    Builder builder = new Builder().withSimulatedTime().withSupervisors(4).withSupervisorSlotPortMin(6710).withDaemonConf(daemonConf);\r\n    try (LocalCluster cluster = builder.build()) {\r\n        TopologyBuilder topologyBuilder = new TopologyBuilder();\r\n        topologyBuilder.setSpout(\"1\", new TestWordSpout(true), 4);\r\n        topologyBuilder.setBolt(\"2\", new TestGlobalCount(), 6).shuffleGrouping(\"1\");\r\n        StormTopology topology = topologyBuilder.createTopology();\r\n        // important for test that tuples = multiple of 4 and 6\r\n        List<FixedTuple> testTuples = Stream.of(\"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\", \"a\", \"b\").map(value -> new FixedTuple(new Values(value))).collect(Collectors.toList());\r\n        MockedSources mockedSources = new MockedSources(Collections.singletonMap(\"1\", testTuples));\r\n        CompleteTopologyParam completeTopologyParams = new CompleteTopologyParam();\r\n        completeTopologyParams.setStormConf(Collections.singletonMap(Config.TOPOLOGY_WORKERS, 3));\r\n        completeTopologyParams.setMockedSources(mockedSources);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, topology, completeTopologyParams);\r\n        List<List<Object>> tuplesRead = Testing.readTuples(results, \"2\");\r\n        String errMsg = \"Tuples Read:\\n\\t\" + String.join(\"\\n\\t\", tuplesRead.stream().map(Object::toString).collect(Collectors.toList())) + \"\\nTuples Expected:\\n\\t\" + String.join(\"\\n\\t\", testTuples.stream().map(FixedTuple::toString).collect(Collectors.toList()));\r\n        assertEquals(6 * 4, tuplesRead.size(), errMsg);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationTest.java",
  "methodName" : "testJavaSerialization",
  "sourceCode" : "@Test\r\npublic void testJavaSerialization() throws IOException {\r\n    Object obj = new TestSerObject(1, 2);\r\n    List<Object> vals = Lists.newArrayList(obj);\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_KRYO_REGISTER, new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"org.apache.storm.testing.TestSerObject\", null);\r\n        }\r\n    });\r\n    conf.put(Config.TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION, false);\r\n    assertThrows(Exception.class, () -> roundtrip(vals, conf));\r\n    conf.clear();\r\n    conf.put(Config.TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION, true);\r\n    assertEquals(vals, roundtrip(vals, conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationTest.java",
  "methodName" : "testKryoDecorator",
  "sourceCode" : "@Test\r\npublic void testKryoDecorator() throws IOException {\r\n    Object obj = new TestSerObject(1, 2);\r\n    List<Object> vals = Lists.newArrayList(obj);\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.TOPOLOGY_FALL_BACK_ON_JAVA_SERIALIZATION, false);\r\n    assertThrows(Exception.class, () -> roundtrip(vals, conf), \"Expected Exception not Thrown for config: \" + conf);\r\n    conf.put(Config.TOPOLOGY_KRYO_DECORATORS, Lists.newArrayList(\"org.apache.storm.testing.TestKryoDecorator\"));\r\n    assertEquals(vals, roundtrip(vals, conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\serialization\\SerializationTest.java",
  "methodName" : "testStringSerialization",
  "sourceCode" : "@Test\r\npublic void testStringSerialization() throws IOException {\r\n    isRoundtrip(Lists.newArrayList(\"a\", \"bb\", \"cbe\"));\r\n    isRoundtrip(Lists.newArrayList(mkString(64 * 1024)));\r\n    isRoundtrip(Lists.newArrayList(mkString(1024 * 1024)));\r\n    isRoundtrip(Lists.newArrayList(mkString(1024 * 1024 * 2)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStats",
  "sourceCode" : "@Test\r\npublic void aggWorkerStats() {\r\n    makeTopoInfo();\r\n    List<WorkerSummary> summaries = checkWorkerStats(true, /*include sys*/\r\n    true, /*user authorized*/\r\n    null);\r\n    WorkerSummary ws = getWorkerSummaryForPort(summaries, 1);\r\n    assertEquals(1, ws.get_component_to_num_tasks().size());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"my-component\").intValue());\r\n    assertEquals(1, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStatsWithSystemComponents",
  "sourceCode" : "@Test\r\npublic void aggWorkerStatsWithSystemComponents() {\r\n    makeTopoInfoWithSysWorker();\r\n    List<WorkerSummary> summaries = checkWorkerStats(true, /*include sys*/\r\n    true, /*user authorized*/\r\n    null);\r\n    WorkerSummary ws = getWorkerSummaryForPort(summaries, 2);\r\n    // since we made sys components visible, the component map has all system components\r\n    assertEquals(3, ws.get_component_to_num_tasks().size());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"__sys1\").intValue());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"__sys2\").intValue());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"__sys3\").intValue());\r\n    assertEquals(2, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStatsWithHiddenSystemComponents",
  "sourceCode" : "@Test\r\npublic void aggWorkerStatsWithHiddenSystemComponents() {\r\n    makeTopoInfoWithSysWorker();\r\n    List<WorkerSummary> summaries = checkWorkerStats(false, /*DON'T include sys*/\r\n    true, /*user authorized*/\r\n    null);\r\n    WorkerSummary ws1 = getWorkerSummaryForPort(summaries, 1);\r\n    WorkerSummary ws2 = getWorkerSummaryForPort(summaries, 2);\r\n    assertEquals(1, ws1.get_component_to_num_tasks().size());\r\n    // since we made sys components hidden, the component map is empty for this worker\r\n    assertEquals(0, ws2.get_component_to_num_tasks().size());\r\n    assertEquals(2, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStatsForUnauthorizedUser",
  "sourceCode" : "@Test\r\npublic void aggWorkerStatsForUnauthorizedUser() {\r\n    makeTopoInfoWithSysWorker();\r\n    List<WorkerSummary> summaries = checkWorkerStats(true, /*include sys (should not matter)*/\r\n    false, /*user NOT authorized*/\r\n    null);\r\n    WorkerSummary ws1 = getWorkerSummaryForPort(summaries, 1);\r\n    WorkerSummary ws2 = getWorkerSummaryForPort(summaries, 2);\r\n    // since we made user not authorized, component map is empty\r\n    assertEquals(0, ws1.get_component_to_num_tasks().size());\r\n    assertEquals(0, ws2.get_component_to_num_tasks().size());\r\n    assertEquals(2, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStatsFilterSupervisor",
  "sourceCode" : "@Test\r\npublic void aggWorkerStatsFilterSupervisor() {\r\n    makeTopoInfoWithMissingBeats();\r\n    List<WorkerSummary> summaries = checkWorkerStats(true, /*include sys*/\r\n    true, /*user authorized*/\r\n    \"node3\");\r\n    WorkerSummary ws = getWorkerSummaryForPort(summaries, 3);\r\n    // only host3 should be returned given filter\r\n    assertEquals(2, ws.get_component_to_num_tasks().size());\r\n    assertEquals(2, ws.get_component_to_num_tasks().get(\"__sys4\").intValue());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"my-component2\").intValue());\r\n    assertEquals(1, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggWorkerStatsFilterSupervisorAndHideSystemComponents",
  "sourceCode" : "@Test\r\npublic void aggWorkerStatsFilterSupervisorAndHideSystemComponents() {\r\n    makeTopoInfoWithMissingBeats();\r\n    List<WorkerSummary> summaries = checkWorkerStats(false, /*DON'T include sys*/\r\n    true, /*user authorized*/\r\n    \"node3\");\r\n    WorkerSummary ws = getWorkerSummaryForPort(summaries, 3);\r\n    // hidden sys component\r\n    assertEquals(1, ws.get_component_to_num_tasks().size());\r\n    assertEquals(1, ws.get_component_to_num_tasks().get(\"my-component2\").intValue());\r\n    assertEquals(1, summaries.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggTopoExecsStats_boltAndSpoutsHaveLastErrorReported",
  "sourceCode" : "/**\r\n * Targeted validation against StatsUtil.aggTopoExecsStats()\r\n * to verify that when a bolt or spout has an error reported,\r\n * it is included in the returned TopologyPageInfo result.\r\n */\r\n@Test\r\npublic void aggTopoExecsStats_boltAndSpoutsHaveLastErrorReported() {\r\n    // Define inputs\r\n    final String expectedBoltErrorMsg = \"This is my test bolt error message\";\r\n    final int expectedBoltErrorTime = (int) TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());\r\n    final int expectedBoltErrorPort = 4321;\r\n    final String expectedBoltErrorHost = \"my.errored.host\";\r\n    final String expectedSpoutErrorMsg = \"This is my test spout error message\";\r\n    final int expectedSpoutErrorTime = (int) TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());\r\n    final int expectedSpoutErrorPort = 1234;\r\n    final String expectedSpoutErrorHost = \"my.errored.host2\";\r\n    // Define our Last Error for the bolt\r\n    final ErrorInfo expectedBoltLastError = new ErrorInfo(expectedBoltErrorMsg, expectedBoltErrorTime);\r\n    expectedBoltLastError.set_port(expectedBoltErrorPort);\r\n    expectedBoltLastError.set_host(expectedBoltErrorHost);\r\n    // Define our Last Error for the spout\r\n    final ErrorInfo expectedSpoutLastError = new ErrorInfo(expectedSpoutErrorMsg, expectedSpoutErrorTime);\r\n    expectedSpoutLastError.set_port(expectedSpoutErrorPort);\r\n    expectedSpoutLastError.set_host(expectedSpoutErrorHost);\r\n    // Create mock StormClusterState\r\n    final IStormClusterState mockStormClusterState = mock(IStormClusterState.class);\r\n    when(mockStormClusterState.lastError(eq(\"my-storm-id\"), eq(\"my-component\"))).thenReturn(expectedBoltLastError);\r\n    when(mockStormClusterState.lastError(eq(\"my-storm-id\"), eq(\"my-spout\"))).thenReturn(expectedSpoutLastError);\r\n    // Setup inputs.\r\n    makeTopoInfoWithSpout();\r\n    // Call method under test.\r\n    final TopologyPageInfo topologyPageInfo = StatsUtil.aggTopoExecsStats(\"my-storm-id\", exec2NodePort, task2Component, beats, null, \":all-time\", false, mockStormClusterState);\r\n    // Validate result\r\n    assertNotNull(topologyPageInfo, \"Should never be null\");\r\n    assertEquals(\"my-storm-id\", topologyPageInfo.get_id());\r\n    assertEquals(8, topologyPageInfo.get_num_tasks(), \"Should have 7 tasks.\");\r\n    assertEquals(2, topologyPageInfo.get_num_workers(), \"Should have 2 workers.\");\r\n    assertEquals(2, topologyPageInfo.get_num_executors(), \"Should have only a single executor.\");\r\n    // Validate Spout aggregate statistics\r\n    assertNotNull(topologyPageInfo.get_id_to_spout_agg_stats(), \"Should be non-null\");\r\n    assertEquals(1, topologyPageInfo.get_id_to_spout_agg_stats().size());\r\n    assertEquals(1, topologyPageInfo.get_id_to_spout_agg_stats_size());\r\n    assertTrue(topologyPageInfo.get_id_to_spout_agg_stats().containsKey(\"my-spout\"));\r\n    assertNotNull(topologyPageInfo.get_id_to_spout_agg_stats().get(\"my-spout\"));\r\n    ComponentAggregateStats componentStats = topologyPageInfo.get_id_to_spout_agg_stats().get(\"my-spout\");\r\n    assertEquals(ComponentType.SPOUT, componentStats.get_type(), \"Should be of type spout\");\r\n    assertNotNull(componentStats.get_last_error(), \"Last error should not be null\");\r\n    ErrorInfo lastError = componentStats.get_last_error();\r\n    assertEquals(expectedSpoutErrorMsg, lastError.get_error());\r\n    assertEquals(expectedSpoutErrorHost, lastError.get_host());\r\n    assertEquals(expectedSpoutErrorPort, lastError.get_port());\r\n    assertEquals(expectedSpoutErrorTime, lastError.get_error_time_secs());\r\n    // Validate Bolt aggregate statistics\r\n    assertNotNull(topologyPageInfo.get_id_to_bolt_agg_stats(), \"Should be non-null\");\r\n    assertEquals(1, topologyPageInfo.get_id_to_bolt_agg_stats().size());\r\n    assertEquals(1, topologyPageInfo.get_id_to_bolt_agg_stats_size());\r\n    assertTrue(topologyPageInfo.get_id_to_bolt_agg_stats().containsKey(\"my-component\"));\r\n    assertNotNull(topologyPageInfo.get_id_to_bolt_agg_stats().get(\"my-component\"));\r\n    componentStats = topologyPageInfo.get_id_to_bolt_agg_stats().get(\"my-component\");\r\n    assertEquals(ComponentType.BOLT, componentStats.get_type(), \"Should be of type bolt\");\r\n    assertNotNull(componentStats.get_last_error(), \"Last error should not be null\");\r\n    lastError = componentStats.get_last_error();\r\n    assertEquals(expectedBoltErrorMsg, lastError.get_error());\r\n    assertEquals(expectedBoltErrorHost, lastError.get_host());\r\n    assertEquals(expectedBoltErrorPort, lastError.get_port());\r\n    assertEquals(expectedBoltErrorTime, lastError.get_error_time_secs());\r\n    // Verify mock interactions\r\n    verify(mockStormClusterState, times(1)).lastError(eq(\"my-storm-id\"), eq(\"my-component\"));\r\n    verify(mockStormClusterState, times(1)).lastError(eq(\"my-storm-id\"), eq(\"my-spout\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\stats\\TestStatsUtil.java",
  "methodName" : "aggTopoExecsStats_boltAndSpoutsHaveNoLastErrorReported",
  "sourceCode" : "/**\r\n * Targeted validation against StatsUtil.aggTopoExecsStats()\r\n * to verify that when a bolt and spout does NOT have an error reported,\r\n * it gracefully handles not having a value.\r\n */\r\n@Test\r\npublic void aggTopoExecsStats_boltAndSpoutsHaveNoLastErrorReported() {\r\n    // Create mock StormClusterState\r\n    final IStormClusterState mockStormClusterState = mock(IStormClusterState.class);\r\n    when(mockStormClusterState.lastError(eq(\"my-storm-id\"), eq(\"my-component\"))).thenReturn(null);\r\n    when(mockStormClusterState.lastError(eq(\"my-storm-id\"), eq(\"my-spout\"))).thenReturn(null);\r\n    // Setup inputs.\r\n    makeTopoInfoWithSpout();\r\n    // Call method under test.\r\n    final TopologyPageInfo topologyPageInfo = StatsUtil.aggTopoExecsStats(\"my-storm-id\", exec2NodePort, task2Component, beats, null, \":all-time\", false, mockStormClusterState);\r\n    // Validate result\r\n    assertNotNull(topologyPageInfo, \"Should never be null\");\r\n    assertEquals(\"my-storm-id\", topologyPageInfo.get_id());\r\n    assertEquals(8, topologyPageInfo.get_num_tasks(), \"Should have 7 tasks.\");\r\n    assertEquals(2, topologyPageInfo.get_num_workers(), \"Should have 2 workers.\");\r\n    assertEquals(2, topologyPageInfo.get_num_executors(), \"Should have only a single executor.\");\r\n    // Validate Spout aggregate statistics\r\n    assertNotNull(topologyPageInfo.get_id_to_spout_agg_stats(), \"Should be non-null\");\r\n    assertEquals(1, topologyPageInfo.get_id_to_spout_agg_stats().size());\r\n    assertEquals(1, topologyPageInfo.get_id_to_spout_agg_stats_size());\r\n    assertTrue(topologyPageInfo.get_id_to_spout_agg_stats().containsKey(\"my-spout\"));\r\n    assertNotNull(topologyPageInfo.get_id_to_spout_agg_stats().get(\"my-spout\"));\r\n    ComponentAggregateStats componentStats = topologyPageInfo.get_id_to_spout_agg_stats().get(\"my-spout\");\r\n    assertEquals(ComponentType.SPOUT, componentStats.get_type(), \"Should be of type spout\");\r\n    assertNull(componentStats.get_last_error(), \"Last error should not be null\");\r\n    // Validate Bolt aggregate statistics\r\n    assertNotNull(topologyPageInfo.get_id_to_bolt_agg_stats(), \"Should be non-null\");\r\n    assertEquals(1, topologyPageInfo.get_id_to_bolt_agg_stats().size());\r\n    assertEquals(1, topologyPageInfo.get_id_to_bolt_agg_stats_size());\r\n    assertTrue(topologyPageInfo.get_id_to_bolt_agg_stats().containsKey(\"my-component\"));\r\n    assertNotNull(topologyPageInfo.get_id_to_bolt_agg_stats().get(\"my-component\"));\r\n    componentStats = topologyPageInfo.get_id_to_bolt_agg_stats().get(\"my-component\");\r\n    assertEquals(ComponentType.BOLT, componentStats.get_type(), \"Should be of type bolt\");\r\n    assertNull(componentStats.get_last_error(), \"Last error should not be null\");\r\n    // Verify mock interactions\r\n    verify(mockStormClusterState, times(1)).lastError(eq(\"my-storm-id\"), eq(\"my-component\"));\r\n    verify(mockStormClusterState, times(1)).lastError(eq(\"my-storm-id\"), eq(\"my-spout\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration01",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration01() {\r\n    // No payload or scheme are generated when already present\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, \"foobar:12345\");\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"anything\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertEquals(\"foobar:12345\", actualPayload);\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration02",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration02() {\r\n    // Scheme is set to digest if not already.\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, \"foobar:12345\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertEquals(\"foobar:12345\", actualPayload);\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration03",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration03() {\r\n    // A payload is generated when no payload is present.\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"anything\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertFalse(StringUtils.isBlank((String) actualPayload));\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration04",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration04() {\r\n    // A payload is generated when payload is not correctly formatted.\r\n    String bogusPayload = \"not-a-valid-payload\";\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, bogusPayload);\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"anything\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertFalse(StormSubmitter.validateZKDigestPayload(bogusPayload));\r\n    assertFalse(StringUtils.isBlank((String) actualPayload));\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration05",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration05() {\r\n    // A payload is generated when payload is null.\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, null);\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"anything\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertFalse(StringUtils.isBlank((String) actualPayload));\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\SubmitterTest.java",
  "methodName" : "testMd5DigestSecretGeneration06",
  "sourceCode" : "@Test\r\npublic void testMd5DigestSecretGeneration06() {\r\n    // A payload is generated when payload is blank.\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, \"\");\r\n    conf.put(Config.STORM_ZOOKEEPER_AUTH_SCHEME, \"anything\");\r\n    Map<String, Object> result = StormSubmitter.prepareZookeeperAuthentication(conf);\r\n    Object actualPayload = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\r\n    Object actualScheme = result.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\r\n    assertFalse(StringUtils.isBlank((String) actualPayload));\r\n    assertEquals(\"digest\", actualScheme);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\trident\\StateTest.java",
  "methodName" : "testOpaqueValue",
  "sourceCode" : "@Test\r\npublic void testOpaqueValue() {\r\n    OpaqueValue<String> opqval = new OpaqueValue<>(8L, \"v1\", \"v0\");\r\n    OpaqueValue<String> upval0 = opqval.update(8L, \"v2\");\r\n    OpaqueValue<String> upval1 = opqval.update(9L, \"v2\");\r\n    assertEquals(opqval.get(null), \"v1\");\r\n    assertEquals(opqval.get(100L), \"v1\");\r\n    assertEquals(opqval.get(9L), \"v1\");\r\n    assertEquals(opqval.get(8L), \"v0\");\r\n    Assertions.assertThrows(Exception.class, () -> opqval.get(7L));\r\n    assertEquals(opqval.getPrev(), \"v0\");\r\n    assertEquals(opqval.getCurr(), \"v1\");\r\n    // update with current\r\n    assertEquals(upval0.getPrev(), \"v0\");\r\n    assertEquals(upval0.getCurr(), \"v2\");\r\n    assertNotEquals(opqval, upval0);\r\n    // update\r\n    assertEquals(upval1.getPrev(), \"v1\");\r\n    assertEquals(upval1.getCurr(), \"v2\");\r\n    assertNotEquals(opqval, upval1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\trident\\StateTest.java",
  "methodName" : "testOpaqueMap",
  "sourceCode" : "@Test\r\npublic void testOpaqueMap() {\r\n    MapState<Object> map = OpaqueMap.build(new MemoryBackingMap<>());\r\n    map.beginCommit(1L);\r\n    assertEquals(singleGet(map, \"a\"), null);\r\n    // tests that intra-batch caching works\r\n    assertEquals(singleUpdate(map, \"a\", 1L), 1L);\r\n    assertEquals(singleGet(map, \"a\"), 1L);\r\n    assertEquals(singleUpdate(map, \"a\", 2L), 3L);\r\n    assertEquals(singleGet(map, \"a\"), 3L);\r\n    map.commit(1L);\r\n    map.beginCommit(1L);\r\n    assertEquals(singleGet(map, \"a\"), null);\r\n    assertEquals(singleUpdate(map, \"a\", 2L), 2L);\r\n    map.commit(1L);\r\n    map.beginCommit(2L);\r\n    assertEquals(singleGet(map, \"a\"), 2L);\r\n    assertEquals(singleUpdate(map, \"a\", 3L), 5L);\r\n    assertEquals(singleUpdate(map, \"a\", 1L), 6L);\r\n    map.commit(2L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\trident\\StateTest.java",
  "methodName" : "testTransactionalMap",
  "sourceCode" : "@Test\r\npublic void testTransactionalMap() {\r\n    MapState<Object> map = TransactionalMap.build(new MemoryBackingMap<>());\r\n    map.beginCommit(1L);\r\n    assertEquals(singleGet(map, \"a\"), null);\r\n    // tests that intra-batch caching works\r\n    assertEquals(singleUpdate(map, \"a\", 1L), 1L);\r\n    assertEquals(singleUpdate(map, \"a\", 2L), 3L);\r\n    map.commit(1L);\r\n    map.beginCommit(1L);\r\n    assertEquals(singleGet(map, \"a\"), 3L);\r\n    // tests that intra-batch caching has no effect if it's the same commit as previous commit\r\n    assertEquals(singleUpdate(map, \"a\", 1L), 3L);\r\n    assertEquals(singleUpdate(map, \"a\", 2L), 3L);\r\n    map.commit(1L);\r\n    map.beginCommit(2L);\r\n    assertEquals(singleGet(map, \"a\"), 3L);\r\n    assertEquals(singleUpdate(map, \"a\", 3L), 6L);\r\n    assertEquals(singleUpdate(map, \"a\", 1L), 7L);\r\n    map.commit(2L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\trident\\StateTest.java",
  "methodName" : "testCreateNodeAcl",
  "sourceCode" : "@Test\r\npublic void testCreateNodeAcl() throws Exception {\r\n    // Creates ZooKeeper nodes with the correct ACLs\r\n    CuratorFramework curator = Mockito.mock(CuratorFramework.class);\r\n    CreateBuilder builder0 = Mockito.mock(CreateBuilder.class);\r\n    ProtectACLCreateModeStatPathAndBytesable builder1 = Mockito.mock(ProtectACLCreateModeStatPathAndBytesable.class);\r\n    List<ACL> expectedAcls = ZooDefs.Ids.CREATOR_ALL_ACL;\r\n    Mockito.when(curator.create()).thenReturn(builder0);\r\n    Mockito.when(builder0.creatingParentsIfNeeded()).thenReturn(builder1);\r\n    Mockito.when(builder1.withMode(ArgumentMatchers.isA(CreateMode.class))).thenReturn(builder1);\r\n    Mockito.when(builder1.withACL(Mockito.anyList())).thenReturn(builder1);\r\n    TestTransactionalState.createNode(curator, \"\", new byte[0], expectedAcls, null);\r\n    Mockito.verify(builder1).withACL(expectedAcls);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-core\\test\\jvm\\org\\apache\\storm\\trident\\StateTest.java",
  "methodName" : "testMemoryMapStateRemove",
  "sourceCode" : "@Test\r\npublic void testMemoryMapStateRemove() {\r\n    MemoryMapState<Object> map = new MemoryMapState<>(Utils.uuid());\r\n    map.beginCommit(1L);\r\n    singlePut(map, \"a\", 1);\r\n    singlePut(map, \"b\", 2);\r\n    map.commit(1L);\r\n    map.beginCommit(2L);\r\n    singleRemove(map, \"a\");\r\n    assertEquals(singleGet(map, \"a\"), null);\r\n    assertEquals(singleGet(map, \"b\"), 2);\r\n    map.commit(2L);\r\n    map.beginCommit(2L);\r\n    assertEquals(singleGet(map, \"a\"), 1);\r\n    assertEquals(singleGet(map, \"b\"), 2);\r\n    singleRemove(map, \"a\");\r\n    map.commit(2L);\r\n    map.beginCommit(3L);\r\n    assertEquals(singleGet(map, \"a\"), null);\r\n    assertEquals(singleGet(map, \"b\"), 2);\r\n    map.commit(3L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\blobstore\\FileBlobStoreImpl.java",
  "methodName" : "getKeyDir",
  "sourceCode" : "@VisibleForTesting\r\nFile getKeyDir(String key) {\r\n    String hash = String.valueOf(Math.abs((long) key.hashCode()) % BUCKETS);\r\n    File ret = new File(new File(fullPath, hash), key);\r\n    LOG.debug(\"{} Looking for {} in {}\", new Object[] { fullPath, key, hash });\r\n    return ret;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStore.java",
  "methodName" : "getKeyDataDir",
  "sourceCode" : "@VisibleForTesting\r\nFile getKeyDataDir(String key) {\r\n    return fbs.getKeyDir(DATA_PREFIX + key);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\container\\cgroup\\CgroupManager.java",
  "methodName" : "getLaunchCommand",
  "sourceCode" : "/**\r\n * To compose launch command based on workerId and existing command.\r\n * @param workerId the worker id\r\n * @param existingCommand the current command to run that may need to be modified\r\n * @return new commandline with necessary additions to launch worker\r\n */\r\n@VisibleForTesting\r\npublic List<String> getLaunchCommand(String workerId, List<String> existingCommand) {\r\n    List<String> newCommand = getLaunchCommandPrefix(workerId);\r\n    if (workerToNumaId.containsKey(workerId)) {\r\n        prefixNumaPinning(newCommand, workerToNumaId.get(workerId));\r\n    }\r\n    newCommand.addAll(existingCommand);\r\n    return newCommand;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\drpc\\DRPC.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\nstatic void checkAuthorization(ReqContext reqContext, IAuthorizer auth, String operation, String function) throws AuthorizationException {\r\n    checkAuthorization(reqContext, auth, operation, function, true);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\HeartbeatCache.java",
  "methodName" : "addEmptyTopoForTests",
  "sourceCode" : "/**\r\n * Add an empty topology to the cache for testing purposes.\r\n * @param topoId the id of the topology to add.\r\n */\r\n@VisibleForTesting\r\npublic void addEmptyTopoForTests(String topoId) {\r\n    cache.put(topoId, new ConcurrentHashMap<>());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\HeartbeatCache.java",
  "methodName" : "getNumToposCached",
  "sourceCode" : "/**\r\n * Get the number of topologies with cached heartbeats.\r\n * @return the number of topologies with cached heartbeats.\r\n */\r\n@VisibleForTesting\r\npublic int getNumToposCached() {\r\n    return cache.size();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\HeartbeatCache.java",
  "methodName" : "getTopologyIds",
  "sourceCode" : "/**\r\n * Get the topology ids with cached heartbeats.\r\n * @return the set of topology ids with cached heartbeats.\r\n */\r\n@VisibleForTesting\r\npublic Set<String> getTopologyIds() {\r\n    return cache.keySet();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "topoIdsToClean",
  "sourceCode" : "@VisibleForTesting\r\npublic static Set<String> topoIdsToClean(IStormClusterState state, BlobStore store, Map<String, Object> conf) {\r\n    Set<String> cleanable = new HashSet<>();\r\n    cleanable.addAll(Utils.OR(state.heartbeatStorms(), EMPTY_STRING_LIST));\r\n    cleanable.addAll(Utils.OR(state.errorTopologies(), EMPTY_STRING_LIST));\r\n    cleanable.addAll(Utils.OR(store.storedTopoIds(), EMPTY_STRING_SET));\r\n    cleanable.addAll(Utils.OR(state.backpressureTopologies(), EMPTY_STRING_LIST));\r\n    cleanable.addAll(Utils.OR(state.idsOfTopologiesWithPrivateWorkerKeys(), EMPTY_STRING_SET));\r\n    Set<String> delayedCleanable = getExpiredTopologyIds(cleanable, conf);\r\n    delayedCleanable.removeAll(Utils.OR(state.activeStorms(), EMPTY_STRING_LIST));\r\n    return delayedCleanable;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "cleanInbox",
  "sourceCode" : "/**\r\n * Deletes jar files in dirLoc older than seconds.\r\n *\r\n * @param dirLoc  the location to look in for file\r\n * @param seconds how old is too old and should be deleted\r\n */\r\n@VisibleForTesting\r\npublic static void cleanInbox(String dirLoc, int seconds) {\r\n    final long now = Time.currentTimeMillis();\r\n    final long ms = Time.secsToMillis(seconds);\r\n    File dir = new File(dirLoc);\r\n    for (File f : dir.listFiles((file) -> file.isFile() && ((file.lastModified() + ms) <= now))) {\r\n        if (f.delete()) {\r\n            LOG.info(\"Cleaning inbox ... deleted: {}\", f.getName());\r\n        } else {\r\n            LOG.error(\"Cleaning inbox ... error deleting: {}\", f.getName());\r\n        }\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "topologiesOnSupervisor",
  "sourceCode" : "@VisibleForTesting\r\npublic static List<String> topologiesOnSupervisor(Map<String, Assignment> assignments, String supervisorId) {\r\n    Set<String> ret = new HashSet<>();\r\n    for (Entry<String, Assignment> entry : assignments.entrySet()) {\r\n        Assignment assignment = entry.getValue();\r\n        for (NodeInfo nodeInfo : assignment.get_executor_node_port().values()) {\r\n            if (supervisorId.equals(nodeInfo.get_node())) {\r\n                ret.add(entry.getKey());\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return new ArrayList<>(ret);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "launchServer",
  "sourceCode" : "@VisibleForTesting\r\npublic void launchServer() throws Exception {\r\n    try {\r\n        LOG.info(\"Starting Nimbus with conf {}\", ConfigUtils.maskPasswords(conf));\r\n        validator.prepare(conf);\r\n        IStormClusterState state = stormClusterState;\r\n        NimbusInfo hpi = nimbusHostPortInfo;\r\n        //add to nimbuses\r\n        NimbusSummary nimbusSummary = new NimbusSummary(hpi.getHost(), hpi.getPort(), Time.currentTimeSecs(), false, STORM_VERSION);\r\n        nimbusSummary.set_tlsPort(hpi.getTlsPort());\r\n        state.addNimbusHost(hpi.getHost(), nimbusSummary);\r\n        leaderElector.addToLeaderLockQueue();\r\n        this.blobStore.startSyncBlobs();\r\n        for (ClusterMetricsConsumerExecutor exec : clusterConsumerExceutors) {\r\n            exec.prepare();\r\n        }\r\n        // Leadership coordination may be incomplete when launchServer is called. Previous behavior did a one time check\r\n        // which could cause Nimbus to not process TopologyActions.GAIN_LEADERSHIP transitions. Similar problem exists for\r\n        // HA Nimbus on being newly elected as leader. Change to a recurring pattern addresses these problems.\r\n        timer.scheduleRecurring(3, 5, () -> {\r\n            try {\r\n                boolean isLeader = isLeader();\r\n                if (isLeader && !wasLeader) {\r\n                    for (String topoId : state.activeStorms()) {\r\n                        transition(topoId, TopologyActions.GAIN_LEADERSHIP, null);\r\n                    }\r\n                    clusterMetricSet.setActive(true);\r\n                }\r\n                wasLeader = isLeader;\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        final boolean doNotReassign = (Boolean) conf.getOrDefault(ServerConfigUtils.NIMBUS_DO_NOT_REASSIGN, false);\r\n        timer.scheduleRecurring(0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_MONITOR_FREQ_SECS)), () -> {\r\n            try {\r\n                if (!doNotReassign) {\r\n                    mkAssignments();\r\n                }\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        // Schedule topology cleanup\r\n        cleanupTimer.scheduleRecurring(0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_MONITOR_FREQ_SECS)), () -> {\r\n            cleanupTimer.schedule(0, () -> doCleanup());\r\n        });\r\n        // Schedule Nimbus inbox cleaner\r\n        final int jarExpSecs = ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_INBOX_JAR_EXPIRATION_SECS));\r\n        timer.scheduleRecurring(0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CLEANUP_INBOX_FREQ_SECS)), () -> {\r\n            try {\r\n                cleanInbox(getInbox(), jarExpSecs);\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        // Schedule topology history cleaner\r\n        Integer interval = ObjectReader.getInt(conf.get(DaemonConfig.LOGVIEWER_CLEANUP_INTERVAL_SECS), null);\r\n        if (interval != null) {\r\n            final int lvCleanupAgeMins = ObjectReader.getInt(conf.get(DaemonConfig.LOGVIEWER_CLEANUP_AGE_MINS));\r\n            timer.scheduleRecurring(0, interval, () -> {\r\n                try {\r\n                    cleanTopologyHistory(lvCleanupAgeMins);\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n            });\r\n        }\r\n        timer.scheduleRecurring(0, ObjectReader.getInt(conf.get(DaemonConfig.NIMBUS_CREDENTIAL_RENEW_FREQ_SECS)), () -> {\r\n            try {\r\n                renewCredentials();\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        // Periodically make sure the blobstore update time is up to date.  This could have failed if Nimbus encountered\r\n        // an exception updating the update time, or due to bugs causing a missed update of the blobstore mod time on a blob\r\n        // update.\r\n        timer.scheduleRecurring(30, ServerConfigUtils.getLocalizerUpdateBlobInterval(conf) * 5, () -> {\r\n            try {\r\n                blobStore.validateBlobUpdateTime();\r\n            } catch (IOException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        metricsRegistry.registerGauge(\"nimbus:total-available-memory-non-negative\", () -> nodeIdToResources.get().values().parallelStream().mapToDouble(supervisorResources -> Math.max(supervisorResources.getAvailableMem(), 0)).sum());\r\n        metricsRegistry.registerGauge(\"nimbus:available-cpu-non-negative\", () -> nodeIdToResources.get().values().parallelStream().mapToDouble(supervisorResources -> Math.max(supervisorResources.getAvailableCpu(), 0)).sum());\r\n        metricsRegistry.registerGauge(\"nimbus:total-memory\", () -> nodeIdToResources.get().values().parallelStream().mapToDouble(SupervisorResources::getTotalMem).sum());\r\n        metricsRegistry.registerGauge(\"nimbus:total-cpu\", () -> nodeIdToResources.get().values().parallelStream().mapToDouble(SupervisorResources::getTotalCpu).sum());\r\n        metricsRegistry.registerGauge(\"nimbus:longest-scheduling-time-ms\", () -> {\r\n            //We want to update longest scheduling time in real time in case scheduler get stuck\r\n            // Get current time before startTime to avoid potential race with scheduler's Timer\r\n            Long currTime = Time.nanoTime();\r\n            Long startTime = schedulingStartTimeNs.get();\r\n            return TimeUnit.NANOSECONDS.toMillis(startTime == null ? longestSchedulingTime.get() : Math.max(currTime - startTime, longestSchedulingTime.get()));\r\n        });\r\n        metricsRegistry.registerMeter(\"nimbus:num-launched\").mark();\r\n        timer.scheduleRecurring(0, ObjectReader.getInt(conf.get(DaemonConfig.STORM_CLUSTER_METRICS_CONSUMER_PUBLISH_INTERVAL_SECS)), () -> {\r\n            try {\r\n                if (isLeader()) {\r\n                    sendClusterMetricsToExecutors();\r\n                }\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        timer.scheduleRecurring(5, 5, clusterMetricSet);\r\n    } catch (Exception e) {\r\n        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)) {\r\n            throw e;\r\n        }\r\n        if (Utils.exceptionCauseIsInstanceOf(InterruptedIOException.class, e)) {\r\n            throw e;\r\n        }\r\n        LOG.error(\"Error on initialization of nimbus\", e);\r\n        Utils.exitProcess(13, \"Error on initialization of nimbus\");\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "setAuthorizationHandler",
  "sourceCode" : "@VisibleForTesting\r\npublic void setAuthorizationHandler(IAuthorizer authorizationHandler) {\r\n    this.authorizationHandler = authorizationHandler;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "getHeartbeatsCache",
  "sourceCode" : "@VisibleForTesting\r\npublic HeartbeatCache getHeartbeatsCache() {\r\n    return heartbeatsCache;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "initWorkerTokenManager",
  "sourceCode" : "@VisibleForTesting\r\nvoid initWorkerTokenManager() {\r\n    if (workerTokenManager == null) {\r\n        workerTokenManager = new WorkerTokenManager(conf, getStormClusterState());\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\npublic void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation) throws AuthorizationException {\r\n    checkAuthorization(topoName, topoConf, operation, null);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\npublic void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation, ReqContext context) throws AuthorizationException {\r\n    IAuthorizer impersonationAuthorizer = impersonationAuthorizationHandler;\r\n    if (context == null) {\r\n        context = ReqContext.context();\r\n    }\r\n    Map<String, Object> checkConf = new HashMap<>();\r\n    if (topoConf != null) {\r\n        checkConf.putAll(topoConf);\r\n    } else if (topoName != null) {\r\n        checkConf.put(Config.TOPOLOGY_NAME, topoName);\r\n    }\r\n    if (context.isImpersonating()) {\r\n        LOG.info(\"principal: {} is trying to impersonate principal: {}\", context.realPrincipal(), context.principal());\r\n        if (impersonationAuthorizer == null) {\r\n            LOG.warn(\"impersonation attempt but {} has no authorizer configured. potential security risk, \" + \"please see SECURITY.MD to learn how to configure impersonation authorizer.\", DaemonConfig.NIMBUS_IMPERSONATION_AUTHORIZER);\r\n        } else {\r\n            if (!impersonationAuthorizer.permit(context, operation, checkConf)) {\r\n                ThriftAccessLogger.logAccess(context.requestID(), context.remoteAddress(), context.principal(), operation, topoName, \"access-denied\");\r\n                throw new WrappedAuthorizationException(\"principal \" + context.realPrincipal() + \" is not authorized to impersonate principal \" + context.principal() + \" from host \" + context.remoteAddress() + \" Please see SECURITY.MD to learn how to configure impersonation acls.\");\r\n            }\r\n        }\r\n    }\r\n    IAuthorizer aclHandler = authorizationHandler;\r\n    if (aclHandler != null) {\r\n        if (!aclHandler.permit(context, operation, checkConf)) {\r\n            ThriftAccessLogger.logAccess(context.requestID(), context.remoteAddress(), context.principal(), operation, topoName, \"access-denied\");\r\n            throw new WrappedAuthorizationException(operation + (topoName != null ? \" on topology \" + topoName : \"\") + \" is not authorized\");\r\n        } else {\r\n            ThriftAccessLogger.logAccess(context.requestID(), context.remoteAddress(), context.principal(), operation, topoName, \"access-granted\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "filterAuthorized",
  "sourceCode" : "@VisibleForTesting\r\npublic Set<String> filterAuthorized(String operation, Collection<String> topoIds) throws NotAliveException, AuthorizationException, IOException {\r\n    Set<String> ret = new HashSet<>();\r\n    for (String topoId : topoIds) {\r\n        if (isAuthorized(operation, topoId)) {\r\n            ret.add(topoId);\r\n        }\r\n    }\r\n    return ret;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "rmDependencyJarsInTopology",
  "sourceCode" : "@VisibleForTesting\r\npublic void rmDependencyJarsInTopology(String topoId) {\r\n    try {\r\n        BlobStore store = blobStore;\r\n        IStormClusterState state = stormClusterState;\r\n        StormTopology topo = readStormTopologyAsNimbus(topoId, topoCache);\r\n        List<String> dependencyJars = topo.get_dependency_jars();\r\n        LOG.info(\"Removing dependency jars from blobs - {}\", dependencyJars);\r\n        if (dependencyJars != null && !dependencyJars.isEmpty()) {\r\n            for (String key : dependencyJars) {\r\n                rmBlobKey(store, key, state);\r\n            }\r\n        }\r\n    } catch (Exception e) {\r\n        //Yes eat the exception\r\n        LOG.info(\"Exception {}\", e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "rmTopologyKeys",
  "sourceCode" : "@VisibleForTesting\r\npublic void rmTopologyKeys(String topoId) {\r\n    BlobStore store = blobStore;\r\n    IStormClusterState state = stormClusterState;\r\n    try {\r\n        topoCache.deleteTopoConf(topoId, NIMBUS_SUBJECT);\r\n    } catch (Exception e) {\r\n        //Just go on and try to delete the others\r\n    }\r\n    try {\r\n        topoCache.deleteTopology(topoId, NIMBUS_SUBJECT);\r\n    } catch (Exception e) {\r\n        //Just go on and try to delte the others\r\n    }\r\n    rmBlobKey(store, ConfigUtils.masterStormJarKey(topoId), state);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "forceDeleteTopoDistDir",
  "sourceCode" : "@VisibleForTesting\r\npublic void forceDeleteTopoDistDir(String topoId) throws IOException {\r\n    Utils.forceDelete(ServerConfigUtils.masterStormDistRoot(conf, topoId));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "doCleanup",
  "sourceCode" : "/**\r\n * Cleanup topologies and Jars.\r\n */\r\n@VisibleForTesting\r\npublic void doCleanup() {\r\n    try {\r\n        if (!isLeader()) {\r\n            LOG.info(\"not a leader, skipping cleanup\");\r\n            return;\r\n        }\r\n        IStormClusterState state = stormClusterState;\r\n        long cleanupStartMs = Time.currentTimeMillis();\r\n        Set<String> toClean = new HashSet<>(topoIdsToClean(state, blobStore, this.conf));\r\n        long topoIdSelectionDurationMs = Time.deltaMs(cleanupStartMs);\r\n        for (String topoId : toClean) {\r\n            LOG.info(\"Cleaning up {}\", topoId);\r\n            state.teardownHeartbeats(topoId);\r\n            state.teardownTopologyErrors(topoId);\r\n            state.removeAllPrivateWorkerKeys(topoId);\r\n            state.removeBackpressure(topoId);\r\n            rmDependencyJarsInTopology(topoId);\r\n            forceDeleteTopoDistDir(topoId);\r\n            rmTopologyKeys(topoId);\r\n            heartbeatsCache.removeTopo(topoId);\r\n            idToExecutors.getAndUpdate(new Dissoc<>(topoId));\r\n        }\r\n        long cleanupDurationMs = Time.deltaMs(cleanupStartMs);\r\n        if (cleanupDurationMs > 10000) {\r\n            LOG.warn(\"doCleanup is taking too long, topoIdSelectionDurationMs={}, cleanupDurationMs={}\", topoIdSelectionDurationMs, cleanupDurationMs);\r\n        }\r\n    } catch (Exception ex) {\r\n        LOG.error(\"Ignoring error in doCleanup()\", ex);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "awaitLeadership",
  "sourceCode" : "@VisibleForTesting\r\npublic boolean awaitLeadership(long timeout, TimeUnit timeUnit) throws InterruptedException {\r\n    return leaderElector.awaitLeadership(timeout, timeUnit);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\nimbus\\Nimbus.java",
  "methodName" : "setUpAckerExecutorConfigs",
  "sourceCode" : "@VisibleForTesting\r\npublic static void setUpAckerExecutorConfigs(String topoName, Map<String, Object> totalConfToSave, Map<String, Object> totalConf, int estimatedNumWorker) {\r\n    int numAckerExecs;\r\n    int numAckerExecsPerWorker;\r\n    if (totalConf.get(Config.TOPOLOGY_ACKER_EXECUTORS) == null) {\r\n        numAckerExecsPerWorker = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER));\r\n        numAckerExecs = estimatedNumWorker * numAckerExecsPerWorker;\r\n    } else {\r\n        numAckerExecs = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_ACKER_EXECUTORS));\r\n        if (estimatedNumWorker == 0) {\r\n            numAckerExecsPerWorker = 0;\r\n        } else {\r\n            numAckerExecsPerWorker = (int) Math.ceil((double) numAckerExecs / (double) estimatedNumWorker);\r\n        }\r\n    }\r\n    totalConfToSave.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numAckerExecsPerWorker);\r\n    totalConfToSave.put(Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs);\r\n    LOG.info(\"Config {} set to: {} for topology: {}\", Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numAckerExecsPerWorker, topoName);\r\n    LOG.info(\"Config {} set to: {} for topology: {}\", Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs, topoName);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\supervisor\\Slot.java",
  "methodName" : "forSameTopology",
  "sourceCode" : "@VisibleForTesting\r\nstatic boolean forSameTopology(LocalAssignment a, LocalAssignment b) {\r\n    if (a == null && b == null) {\r\n        return true;\r\n    }\r\n    if (a != null && b != null) {\r\n        if (a.get_topology_id().equals(b.get_topology_id())) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\supervisor\\Supervisor.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\npublic void checkAuthorization(String operation) throws AuthorizationException {\r\n    checkAuthorization(null, null, operation, null);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\supervisor\\Supervisor.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\npublic void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation) throws AuthorizationException {\r\n    checkAuthorization(topoName, topoConf, operation, null);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\daemon\\supervisor\\Supervisor.java",
  "methodName" : "checkAuthorization",
  "sourceCode" : "@VisibleForTesting\r\npublic void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation, ReqContext context) throws AuthorizationException {\r\n    if (context == null) {\r\n        context = ReqContext.context();\r\n    }\r\n    Map<String, Object> checkConf = new HashMap<>();\r\n    if (topoConf != null) {\r\n        checkConf.putAll(topoConf);\r\n    } else if (topoName != null) {\r\n        checkConf.put(Config.TOPOLOGY_NAME, topoName);\r\n    }\r\n    if (context.isImpersonating()) {\r\n        LOG.info(\"principal: {} is trying to impersonate principal: {}\", context.realPrincipal(), context.principal());\r\n        throw new WrappedAuthorizationException(\"Supervisor does not support impersonation\");\r\n    }\r\n    IAuthorizer aclHandler = authorizationHandler;\r\n    if (aclHandler != null) {\r\n        if (!aclHandler.permit(context, operation, checkConf)) {\r\n            ThriftAccessLogger.logAccess(context.requestID(), context.remoteAddress(), context.principal(), operation, topoName, \"access-denied\");\r\n            throw new WrappedAuthorizationException(operation + (topoName != null ? \" on topology \" + topoName : \"\") + \" is not authorized\");\r\n        } else {\r\n            ThriftAccessLogger.logAccess(context.requestID(), context.remoteAddress(), context.principal(), operation, topoName, \"access-granted\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "getTopoJar",
  "sourceCode" : "@VisibleForTesting\r\nLocallyCachedBlob getTopoJar(final String topologyId, String owner) {\r\n    return topologyBlobs.computeIfAbsent(ConfigUtils.masterStormJarKey(topologyId), (tjk) -> {\r\n        try {\r\n            return new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps, LocallyCachedTopologyBlob.TopologyBlobType.TOPO_JAR, owner, metricsRegistry);\r\n        } catch (IOException e) {\r\n            String message = \"Failed getTopoJar for \" + topologyId;\r\n            LOG.error(message, e);\r\n            throw new RuntimeException(message, e);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "getTopoCode",
  "sourceCode" : "@VisibleForTesting\r\nLocallyCachedBlob getTopoCode(final String topologyId, String owner) {\r\n    return topologyBlobs.computeIfAbsent(ConfigUtils.masterStormCodeKey(topologyId), (tck) -> {\r\n        try {\r\n            return new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps, LocallyCachedTopologyBlob.TopologyBlobType.TOPO_CODE, owner, metricsRegistry);\r\n        } catch (IOException e) {\r\n            String message = \"Failed getTopoCode for \" + topologyId;\r\n            LOG.error(message, e);\r\n            throw new RuntimeException(message, e);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "getTopoConf",
  "sourceCode" : "@VisibleForTesting\r\nLocallyCachedBlob getTopoConf(final String topologyId, String owner) {\r\n    return topologyBlobs.computeIfAbsent(ConfigUtils.masterStormConfKey(topologyId), (tck) -> {\r\n        try {\r\n            return new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps, LocallyCachedTopologyBlob.TopologyBlobType.TOPO_CONF, owner, metricsRegistry);\r\n        } catch (IOException e) {\r\n            String message = \"Failed getTopoConf for \" + topologyId;\r\n            LOG.error(message, e);\r\n            throw new RuntimeException(message, e);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "requestDownloadBaseTopologyBlobs",
  "sourceCode" : "@VisibleForTesting\r\nCompletableFuture<Void> requestDownloadBaseTopologyBlobs(PortAndAssignment pna, BlobChangingCallback cb) {\r\n    final String topologyId = pna.getToplogyId();\r\n    final LocallyCachedBlob topoJar = getTopoJar(topologyId, pna.getAssignment().get_owner());\r\n    topoJar.addReference(pna, cb);\r\n    final LocallyCachedBlob topoCode = getTopoCode(topologyId, pna.getAssignment().get_owner());\r\n    topoCode.addReference(pna, cb);\r\n    final LocallyCachedBlob topoConf = getTopoConf(topologyId, pna.getAssignment().get_owner());\r\n    topoConf.addReference(pna, cb);\r\n    return topologyBasicDownloaded.computeIfAbsent(topologyId, (tid) -> downloadOrUpdate(topoJar, topoCode, topoConf));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "updateBlobs",
  "sourceCode" : "/**\r\n * Downloads all blobs listed in the topology configuration for all topologies assigned to this supervisor, and creates version files\r\n * with a suffix. The runnable is intended to be run periodically by a timer, created elsewhere.\r\n */\r\n@VisibleForTesting\r\nvoid updateBlobs() {\r\n    try (Timer.Context t = blobCacheUpdateDuration.time()) {\r\n        List<CompletableFuture<?>> futures = new ArrayList<>();\r\n        futures.add(downloadOrUpdate(topologyBlobs.values()));\r\n        if (symlinksDisabled) {\r\n            LOG.warn(\"symlinks are disabled so blobs cannot be downloaded.\");\r\n        } else {\r\n            for (ConcurrentMap<String, LocalizedResource> map : userArchives.values()) {\r\n                futures.add(downloadOrUpdate(map.values()));\r\n            }\r\n            for (ConcurrentMap<String, LocalizedResource> map : userFiles.values()) {\r\n                futures.add(downloadOrUpdate(map.values()));\r\n            }\r\n        }\r\n        for (CompletableFuture<?> f : futures) {\r\n            f.get();\r\n        }\r\n    } catch (Exception e) {\r\n        updateBlobExceptions.mark();\r\n        LOG.warn(\"Could not update blob ({}), will retry again later.\", e.getClass().getName());\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "addReferencesToBlobs",
  "sourceCode" : "@VisibleForTesting\r\nvoid addReferencesToBlobs(PortAndAssignment pna, BlobChangingCallback cb) throws IOException, KeyNotFoundException, AuthorizationException {\r\n    List<LocalResource> localResourceList = getLocalResources(pna);\r\n    if (!localResourceList.isEmpty()) {\r\n        getBlobs(localResourceList, pna, cb);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "getLocalUserDir",
  "sourceCode" : "// baseDir/supervisor/usercache/user1/\r\n@VisibleForTesting\r\nFile getLocalUserDir(String userName) {\r\n    return LocalizedResource.getLocalUserDir(localBaseDir, userName).toFile();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "getLocalUserFileCacheDir",
  "sourceCode" : "// baseDir/supervisor/usercache/user1/filecache\r\n@VisibleForTesting\r\nFile getLocalUserFileCacheDir(String userName) {\r\n    return LocalizedResource.getLocalUserFileCacheDir(localBaseDir, userName).toFile();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\AsyncLocalizer.java",
  "methodName" : "cleanup",
  "sourceCode" : "@VisibleForTesting\r\nvoid cleanup() {\r\n    try {\r\n        LOG.info(\"Starting cleanup\");\r\n        LocalizedResourceRetentionSet toClean = new LocalizedResourceRetentionSet(cacheTargetSize);\r\n        // need one large set of all and then clean via LRU\r\n        for (Map.Entry<String, ConcurrentHashMap<String, LocalizedResource>> t : userArchives.entrySet()) {\r\n            toClean.addResources(t.getValue());\r\n            LOG.debug(\"Resources to be cleaned after adding {} archives : {}\", t.getKey(), toClean);\r\n        }\r\n        for (Map.Entry<String, ConcurrentHashMap<String, LocalizedResource>> t : userFiles.entrySet()) {\r\n            toClean.addResources(t.getValue());\r\n            LOG.debug(\"Resources to be cleaned after adding {} files : {}\", t.getKey(), toClean);\r\n        }\r\n        toClean.addResources(topologyBlobs);\r\n        Set<String> topologiesWithDeletes = new HashSet<>();\r\n        try (ClientBlobStore store = getClientBlobStore()) {\r\n            Set<LocallyCachedBlob> deletedBlobs = toClean.cleanup(store);\r\n            for (LocallyCachedBlob deletedBlob : deletedBlobs) {\r\n                String topologyId = ConfigUtils.getIdFromBlobKey(deletedBlob.getKey());\r\n                if (topologyId != null) {\r\n                    topologiesWithDeletes.add(topologyId);\r\n                }\r\n            }\r\n        }\r\n        HashSet<String> safeTopologyIds = new HashSet<>();\r\n        for (String blobKey : topologyBlobs.keySet()) {\r\n            safeTopologyIds.add(ConfigUtils.getIdFromBlobKey(blobKey));\r\n        }\r\n        LOG.debug(\"Topologies {} can no longer be considered fully downloaded\", topologiesWithDeletes);\r\n        safeTopologyIds.removeAll(topologiesWithDeletes);\r\n        //Deleting this early does not hurt anything\r\n        topologyBasicDownloaded.keySet().removeIf(topoId -> !safeTopologyIds.contains(topoId));\r\n        blobPending.keySet().removeIf(topoId -> !safeTopologyIds.contains(topoId));\r\n        try {\r\n            forEachTopologyDistDir((p, topologyId) -> {\r\n                String topoJarKey = ConfigUtils.masterStormJarKey(topologyId);\r\n                String topoCodeKey = ConfigUtils.masterStormCodeKey(topologyId);\r\n                String topoConfKey = ConfigUtils.masterStormConfKey(topologyId);\r\n                if (!topologyBlobs.containsKey(topoJarKey) && !topologyBlobs.containsKey(topoCodeKey) && !topologyBlobs.containsKey(topoConfKey)) {\r\n                    fsOps.deleteIfExists(p.toFile());\r\n                }\r\n            });\r\n        } catch (Exception e) {\r\n            LOG.error(\"Could not read topology directories for cleanup\", e);\r\n        }\r\n        LOG.debug(\"Resource cleanup: {}\", toClean);\r\n        Set<String> allUsers = new HashSet<>(userArchives.keySet());\r\n        allUsers.addAll(userFiles.keySet());\r\n        for (String user : allUsers) {\r\n            ConcurrentMap<String, LocalizedResource> filesForUser = userFiles.get(user);\r\n            ConcurrentMap<String, LocalizedResource> archivesForUser = userArchives.get(user);\r\n            if ((filesForUser == null || filesForUser.size() == 0) && (archivesForUser == null || archivesForUser.size() == 0)) {\r\n                LOG.debug(\"removing empty set: {}\", user);\r\n                try {\r\n                    LocalizedResource.completelyRemoveUnusedUser(localBaseDir, user);\r\n                    userFiles.remove(user);\r\n                    userArchives.remove(user);\r\n                } catch (IOException e) {\r\n                    LOG.error(\"Error trying to delete cached user files\", e);\r\n                }\r\n            }\r\n        }\r\n    } catch (Exception ex) {\r\n        LOG.error(\"AsyncLocalizer cleanup failure\", ex);\r\n    } catch (Error error) {\r\n        LOG.error(\"AsyncLocalizer cleanup failure\", error);\r\n        Utils.exitProcess(20, \"AsyncLocalizer cleanup failure\");\r\n    } finally {\r\n        LOG.info(\"Finish cleanup\");\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\LocalizedResource.java",
  "methodName" : "localVersionOfBlob",
  "sourceCode" : "@VisibleForTesting\r\nstatic long localVersionOfBlob(Path versionFile) {\r\n    long currentVersion = -1;\r\n    if (Files.exists(versionFile) && !(Files.isDirectory(versionFile))) {\r\n        try (BufferedReader br = new BufferedReader(new FileReader(versionFile.toFile()))) {\r\n            String line = br.readLine();\r\n            currentVersion = Long.parseLong(line);\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    return currentVersion;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\LocalizedResource.java",
  "methodName" : "getFilePathWithVersion",
  "sourceCode" : "@VisibleForTesting\r\nPath getFilePathWithVersion() {\r\n    return constructBlobWithVersionFileName(baseDir, getKey(), getLocalVersion());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\localizer\\LocalizedResource.java",
  "methodName" : "setSize",
  "sourceCode" : "@VisibleForTesting\r\nprotected void setSize(long size) {\r\n    this.size = size;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\scheduler\\Cluster.java",
  "methodName" : "setNetworkTopography",
  "sourceCode" : "@VisibleForTesting\r\npublic void setNetworkTopography(Map<String, List<String>> networkTopography) {\r\n    this.networkTopography.clear();\r\n    this.networkTopography.putAll(networkTopography);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\scheduler\\EvenScheduler.java",
  "methodName" : "sortSlots",
  "sourceCode" : "@VisibleForTesting\r\npublic static List<WorkerSlot> sortSlots(List<WorkerSlot> availableSlots) {\r\n    //For example, we have a three nodes(supervisor1, supervisor2, supervisor3) cluster:\r\n    //slots before sort:\r\n    //supervisor1:6700, supervisor1:6701,\r\n    //supervisor2:6700, supervisor2:6701, supervisor2:6702,\r\n    //supervisor3:6700, supervisor3:6703, supervisor3:6702, supervisor3:6701\r\n    //slots after sort:\r\n    //supervisor3:6700, supervisor2:6700, supervisor1:6700,\r\n    //supervisor3:6701, supervisor2:6701, supervisor1:6701,\r\n    //supervisor3:6702, supervisor2:6702,\r\n    //supervisor3:6703\r\n    if (availableSlots != null && availableSlots.size() > 0) {\r\n        // group by node\r\n        Map<String, List<WorkerSlot>> slotGroups = new TreeMap<>();\r\n        for (WorkerSlot slot : availableSlots) {\r\n            String node = slot.getNodeId();\r\n            List<WorkerSlot> slots = null;\r\n            if (slotGroups.containsKey(node)) {\r\n                slots = slotGroups.get(node);\r\n            } else {\r\n                slots = new ArrayList<WorkerSlot>();\r\n                slotGroups.put(node, slots);\r\n            }\r\n            slots.add(slot);\r\n        }\r\n        // sort by port: from small to large\r\n        for (List<WorkerSlot> slots : slotGroups.values()) {\r\n            Collections.sort(slots, new Comparator<WorkerSlot>() {\r\n\r\n                @Override\r\n                public int compare(WorkerSlot o1, WorkerSlot o2) {\r\n                    return o1.getPort() - o2.getPort();\r\n                }\r\n            });\r\n        }\r\n        // sort by available slots size: from large to small\r\n        List<List<WorkerSlot>> list = new ArrayList<List<WorkerSlot>>(slotGroups.values());\r\n        Collections.sort(list, new Comparator<List<WorkerSlot>>() {\r\n\r\n            @Override\r\n            public int compare(List<WorkerSlot> o1, List<WorkerSlot> o2) {\r\n                return o2.size() - o1.size();\r\n            }\r\n        });\r\n        return ServerUtils.interleaveAll(list);\r\n    }\r\n    return null;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResources.java",
  "methodName" : "resetResourceNames",
  "sourceCode" : "/**\r\n * This is for testing only. It allows a test to reset the static state relating to resource names. We reset the mapping because some\r\n * algorithms sadly have different behavior if a resource exists or not.\r\n */\r\n@VisibleForTesting\r\npublic static void resetResourceNames() {\r\n    RESOURCE_NAME_NORMALIZER = new ResourceNameNormalizer();\r\n    RESOURCE_MAP_ARRAY_BRIDGE = new ResourceMapArrayBridge();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\ConstraintSolverStrategy.java",
  "methodName" : "validateSolution",
  "sourceCode" : "/**\r\n * Determines if a scheduling is valid and all constraints are satisfied (for use in testing).\r\n * This is done in three steps.\r\n *\r\n * <li>Check if nodeCoLocationCnt-constraints are satisfied. Some components may allow only a certain number of\r\n * executors to exist on the same node {@link ConstraintSolverConfig#getMaxNodeCoLocationCnts()}.\r\n * </li>\r\n *\r\n * <li>\r\n * Check if incompatibility-constraints are satisfied. Incompatible components\r\n * {@link ConstraintSolverConfig#getIncompatibleComponentSets()} should not be put on the same worker.\r\n * </li>\r\n *\r\n * <li>\r\n * Check if CPU and Memory resources do not exceed availability on the node and total matches what is expected\r\n * when fully scheduled.\r\n * </li>\r\n *\r\n * @param cluster on which scheduling was done.\r\n * @param topo TopologyDetails being scheduled.\r\n * @return true if solution is valid, false otherwise.\r\n */\r\n@VisibleForTesting\r\npublic static boolean validateSolution(Cluster cluster, TopologyDetails topo) {\r\n    LOG.debug(\"Checking for a valid scheduling for topology {}...\", topo.getName());\r\n    if (cluster.getAssignmentById(topo.getId()) == null) {\r\n        String err = \"cluster.getAssignmentById(\\\"\" + topo.getId() + \"\\\") returned null\";\r\n        LOG.error(err);\r\n        throw new AssertionError(\"No assignments for topologyId \" + topo.getId());\r\n    }\r\n    ConstraintSolverConfig constraintSolverConfig = new ConstraintSolverConfig(topo);\r\n    // First check NodeCoLocationCnt constraints\r\n    Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();\r\n    // this is the critical count\r\n    Map<String, Map<String, Integer>> nodeCompMap = new HashMap<>();\r\n    Map<WorkerSlot, RasNode> workerToNodes = new HashMap<>();\r\n    RasNodes.getAllNodesFrom(cluster).values().forEach(node -> node.getUsedSlots().forEach(workerSlot -> workerToNodes.put(workerSlot, node)));\r\n    List<String> errors = new ArrayList<>();\r\n    for (Map.Entry<ExecutorDetails, WorkerSlot> entry : cluster.getAssignmentById(topo.getId()).getExecutorToSlot().entrySet()) {\r\n        ExecutorDetails exec = entry.getKey();\r\n        String comp = execToComp.get(exec);\r\n        WorkerSlot worker = entry.getValue();\r\n        RasNode node = workerToNodes.get(worker);\r\n        String nodeId = node.getId();\r\n        if (!constraintSolverConfig.getMaxNodeCoLocationCnts().containsKey(comp)) {\r\n            continue;\r\n        }\r\n        int allowedColocationMaxCnt = constraintSolverConfig.getMaxNodeCoLocationCnts().get(comp);\r\n        Map<String, Integer> oneNodeCompMap = nodeCompMap.computeIfAbsent(nodeId, (k) -> new HashMap<>());\r\n        oneNodeCompMap.put(comp, oneNodeCompMap.getOrDefault(comp, 0) + 1);\r\n        if (allowedColocationMaxCnt < oneNodeCompMap.get(comp)) {\r\n            String err = String.format(\"MaxNodeCoLocation: Component %s (exec=%s) on node %s, cnt %d > allowed %d\", comp, exec, nodeId, oneNodeCompMap.get(comp), allowedColocationMaxCnt);\r\n            errors.add(err);\r\n        }\r\n    }\r\n    // Second check IncompatibileComponent Constraints\r\n    Map<WorkerSlot, Set<String>> workerCompMap = new HashMap<>();\r\n    cluster.getAssignmentById(topo.getId()).getExecutorToSlot().forEach((exec, worker) -> {\r\n        String comp = execToComp.get(exec);\r\n        workerCompMap.computeIfAbsent(worker, (k) -> new HashSet<>()).add(comp);\r\n    });\r\n    for (Map.Entry<WorkerSlot, Set<String>> entry : workerCompMap.entrySet()) {\r\n        Set<String> comps = entry.getValue();\r\n        for (String comp1 : comps) {\r\n            for (String comp2 : comps) {\r\n                if (!comp1.equals(comp2) && constraintSolverConfig.getIncompatibleComponentSets().containsKey(comp1) && constraintSolverConfig.getIncompatibleComponentSets().get(comp1).contains(comp2)) {\r\n                    String err = String.format(\"IncompatibleComponents: %s and %s on WorkerSlot: %s\", comp1, comp2, entry.getKey());\r\n                    errors.add(err);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    // Third check resources\r\n    SchedulerAssignment schedulerAssignment = cluster.getAssignmentById(topo.getId());\r\n    Map<ExecutorDetails, WorkerSlot> execToWorker = new HashMap<>();\r\n    if (schedulerAssignment.getExecutorToSlot() != null) {\r\n        execToWorker.putAll(schedulerAssignment.getExecutorToSlot());\r\n    }\r\n    Map<String, RasNode> nodes = RasNodes.getAllNodesFrom(cluster);\r\n    Map<RasNode, Collection<ExecutorDetails>> nodeToExecs = new HashMap<>();\r\n    for (Map.Entry<ExecutorDetails, WorkerSlot> entry : execToWorker.entrySet()) {\r\n        ExecutorDetails exec = entry.getKey();\r\n        WorkerSlot worker = entry.getValue();\r\n        RasNode node = nodes.get(worker.getNodeId());\r\n        if (node.getAvailableMemoryResources() < 0.0) {\r\n            String err = String.format(\"Resource Exhausted: Found node %s with negative available memory %,.2f\", node.getId(), node.getAvailableMemoryResources());\r\n            errors.add(err);\r\n            continue;\r\n        }\r\n        if (node.getAvailableCpuResources() < 0.0) {\r\n            String err = String.format(\"Resource Exhausted: Found node %s with negative available CPU %,.2f\", node.getId(), node.getAvailableCpuResources());\r\n            errors.add(err);\r\n            continue;\r\n        }\r\n        nodeToExecs.computeIfAbsent(node, (k) -> new HashSet<>()).add(exec);\r\n    }\r\n    for (Map.Entry<RasNode, Collection<ExecutorDetails>> entry : nodeToExecs.entrySet()) {\r\n        RasNode node = entry.getKey();\r\n        Collection<ExecutorDetails> execs = entry.getValue();\r\n        double cpuUsed = 0.0;\r\n        double memoryUsed = 0.0;\r\n        for (ExecutorDetails exec : execs) {\r\n            cpuUsed += topo.getTotalCpuReqTask(exec);\r\n            memoryUsed += topo.getTotalMemReqTask(exec);\r\n        }\r\n        if (node.getAvailableCpuResources() != (node.getTotalCpuResources() - cpuUsed)) {\r\n            String err = String.format(\"Incorrect CPU Resources: Node %s CPU available is %,.2f, expected %,.2f, \" + \"Executors scheduled on node: %s\", node.getId(), node.getAvailableCpuResources(), (node.getTotalCpuResources() - cpuUsed), execs);\r\n            errors.add(err);\r\n        }\r\n        if (node.getAvailableMemoryResources() != (node.getTotalMemoryResources() - memoryUsed)) {\r\n            String err = String.format(\"Incorrect Memory Resources: Node %s Memory available is %,.2f, expected %,.2f, \" + \"Executors scheduled on node: %s\", node.getId(), node.getAvailableMemoryResources(), (node.getTotalMemoryResources() - memoryUsed), execs);\r\n            errors.add(err);\r\n        }\r\n    }\r\n    if (!errors.isEmpty()) {\r\n        LOG.error(\"Topology {} solution is invalid\\n\\t{}\", topo.getName(), String.join(\"\\n\\t\", errors));\r\n    }\r\n    return errors.isEmpty();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\NodeSorterHostProximity.java",
  "methodName" : "getRackIdToHosts",
  "sourceCode" : "@VisibleForTesting\r\npublic Map<String, Set<String>> getRackIdToHosts() {\r\n    return rackIdToHosts;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\security\\auth\\workertoken\\WorkerTokenManager.java",
  "methodName" : "shouldRenewWorkerToken",
  "sourceCode" : "@VisibleForTesting\r\npublic boolean shouldRenewWorkerToken(Map<String, String> creds, WorkerTokenServiceType type) {\r\n    boolean shouldAdd = true;\r\n    WorkerToken oldToken = ClientAuthUtils.readWorkerToken(creds, type);\r\n    if (oldToken != null) {\r\n        try {\r\n            WorkerTokenInfo info = ClientAuthUtils.getWorkerTokenInfo(oldToken);\r\n            if (!info.is_set_expirationTimeMillis() || info.get_expirationTimeMillis() - Time.currentTimeMillis() > (tokenLifetimeMillis / 2)) {\r\n                //Found an existing token and it is not going to expire any time soon, so don't bother adding in a new\r\n                // token.\r\n                shouldAdd = false;\r\n            }\r\n        } catch (Exception e) {\r\n            //The old token could not be deserialized.  This is bad, but we are going to replace it anyways so just keep going.\r\n            LOG.error(\"Could not deserialize token info\", e);\r\n        }\r\n    }\r\n    return shouldAdd;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\utils\\EquivalenceUtils.java",
  "methodName" : "customWorkerResourcesEquality",
  "sourceCode" : "/**\r\n * This method compares WorkerResources while considering any resources are NULL to be 0.0\r\n *\r\n * @param first  WorkerResources A\r\n * @param second WorkerResources B\r\n * @return True if A and B are equivalent, treating the absent resources as 0.0\r\n */\r\n@VisibleForTesting\r\nstatic boolean customWorkerResourcesEquality(WorkerResources first, WorkerResources second) {\r\n    if (first == null) {\r\n        return false;\r\n    }\r\n    if (second == null) {\r\n        return false;\r\n    }\r\n    if (first == second) {\r\n        return true;\r\n    }\r\n    if (first.equals(second)) {\r\n        return true;\r\n    }\r\n    if (first.get_cpu() != second.get_cpu()) {\r\n        return false;\r\n    }\r\n    if (first.get_mem_on_heap() != second.get_mem_on_heap()) {\r\n        return false;\r\n    }\r\n    if (first.get_mem_off_heap() != second.get_mem_off_heap()) {\r\n        return false;\r\n    }\r\n    if (first.get_shared_mem_off_heap() != second.get_shared_mem_off_heap()) {\r\n        return false;\r\n    }\r\n    if (first.get_shared_mem_on_heap() != second.get_shared_mem_on_heap()) {\r\n        return false;\r\n    }\r\n    if (!customResourceMapEquality(first.get_resources(), second.get_resources())) {\r\n        return false;\r\n    }\r\n    if (!customResourceMapEquality(first.get_shared_resources(), second.get_shared_resources())) {\r\n        return false;\r\n    }\r\n    return true;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\utils\\ServerUtils.java",
  "methodName" : "isAnyPosixProcessPidDirAlive",
  "sourceCode" : "/**\r\n * Find if the process is alive using the existence of /proc/&lt;pid&gt; directory\r\n * owned by the supplied expectedUser. This is an alternative to \"ps -p pid -u uid\" command\r\n * used in {@link #isAnyPosixProcessAlive(Collection, int)}\r\n *\r\n * <p>\r\n * Processes are tracked using the existence of the directory \"/proc/&lt;pid&gt;\r\n * For each of the supplied PIDs, their PID directory is checked for existence and ownership\r\n * by the specified uid.\r\n * </p>\r\n *\r\n * @param pids Process IDs that need to be monitored for liveness\r\n * @param expectedUser the userId that is expected to own that process\r\n * @param mockFileOwnerToUid if true (used for testing), then convert File.owner to UID\r\n * @return true if any one of the processes is owned by expectedUser and alive, else false\r\n * @throws IOException on I/O exception\r\n */\r\n@VisibleForTesting\r\npublic static boolean isAnyPosixProcessPidDirAlive(Collection<Long> pids, String expectedUser, boolean mockFileOwnerToUid) throws IOException {\r\n    File procDir = new File(\"/proc\");\r\n    if (!procDir.exists()) {\r\n        throw new IOException(\"Missing process directory \" + procDir.getAbsolutePath() + \": method not supported on \" + \"os.name=\" + System.getProperty(\"os.name\"));\r\n    }\r\n    for (long pid : pids) {\r\n        File pidDir = new File(procDir, String.valueOf(pid));\r\n        if (!pidDir.exists()) {\r\n            continue;\r\n        }\r\n        // check if existing process is owned by the specified expectedUser, if not, the process is dead\r\n        String actualUser;\r\n        try {\r\n            actualUser = Files.getOwner(pidDir.toPath()).getName();\r\n        } catch (NoSuchFileException ex) {\r\n            // process died before the expectedUser can be checked\r\n            continue;\r\n        }\r\n        if (mockFileOwnerToUid) {\r\n            // code activated in testing to simulate Files.getOwner returning UID (which sometimes happens in runtime)\r\n            if (StringUtils.isNumeric(actualUser)) {\r\n                LOG.info(\"Skip mocking, since owner {} of pidDir {} is already numeric\", actualUser, pidDir);\r\n            } else {\r\n                Integer actualUid = cachedUserToUidMap.get(actualUser);\r\n                if (actualUid == null) {\r\n                    actualUid = ServerUtils.getUserId(actualUser);\r\n                    if (actualUid < 0) {\r\n                        String err = String.format(\"Cannot get UID for %s, while mocking the owner of pidDir %s\", actualUser, pidDir.getAbsolutePath());\r\n                        throw new IOException(err);\r\n                    }\r\n                    cachedUserToUidMap.put(actualUser, actualUid);\r\n                    LOG.info(\"Found UID {} for {}, while mocking the owner of pidDir {}\", actualUid, actualUser, pidDir);\r\n                } else {\r\n                    LOG.info(\"Found cached UID {} for {}, while mocking the owner of pidDir {}\", actualUid, actualUser, pidDir);\r\n                }\r\n                actualUser = String.valueOf(actualUid);\r\n            }\r\n        }\r\n        //sometimes uid is returned instead of username - if so, try to convert and compare with uid\r\n        if (StringUtils.isNumeric(actualUser)) {\r\n            // numeric actualUser - this is UID not user\r\n            LOG.debug(\"Process directory {} owner is uid={}\", pidDir, actualUser);\r\n            int actualUid = Integer.parseInt(actualUser);\r\n            Integer expectedUid = cachedUserToUidMap.get(expectedUser);\r\n            if (expectedUid == null) {\r\n                expectedUid = ServerUtils.getUserId(expectedUser);\r\n                if (expectedUid < 0) {\r\n                    String err = String.format(\"Cannot get uid for %s to compare with owner id=%d of process directory %s\", expectedUser, actualUid, pidDir.getAbsolutePath());\r\n                    throw new IOException(err);\r\n                }\r\n                cachedUserToUidMap.put(expectedUser, expectedUid);\r\n            }\r\n            if (expectedUid == actualUid) {\r\n                LOG.debug(\"Process {} is alive and owned by expectedUser {}/{}\", pid, expectedUser, expectedUid);\r\n                return true;\r\n            }\r\n            LOG.info(\"Prior process is dead, since directory {} owner {} is not same as expectedUser {}/{}, \" + \"likely pid {} was reused for a new process for uid {}, {}\", pidDir, actualUser, expectedUser, expectedUid, pid, actualUid, getProcessDesc(pidDir));\r\n        } else {\r\n            // actualUser is a string\r\n            LOG.debug(\"Process directory {} owner is {}\", pidDir, actualUser);\r\n            if (expectedUser.equals(actualUser)) {\r\n                LOG.debug(\"Process {} is alive and owned by expectedUser {}\", pid, expectedUser);\r\n                return true;\r\n            }\r\n            LOG.info(\"Prior process is dead, since directory {} owner {} is not same as expectedUser {}, \" + \"likely pid {} was reused for a new process for actualUser {}, {}}\", pidDir, actualUser, expectedUser, pid, actualUser, getProcessDesc(pidDir));\r\n        }\r\n    }\r\n    LOG.info(\"None of the processes {} are alive AND owned by expectedUser {}\", pids, expectedUser);\r\n    return false;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\utils\\ServerUtils.java",
  "methodName" : "validateTopologyWorkerMaxHeapSizeConfigs",
  "sourceCode" : "@VisibleForTesting\r\npublic static void validateTopologyWorkerMaxHeapSizeConfigs(Map<String, Object> stormConf, StormTopology topology, double defaultWorkerMaxHeapSizeMb) throws InvalidTopologyException {\r\n    double largestMemReq = getMaxExecutorMemoryUsageForTopo(topology, stormConf);\r\n    double topologyWorkerMaxHeapSize = ObjectReader.getDouble(stormConf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB), defaultWorkerMaxHeapSizeMb);\r\n    if (topologyWorkerMaxHeapSize < largestMemReq) {\r\n        throw new InvalidTopologyException(\"Topology will not be able to be successfully scheduled: Config \" + \"TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB=\" + topologyWorkerMaxHeapSize + \" < \" + largestMemReq + \" (Largest memory requirement of a component in the topology).\" + \" Perhaps set TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB to a larger amount\");\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\main\\java\\org\\apache\\storm\\zookeeper\\LeaderElectorImp.java",
  "methodName" : "awaitLeadership",
  "sourceCode" : "@Override\r\n@VisibleForTesting\r\npublic boolean awaitLeadership(long timeout, TimeUnit timeUnit) throws InterruptedException {\r\n    return leaderLatch.get().await(timeout, timeUnit);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_nullNimbusInfo",
  "sourceCode" : "/**\r\n * If nimbusDetails are null, the method returns without any Zookeeper calls.\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_nullNimbusInfo() {\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, null);\r\n    zkClientBuilder.verifyExists(false);\r\n    zkClientBuilder.verifyGetChildren(false);\r\n    verify(nimbusDetails, never()).getHost();\r\n    verify(conf, never()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_missingNode",
  "sourceCode" : "/**\r\n * If the node doesn't exist, the method returns before attempting to fetch children.\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_missingNode() {\r\n    zkClientBuilder.withExists(BLOBSTORE_KEY, false);\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, nimbusDetails);\r\n    zkClientBuilder.verifyExists(true);\r\n    zkClientBuilder.verifyGetChildren(false);\r\n    verify(nimbusDetails, never()).getHost();\r\n    verify(conf, never()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_nodeWithNullChildren",
  "sourceCode" : "/**\r\n * If the node has null children, the method will exit before calling downloadUpdatedBlob\r\n * (the config map is first accessed by downloadUpdatedBlob).\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_nodeWithNullChildren() {\r\n    zkClientBuilder.withExists(BLOBSTORE_KEY, true);\r\n    zkClientBuilder.withGetChildren(BLOBSTORE_KEY, (List<String>) null);\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, nimbusDetails);\r\n    zkClientBuilder.verifyExists(true);\r\n    zkClientBuilder.verifyGetChildren();\r\n    verify(nimbusDetails, never()).getHost();\r\n    verify(conf, never()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_nodeWithEmptyChildren",
  "sourceCode" : "/**\r\n * If the node has no children, the method behaves the same as for null children.\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_nodeWithEmptyChildren() {\r\n    zkClientBuilder.withExists(BLOBSTORE_KEY, true);\r\n    zkClientBuilder.withGetChildren(BLOBSTORE_KEY);\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, nimbusDetails);\r\n    zkClientBuilder.verifyExists(true);\r\n    zkClientBuilder.verifyGetChildren();\r\n    verify(nimbusDetails, never()).getHost();\r\n    verify(conf, never()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_hostsMatch",
  "sourceCode" : "/**\r\n * If the node has children, their hostnames will be checked and if they match,\r\n * downloadUpdatedBlob will not be called.\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_hostsMatch() {\r\n    zkClientBuilder.withExists(BLOBSTORE_KEY, true);\r\n    zkClientBuilder.withGetChildren(BLOBSTORE_KEY, \"localhost:1111-1\");\r\n    when(nimbusDetails.getHost()).thenReturn(\"localhost\");\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, nimbusDetails);\r\n    zkClientBuilder.verifyExists(true);\r\n    zkClientBuilder.verifyGetChildren(2);\r\n    verify(nimbusDetails).getHost();\r\n    verify(conf, never()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\BlobStoreUtilsTest.java",
  "methodName" : "testUpdateKeyForBlobStore_noMatch",
  "sourceCode" : "/**\r\n * If the node has children, their hostnames will be checked and if they don't match,\r\n * downloadUpdatedBlob will be called.\r\n */\r\n@Test\r\npublic void testUpdateKeyForBlobStore_noMatch() {\r\n    zkClientBuilder.withExists(BLOBSTORE_KEY, true);\r\n    zkClientBuilder.withGetChildren(BLOBSTORE_KEY, \"localhost:1111-1\");\r\n    when(nimbusDetails.getHost()).thenReturn(\"no match\");\r\n    BlobStoreUtils.updateKeyForBlobStore(conf, blobStore, zkClientBuilder.build(), KEY, nimbusDetails);\r\n    zkClientBuilder.verifyExists(true);\r\n    zkClientBuilder.verifyGetChildren(2);\r\n    verify(nimbusDetails).getHost();\r\n    verify(conf, atLeastOnce()).get(anyString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreFileTest.java",
  "methodName" : "testGetVersion",
  "sourceCode" : "@Test\r\nvoid testGetVersion() throws IOException {\r\n    long expectedVersion = FileUtils.checksum(tempFile, checksumAlgorithm).getValue();\r\n    long actualVersion = blobStoreFile.getVersion();\r\n    assertEquals(expectedVersion, actualVersion, \"The version should match the expected checksum value.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreFileTest.java",
  "methodName" : "testGetVersion_Mismatch",
  "sourceCode" : "@Test\r\nvoid testGetVersion_Mismatch() throws IOException {\r\n    long expectedVersion = FileUtils.checksum(tempFile, checksumAlgorithm).getValue();\r\n    try (FileOutputStream fs = new FileOutputStream(tempFile)) {\r\n        fs.write(\"Different content\".getBytes());\r\n    }\r\n    long actualVersion = blobStoreFile.getVersion();\r\n    assertNotEquals(expectedVersion, actualVersion, \"The version shouldn't match the checksum value of different content.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreFileTest.java",
  "methodName" : "testGetModTime",
  "sourceCode" : "@Test\r\nvoid testGetModTime() throws IOException {\r\n    long expectedModTime = tempFile.lastModified();\r\n    long actualModTime = blobStoreFile.getModTime();\r\n    assertEquals(expectedModTime, actualModTime, \"The modification time should match the expected value.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreSynchronizerTest.java",
  "methodName" : "testBlobSynchronizerForKeysToDownload",
  "sourceCode" : "@Test\r\npublic void testBlobSynchronizerForKeysToDownload() {\r\n    BlobStore store = initLocalFs();\r\n    LocalFsBlobStoreSynchronizer sync = new LocalFsBlobStoreSynchronizer(store, conf);\r\n    // test for keylist to download\r\n    Set<String> zkSet = new HashSet<>();\r\n    zkSet.add(\"key1\");\r\n    Set<String> blobStoreSet = new HashSet<>();\r\n    blobStoreSet.add(\"key1\");\r\n    Set<String> resultSet = sync.getKeySetToDownload(blobStoreSet, zkSet);\r\n    assertTrue(resultSet.isEmpty(), \"Not Empty\");\r\n    zkSet.add(\"key1\");\r\n    blobStoreSet.add(\"key2\");\r\n    resultSet = sync.getKeySetToDownload(blobStoreSet, zkSet);\r\n    assertTrue(resultSet.isEmpty(), \"Not Empty\");\r\n    blobStoreSet.remove(\"key1\");\r\n    blobStoreSet.remove(\"key2\");\r\n    zkSet.add(\"key1\");\r\n    resultSet = sync.getKeySetToDownload(blobStoreSet, zkSet);\r\n    assertTrue((resultSet.size() == 1) && (resultSet.contains(\"key1\")), \"Unexpected keys to download\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreSynchronizerTest.java",
  "methodName" : "testGetLatestSequenceNumber",
  "sourceCode" : "@Test\r\npublic void testGetLatestSequenceNumber() {\r\n    List<String> stateInfoList = new ArrayList<>();\r\n    stateInfoList.add(\"nimbus1:8000-2\");\r\n    stateInfoList.add(\"nimbus-1:8000-4\");\r\n    assertEquals(4, BlobStoreUtils.getLatestSequenceNumber(stateInfoList), \"Failed to get the latest version\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreSynchronizerTest.java",
  "methodName" : "testNimbodesWithLatestVersionOfBlob",
  "sourceCode" : "@Test\r\npublic void testNimbodesWithLatestVersionOfBlob() throws Exception {\r\n    try (TestingServer server = new TestingServer();\r\n        CuratorFramework zkClient = CuratorFrameworkFactory.newClient(server.getConnectString(), new ExponentialBackoffRetry(1000, 3))) {\r\n        zkClient.start();\r\n        // Creating nimbus hosts containing the latest version of blob\r\n        zkClient.create().creatingParentContainersIfNeeded().forPath(\"/blobstore/key1/nimbus1:7800-1\");\r\n        zkClient.create().creatingParentContainersIfNeeded().forPath(\"/blobstore/key1/nimbus2:7800-2\");\r\n        Set<NimbusInfo> set = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, \"key1\");\r\n        assertEquals(\"nimbus2\", (set.iterator().next()).getHost(), \"Failed to get the correct nimbus hosts with latest blob version\");\r\n        zkClient.delete().deletingChildrenIfNeeded().forPath(\"/blobstore/key1/nimbus1:7800-1\");\r\n        zkClient.delete().deletingChildrenIfNeeded().forPath(\"/blobstore/key1/nimbus2:7800-2\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreSynchronizerTest.java",
  "methodName" : "testNormalizeVersionInfo",
  "sourceCode" : "@Test\r\npublic void testNormalizeVersionInfo() {\r\n    BlobKeySequenceInfo info1 = BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(\"nimbus1:7800-1\");\r\n    assertEquals(\"nimbus1:7800\", info1.getNimbusHostPort());\r\n    assertEquals(\"1\", info1.getSequenceNumber());\r\n    BlobKeySequenceInfo info2 = BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(\"nimbus-1:7800-1\");\r\n    assertEquals(\"nimbus-1:7800\", info2.getNimbusHostPort());\r\n    assertEquals(\"1\", info2.getSequenceNumber());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreTest.java",
  "methodName" : "testLocalFsWithAuth",
  "sourceCode" : "@Test\r\npublic void testLocalFsWithAuth() throws Exception {\r\n    testWithAuthentication(initLocalFs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreTest.java",
  "methodName" : "testBasicLocalFs",
  "sourceCode" : "@Test\r\npublic void testBasicLocalFs() throws Exception {\r\n    testBasic(initLocalFs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreTest.java",
  "methodName" : "testMultipleLocalFs",
  "sourceCode" : "@Test\r\npublic void testMultipleLocalFs() throws Exception {\r\n    testMultiple(initLocalFs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreTest.java",
  "methodName" : "testDeleteAfterFailedCreate",
  "sourceCode" : "@Test\r\npublic void testDeleteAfterFailedCreate() throws Exception {\r\n    //Check that a blob can be deleted when a temporary file exists in the blob directory\r\n    LocalFsBlobStore store = initLocalFs();\r\n    String key = \"test\";\r\n    SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING);\r\n    try (AtomicOutputStream out = store.createBlob(key, metadata, null)) {\r\n        out.write(1);\r\n        File blobDir = store.getKeyDataDir(key);\r\n        Files.createFile(blobDir.toPath().resolve(\"tempFile.tmp\"));\r\n    }\r\n    store.deleteBlob(\"test\", null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\blobstore\\LocalFsBlobStoreTest.java",
  "methodName" : "testGetFileLength",
  "sourceCode" : "@Test\r\npublic void testGetFileLength() throws AuthorizationException, KeyNotFoundException, KeyAlreadyExistsException, IOException {\r\n    LocalFsBlobStore store = initLocalFs();\r\n    try (AtomicOutputStream out = store.createBlob(\"test\", new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING), null)) {\r\n        out.write(1);\r\n    }\r\n    try (InputStreamWithMeta blobInputStream = store.getBlob(\"test\", null)) {\r\n        assertEquals(1, blobInputStream.getFileLength());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerExecCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"exec\", dockerExecCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerExecCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    dockerExecCommand.addExecCommand(Arrays.asList(\"ls\", \"-l\"));\r\n    assertEquals(\"exec container_name ls -l\", dockerExecCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerInspectCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"inspect\", dockerInspectCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerInspectCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    dockerInspectCommand.withGettingContainerStatus();\r\n    assertEquals(\"inspect --format='{{.State.Status}}' container_name\", dockerInspectCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerPsCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"ps\", dockerPsCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerPsCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    dockerPsCommand.withNameFilter(\"container_name\");\r\n    dockerPsCommand.withQuietOption();\r\n    assertEquals(\"ps --filter=name=container_name --quiet=true\", dockerPsCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerRmCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"rm\", dockerRmCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerRmCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    dockerRmCommand.withForce();\r\n    assertEquals(\"rm --force container_name\", dockerRmCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerRunCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    assertEquals(\"run\", dockerRunCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerRunCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() throws IOException {\r\n    String sourcePath = \"source\";\r\n    String destPath = \"dest\";\r\n    dockerRunCommand.detachOnRun().addReadWriteMountLocation(sourcePath, destPath);\r\n    List<String> commands = Arrays.asList(\"bash\", \"launch_command\");\r\n    dockerRunCommand.setOverrideCommandWithArgs(commands);\r\n    dockerRunCommand.removeContainerOnExit();\r\n    assertEquals(\"run --name=foo --user=user_id:group_id -d -v source:dest --rm \" + \"image_name bash launch_command\", dockerRunCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerStopCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"stop\", dockerStopCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerStopCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    dockerStopCommand.setGracePeriod(3);\r\n    assertEquals(\"stop --time=3 container_name\", dockerStopCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerWaitCommandTest.java",
  "methodName" : "getCommandOption",
  "sourceCode" : "@Test\r\npublic void getCommandOption() {\r\n    assertEquals(\"wait\", dockerWaitCommand.getCommandOption());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\docker\\DockerWaitCommandTest.java",
  "methodName" : "getCommandWithArguments",
  "sourceCode" : "@Test\r\npublic void getCommandWithArguments() {\r\n    assertEquals(\"wait container_name\", dockerWaitCommand.getCommandWithArguments());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "validateImageInDaemonConfSkipped",
  "sourceCode" : "@Test\r\npublic void validateImageInDaemonConfSkipped() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(DaemonConfig.STORM_OCI_IMAGE, \"storm/rhel7:dev_test\");\r\n    //this is essentially a no-op\r\n    OciUtils.validateImageInDaemonConf(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "validateImageInDaemonConfTest",
  "sourceCode" : "@Test\r\npublic void validateImageInDaemonConfTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    List<String> allowedImages = new ArrayList<>();\r\n    allowedImages.add(\"storm/rhel7:dev_test\");\r\n    allowedImages.add(\"storm/rhel7:dev_current\");\r\n    conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n    conf.put(DaemonConfig.STORM_OCI_IMAGE, \"storm/rhel7:dev_test\");\r\n    OciUtils.validateImageInDaemonConf(conf);\r\n    allowedImages.add(\"*\");\r\n    conf.put(DaemonConfig.STORM_OCI_IMAGE, \"storm/rhel7:wow\");\r\n    OciUtils.validateImageInDaemonConf(conf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "validateImageInDaemonConfNotInAllowedList",
  "sourceCode" : "@Test\r\npublic void validateImageInDaemonConfNotInAllowedList() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        List<String> allowedImages = new ArrayList<>();\r\n        allowedImages.add(\"storm/rhel7:dev_test\");\r\n        conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n        conf.put(DaemonConfig.STORM_OCI_IMAGE, \"storm/rhel7:wow\");\r\n        OciUtils.validateImageInDaemonConf(conf);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "validateImageInDaemonConfWithNullDefault",
  "sourceCode" : "@Test\r\npublic void validateImageInDaemonConfWithNullDefault() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        List<String> allowedImages = new ArrayList<>();\r\n        allowedImages.add(\"storm/rhel7:dev_test\");\r\n        conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n        //or not set\r\n        conf.put(DaemonConfig.STORM_OCI_IMAGE, null);\r\n        OciUtils.validateImageInDaemonConf(conf);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "validateImageInDaemonConfWrongPattern",
  "sourceCode" : "@Test\r\npublic void validateImageInDaemonConfWrongPattern() {\r\n    assertThrows(IllegalArgumentException.class, () -> {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        List<String> allowedImages = new ArrayList<>();\r\n        allowedImages.add(\"*\");\r\n        conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n        conf.put(DaemonConfig.STORM_OCI_IMAGE, \"a-strange@image-name\");\r\n        OciUtils.validateImageInDaemonConf(conf);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "adjustImageConfigForTopoTest",
  "sourceCode" : "@Test\r\npublic void adjustImageConfigForTopoTest() throws InvalidTopologyException {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    //or not set\r\n    conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, null);\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    String topoId = \"topo1\";\r\n    //case 1: nothing is not; nothing will happen\r\n    OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId);\r\n    String image1 = \"storm/rhel7:dev_test\";\r\n    String defaultImage = \"storm/rhel7:dev_current\";\r\n    //case 2: allowed list is not set; topology oci image will be set to null\r\n    topoConf.put(Config.TOPOLOGY_OCI_IMAGE, image1);\r\n    OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId);\r\n    assertNull(topoConf.get(Config.TOPOLOGY_OCI_IMAGE), Config.TOPOLOGY_OCI_IMAGE + \" is not removed\");\r\n    //set up daemon conf properly\r\n    List<String> allowedImages = new ArrayList<>();\r\n    allowedImages.add(image1);\r\n    allowedImages.add(defaultImage);\r\n    conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n    conf.put(DaemonConfig.STORM_OCI_IMAGE, defaultImage);\r\n    //case 3: configs are set properly; nothing will happen\r\n    topoConf.put(Config.TOPOLOGY_OCI_IMAGE, image1);\r\n    OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId);\r\n    assertEquals(image1, topoConf.get(Config.TOPOLOGY_OCI_IMAGE));\r\n    //case 4: topology oci image is not set; will be set to default image\r\n    topoConf.remove(Config.TOPOLOGY_OCI_IMAGE);\r\n    OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId);\r\n    assertEquals(defaultImage, topoConf.get(Config.TOPOLOGY_OCI_IMAGE));\r\n    //case 5: any topology oci image is allowed\r\n    allowedImages.add(\"*\");\r\n    String image2 = \"storm/rhel7:dev_wow\";\r\n    topoConf.put(Config.TOPOLOGY_OCI_IMAGE, image2);\r\n    OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId);\r\n    assertEquals(image2, topoConf.get(Config.TOPOLOGY_OCI_IMAGE));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\container\\oci\\OciUtilsTest.java",
  "methodName" : "adjustImageConfigForTopoNotInAllowedList",
  "sourceCode" : "@Test\r\npublic void adjustImageConfigForTopoNotInAllowedList() {\r\n    String image1 = \"storm/rhel7:dev_test\";\r\n    String image2 = \"storm/rhel7:dev_current\";\r\n    Map<String, Object> conf = new HashMap<>();\r\n    List<String> allowedImages = new ArrayList<>();\r\n    allowedImages.add(image1);\r\n    conf.put(DaemonConfig.STORM_OCI_ALLOWED_IMAGES, allowedImages);\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    String topoId = \"topo1\";\r\n    topoConf.put(Config.TOPOLOGY_OCI_IMAGE, image2);\r\n    assertThrows(WrappedInvalidTopologyException.class, () -> OciUtils.adjustImageConfigForTopo(conf, topoConf, topoId));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testGoodBlocking",
  "sourceCode" : "@Test\r\npublic void testGoodBlocking() throws Exception {\r\n    try (DRPC server = new DRPC(new StormMetricsRegistry(), null, 100)) {\r\n        Future<String> found = exec.submit(() -> server.executeBlocking(\"testing\", \"test\"));\r\n        DRPCRequest request = getNextAvailableRequest(server, \"testing\");\r\n        assertNotNull(request);\r\n        assertEquals(\"test\", request.get_func_args());\r\n        assertNotNull(request.get_request_id());\r\n        server.returnResult(request.get_request_id(), \"tested\");\r\n        String result = found.get(10, TimeUnit.MILLISECONDS);\r\n        assertEquals(\"tested\", result);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testFailedBlocking",
  "sourceCode" : "@Test\r\npublic void testFailedBlocking() throws Exception {\r\n    try (DRPC server = new DRPC(new StormMetricsRegistry(), null, 100)) {\r\n        Future<String> found = exec.submit(() -> server.executeBlocking(\"testing\", \"test\"));\r\n        DRPCRequest request = getNextAvailableRequest(server, \"testing\");\r\n        assertNotNull(request);\r\n        assertEquals(\"test\", request.get_func_args());\r\n        assertNotNull(request.get_request_id());\r\n        server.failRequest(request.get_request_id(), null);\r\n        try {\r\n            found.get(100, TimeUnit.MILLISECONDS);\r\n            fail(\"exec did not throw an exception\");\r\n        } catch (ExecutionException e) {\r\n            Throwable t = e.getCause();\r\n            assertTrue(t instanceof DRPCExecutionException);\r\n            //Don't know a better way to validate that it failed.\r\n            assertEquals(DRPCExceptionType.FAILED_REQUEST, ((DRPCExecutionException) t).get_type());\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testDequeueAfterTimeout",
  "sourceCode" : "@Test\r\npublic void testDequeueAfterTimeout() throws Exception {\r\n    long timeout = 1000;\r\n    try (DRPC server = new DRPC(new StormMetricsRegistry(), null, timeout)) {\r\n        long start = Time.currentTimeMillis();\r\n        try {\r\n            server.executeBlocking(\"testing\", \"test\");\r\n            fail(\"Should have timed out....\");\r\n        } catch (DRPCExecutionException e) {\r\n            long spent = Time.currentTimeMillis() - start;\r\n            assertTrue(spent < timeout * 2);\r\n            assertTrue(spent >= timeout);\r\n            assertEquals(DRPCExceptionType.SERVER_TIMEOUT, e.get_type());\r\n        }\r\n        DRPCRequest request = server.fetchRequest(\"testing\");\r\n        assertNotNull(request);\r\n        assertEquals(\"\", request.get_request_id());\r\n        assertEquals(\"\", request.get_func_args());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testDeny",
  "sourceCode" : "@Test\r\npublic void testDeny() {\r\n    try (DRPC server = new DRPC(new StormMetricsRegistry(), new DenyAuthorizer(), 100)) {\r\n        assertThrows(() -> server.executeBlocking(\"testing\", \"test\"), AuthorizationException.class);\r\n        assertThrows(() -> server.fetchRequest(\"testing\"), AuthorizationException.class);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testStrict",
  "sourceCode" : "@Test\r\npublic void testStrict() throws Exception {\r\n    ReqContext jt = new ReqContext(new Subject());\r\n    SingleUserPrincipal jumpTopo = new SingleUserPrincipal(\"jump_topo\");\r\n    jt.subject().getPrincipals().add(jumpTopo);\r\n    ReqContext jc = new ReqContext(new Subject());\r\n    SingleUserPrincipal jumpClient = new SingleUserPrincipal(\"jump_client\");\r\n    jc.subject().getPrincipals().add(jumpClient);\r\n    ReqContext other = new ReqContext(new Subject());\r\n    SingleUserPrincipal otherUser = new SingleUserPrincipal(\"other\");\r\n    other.subject().getPrincipals().add(otherUser);\r\n    Map<String, AclFunctionEntry> acl = new HashMap<>();\r\n    acl.put(\"jump\", new AclFunctionEntry(Collections.singletonList(jumpClient.getName()), jumpTopo.getName()));\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.DRPC_AUTHORIZER_ACL_STRICT, true);\r\n    conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, DefaultPrincipalToLocal.class.getName());\r\n    DRPCSimpleACLAuthorizer auth = new DRPCSimpleACLAuthorizer() {\r\n\r\n        @Override\r\n        protected Map<String, AclFunctionEntry> readAclFromConfig() {\r\n            return acl;\r\n        }\r\n    };\r\n    auth.prepare(conf);\r\n    //JUMP\r\n    DRPC.checkAuthorization(jt, auth, \"fetchRequest\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"fetchRequest\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"fetchRequest\", \"jump\"), AuthorizationException.class);\r\n    DRPC.checkAuthorization(jt, auth, \"result\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"result\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"result\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jt, auth, \"execute\", \"jump\"), AuthorizationException.class);\r\n    DRPC.checkAuthorization(jc, auth, \"execute\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"execute\", \"jump\"), AuthorizationException.class);\r\n    //not_jump (closed in strict mode)\r\n    assertThrows(() -> DRPC.checkAuthorization(jt, auth, \"fetchRequest\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"fetchRequest\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"fetchRequest\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jt, auth, \"result\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"result\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"result\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jt, auth, \"execute\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"execute\", \"not_jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"execute\", \"not_jump\"), AuthorizationException.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCTest.java",
  "methodName" : "testNotStrict",
  "sourceCode" : "@Test\r\npublic void testNotStrict() throws Exception {\r\n    ReqContext jt = new ReqContext(new Subject());\r\n    SingleUserPrincipal jumpTopo = new SingleUserPrincipal(\"jump_topo\");\r\n    jt.subject().getPrincipals().add(jumpTopo);\r\n    ReqContext jc = new ReqContext(new Subject());\r\n    SingleUserPrincipal jumpClient = new SingleUserPrincipal(\"jump_client\");\r\n    jc.subject().getPrincipals().add(jumpClient);\r\n    ReqContext other = new ReqContext(new Subject());\r\n    SingleUserPrincipal otherUser = new SingleUserPrincipal(\"other\");\r\n    other.subject().getPrincipals().add(otherUser);\r\n    Map<String, AclFunctionEntry> acl = new HashMap<>();\r\n    acl.put(\"jump\", new AclFunctionEntry(Collections.singletonList(jumpClient.getName()), jumpTopo.getName()));\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.DRPC_AUTHORIZER_ACL_STRICT, false);\r\n    conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, DefaultPrincipalToLocal.class.getName());\r\n    DRPCSimpleACLAuthorizer auth = new DRPCSimpleACLAuthorizer() {\r\n\r\n        @Override\r\n        protected Map<String, AclFunctionEntry> readAclFromConfig() {\r\n            return acl;\r\n        }\r\n    };\r\n    auth.prepare(conf);\r\n    //JUMP\r\n    DRPC.checkAuthorization(jt, auth, \"fetchRequest\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"fetchRequest\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"fetchRequest\", \"jump\"), AuthorizationException.class);\r\n    DRPC.checkAuthorization(jt, auth, \"result\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(jc, auth, \"result\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"result\", \"jump\"), AuthorizationException.class);\r\n    assertThrows(() -> DRPC.checkAuthorization(jt, auth, \"execute\", \"jump\"), AuthorizationException.class);\r\n    DRPC.checkAuthorization(jc, auth, \"execute\", \"jump\");\r\n    assertThrows(() -> DRPC.checkAuthorization(other, auth, \"execute\", \"jump\"), AuthorizationException.class);\r\n    //not_jump (open in not strict mode)\r\n    DRPC.checkAuthorization(jt, auth, \"fetchRequest\", \"not_jump\");\r\n    DRPC.checkAuthorization(jc, auth, \"fetchRequest\", \"not_jump\");\r\n    DRPC.checkAuthorization(other, auth, \"fetchRequest\", \"not_jump\");\r\n    DRPC.checkAuthorization(jt, auth, \"result\", \"not_jump\");\r\n    DRPC.checkAuthorization(jc, auth, \"result\", \"not_jump\");\r\n    DRPC.checkAuthorization(other, auth, \"result\", \"not_jump\");\r\n    DRPC.checkAuthorization(jt, auth, \"execute\", \"not_jump\");\r\n    DRPC.checkAuthorization(jc, auth, \"execute\", \"not_jump\");\r\n    DRPC.checkAuthorization(other, auth, \"execute\", \"not_jump\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\metrics\\MetricsUtilsTest.java",
  "methodName" : "getPreparableReporters",
  "sourceCode" : "@Test\r\npublic void getPreparableReporters() {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    List<PreparableReporter> reporters = MetricsUtils.getPreparableReporters(daemonConf);\r\n    assertEquals(1, reporters.size());\r\n    assertTrue(reporters.get(0) instanceof JmxPreparableReporter);\r\n    List<String> reporterPlugins = Arrays.asList(\"org.apache.storm.daemon.metrics.reporters.ConsolePreparableReporter\", \"org.apache.storm.daemon.metrics.reporters.CsvPreparableReporter\");\r\n    daemonConf.put(DaemonConfig.STORM_DAEMON_METRICS_REPORTER_PLUGINS, reporterPlugins);\r\n    reporters = MetricsUtils.getPreparableReporters(daemonConf);\r\n    assertEquals(2, reporters.size());\r\n    assertTrue(reporters.get(0) instanceof ConsolePreparableReporter);\r\n    assertTrue(reporters.get(1) instanceof CsvPreparableReporter);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\metrics\\MetricsUtilsTest.java",
  "methodName" : "getCsvLogDir",
  "sourceCode" : "@Test\r\npublic void getCsvLogDir() {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    String currentPath = new File(\"\").getAbsolutePath();\r\n    daemonConf.put(Config.STORM_LOCAL_DIR, currentPath);\r\n    File dir = new File(currentPath, \"csvmetrics\");\r\n    assertEquals(dir, MetricsUtils.getCsvLogDir(daemonConf));\r\n    daemonConf.put(DaemonConfig.STORM_DAEMON_METRICS_REPORTER_CSV_LOG_DIR, \"./\");\r\n    assertEquals(new File(\"./\"), MetricsUtils.getCsvLogDir(daemonConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\metrics\\MetricsUtilsTest.java",
  "methodName" : "getMetricsRateUnit",
  "sourceCode" : "@Test\r\npublic void getMetricsRateUnit() {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    assertNull(MetricsUtils.getMetricsRateUnit(daemonConf));\r\n    daemonConf.put(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_RATE_UNIT, \"SECONDS\");\r\n    assertEquals(TimeUnit.SECONDS, MetricsUtils.getMetricsRateUnit(daemonConf));\r\n    daemonConf.put(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_RATE_UNIT, \"MINUTES\");\r\n    assertEquals(TimeUnit.MINUTES, MetricsUtils.getMetricsRateUnit(daemonConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\metrics\\MetricsUtilsTest.java",
  "methodName" : "getMetricsDurationUnit",
  "sourceCode" : "@Test\r\npublic void getMetricsDurationUnit() {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    assertNull(MetricsUtils.getMetricsDurationUnit(daemonConf));\r\n    daemonConf.put(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_DURATION_UNIT, \"SECONDS\");\r\n    assertEquals(TimeUnit.SECONDS, MetricsUtils.getMetricsDurationUnit(daemonConf));\r\n    daemonConf.put(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_DURATION_UNIT, \"MINUTES\");\r\n    assertEquals(TimeUnit.MINUTES, MetricsUtils.getMetricsDurationUnit(daemonConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\metrics\\MetricsUtilsTest.java",
  "methodName" : "getMetricsReporterLocale",
  "sourceCode" : "@Test\r\npublic void getMetricsReporterLocale() {\r\n    Map<String, Object> daemonConf = new HashMap<>();\r\n    assertNull(MetricsUtils.getMetricsReporterLocale(daemonConf));\r\n    daemonConf.put(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_LOCALE, \"en-US\");\r\n    assertEquals(Locale.US, MetricsUtils.getMetricsReporterLocale(daemonConf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "testMemoryLoadLargerThanMaxHeapSize",
  "sourceCode" : "@Test\r\npublic void testMemoryLoadLargerThanMaxHeapSize() {\r\n    // Topology will not be able to be successfully scheduled: Config TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB=128.0 < 129.0,\r\n    // Largest memory requirement of a component in the topology).\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout1\", new TestWordSpout(), 4);\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config1 = new Config();\r\n    config1.put(Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN, \"org.apache.storm.networktopography.DefaultRackDNSToSwitchMapping\");\r\n    config1.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY, DefaultSchedulingPriorityStrategy.class.getName());\r\n    config1.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 10.0);\r\n    config1.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 0.0);\r\n    config1.put(Config.TOPOLOGY_PRIORITY, 0);\r\n    config1.put(Config.TOPOLOGY_SUBMITTER_USER, \"zhuo\");\r\n    config1.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 128.0);\r\n    config1.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 129.0);\r\n    Class[] strategyClasses = { DefaultResourceAwareStrategy.class, RoundRobinResourceAwareStrategy.class, GenericResourceAwareStrategyOld.class };\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        config1.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, strategyClassName);\r\n        try {\r\n            ServerUtils.validateTopologyWorkerMaxHeapSizeConfigs(config1, stormTopology1, 768.0);\r\n            fail(\"Expected exception not thrown when using Strategy \" + strategyClassName);\r\n        } catch (InvalidTopologyException e) {\r\n            //Expected...\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "uploadedBlobPersistsMinimumTime",
  "sourceCode" : "@Test\r\npublic void uploadedBlobPersistsMinimumTime() {\r\n    Set<String> idleTopologies = new HashSet<>();\r\n    idleTopologies.add(\"topology1\");\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(DaemonConfig.NIMBUS_TOPOLOGY_BLOBSTORE_DELETION_DELAY_MS, 300000);\r\n    try (Time.SimulatedTime ignored = new Time.SimulatedTime(null)) {\r\n        Set<String> toDelete = Nimbus.getExpiredTopologyIds(idleTopologies, conf);\r\n        assertTrue(toDelete.isEmpty());\r\n        Time.advanceTime(10 * 60 * 1000L);\r\n        toDelete = Nimbus.getExpiredTopologyIds(idleTopologies, conf);\r\n        assertTrue(toDelete.contains(\"topology1\"));\r\n        assertEquals(1, toDelete.size());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "validateNoTopoConfOverrides",
  "sourceCode" : "@Test\r\npublic void validateNoTopoConfOverrides() {\r\n    StormTopology topology = new StormTopology();\r\n    topology.set_spouts(new HashMap<>());\r\n    topology.set_bolts(new HashMap<>());\r\n    topology.set_state_spouts(new HashMap<>());\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(Config.STORM_MESSAGING_NETTY_AUTHENTICATION, false);\r\n    conf.put(Config.TOPOLOGY_WORKER_NIMBUS_THRIFT_CLIENT_USE_TLS, false);\r\n    conf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, \"a\");\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    topoConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, \"b\");\r\n    Map<String, Object> normalized = Nimbus.normalizeConf(conf, topoConf, topology);\r\n    assertNull(normalized.get(Config.STORM_WORKERS_ARTIFACTS_DIR));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "testCreateStateInZookeeper",
  "sourceCode" : "@Test\r\nvoid testCreateStateInZookeeper() throws TException {\r\n    nimbus.createStateInZookeeper(BLOB_FILE_KEY);\r\n    verify(stormClusterState).setupBlob(eq(BLOB_FILE_KEY), eq(nimbusInfo), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "testCreateStateInZookeeperWithoutLocalFsBlobStoreInstanceShouldNotCreate",
  "sourceCode" : "@Test\r\nvoid testCreateStateInZookeeperWithoutLocalFsBlobStoreInstanceShouldNotCreate() throws Exception {\r\n    BlobStore blobStore = mock(BlobStore.class);\r\n    Map<String, Object> conf = Map.of(DaemonConfig.NIMBUS_MONITOR_FREQ_SECS, 10);\r\n    nimbus = new Nimbus(conf, iNimbus, stormClusterState, nimbusInfo, blobStore, leaderElector, groupMapper, metricRegistry);\r\n    nimbus.createStateInZookeeper(BLOB_FILE_KEY);\r\n    verify(stormClusterState, never()).setupBlob(eq(BLOB_FILE_KEY), eq(nimbusInfo), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "testCreateStateInZookeeperWhenFailToSetupBlobWithRuntimeExceptionThrowsRuntimeException",
  "sourceCode" : "@Test\r\nvoid testCreateStateInZookeeperWhenFailToSetupBlobWithRuntimeExceptionThrowsRuntimeException() {\r\n    doThrow(new RuntimeException(\"Failed to setup blob\")).when(stormClusterState).setupBlob(eq(BLOB_FILE_KEY), eq(nimbusInfo), any());\r\n    assertThrows(RuntimeException.class, () -> nimbus.createStateInZookeeper(BLOB_FILE_KEY));\r\n    verify(stormClusterState).setupBlob(eq(BLOB_FILE_KEY), eq(nimbusInfo), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\nimbus\\NimbusTest.java",
  "methodName" : "testCreateStateInZookeeperWhenKeyNotFoundHandlesException",
  "sourceCode" : "@Test\r\nvoid testCreateStateInZookeeperWhenKeyNotFoundHandlesException() throws Exception {\r\n    try (MockedConstruction<KeySequenceNumber> keySequenceNumber = mockConstruction(KeySequenceNumber.class, (mock, context) -> when(mock.getKeySequenceNumber(any())).thenThrow(new KeyNotFoundException(\"Failed to setup blob\")))) {\r\n        nimbus.createStateInZookeeper(BLOB_FILE_KEY);\r\n        verify(keySequenceNumber.constructed().get(0)).getKeySequenceNumber(any());\r\n        verify(stormClusterState, never()).setupBlob(eq(BLOB_FILE_KEY), eq(nimbusInfo), any());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testCreateNewWorkerId",
  "sourceCode" : "@Test\r\npublic void testCreateNewWorkerId() throws Exception {\r\n    final String topoId = \"test_topology\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    Map<String, Object> superConf = new HashMap<>();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    LocalState ls = mock(LocalState.class);\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, null, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n    //null worker id means generate one...\r\n    assertNotNull(mc.workerId);\r\n    verify(ls).getApprovedWorkers();\r\n    Map<String, Integer> expectedNewState = new HashMap<>();\r\n    expectedNewState.put(mc.workerId, port);\r\n    verify(ls).setApprovedWorkers(expectedNewState);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testRecovery",
  "sourceCode" : "@Test\r\npublic void testRecovery() throws Exception {\r\n    final String topoId = \"test_topology\";\r\n    final String workerId = \"myWorker\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    Map<String, Integer> workerState = new HashMap<>();\r\n    workerState.put(workerId, port);\r\n    LocalState ls = mock(LocalState.class);\r\n    when(ls.getApprovedWorkers()).thenReturn(workerState);\r\n    Map<String, Object> superConf = new HashMap<>();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    MockBasicContainer mc = new MockBasicContainer(ContainerType.RECOVER_FULL, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, null, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n    assertEquals(workerId, mc.workerId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testRecoveryMiss",
  "sourceCode" : "@Test\r\npublic void testRecoveryMiss() throws Exception {\r\n    final String topoId = \"test_topology\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    Map<String, Integer> workerState = new HashMap<>();\r\n    workerState.put(\"somethingelse\", port + 1);\r\n    LocalState ls = mock(LocalState.class);\r\n    when(ls.getApprovedWorkers()).thenReturn(workerState);\r\n    try {\r\n        new MockBasicContainer(ContainerType.RECOVER_FULL, new HashMap<>(), \"SUPERVISOR\", supervisorPort, port, la, null, ls, null, new StormMetricsRegistry(), new HashMap<>(), null, \"profile\");\r\n        fail(\"Container recovered worker incorrectly\");\r\n    } catch (ContainerRecoveryException e) {\r\n        //Expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testCleanUp",
  "sourceCode" : "@Test\r\npublic void testCleanUp() throws Exception {\r\n    final String topoId = \"test_topology\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String workerId = \"worker-id\";\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    Map<String, Object> superConf = new HashMap<>();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    Map<String, Integer> workerState = new HashMap<>();\r\n    workerState.put(workerId, port);\r\n    LocalState ls = mock(LocalState.class);\r\n    when(ls.getApprovedWorkers()).thenReturn(new HashMap<>(workerState));\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n    mc.cleanUp();\r\n    assertNull(mc.workerId);\r\n    verify(ls).getApprovedWorkers();\r\n    Map<String, Integer> expectedNewState = new HashMap<>();\r\n    verify(ls).setApprovedWorkers(expectedNewState);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testRunProfiling",
  "sourceCode" : "@Test\r\npublic void testRunProfiling() throws Exception {\r\n    final long pid = 100;\r\n    final String topoId = \"test_topology\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String workerId = \"worker-id\";\r\n    final String stormLocal = ContainerTest.asAbsPath(\"tmp\", \"testing\");\r\n    final String topoRoot = ContainerTest.asAbsPath(stormLocal, topoId, String.valueOf(port));\r\n    final File workerArtifactsPid = ContainerTest.asAbsFile(topoRoot, \"worker.pid\");\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.slurpString(workerArtifactsPid)).thenReturn(String.valueOf(pid));\r\n    LocalState ls = mock(LocalState.class);\r\n    MockResourceIsolationManager iso = new MockResourceIsolationManager();\r\n    MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n    //HEAP DUMP\r\n    ProfileRequest req = new ProfileRequest();\r\n    req.set_action(ProfileAction.JMAP_DUMP);\r\n    mc.runProfiling(req, false);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    CommandRun cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"jmap\", topoRoot), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n    // JSTACK DUMP\r\n    req.set_action(ProfileAction.JSTACK_DUMP);\r\n    mc.runProfiling(req, false);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"jstack\", topoRoot), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n    //RESTART\r\n    req.set_action(ProfileAction.JVM_RESTART);\r\n    mc.runProfiling(req, false);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"kill\"), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n    //JPROFILE DUMP\r\n    req.set_action(ProfileAction.JPROFILE_DUMP);\r\n    mc.runProfiling(req, false);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"dump\", topoRoot), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n    //JPROFILE START\r\n    req.set_action(ProfileAction.JPROFILE_STOP);\r\n    mc.runProfiling(req, false);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"start\"), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n    //JPROFILE STOP\r\n    req.set_action(ProfileAction.JPROFILE_STOP);\r\n    mc.runProfiling(req, true);\r\n    assertEquals(1, iso.profileCmds.size());\r\n    cmd = iso.profileCmds.get(0);\r\n    iso.profileCmds.clear();\r\n    assertEquals(Arrays.asList(\"profile\", String.valueOf(pid), \"stop\", topoRoot), cmd.cmd);\r\n    assertEquals(new File(topoRoot), cmd.pwd);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testLaunch",
  "sourceCode" : "@Test\r\npublic void testLaunch() throws Exception {\r\n    final String topoId = \"test_topology_current\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String stormHome = ContainerTest.asAbsPath(\"tmp\", \"storm-home\");\r\n    final String stormLogDir = ContainerTest.asFile(\".\", \"target\").getCanonicalPath();\r\n    final String workerId = \"worker-id\";\r\n    final String stormLocal = ContainerTest.asAbsPath(\"tmp\", \"storm-local\");\r\n    final String distRoot = ContainerTest.asAbsPath(stormLocal, \"supervisor\", \"stormdist\", topoId);\r\n    final File stormcode = new File(distRoot, \"stormcode.ser\");\r\n    final File stormjar = new File(distRoot, \"stormjar.jar\");\r\n    final String log4jdir = ContainerTest.asAbsPath(stormHome, \"conf\");\r\n    final String workerConf = ContainerTest.asAbsPath(log4jdir, \"worker.xml\");\r\n    final String workerRoot = ContainerTest.asAbsPath(stormLocal, \"workers\", workerId);\r\n    final String workerTmpDir = ContainerTest.asAbsPath(workerRoot, \"tmp\");\r\n    final StormTopology st = new StormTopology();\r\n    st.set_spouts(new HashMap<>());\r\n    st.set_bolts(new HashMap<>());\r\n    st.set_state_spouts(new HashMap<>());\r\n    byte[] serializedState = Utils.gzip(Utils.thriftSerialize(st));\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    superConf.put(DaemonConfig.STORM_LOG4J2_CONF_DIR, log4jdir);\r\n    superConf.put(Config.WORKER_CHILDOPTS, \" -Dtesting=true\");\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.slurp(stormcode)).thenReturn(serializedState);\r\n    LocalState ls = mock(LocalState.class);\r\n    MockResourceIsolationManager iso = new MockResourceIsolationManager();\r\n    checkpoint(() -> {\r\n        MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n        mc.launch();\r\n        assertEquals(1, iso.workerCmds.size());\r\n        CommandRun cmd = iso.workerCmds.get(0);\r\n        iso.workerCmds.clear();\r\n        assertListEquals(Arrays.asList(\"java\", \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"org.apache.storm.LogWriter\", \"java\", \"-server\", \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"-Dtesting=true\", \"-Djava.library.path=JLP\", \"-Dstorm.conf.file=\", \"-Dstorm.options=\", \"-Djava.io.tmpdir=\" + workerTmpDir, \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"org.apache.storm.daemon.worker.Worker\", topoId, \"SUPERVISOR\", String.valueOf(supervisorPort), String.valueOf(port), workerId), cmd.cmd);\r\n        assertEquals(new File(workerRoot), cmd.pwd);\r\n    }, ConfigUtils.STORM_HOME, stormHome, \"storm.log.dir\", stormLogDir);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testLaunchStorm1version",
  "sourceCode" : "@Test\r\npublic void testLaunchStorm1version() throws Exception {\r\n    final String topoId = \"test_topology_storm_1.x\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String stormHome = ContainerTest.asAbsPath(\"tmp\", \"storm-home\");\r\n    final String stormLogDir = ContainerTest.asFile(\".\", \"target\").getCanonicalPath();\r\n    final String workerId = \"worker-id\";\r\n    final String stormLocal = ContainerTest.asAbsPath(\"tmp\", \"storm-local\");\r\n    final String distRoot = ContainerTest.asAbsPath(stormLocal, \"supervisor\", \"stormdist\", topoId);\r\n    final File stormcode = new File(distRoot, \"stormcode.ser\");\r\n    final File stormjar = new File(distRoot, \"stormjar.jar\");\r\n    final String log4jdir = ContainerTest.asAbsPath(stormHome, \"conf\");\r\n    final String workerConf = ContainerTest.asAbsPath(log4jdir, \"worker.xml\");\r\n    final String workerRoot = ContainerTest.asAbsPath(stormLocal, \"workers\", workerId);\r\n    final String workerTmpDir = ContainerTest.asAbsPath(workerRoot, \"tmp\");\r\n    final StormTopology st = new StormTopology();\r\n    st.set_spouts(new HashMap<>());\r\n    st.set_bolts(new HashMap<>());\r\n    st.set_state_spouts(new HashMap<>());\r\n    // minimum 1.x version of supporting STORM-2448 would be 1.0.4\r\n    st.set_storm_version(\"1.0.4\");\r\n    byte[] serializedState = Utils.gzip(Utils.thriftSerialize(st));\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    superConf.put(DaemonConfig.STORM_LOG4J2_CONF_DIR, log4jdir);\r\n    superConf.put(Config.WORKER_CHILDOPTS, \" -Dtesting=true\");\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.slurp(stormcode)).thenReturn(serializedState);\r\n    LocalState ls = mock(LocalState.class);\r\n    MockResourceIsolationManager iso = new MockResourceIsolationManager();\r\n    checkpoint(() -> {\r\n        MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n        mc.launch();\r\n        assertEquals(1, iso.workerCmds.size());\r\n        CommandRun cmd = iso.workerCmds.get(0);\r\n        iso.workerCmds.clear();\r\n        assertListEquals(Arrays.asList(\"java\", \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"org.apache.storm.LogWriter\", \"java\", \"-server\", \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"-Dtesting=true\", \"-Djava.library.path=JLP\", \"-Dstorm.conf.file=\", \"-Dstorm.options=\", \"-Djava.io.tmpdir=\" + workerTmpDir, \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"org.apache.storm.daemon.worker\", topoId, \"SUPERVISOR\", String.valueOf(port), workerId), cmd.cmd);\r\n        assertEquals(new File(workerRoot), cmd.pwd);\r\n    }, ConfigUtils.STORM_HOME, stormHome, \"storm.log.dir\", stormLogDir);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testLaunchStorm0version",
  "sourceCode" : "@Test\r\npublic void testLaunchStorm0version() throws Exception {\r\n    final String topoId = \"test_topology_storm_0.x\";\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String stormHome = ContainerTest.asAbsPath(\"tmp\", \"storm-home\");\r\n    final String stormLogDir = ContainerTest.asFile(\".\", \"target\").getCanonicalPath();\r\n    final String workerId = \"worker-id\";\r\n    final String stormLocal = ContainerTest.asAbsPath(\"tmp\", \"storm-local\");\r\n    final String distRoot = ContainerTest.asAbsPath(stormLocal, \"supervisor\", \"stormdist\", topoId);\r\n    final File stormcode = new File(distRoot, \"stormcode.ser\");\r\n    final File stormjar = new File(distRoot, \"stormjar.jar\");\r\n    final String log4jdir = ContainerTest.asAbsPath(stormHome, \"conf\");\r\n    final String workerConf = ContainerTest.asAbsPath(log4jdir, \"worker.xml\");\r\n    final String workerRoot = ContainerTest.asAbsPath(stormLocal, \"workers\", workerId);\r\n    final String workerTmpDir = ContainerTest.asAbsPath(workerRoot, \"tmp\");\r\n    final StormTopology st = new StormTopology();\r\n    st.set_spouts(new HashMap<>());\r\n    st.set_bolts(new HashMap<>());\r\n    st.set_state_spouts(new HashMap<>());\r\n    // minimum 0.x version of supporting STORM-2448 would be 0.10.3\r\n    st.set_storm_version(\"0.10.3\");\r\n    byte[] serializedState = Utils.gzip(Utils.thriftSerialize(st));\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    superConf.put(DaemonConfig.STORM_LOG4J2_CONF_DIR, log4jdir);\r\n    superConf.put(Config.WORKER_CHILDOPTS, \" -Dtesting=true\");\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.slurp(stormcode)).thenReturn(serializedState);\r\n    LocalState ls = mock(LocalState.class);\r\n    MockResourceIsolationManager iso = new MockResourceIsolationManager();\r\n    checkpoint(() -> {\r\n        MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n        mc.launch();\r\n        assertEquals(1, iso.workerCmds.size());\r\n        CommandRun cmd = iso.workerCmds.get(0);\r\n        iso.workerCmds.clear();\r\n        assertListEquals(Arrays.asList(\"java\", \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"backtype.storm.LogWriter\", \"java\", \"-server\", \"-Dlogging.sensitivity=S3\", \"-Dlogfile.name=worker.log\", \"-Dstorm.home=\" + stormHome, \"-Dworkers.artifacts=\" + stormLocal, \"-Dstorm.id=\" + topoId, \"-Dworker.id=\" + workerId, \"-Dworker.port=\" + port, \"-Dstorm.log.dir=\" + stormLogDir, \"-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector\", \"-Dstorm.local.dir=\" + stormLocal, \"-Dworker.memory_limit_mb=768\", \"-Dlog4j.configurationFile=\" + workerConf, \"-Dtesting=true\", \"-Djava.library.path=JLP\", \"-Dstorm.conf.file=\", \"-Dstorm.options=\", \"-Djava.io.tmpdir=\" + workerTmpDir, \"-cp\", \"FRAMEWORK_CP:\" + stormjar.getAbsolutePath(), \"backtype.storm.daemon.worker\", topoId, \"SUPERVISOR\", String.valueOf(port), workerId), cmd.cmd);\r\n        assertEquals(new File(workerRoot), cmd.pwd);\r\n    }, ConfigUtils.STORM_HOME, stormHome, \"storm.log.dir\", stormLogDir);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\BasicContainerTest.java",
  "methodName" : "testSubstChildOpts",
  "sourceCode" : "@Test\r\npublic void testSubstChildOpts() throws Exception {\r\n    String workerId = \"w-01\";\r\n    String topoId = \"s-01\";\r\n    int supervisorPort = 6628;\r\n    int port = 9999;\r\n    int memOnheap = 512;\r\n    int memOffheap = 256;\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    Map<String, Object> superConf = new HashMap<>();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    LocalState ls = mock(LocalState.class);\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    MockBasicContainer mc = new MockBasicContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, ls, workerId, new StormMetricsRegistry(), new HashMap<>(), ops, \"profile\");\r\n    assertListEquals(Arrays.asList(\"-Xloggc:/tmp/storm/logs/gc.worker-9999-s-01-w-01-9999.log\", \"-Xms256m\", \"-Xmx512m\", \"-XX:MaxDirectMemorySize=256m\"), mc.substituteChildopts(\"-Xloggc:/tmp/storm/logs/gc.worker-%ID%-%TOPOLOGY-ID%-%WORKER-ID%-%WORKER-PORT%.log -Xms256m -Xmx%HEAP-MEM%m -XX:MaxDirectMemorySize=%OFF-HEAP-MEM%m\", memOnheap, memOffheap));\r\n    assertListEquals(Arrays.asList(\"-Xloggc:/tmp/storm/logs/gc.worker-9999-s-01-w-01-9999.log\", \"-Xms256m\", \"-Xmx512m\"), mc.substituteChildopts(Arrays.asList(\"-Xloggc:/tmp/storm/logs/gc.worker-%ID%-%TOPOLOGY-ID%-%WORKER-ID%-%WORKER-PORT%.log\", \"-Xms256m\", \"-Xmx%HEAP-MEM%m\"), memOnheap, memOffheap));\r\n    assertListEquals(Collections.emptyList(), mc.substituteChildopts(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\ContainerTest.java",
  "methodName" : "testKill",
  "sourceCode" : "@Test\r\npublic void testKill() throws Exception {\r\n    final String topoId = \"test_topology\";\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    MockResourceIsolationManager iso = new MockResourceIsolationManager();\r\n    String workerId = \"worker-id\";\r\n    MockContainer mc = new MockContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", 6628, 8080, la, iso, workerId, new HashMap<>(), ops, new StormMetricsRegistry());\r\n    iso.allWorkerIds.add(workerId);\r\n    assertEquals(Collections.EMPTY_LIST, iso.killedWorkerIds);\r\n    assertEquals(Collections.EMPTY_LIST, iso.forceKilledWorkerIds);\r\n    mc.kill();\r\n    assertEquals(iso.allWorkerIds, iso.killedWorkerIds);\r\n    assertEquals(Collections.EMPTY_LIST, iso.forceKilledWorkerIds);\r\n    iso.killedWorkerIds.clear();\r\n    mc.forceKill();\r\n    assertEquals(Collections.EMPTY_LIST, iso.killedWorkerIds);\r\n    assertEquals(iso.allWorkerIds, iso.forceKilledWorkerIds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\ContainerTest.java",
  "methodName" : "testSetup",
  "sourceCode" : "@Test\r\npublic void testSetup() throws Exception {\r\n    final int port = 8080;\r\n    final String topoId = \"test_topology\";\r\n    final String workerId = \"worker_id\";\r\n    final String user = \"me\";\r\n    final String stormLocal = asAbsPath(\"tmp\", \"testing\");\r\n    final File workerArtifacts = asAbsFile(stormLocal, topoId, String.valueOf(port));\r\n    final File logMetadataFile = new File(workerArtifacts, \"worker.yaml\");\r\n    final File workerUserFile = asAbsFile(stormLocal, \"workers-users\", workerId);\r\n    final File workerRoot = asAbsFile(stormLocal, \"workers\", workerId);\r\n    final File distRoot = asAbsFile(stormLocal, \"supervisor\", \"stormdist\", topoId);\r\n    final Map<String, Object> topoConf = new HashMap<>();\r\n    final List<String> topoUsers = Arrays.asList(\"t-user-a\", \"t-user-b\");\r\n    final List<String> logUsers = Arrays.asList(\"l-user-a\", \"l-user-b\");\r\n    final List<String> topoGroups = Arrays.asList(\"t-group-a\", \"t-group-b\");\r\n    final List<String> logGroups = Arrays.asList(\"l-group-a\", \"l-group-b\");\r\n    topoConf.put(DaemonConfig.LOGS_GROUPS, logGroups);\r\n    topoConf.put(Config.TOPOLOGY_GROUPS, topoGroups);\r\n    topoConf.put(DaemonConfig.LOGS_USERS, logUsers);\r\n    topoConf.put(Config.TOPOLOGY_USERS, topoUsers);\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    final StringWriter yamlDump = new StringWriter();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.fileExists(workerArtifacts)).thenReturn(true);\r\n    when(ops.fileExists(workerRoot)).thenReturn(true);\r\n    when(ops.getWriter(logMetadataFile)).thenReturn(yamlDump);\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_topology_id(topoId);\r\n    la.set_owner(user);\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    MockContainer mc = new MockContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", 6628, 8080, la, iso, workerId, topoConf, ops, new StormMetricsRegistry());\r\n    mc.setup();\r\n    //Initial Setup\r\n    verify(ops).forceMkdir(new File(workerRoot, \"pids\"));\r\n    verify(ops).forceMkdir(new File(workerRoot, \"tmp\"));\r\n    verify(ops).forceMkdir(new File(workerRoot, \"heartbeats\"));\r\n    verify(ops).fileExists(workerArtifacts);\r\n    //Log file permissions\r\n    verify(ops).getWriter(logMetadataFile);\r\n    String yamlResult = yamlDump.toString();\r\n    Yaml yaml = new Yaml();\r\n    Map<String, Object> result = yaml.load(yamlResult);\r\n    assertEquals(workerId, result.get(\"worker-id\"));\r\n    assertEquals(user, result.get(Config.TOPOLOGY_SUBMITTER_USER));\r\n    HashSet<String> allowedUsers = new HashSet<>(topoUsers);\r\n    allowedUsers.addAll(logUsers);\r\n    assertEquals(allowedUsers, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_USERS))));\r\n    HashSet<String> allowedGroups = new HashSet<>(topoGroups);\r\n    allowedGroups.addAll(logGroups);\r\n    assertEquals(allowedGroups, new HashSet<>(ObjectReader.getStrings(result.get(DaemonConfig.LOGS_GROUPS))));\r\n    //Save the current user to help with recovery\r\n    verify(ops).dump(workerUserFile, user);\r\n    //Create links to artifacts dir\r\n    verify(ops).createSymlink(new File(workerRoot, \"artifacts\"), workerArtifacts);\r\n    //Create links to blobs\r\n    verify(ops, never()).createSymlink(new File(workerRoot, \"resources\"), new File(distRoot, \"resources\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\ContainerTest.java",
  "methodName" : "testCleanup",
  "sourceCode" : "@Test\r\npublic void testCleanup() throws Exception {\r\n    final int supervisorPort = 6628;\r\n    final int port = 8080;\r\n    final String topoId = \"test_topology\";\r\n    final String workerId = \"worker_id\";\r\n    final String user = \"me\";\r\n    final String stormLocal = asAbsPath(\"tmp\", \"testing\");\r\n    final File workerArtifacts = asAbsFile(stormLocal, topoId, String.valueOf(port));\r\n    final File logMetadataFile = new File(workerArtifacts, \"worker.yaml\");\r\n    final File workerUserFile = asAbsFile(stormLocal, \"workers-users\", workerId);\r\n    final File workerRoot = asAbsFile(stormLocal, \"workers\", workerId);\r\n    final Map<String, Object> topoConf = new HashMap<>();\r\n    final Map<String, Object> superConf = new HashMap<>();\r\n    superConf.put(Config.STORM_LOCAL_DIR, stormLocal);\r\n    superConf.put(Config.STORM_WORKERS_ARTIFACTS_DIR, stormLocal);\r\n    final StringWriter yamlDump = new StringWriter();\r\n    AdvancedFSOps ops = mock(AdvancedFSOps.class);\r\n    when(ops.doRequiredTopoFilesExist(superConf, topoId)).thenReturn(true);\r\n    when(ops.fileExists(workerArtifacts)).thenReturn(true);\r\n    when(ops.fileExists(workerRoot)).thenReturn(true);\r\n    when(ops.getWriter(logMetadataFile)).thenReturn(yamlDump);\r\n    ResourceIsolationInterface iso = mock(ResourceIsolationInterface.class);\r\n    when(iso.isResourceManaged()).thenReturn(true);\r\n    LocalAssignment la = new LocalAssignment();\r\n    la.set_owner(user);\r\n    la.set_topology_id(topoId);\r\n    MockContainer mc = new MockContainer(ContainerType.LAUNCH, superConf, \"SUPERVISOR\", supervisorPort, port, la, iso, workerId, topoConf, ops, new StormMetricsRegistry());\r\n    mc.cleanUp();\r\n    verify(iso).cleanup(user, workerId, port);\r\n    verify(ops).deleteIfExists(eq(new File(workerRoot, \"pids\")), eq(user), anyString());\r\n    verify(ops).deleteIfExists(eq(new File(workerRoot, \"tmp\")), eq(user), anyString());\r\n    verify(ops).deleteIfExists(eq(new File(workerRoot, \"heartbeats\")), eq(user), anyString());\r\n    verify(ops).deleteIfExists(eq(workerRoot), eq(user), anyString());\r\n    verify(ops).deleteIfExists(workerUserFile);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testForSameTopology",
  "sourceCode" : "@Test\r\npublic void testForSameTopology() {\r\n    LocalAssignment a = mkLocalAssignment(\"A\", mkExecutorInfoList(1, 2, 3, 4, 5), mkWorkerResources(100.0, 100.0, 100.0));\r\n    LocalAssignment aResized = mkLocalAssignment(\"A\", mkExecutorInfoList(1, 2, 3, 4, 5), mkWorkerResources(100.0, 200.0, 100.0));\r\n    LocalAssignment b = mkLocalAssignment(\"B\", mkExecutorInfoList(1, 2, 3, 4, 5, 6), mkWorkerResources(100.0, 100.0, 100.0));\r\n    LocalAssignment bReordered = mkLocalAssignment(\"B\", mkExecutorInfoList(6, 5, 4, 3, 2, 1), mkWorkerResources(100.0, 100.0, 100.0));\r\n    assertTrue(Slot.forSameTopology(null, null));\r\n    assertTrue(Slot.forSameTopology(a, a));\r\n    assertTrue(Slot.forSameTopology(a, aResized));\r\n    assertTrue(Slot.forSameTopology(aResized, a));\r\n    assertTrue(Slot.forSameTopology(b, bReordered));\r\n    assertTrue(Slot.forSameTopology(bReordered, b));\r\n    assertFalse(Slot.forSameTopology(a, null));\r\n    assertFalse(Slot.forSameTopology(null, b));\r\n    assertFalse(Slot.forSameTopology(a, b));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testEmptyToEmpty",
  "sourceCode" : "@Test\r\npublic void testEmptyToEmpty() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        LocalState state = mock(LocalState.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 1000, 1000, 1000, 1000, containerLauncher, \"localhost\", 8080, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(null, null, null, slotMetrics);\r\n        DynamicState nextState = Slot.handleEmpty(dynamicState, staticState);\r\n        assertEquals(MachineState.EMPTY, nextState.state);\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testLaunchContainerFromEmpty",
  "sourceCode" : "@Test\r\npublic void testLaunchContainerFromEmpty() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String topoId = \"NEW\";\r\n        List<ExecutorInfo> execList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment newAssignment = mkLocalAssignment(topoId, execList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        Container container = mock(Container.class);\r\n        LocalState state = mock(LocalState.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        when(containerLauncher.launchContainer(port, newAssignment, state)).thenReturn(container);\r\n        LSWorkerHeartbeat hb = mkWorkerHB(topoId, port, execList, Time.currentTimeSecs());\r\n        when(container.readHeartbeat()).thenReturn(hb, hb);\r\n        @SuppressWarnings(\"unchecked\")\r\n        CompletableFuture<Void> blobFuture = mock(CompletableFuture.class);\r\n        when(localizer.requestDownloadTopologyBlobs(newAssignment, port, cb)).thenReturn(blobFuture);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(null, null, null, slotMetrics).withNewAssignment(newAssignment);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        verify(localizer).requestDownloadTopologyBlobs(newAssignment, port, cb);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_LOCALIZATION, nextState.state);\r\n        assertSame(blobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(newAssignment, nextState.pendingLocalization);\r\n        assertEquals(0, Time.currentTimeMillis());\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        verify(blobFuture).get(1000, TimeUnit.MILLISECONDS);\r\n        verify(containerLauncher).launchContainer(port, newAssignment, state);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(newAssignment, nextState.currentAssignment);\r\n        assertSame(container, nextState.container);\r\n        assertEquals(0, Time.currentTimeMillis());\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(newAssignment, nextState.currentAssignment);\r\n        assertSame(container, nextState.container);\r\n        assertEquals(0, Time.currentTimeMillis());\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(newAssignment, nextState.currentAssignment);\r\n        assertSame(container, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(newAssignment, nextState.currentAssignment);\r\n        assertSame(container, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testErrorHandlingWhenLocalizationFails",
  "sourceCode" : "@Test\r\npublic void testErrorHandlingWhenLocalizationFails() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String topoId = \"NEW\";\r\n        List<ExecutorInfo> execList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment newAssignment = mkLocalAssignment(topoId, execList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        Container container = mock(Container.class);\r\n        LocalState state = mock(LocalState.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        when(containerLauncher.launchContainer(port, newAssignment, state)).thenReturn(container);\r\n        LSWorkerHeartbeat hb = mkWorkerHB(topoId, port, execList, Time.currentTimeSecs());\r\n        when(container.readHeartbeat()).thenReturn(hb, hb);\r\n        @SuppressWarnings(\"unchecked\")\r\n        CompletableFuture<Void> blobFuture = mock(CompletableFuture.class);\r\n        CompletableFuture<Void> secondBlobFuture = mock(CompletableFuture.class);\r\n        when(secondBlobFuture.get(anyLong(), any())).thenThrow(new ExecutionException(new RuntimeException(\"Localization failure\")));\r\n        CompletableFuture<Void> thirdBlobFuture = mock(CompletableFuture.class);\r\n        when(localizer.requestDownloadTopologyBlobs(newAssignment, port, cb)).thenReturn(blobFuture).thenReturn(secondBlobFuture).thenReturn(thirdBlobFuture);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(null, null, null, slotMetrics).withNewAssignment(newAssignment);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        verify(localizer).requestDownloadTopologyBlobs(newAssignment, port, cb);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_LOCALIZATION, nextState.state);\r\n        assertSame(blobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(newAssignment, nextState.pendingLocalization);\r\n        assertEquals(0, Time.currentTimeMillis());\r\n        //Assignment has changed\r\n        nextState = Slot.stateMachineStep(nextState.withNewAssignment(null), staticState);\r\n        assertThat(nextState.state, is(MachineState.EMPTY));\r\n        assertThat(nextState.pendingChangingBlobs, is(Collections.emptySet()));\r\n        assertThat(nextState.pendingChangingBlobsAssignment, nullValue());\r\n        assertThat(nextState.pendingLocalization, nullValue());\r\n        assertThat(nextState.pendingDownload, nullValue());\r\n        clearInvocations(localizer);\r\n        nextState = Slot.stateMachineStep(dynamicState.withNewAssignment(newAssignment), staticState);\r\n        verify(localizer).requestDownloadTopologyBlobs(newAssignment, port, cb);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_LOCALIZATION, nextState.state);\r\n        assertSame(secondBlobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(newAssignment, nextState.pendingLocalization);\r\n        //Error occurs, but assignment has not changed\r\n        clearInvocations(localizer);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        verify(localizer).requestDownloadTopologyBlobs(newAssignment, port, cb);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_LOCALIZATION, nextState.state);\r\n        assertSame(thirdBlobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(newAssignment, nextState.pendingLocalization);\r\n        assertThat(Time.currentTimeMillis(), greaterThan(3L));\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        verify(thirdBlobFuture).get(1000, TimeUnit.MILLISECONDS);\r\n        verify(containerLauncher).launchContainer(port, newAssignment, state);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(newAssignment, nextState.currentAssignment);\r\n        assertSame(container, nextState.container);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testRelaunch",
  "sourceCode" : "@Test\r\npublic void testRelaunch() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String topoId = \"CURRENT\";\r\n        List<ExecutorInfo> execList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment assignment = mkLocalAssignment(topoId, execList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        Container container = mock(Container.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        LSWorkerHeartbeat oldhb = mkWorkerHB(topoId, port, execList, Time.currentTimeSecs() - 10);\r\n        LSWorkerHeartbeat goodhb = mkWorkerHB(topoId, port, execList, Time.currentTimeSecs());\r\n        when(container.readHeartbeat()).thenReturn(oldhb, oldhb, goodhb, goodhb);\r\n        when(container.areAllProcessesDead()).thenReturn(false, false, true);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        LocalState state = mock(LocalState.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(assignment, container, assignment, slotMetrics);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        assertEquals(MachineState.KILL_AND_RELAUNCH, nextState.state);\r\n        verify(container).kill();\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.KILL_AND_RELAUNCH, nextState.state);\r\n        verify(container).forceKill();\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        verify(container).relaunch();\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testReschedule",
  "sourceCode" : "@Test\r\npublic void testReschedule() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String cTopoId = \"CURRENT\";\r\n        List<ExecutorInfo> cExecList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment cAssignment = mkLocalAssignment(cTopoId, cExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        Container cContainer = mock(Container.class);\r\n        LSWorkerHeartbeat chb = mkWorkerHB(cTopoId, port, cExecList, Time.currentTimeSecs());\r\n        when(cContainer.readHeartbeat()).thenReturn(chb);\r\n        when(cContainer.areAllProcessesDead()).thenReturn(false, false, true);\r\n        String nTopoId = \"NEW\";\r\n        List<ExecutorInfo> nExecList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment nAssignment = mkLocalAssignment(nTopoId, nExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        Container nContainer = mock(Container.class);\r\n        LocalState state = mock(LocalState.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        when(containerLauncher.launchContainer(port, nAssignment, state)).thenReturn(nContainer);\r\n        LSWorkerHeartbeat nhb = mkWorkerHB(nTopoId, 100, nExecList, Time.currentTimeSecs());\r\n        when(nContainer.readHeartbeat()).thenReturn(nhb, nhb);\r\n        @SuppressWarnings(\"unchecked\")\r\n        CompletableFuture<Void> blobFuture = mock(CompletableFuture.class);\r\n        when(localizer.requestDownloadTopologyBlobs(nAssignment, port, cb)).thenReturn(blobFuture);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(cAssignment, cContainer, nAssignment, slotMetrics);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        assertEquals(MachineState.KILL, nextState.state);\r\n        verify(cContainer).kill();\r\n        verify(localizer).requestDownloadTopologyBlobs(nAssignment, port, cb);\r\n        assertSame(blobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(nAssignment, nextState.pendingLocalization);\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.KILL, nextState.state);\r\n        verify(cContainer).forceKill();\r\n        assertSame(blobFuture, nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertEquals(nAssignment, nextState.pendingLocalization);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_LOCALIZATION, nextState.state);\r\n        verify(cContainer).cleanUp();\r\n        verify(localizer).releaseSlotFor(cAssignment, port);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        verify(blobFuture).get(1000, TimeUnit.MILLISECONDS);\r\n        verify(containerLauncher).launchContainer(port, nAssignment, state);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(nAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(nAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(nAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload is not null\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertSame(nAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 4000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testRunningToEmpty",
  "sourceCode" : "@Test\r\npublic void testRunningToEmpty() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String cTopoId = \"CURRENT\";\r\n        List<ExecutorInfo> cExecList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment cAssignment = mkLocalAssignment(cTopoId, cExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        Container cContainer = mock(Container.class);\r\n        LSWorkerHeartbeat chb = mkWorkerHB(cTopoId, port, cExecList, Time.currentTimeSecs());\r\n        when(cContainer.readHeartbeat()).thenReturn(chb);\r\n        when(cContainer.areAllProcessesDead()).thenReturn(false, false, true);\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        LocalState state = mock(LocalState.class);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, slotMetrics);\r\n        DynamicState dynamicState = new DynamicState(cAssignment, cContainer, null, slotMetrics);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        assertEquals(MachineState.KILL, nextState.state);\r\n        verify(cContainer).kill();\r\n        verify(localizer, never()).requestDownloadTopologyBlobs(null, port, cb);\r\n        assertNull(nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.KILL, nextState.state);\r\n        verify(cContainer).forceKill();\r\n        assertNull(nextState.pendingDownload, \"pendingDownload not set properly\");\r\n        assertNull(nextState.pendingLocalization);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.EMPTY, nextState.state);\r\n        verify(cContainer).cleanUp();\r\n        verify(localizer).releaseSlotFor(cAssignment, port);\r\n        assertNull(nextState.container);\r\n        assertNull(nextState.currentAssignment);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.EMPTY, nextState.state);\r\n        assertNull(nextState.container);\r\n        assertNull(nextState.currentAssignment);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.EMPTY, nextState.state);\r\n        assertNull(nextState.container);\r\n        assertNull(nextState.currentAssignment);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testRunWithProfileActions",
  "sourceCode" : "@Test\r\npublic void testRunWithProfileActions() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String cTopoId = \"CURRENT\";\r\n        List<ExecutorInfo> cExecList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment cAssignment = mkLocalAssignment(cTopoId, cExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        Container cContainer = mock(Container.class);\r\n        //NOT going to timeout for a while\r\n        LSWorkerHeartbeat chb = mkWorkerHB(cTopoId, port, cExecList, Time.currentTimeSecs() + 100);\r\n        when(cContainer.readHeartbeat()).thenReturn(chb, chb, chb, chb, chb, chb);\r\n        when(cContainer.runProfiling(any(ProfileRequest.class), anyBoolean())).thenReturn(true);\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        LocalState state = mock(LocalState.class);\r\n        StaticState staticState = new StaticState(localizer, 5000, 120000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, new SlotMetrics(new StormMetricsRegistry()));\r\n        Set<TopoProfileAction> profileActions = new HashSet<>();\r\n        ProfileRequest request = new ProfileRequest();\r\n        request.set_action(ProfileAction.JPROFILE_STOP);\r\n        NodeInfo info = new NodeInfo();\r\n        info.set_node(\"localhost\");\r\n        info.add_to_port(port);\r\n        request.set_nodeInfo(info);\r\n        //3 seconds from now\r\n        request.set_time_stamp(Time.currentTimeMillis() + 3000);\r\n        TopoProfileAction profile = new TopoProfileAction(cTopoId, request);\r\n        profileActions.add(profile);\r\n        Set<TopoProfileAction> expectedPending = new HashSet<>();\r\n        expectedPending.add(profile);\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        DynamicState dynamicState = new DynamicState(cAssignment, cContainer, cAssignment, slotMetrics).withProfileActions(profileActions, Collections.emptySet());\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        verify(cContainer).runProfiling(request, false);\r\n        assertEquals(expectedPending, nextState.pendingStopProfileActions);\r\n        assertEquals(expectedPending, nextState.profileActions);\r\n        assertTrue(Time.currentTimeMillis() > 1000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertEquals(expectedPending, nextState.pendingStopProfileActions);\r\n        assertEquals(expectedPending, nextState.profileActions);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertEquals(expectedPending, nextState.pendingStopProfileActions);\r\n        assertEquals(expectedPending, nextState.profileActions);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        verify(cContainer).runProfiling(request, true);\r\n        assertEquals(Collections.<TopoProfileAction>emptySet(), nextState.pendingStopProfileActions);\r\n        assertEquals(Collections.<TopoProfileAction>emptySet(), nextState.profileActions);\r\n        assertTrue(Time.currentTimeMillis() > 4000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertEquals(Collections.<TopoProfileAction>emptySet(), nextState.pendingStopProfileActions);\r\n        assertEquals(Collections.<TopoProfileAction>emptySet(), nextState.profileActions);\r\n        assertTrue(Time.currentTimeMillis() > 5000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\daemon\\supervisor\\SlotTest.java",
  "methodName" : "testResourcesChangedFiltered",
  "sourceCode" : "@Test\r\npublic void testResourcesChangedFiltered() throws Exception {\r\n    try (SimulatedTime ignored = new SimulatedTime(1010)) {\r\n        int port = 8080;\r\n        String cTopoId = \"CURRENT\";\r\n        List<ExecutorInfo> cExecList = mkExecutorInfoList(1, 2, 3, 4, 5);\r\n        LocalAssignment cAssignment = mkLocalAssignment(cTopoId, cExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        String otherTopoId = \"OTHER\";\r\n        LocalAssignment otherAssignment = mkLocalAssignment(otherTopoId, cExecList, mkWorkerResources(100.0, 100.0, 100.0));\r\n        BlobChangingCallback cb = mock(BlobChangingCallback.class);\r\n        Container cContainer = mock(Container.class);\r\n        LSWorkerHeartbeat chb = mkWorkerHB(cTopoId, port, cExecList, Time.currentTimeSecs());\r\n        when(cContainer.readHeartbeat()).thenReturn(chb);\r\n        when(cContainer.areAllProcessesDead()).thenReturn(false, false, true);\r\n        AsyncLocalizer localizer = mock(AsyncLocalizer.class);\r\n        Container nContainer = mock(Container.class);\r\n        LocalState state = mock(LocalState.class);\r\n        ContainerLauncher containerLauncher = mock(ContainerLauncher.class);\r\n        when(containerLauncher.launchContainer(port, cAssignment, state)).thenReturn(nContainer);\r\n        when(nContainer.readHeartbeat()).thenReturn(chb, chb);\r\n        ISupervisor iSuper = mock(ISupervisor.class);\r\n        long heartbeatTimeoutMs = 5000;\r\n        StaticState staticState = new StaticState(localizer, heartbeatTimeoutMs, 120_000, 1000, 1000, containerLauncher, \"localhost\", port, iSuper, state, cb, null, null, new SlotMetrics(new StormMetricsRegistry()));\r\n        Set<Slot.BlobChanging> changing = new HashSet<>();\r\n        LocallyCachedBlob stormJar = mock(LocallyCachedBlob.class);\r\n        GoodToGo.GoodToGoLatch stormJarLatch = mock(GoodToGo.GoodToGoLatch.class);\r\n        CompletableFuture<Void> stormJarLatchFuture = mock(CompletableFuture.class);\r\n        when(stormJarLatch.countDown()).thenReturn(stormJarLatchFuture);\r\n        changing.add(new Slot.BlobChanging(cAssignment, stormJar, stormJarLatch));\r\n        Set<Slot.BlobChanging> desired = new HashSet<>(changing);\r\n        LocallyCachedBlob otherJar = mock(LocallyCachedBlob.class);\r\n        GoodToGo.GoodToGoLatch otherJarLatch = mock(GoodToGo.GoodToGoLatch.class);\r\n        changing.add(new Slot.BlobChanging(otherAssignment, otherJar, otherJarLatch));\r\n        SlotMetrics slotMetrics = new SlotMetrics(new StormMetricsRegistry());\r\n        DynamicState dynamicState = new DynamicState(cAssignment, cContainer, cAssignment, slotMetrics).withChangingBlobs(changing);\r\n        DynamicState nextState = Slot.stateMachineStep(dynamicState, staticState);\r\n        assertEquals(MachineState.KILL_BLOB_UPDATE, nextState.state);\r\n        verify(iSuper).killedWorker(port);\r\n        verify(cContainer).kill();\r\n        verify(localizer, never()).requestDownloadTopologyBlobs(any(), anyInt(), any());\r\n        verify(stormJarLatch, never()).countDown();\r\n        verify(otherJarLatch, times(1)).countDown();\r\n        assertNull(nextState.pendingDownload);\r\n        assertNull(nextState.pendingLocalization);\r\n        assertEquals(desired, nextState.changingBlobs);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertThat(Time.currentTimeMillis(), greaterThan(1000L));\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.KILL_BLOB_UPDATE, nextState.state);\r\n        verify(cContainer).forceKill();\r\n        assertNull(nextState.pendingDownload);\r\n        assertNull(nextState.pendingLocalization);\r\n        assertEquals(desired, nextState.changingBlobs);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertThat(Time.currentTimeMillis(), greaterThan(2000L));\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.WAITING_FOR_BLOB_UPDATE, nextState.state);\r\n        verify(cContainer).cleanUp();\r\n        assertThat(Time.currentTimeMillis(), greaterThan(2000L));\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        verify(stormJarLatchFuture).get(anyLong(), any());\r\n        verify(containerLauncher).launchContainer(port, cAssignment, state);\r\n        assertEquals(MachineState.WAITING_FOR_WORKER_START, nextState.state);\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertSame(cAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertThat(Time.currentTimeMillis(), greaterThan(2000L));\r\n        assertThat(Time.currentTimeMillis(), lessThan(heartbeatTimeoutMs));\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertSame(cAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 2000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertSame(cAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 3000);\r\n        nextState = Slot.stateMachineStep(nextState, staticState);\r\n        assertEquals(MachineState.RUNNING, nextState.state);\r\n        assertNull(nextState.pendingChangingBlobsAssignment);\r\n        assertTrue(nextState.pendingChangingBlobs.isEmpty());\r\n        assertSame(cAssignment, nextState.currentAssignment);\r\n        assertSame(nContainer, nextState.container);\r\n        assertTrue(Time.currentTimeMillis() > 4000);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testNimbusChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testNimbusChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.NIMBUS_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testLogviewerChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testLogviewerChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.LOGVIEWER_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testUiChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testUiChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.UI_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testPacemakerChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testPacemakerChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.PACEMAKER_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testDrpcChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testDrpcChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.DRPC_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testSupervisorChildoptsIsStringOrStringList",
  "sourceCode" : "@Test\r\npublic void testSupervisorChildoptsIsStringOrStringList() {\r\n    stringOrStringListTest(DaemonConfig.SUPERVISOR_CHILDOPTS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\DaemonConfigTest.java",
  "methodName" : "testMaskPasswords",
  "sourceCode" : "@Test\r\npublic void testMaskPasswords() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(DaemonConfig.LOGVIEWER_HTTPS_KEY_PASSWORD, \"pass1\");\r\n    conf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, 100);\r\n    Map<String, Object> result = ConfigUtils.maskPasswords(conf);\r\n    assertEquals(\"*****\", result.get(DaemonConfig.LOGVIEWER_HTTPS_KEY_PASSWORD));\r\n    assertEquals(100, result.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testRequestDownloadBaseTopologyBlobs",
  "sourceCode" : "@Test\r\npublic void testRequestDownloadBaseTopologyBlobs() throws Exception {\r\n    ReflectionUtils mockedReflectionUtils = mock(ReflectionUtils.class);\r\n    ServerUtils mockedServerUtils = mock(ServerUtils.class);\r\n    ReflectionUtils previousReflectionUtils = ReflectionUtils.setInstance(mockedReflectionUtils);\r\n    ServerUtils previousServerUtils = ServerUtils.setInstance(mockedServerUtils);\r\n    // cannot use automatic resource management here in this try because the AsyncLocalizer depends on a config map,\r\n    // which should take the storm local dir, and that storm local dir is declared in the try-with-resources.\r\n    AsyncLocalizer victim = null;\r\n    try (TmpPath stormRoot = new TmpPath();\r\n        TmpPath localizerRoot = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(DaemonConfig.SUPERVISOR_BLOBSTORE, ClientBlobStore.class.getName());\r\n        conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, DefaultPrincipalToLocal.class.getName());\r\n        conf.put(Config.STORM_CLUSTER_MODE, \"distributed\");\r\n        conf.put(Config.STORM_LOCAL_DIR, stormRoot.getPath());\r\n        AdvancedFSOps ops = AdvancedFSOps.make(conf);\r\n        victim = spy(new AsyncLocalizer(conf, ops, localizerRoot.getPath(), new StormMetricsRegistry()));\r\n        final String topoId = \"TOPO\";\r\n        final LocalAssignment localAssignment = constructLocalAssignment(topoId, \"user\");\r\n        final int port = 8080;\r\n        ClientBlobStore blobStore = mock(ClientBlobStore.class);\r\n        when(blobStore.getRemoteBlobstoreUpdateTime()).thenReturn(-1L);\r\n        LocallyCachedTopologyBlob jarBlob = mock(LocallyCachedTopologyBlob.class);\r\n        doReturn(jarBlob).when(victim).getTopoJar(topoId, localAssignment.get_owner());\r\n        when(jarBlob.getLocalVersion()).thenReturn(-1L);\r\n        when(jarBlob.getRemoteVersion(any())).thenReturn(100L);\r\n        when(jarBlob.fetchUnzipToTemp(any())).thenReturn(100L);\r\n        when(jarBlob.isUsed()).thenReturn(true);\r\n        LocallyCachedTopologyBlob codeBlob = mock(LocallyCachedTopologyBlob.class);\r\n        doReturn(codeBlob).when(victim).getTopoCode(topoId, localAssignment.get_owner());\r\n        when(codeBlob.getLocalVersion()).thenReturn(-1L);\r\n        when(codeBlob.getRemoteVersion(any())).thenReturn(200L);\r\n        when(codeBlob.fetchUnzipToTemp(any())).thenReturn(200L);\r\n        when(codeBlob.isUsed()).thenReturn(true);\r\n        LocallyCachedTopologyBlob confBlob = mock(LocallyCachedTopologyBlob.class);\r\n        doReturn(confBlob).when(victim).getTopoConf(topoId, localAssignment.get_owner());\r\n        when(confBlob.getLocalVersion()).thenReturn(-1L);\r\n        when(confBlob.getRemoteVersion(any())).thenReturn(300L);\r\n        when(confBlob.fetchUnzipToTemp(any())).thenReturn(300L);\r\n        when(confBlob.isUsed()).thenReturn(true);\r\n        when(mockedReflectionUtils.newInstanceImpl(ClientBlobStore.class)).thenReturn(blobStore);\r\n        PortAndAssignment pna = new PortAndAssignmentImpl(port, localAssignment);\r\n        Future<Void> f = victim.requestDownloadBaseTopologyBlobs(pna, null);\r\n        f.get(20, TimeUnit.SECONDS);\r\n        verify(jarBlob).update(eq(blobStore), eq(-1L));\r\n        verify(codeBlob).update(eq(blobStore), eq(-1L));\r\n        verify(confBlob).update(eq(blobStore), eq(-1L));\r\n    } finally {\r\n        ReflectionUtils.setInstance(previousReflectionUtils);\r\n        ServerUtils.setInstance(previousServerUtils);\r\n        if (victim != null) {\r\n            victim.close();\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testRequestDownloadTopologyBlobs",
  "sourceCode" : "@Test\r\npublic void testRequestDownloadTopologyBlobs() throws Exception {\r\n    ConfigUtils mockedConfigUtils = mock(ConfigUtils.class);\r\n    ConfigUtils previousConfigUtils = ConfigUtils.setInstance(mockedConfigUtils);\r\n    AsyncLocalizer victim = null;\r\n    try (TmpPath stormLocal = new TmpPath();\r\n        TmpPath localizerRoot = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(Config.STORM_LOCAL_DIR, stormLocal.getPath());\r\n        AdvancedFSOps ops = AdvancedFSOps.make(conf);\r\n        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n        victim = spy(new AsyncLocalizer(conf, ops, localizerRoot.getPath(), metricsRegistry));\r\n        final String topoId = \"TOPO-12345\";\r\n        final String user = \"user\";\r\n        final Path userDir = Paths.get(stormLocal.getPath(), user);\r\n        final Path topologyDirRoot = Paths.get(stormLocal.getPath(), topoId);\r\n        final String simpleLocalName = \"simple.txt\";\r\n        final String simpleKey = \"simple\";\r\n        Map<String, Map<String, Object>> topoBlobMap = new HashMap<>();\r\n        Map<String, Object> simple = new HashMap<>();\r\n        simple.put(\"localname\", simpleLocalName);\r\n        simple.put(\"uncompress\", false);\r\n        topoBlobMap.put(simpleKey, simple);\r\n        final int port = 8080;\r\n        Map<String, Object> topoConf = new HashMap<>(conf);\r\n        topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topoBlobMap);\r\n        topoConf.put(Config.TOPOLOGY_NAME, \"TOPO\");\r\n        List<LocalizedResource> localizedList = new ArrayList<>();\r\n        LocalizedResource simpleLocal = new LocalizedResource(simpleKey, localizerRoot.getFile().toPath(), false, ops, conf, user, metricsRegistry);\r\n        localizedList.add(simpleLocal);\r\n        when(mockedConfigUtils.supervisorStormDistRootImpl(conf, topoId)).thenReturn(topologyDirRoot.toString());\r\n        when(mockedConfigUtils.readSupervisorStormConfImpl(conf, topoId)).thenReturn(topoConf);\r\n        when(mockedConfigUtils.readSupervisorTopologyImpl(conf, topoId, ops)).thenReturn(constructEmptyStormTopology());\r\n        //Write the mocking backwards so the actual method is not called on the spy object\r\n        doReturn(CompletableFuture.supplyAsync(() -> null)).when(victim).requestDownloadBaseTopologyBlobs(any(), eq(null));\r\n        Files.createDirectories(topologyDirRoot);\r\n        doReturn(userDir.toFile()).when(victim).getLocalUserFileCacheDir(user);\r\n        doReturn(localizedList).when(victim).getBlobs(any(List.class), any(), any());\r\n        Future<Void> f = victim.requestDownloadTopologyBlobs(constructLocalAssignment(topoId, user), port, null);\r\n        f.get(20, TimeUnit.SECONDS);\r\n        // We should be done now...\r\n        verify(victim).getLocalUserFileCacheDir(user);\r\n        assertTrue(ops.fileExists(userDir));\r\n        verify(victim).getBlobs(any(List.class), any(), any());\r\n        // symlink was created\r\n        assertTrue(Files.isSymbolicLink(topologyDirRoot.resolve(simpleLocalName)));\r\n    } finally {\r\n        ConfigUtils.setInstance(previousConfigUtils);\r\n        if (victim != null) {\r\n            victim.close();\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testRequestDownloadTopologyBlobsLocalMode",
  "sourceCode" : "@Test\r\npublic void testRequestDownloadTopologyBlobsLocalMode() throws Exception {\r\n    // tests download of topology blobs in local mode on a topology without resources folder\r\n    ConfigUtils mockedConfigUtils = mock(ConfigUtils.class);\r\n    ServerUtils mockedServerUtils = mock(ServerUtils.class);\r\n    ConfigUtils previousConfigUtils = ConfigUtils.setInstance(mockedConfigUtils);\r\n    ServerUtils previousServerUtils = ServerUtils.setInstance(mockedServerUtils);\r\n    AsyncLocalizer victim = null;\r\n    try (TmpPath stormLocal = new TmpPath();\r\n        TmpPath localizerRoot = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(Config.STORM_LOCAL_DIR, stormLocal.getPath());\r\n        conf.put(Config.STORM_CLUSTER_MODE, \"local\");\r\n        StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n        AdvancedFSOps ops = AdvancedFSOps.make(conf);\r\n        victim = spy(new AsyncLocalizer(conf, ops, localizerRoot.getPath(), metricsRegistry));\r\n        final String topoId = \"TOPO-12345\";\r\n        final String user = \"user\";\r\n        final int port = 8080;\r\n        final Path userDir = Paths.get(stormLocal.getPath(), user);\r\n        final Path stormRoot = Paths.get(stormLocal.getPath(), topoId);\r\n        final String simpleLocalName = \"simple.txt\";\r\n        final String simpleKey = \"simple\";\r\n        Map<String, Map<String, Object>> topoBlobMap = new HashMap<>();\r\n        Map<String, Object> simple = new HashMap<>();\r\n        simple.put(\"localname\", simpleLocalName);\r\n        simple.put(\"uncompress\", false);\r\n        topoBlobMap.put(simpleKey, simple);\r\n        Map<String, Object> topoConf = new HashMap<>(conf);\r\n        topoConf.put(Config.TOPOLOGY_BLOBSTORE_MAP, topoBlobMap);\r\n        topoConf.put(Config.TOPOLOGY_NAME, \"TOPO\");\r\n        List<LocalizedResource> localizedList = new ArrayList<>();\r\n        LocalizedResource simpleLocal = new LocalizedResource(simpleKey, localizerRoot.getFile().toPath(), false, ops, conf, user, metricsRegistry);\r\n        localizedList.add(simpleLocal);\r\n        when(mockedConfigUtils.supervisorStormDistRootImpl(conf, topoId)).thenReturn(stormRoot.toString());\r\n        when(mockedConfigUtils.readSupervisorStormConfImpl(conf, topoId)).thenReturn(topoConf);\r\n        when(mockedConfigUtils.readSupervisorTopologyImpl(conf, topoId, ops)).thenReturn(constructEmptyStormTopology());\r\n        doReturn(mockBlobStore).when(victim).getClientBlobStore();\r\n        doReturn(userDir.toFile()).when(victim).getLocalUserFileCacheDir(user);\r\n        doReturn(localizedList).when(victim).getBlobs(any(List.class), any(), any());\r\n        ReadableBlobMeta blobMeta = new ReadableBlobMeta();\r\n        blobMeta.set_version(1);\r\n        doReturn(blobMeta).when(mockBlobStore).getBlobMeta(any());\r\n        when(mockBlobStore.getBlob(any())).thenAnswer(invocation -> new TestInputStreamWithMeta(LOCAL_MODE_JAR_VERSION));\r\n        Future<Void> f = victim.requestDownloadTopologyBlobs(constructLocalAssignment(topoId, user), port, null);\r\n        f.get(20, TimeUnit.SECONDS);\r\n        verify(victim).getLocalUserFileCacheDir(user);\r\n        assertTrue(ops.fileExists(userDir));\r\n        verify(victim).getBlobs(any(List.class), any(), any());\r\n        // make sure resources directory after blob version commit is created.\r\n        Path extractionDir = stormRoot.resolve(LocallyCachedTopologyBlob.TopologyBlobType.TOPO_JAR.getExtractionDir());\r\n        assertTrue(ops.fileExists(extractionDir));\r\n    } finally {\r\n        ConfigUtils.setInstance(previousConfigUtils);\r\n        ServerUtils.setInstance(previousServerUtils);\r\n        if (victim != null) {\r\n            victim.close();\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testDirPaths",
  "sourceCode" : "@Test\r\npublic void testDirPaths() throws Exception {\r\n    try (TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        AsyncLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        String expectedDir = constructUserCacheDir(tmp.getPath(), user1);\r\n        assertEquals(expectedDir, localizer.getLocalUserDir(user1).toString(), \"get local user dir doesn't return right value\");\r\n        String expectedFileDir = joinPath(expectedDir, LocalizedResource.FILECACHE);\r\n        assertEquals(expectedFileDir, localizer.getLocalUserFileCacheDir(user1).toString(), \"get local user file dir doesn't return right value\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testReconstruct",
  "sourceCode" : "@Test\r\npublic void testReconstruct() throws Exception {\r\n    try (TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        String expectedFileDir1 = constructExpectedFilesDir(tmp.getPath(), user1);\r\n        String expectedArchiveDir1 = constructExpectedArchivesDir(tmp.getPath(), user1);\r\n        String expectedFileDir2 = constructExpectedFilesDir(tmp.getPath(), user2);\r\n        String expectedArchiveDir2 = constructExpectedArchivesDir(tmp.getPath(), user2);\r\n        String key1 = \"testfile1.txt\";\r\n        String key2 = \"testfile2.txt\";\r\n        String key3 = \"testfile3.txt\";\r\n        String key4 = \"testfile4.txt\";\r\n        String archive1 = \"archive1\";\r\n        String archive2 = \"archive2\";\r\n        File user1file1 = new File(expectedFileDir1, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user1file2 = new File(expectedFileDir1, key2 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user2file3 = new File(expectedFileDir2, key3 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user2file4 = new File(expectedFileDir2, key4 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user1archive1 = new File(expectedArchiveDir1, archive1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user2archive2 = new File(expectedArchiveDir2, archive2 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File user1archive1file = new File(user1archive1, \"file1\");\r\n        File user2archive2file = new File(user2archive2, \"file2\");\r\n        // setup some files/dirs to emulate supervisor restart\r\n        assertTrue(new File(expectedFileDir1).mkdirs(), \"Failed setup filecache dir1\");\r\n        assertTrue(new File(expectedFileDir2).mkdirs(), \"Failed setup filecache dir2\");\r\n        assertTrue(user1file1.createNewFile(), \"Failed setup file1\");\r\n        assertTrue(user1file2.createNewFile(), \"Failed setup file2\");\r\n        assertTrue(user2file3.createNewFile(), \"Failed setup file3\");\r\n        assertTrue(user2file4.createNewFile(), \"Failed setup file4\");\r\n        assertTrue(user1archive1.mkdirs(), \"Failed setup archive dir1\");\r\n        assertTrue(user2archive2.mkdirs(), \"Failed setup archive dir2\");\r\n        assertTrue(user1archive1file.createNewFile(), \"Failed setup file in archivedir1\");\r\n        assertTrue(user2archive2file.createNewFile(), \"Failed setup file in archivedir2\");\r\n        TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        ArrayList<LocalResource> arrUser1Keys = new ArrayList<>();\r\n        arrUser1Keys.add(new LocalResource(key1, false, false));\r\n        arrUser1Keys.add(new LocalResource(archive1, true, false));\r\n        LocalAssignment topo1 = constructLocalAssignment(\"topo1\", user1, Collections.emptyList());\r\n        localizer.addReferences(arrUser1Keys, new PortAndAssignmentImpl(1, topo1), null);\r\n        ConcurrentMap<String, LocalizedResource> lrsrcFiles = localizer.getUserFiles().get(user1);\r\n        ConcurrentMap<String, LocalizedResource> lrsrcArchives = localizer.getUserArchives().get(user1);\r\n        assertEquals(3, lrsrcFiles.size() + lrsrcArchives.size(), \"local resource set size wrong\");\r\n        LocalizedResource key1rsrc = lrsrcFiles.get(key1);\r\n        assertNotNull(key1rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(key1, key1rsrc.getKey(), \"key doesn't match\");\r\n        assertTrue(key1rsrc.isUsed(), \"references doesn't match \" + key1rsrc.getDependencies());\r\n        LocalizedResource key2rsrc = lrsrcFiles.get(key2);\r\n        assertNotNull(key2rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(key2, key2rsrc.getKey(), \"key doesn't match\");\r\n        assertFalse(key2rsrc.isUsed(), \"refcount doesn't match \" + key2rsrc.getDependencies());\r\n        LocalizedResource archive1rsrc = lrsrcArchives.get(archive1);\r\n        assertNotNull(archive1rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(archive1, archive1rsrc.getKey(), \"key doesn't match\");\r\n        assertTrue(archive1rsrc.isUsed(), \"refcount doesn't match \" + archive1rsrc.getDependencies());\r\n        ConcurrentMap<String, LocalizedResource> lrsrcFiles2 = localizer.getUserFiles().get(user2);\r\n        ConcurrentMap<String, LocalizedResource> lrsrcArchives2 = localizer.getUserArchives().get(user2);\r\n        assertEquals(3, lrsrcFiles2.size() + lrsrcArchives2.size(), \"local resource set size wrong\");\r\n        LocalizedResource key3rsrc = lrsrcFiles2.get(key3);\r\n        assertNotNull(key3rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(key3, key3rsrc.getKey(), \"key doesn't match\");\r\n        assertFalse(key3rsrc.isUsed(), \"refcount doesn't match \" + key3rsrc.getDependencies());\r\n        LocalizedResource key4rsrc = lrsrcFiles2.get(key4);\r\n        assertNotNull(key4rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(key4, key4rsrc.getKey(), \"key doesn't match\");\r\n        assertFalse(key4rsrc.isUsed(), \"refcount doesn't match \" + key4rsrc.getDependencies());\r\n        LocalizedResource archive2rsrc = lrsrcArchives2.get(archive2);\r\n        assertNotNull(archive2rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(archive2, archive2rsrc.getKey(), \"key doesn't match\");\r\n        assertFalse(archive2rsrc.isUsed(), \"refcount doesn't match \" + archive2rsrc.getDependencies());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testArchivesTgz",
  "sourceCode" : "@Test\r\npublic void testArchivesTgz() throws Exception {\r\n    testArchives(getFileFromResource(joinPath(\"localizer\", \"localtestwithsymlink.tgz\")), true, 21344);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testArchivesZip",
  "sourceCode" : "@Test\r\npublic void testArchivesZip() throws Exception {\r\n    testArchives(getFileFromResource(joinPath(\"localizer\", \"localtest.zip\")), false, 21348);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testArchivesTarGz",
  "sourceCode" : "@Test\r\npublic void testArchivesTarGz() throws Exception {\r\n    testArchives(getFileFromResource(joinPath(\"localizer\", \"localtestwithsymlink.tar.gz\")), true, 21344);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testArchivesTar",
  "sourceCode" : "@Test\r\npublic void testArchivesTar() throws Exception {\r\n    testArchives(getFileFromResource(joinPath(\"localizer\", \"localtestwithsymlink.tar\")), true, 21344);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testArchivesJar",
  "sourceCode" : "@Test\r\npublic void testArchivesJar() throws Exception {\r\n    testArchives(getFileFromResource(joinPath(\"localizer\", \"localtestwithsymlink.jar\")), false, 21416);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testBasic",
  "sourceCode" : "@Test\r\npublic void testBasic() throws Exception {\r\n    try (Time.SimulatedTime ignored = new Time.SimulatedTime();\r\n        TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        // set clean time really high so doesn't kick in\r\n        conf.put(DaemonConfig.SUPERVISOR_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, 60 * 60 * 1000);\r\n        String key1 = \"key1\";\r\n        String topo1 = \"topo1\";\r\n        TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        // set really small so will do cleanup\r\n        localizer.setTargetCacheSize(1);\r\n        ReadableBlobMeta rbm = new ReadableBlobMeta();\r\n        rbm.set_settable(new SettableBlobMeta(WORLD_EVERYTHING));\r\n        when(mockBlobStore.getBlobMeta(key1)).thenReturn(rbm);\r\n        when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(1));\r\n        long timeBefore = Time.currentTimeMillis();\r\n        Time.advanceTime(10);\r\n        File user1Dir = localizer.getLocalUserFileCacheDir(user1);\r\n        assertTrue(user1Dir.mkdirs(), \"failed to create user dir\");\r\n        Time.advanceTime(10);\r\n        LocalAssignment topo1Assignment = constructLocalAssignment(topo1, user1, Collections.emptyList());\r\n        PortAndAssignment topo1Pna = new PortAndAssignmentImpl(1, topo1Assignment);\r\n        LocalizedResource lrsrc = localizer.getBlob(new LocalResource(key1, false, false), topo1Pna, null);\r\n        long timeAfter = Time.currentTimeMillis();\r\n        Time.advanceTime(10);\r\n        String expectedUserDir = joinPath(tmp.getPath(), USERCACHE, user1);\r\n        String expectedFileDir = joinPath(expectedUserDir, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        assertTrue(new File(expectedFileDir).exists(), \"user filecache dir not created\");\r\n        File keyFile = new File(expectedFileDir, key1 + \".current\");\r\n        File keyFileCurrentSymlink = new File(expectedFileDir, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        assertTrue(keyFileCurrentSymlink.exists(), \"blob not created\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(1, lrsrcSet.size(), \"local resource set size wrong\");\r\n        LocalizedResource key1rsrc = lrsrcSet.get(key1);\r\n        assertNotNull(key1rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(key1, key1rsrc.getKey(), \"key doesn't match\");\r\n        assertEquals(true, key1rsrc.isUsed(), \"refcount doesn't match \" + key1rsrc.getDependencies());\r\n        assertEquals(keyFile.toPath(), key1rsrc.getCurrentSymlinkPath(), \"file path doesn't match\");\r\n        assertEquals(34, key1rsrc.getSizeOnDisk(), \"size doesn't match\");\r\n        assertTrue((key1rsrc.getLastUsed() >= timeBefore && key1rsrc.getLastUsed() <= timeAfter), \"timestamp not within range\");\r\n        timeBefore = Time.currentTimeMillis();\r\n        Time.advanceTime(10);\r\n        localizer.removeBlobReference(lrsrc.getKey(), topo1Pna, false);\r\n        Time.advanceTime(10);\r\n        timeAfter = Time.currentTimeMillis();\r\n        Time.advanceTime(10);\r\n        lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(1, lrsrcSet.size(), \"local resource set size wrong\");\r\n        key1rsrc = lrsrcSet.get(key1);\r\n        assertNotNull(key1rsrc, \"Local resource doesn't exist but should\");\r\n        assertEquals(false, key1rsrc.isUsed(), \"refcount doesn't match \" + key1rsrc.getDependencies());\r\n        assertTrue((key1rsrc.getLastUsed() >= timeBefore && key1rsrc.getLastUsed() <= timeAfter), \"timestamp not within range \" + timeBefore + \" \" + key1rsrc.getLastUsed() + \" \" + timeAfter);\r\n        // should remove the blob since cache size set really small\r\n        localizer.cleanup();\r\n        lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertNull(lrsrcSet, \"user set should be null\");\r\n        assertFalse(keyFile.exists(), \"blob not deleted\");\r\n        assertFalse(new File(expectedFileDir).exists(), \"blob dir not deleted\");\r\n        assertFalse(new File(expectedUserDir).exists(), \"blob dir not deleted\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testMultipleKeysOneUser",
  "sourceCode" : "@Test\r\npublic void testMultipleKeysOneUser() throws Exception {\r\n    try (Time.SimulatedTime ignored = new Time.SimulatedTime();\r\n        TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        // set clean time really high so doesn't kick in\r\n        conf.put(DaemonConfig.SUPERVISOR_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, 60 * 60 * 1_000);\r\n        String key1 = \"key1\";\r\n        String topo1 = \"topo1\";\r\n        String key2 = \"key2\";\r\n        String key3 = \"key3\";\r\n        TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        // set to keep 2 blobs (each of size 34)\r\n        localizer.setTargetCacheSize(68);\r\n        ReadableBlobMeta rbm = new ReadableBlobMeta();\r\n        rbm.set_settable(new SettableBlobMeta(WORLD_EVERYTHING));\r\n        when(mockBlobStore.getBlobMeta(anyString())).thenReturn(rbm);\r\n        when(mockBlobStore.isRemoteBlobExists(anyString())).thenReturn(true);\r\n        when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(0));\r\n        when(mockBlobStore.getBlob(key2)).thenReturn(new TestInputStreamWithMeta(0));\r\n        when(mockBlobStore.getBlob(key3)).thenReturn(new TestInputStreamWithMeta(0));\r\n        List<LocalResource> keys = Arrays.asList(new LocalResource(key1, false, false), new LocalResource(key2, false, false), new LocalResource(key3, false, false));\r\n        File user1Dir = localizer.getLocalUserFileCacheDir(user1);\r\n        assertTrue(user1Dir.mkdirs(), \"failed to create user dir\");\r\n        LocalAssignment topo1Assignment = constructLocalAssignment(topo1, user1, Collections.emptyList());\r\n        PortAndAssignment topo1Pna = new PortAndAssignmentImpl(1, topo1Assignment);\r\n        List<LocalizedResource> lrsrcs = localizer.getBlobs(keys, topo1Pna, null);\r\n        LocalizedResource lrsrc = lrsrcs.get(0);\r\n        LocalizedResource lrsrc2 = lrsrcs.get(1);\r\n        LocalizedResource lrsrc3 = lrsrcs.get(2);\r\n        String expectedFileDir = joinPath(tmp.getPath(), USERCACHE, user1, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        assertTrue(new File(expectedFileDir).exists(), \"user filecache dir not created\");\r\n        File keyFile = new File(expectedFileDir, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File keyFile2 = new File(expectedFileDir, key2 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File keyFile3 = new File(expectedFileDir, key3 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        assertTrue(keyFile.exists(), \"blob not created\");\r\n        assertTrue(keyFile2.exists(), \"blob not created\");\r\n        assertTrue(keyFile3.exists(), \"blob not created\");\r\n        assertEquals(34, keyFile.length(), \"size doesn't match\");\r\n        assertEquals(34, keyFile2.length(), \"size doesn't match\");\r\n        assertEquals(34, keyFile3.length(), \"size doesn't match\");\r\n        assertEquals(34, lrsrc.getSizeOnDisk(), \"size doesn't match\");\r\n        assertEquals(34, lrsrc3.getSizeOnDisk(), \"size doesn't match\");\r\n        assertEquals(34, lrsrc2.getSizeOnDisk(), \"size doesn't match\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(3, lrsrcSet.size(), \"local resource set size wrong\");\r\n        LOG.info(\"Removing blob references...\");\r\n        long timeBefore = Time.nanoTime();\r\n        Time.advanceTime(10);\r\n        localizer.removeBlobReference(lrsrc.getKey(), topo1Pna, false);\r\n        Time.advanceTime(10);\r\n        localizer.removeBlobReference(lrsrc2.getKey(), topo1Pna, false);\r\n        Time.advanceTime(10);\r\n        localizer.removeBlobReference(lrsrc3.getKey(), topo1Pna, false);\r\n        Time.advanceTime(10);\r\n        long timeAfter = Time.nanoTime();\r\n        LOG.info(\"Done removing blob references...\");\r\n        // add reference to one and then remove reference again so it has newer timestamp\r\n        LOG.info(\"Get Blob...\");\r\n        lrsrc = localizer.getBlob(new LocalResource(key1, false, false), topo1Pna, null);\r\n        LOG.info(\"Got Blob...\");\r\n        assertTrue((lrsrc.getLastUsed() >= timeBefore && lrsrc.getLastUsed() <= timeAfter), \"timestamp not within range \" + timeBefore + \" <= \" + lrsrc.getLastUsed() + \" <= \" + timeAfter);\r\n        //Resets the last access time for key1\r\n        localizer.removeBlobReference(lrsrc.getKey(), topo1Pna, false);\r\n        // should remove the second blob first\r\n        localizer.cleanup();\r\n        lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(2, lrsrcSet.size(), \"local resource set size wrong\");\r\n        long end = System.currentTimeMillis() + 100;\r\n        while ((end - System.currentTimeMillis()) >= 0 && keyFile2.exists()) {\r\n            Thread.sleep(1);\r\n        }\r\n        assertTrue(keyFile.exists(), \"blob deleted\");\r\n        assertFalse(keyFile2.exists(), \"blob not deleted\");\r\n        assertTrue(keyFile3.exists(), \"blob deleted\");\r\n        // set size to cleanup another one\r\n        localizer.setTargetCacheSize(34);\r\n        // should remove the third blob, because the first has the reset timestamp\r\n        localizer.cleanup();\r\n        lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(1, lrsrcSet.size(), \"local resource set size wrong\");\r\n        assertTrue(keyFile.exists(), \"blob deleted\");\r\n        assertFalse(keyFile2.exists(), \"blob not deleted\");\r\n        assertFalse(keyFile3.exists(), \"blob not deleted\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testFailAcls",
  "sourceCode" : "@Test\r\npublic void testFailAcls() {\r\n    assertThrows(AuthorizationException.class, () -> {\r\n        try (TmpPath tmp = new TmpPath()) {\r\n            Map<String, Object> conf = new HashMap<>();\r\n            // set clean time really high so doesn't kick in\r\n            conf.put(DaemonConfig.SUPERVISOR_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, 60 * 60 * 1000);\r\n            // enable blobstore acl validation\r\n            conf.put(Config.STORM_BLOBSTORE_ACL_VALIDATION_ENABLED, true);\r\n            String topo1 = \"topo1\";\r\n            String key1 = \"key1\";\r\n            TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n            ReadableBlobMeta rbm = new ReadableBlobMeta();\r\n            // set acl so user doesn't have read access\r\n            AccessControl acl = new AccessControl(AccessControlType.USER, BlobStoreAclHandler.ADMIN);\r\n            acl.set_name(user1);\r\n            rbm.set_settable(new SettableBlobMeta(Collections.singletonList(acl)));\r\n            when(mockBlobStore.getBlobMeta(anyString())).thenReturn(rbm);\r\n            when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(1));\r\n            File user1Dir = localizer.getLocalUserFileCacheDir(user1);\r\n            assertTrue(user1Dir.mkdirs(), \"failed to create user dir\");\r\n            LocalAssignment topo1Assignment = constructLocalAssignment(topo1, user1, Collections.emptyList());\r\n            PortAndAssignment topo1Pna = new PortAndAssignmentImpl(1, topo1Assignment);\r\n            // This should throw AuthorizationException because auth failed\r\n            localizer.getBlob(new LocalResource(key1, false, false), topo1Pna, null);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testKeyNotFoundException",
  "sourceCode" : "@Test\r\npublic void testKeyNotFoundException() {\r\n    assertThrows(KeyNotFoundException.class, () -> {\r\n        Map<String, Object> conf = Utils.readStormConfig();\r\n        String key1 = \"key1\";\r\n        conf.put(Config.STORM_LOCAL_DIR, \"target\");\r\n        LocalFsBlobStore bs = new LocalFsBlobStore();\r\n        LocalFsBlobStore spy = spy(bs);\r\n        Mockito.doReturn(true).when(spy).checkForBlobOrDownload(key1);\r\n        Mockito.doNothing().when(spy).checkForBlobUpdate(key1);\r\n        spy.prepare(conf, null, null, null);\r\n        spy.getBlob(key1, null);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testMultipleUsers",
  "sourceCode" : "@Test\r\npublic void testMultipleUsers() throws Exception {\r\n    try (TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        // set clean time really high so doesn't kick in\r\n        conf.put(DaemonConfig.SUPERVISOR_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, 60 * 60 * 1000);\r\n        String topo1 = \"topo1\";\r\n        String topo2 = \"topo2\";\r\n        String topo3 = \"topo3\";\r\n        String key1 = \"key1\";\r\n        String key2 = \"key2\";\r\n        String key3 = \"key3\";\r\n        TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        // set to keep 2 blobs (each of size 34)\r\n        localizer.setTargetCacheSize(68);\r\n        ReadableBlobMeta rbm = new ReadableBlobMeta();\r\n        rbm.set_settable(new SettableBlobMeta(WORLD_EVERYTHING));\r\n        when(mockBlobStore.getBlobMeta(anyString())).thenReturn(rbm);\r\n        //thenReturn always returns the same object, which is already consumed by the time User3 tries to getBlob!\r\n        when(mockBlobStore.getBlob(key1)).thenAnswer((i) -> new TestInputStreamWithMeta(1));\r\n        when(mockBlobStore.getBlob(key2)).thenReturn(new TestInputStreamWithMeta(1));\r\n        when(mockBlobStore.getBlob(key3)).thenReturn(new TestInputStreamWithMeta(1));\r\n        File user1Dir = localizer.getLocalUserFileCacheDir(user1);\r\n        assertTrue(user1Dir.mkdirs(), \"failed to create user dir\");\r\n        File user2Dir = localizer.getLocalUserFileCacheDir(user2);\r\n        assertTrue(user2Dir.mkdirs(), \"failed to create user dir\");\r\n        File user3Dir = localizer.getLocalUserFileCacheDir(user3);\r\n        assertTrue(user3Dir.mkdirs(), \"failed to create user dir\");\r\n        LocalAssignment topo1Assignment = constructLocalAssignment(topo1, user1, Collections.emptyList());\r\n        PortAndAssignment topo1Pna = new PortAndAssignmentImpl(1, topo1Assignment);\r\n        LocalizedResource lrsrc = localizer.getBlob(new LocalResource(key1, false, false), topo1Pna, null);\r\n        LocalAssignment topo2Assignment = constructLocalAssignment(topo2, user2, Collections.emptyList());\r\n        PortAndAssignment topo2Pna = new PortAndAssignmentImpl(2, topo2Assignment);\r\n        LocalizedResource lrsrc2 = localizer.getBlob(new LocalResource(key2, false, false), topo2Pna, null);\r\n        LocalAssignment topo3Assignment = constructLocalAssignment(topo3, user3, Collections.emptyList());\r\n        PortAndAssignment topo3Pna = new PortAndAssignmentImpl(3, topo3Assignment);\r\n        LocalizedResource lrsrc3 = localizer.getBlob(new LocalResource(key3, false, false), topo3Pna, null);\r\n        // make sure we support different user reading same blob\r\n        LocalizedResource lrsrc1_user3 = localizer.getBlob(new LocalResource(key1, false, false), topo3Pna, null);\r\n        String expectedUserDir1 = joinPath(tmp.getPath(), USERCACHE, user1);\r\n        String expectedFileDirUser1 = joinPath(expectedUserDir1, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        String expectedFileDirUser2 = joinPath(tmp.getPath(), USERCACHE, user2, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        String expectedFileDirUser3 = joinPath(tmp.getPath(), USERCACHE, user3, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        assertTrue(new File(expectedFileDirUser1).exists(), \"user filecache dir user1 not created\");\r\n        assertTrue(new File(expectedFileDirUser2).exists(), \"user filecache dir user2 not created\");\r\n        assertTrue(new File(expectedFileDirUser3).exists(), \"user filecache dir user3 not created\");\r\n        File keyFile = new File(expectedFileDirUser1, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File keyFile2 = new File(expectedFileDirUser2, key2 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File keyFile3 = new File(expectedFileDirUser3, key3 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        File keyFile1user3 = new File(expectedFileDirUser3, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        assertTrue(keyFile.exists(), \"blob not created\");\r\n        assertTrue(keyFile2.exists(), \"blob not created\");\r\n        assertTrue(keyFile3.exists(), \"blob not created\");\r\n        assertTrue(keyFile1user3.exists(), \"blob not created\");\r\n        //Should assert file size\r\n        assertEquals(34, lrsrc.getSizeOnDisk(), \"size doesn't match\");\r\n        assertEquals(34, lrsrc2.getSizeOnDisk(), \"size doesn't match\");\r\n        assertEquals(34, lrsrc3.getSizeOnDisk(), \"size doesn't match\");\r\n        //This was 0 byte in test\r\n        assertEquals(34, lrsrc1_user3.getSizeOnDisk(), \"size doesn't match\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(1, lrsrcSet.size(), \"local resource set size wrong\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet2 = localizer.getUserFiles().get(user2);\r\n        assertEquals(1, lrsrcSet2.size(), \"local resource set size wrong\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet3 = localizer.getUserFiles().get(user3);\r\n        assertEquals(2, lrsrcSet3.size(), \"local resource set size wrong\");\r\n        localizer.removeBlobReference(lrsrc.getKey(), topo1Pna, false);\r\n        // should remove key1\r\n        localizer.cleanup();\r\n        lrsrcSet = localizer.getUserFiles().get(user1);\r\n        lrsrcSet3 = localizer.getUserFiles().get(user3);\r\n        assertNull(lrsrcSet, \"user set should be null\");\r\n        assertFalse(new File(expectedFileDirUser1).exists(), \"blob dir not deleted\");\r\n        assertFalse(new File(expectedUserDir1).exists(), \"blob dir not deleted\");\r\n        assertEquals(2, lrsrcSet3.size(), \"local resource set size wrong\");\r\n        assertTrue(keyFile2.exists(), \"blob deleted\");\r\n        assertFalse(keyFile.exists(), \"blob not deleted\");\r\n        assertTrue(keyFile3.exists(), \"blob deleted\");\r\n        assertTrue(keyFile1user3.exists(), \"blob deleted\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "testUpdate",
  "sourceCode" : "@Test\r\npublic void testUpdate() throws Exception {\r\n    try (TmpPath tmp = new TmpPath()) {\r\n        Map<String, Object> conf = new HashMap<>();\r\n        // set clean time really high so doesn't kick in\r\n        conf.put(DaemonConfig.SUPERVISOR_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, 60 * 60 * 1000);\r\n        String key1 = \"key1\";\r\n        String topo1 = \"topo1\";\r\n        String topo2 = \"topo2\";\r\n        TestLocalizer localizer = new TestLocalizer(conf, tmp.getPath());\r\n        ReadableBlobMeta rbm = new ReadableBlobMeta();\r\n        rbm.set_version(1);\r\n        rbm.set_settable(new SettableBlobMeta(WORLD_EVERYTHING));\r\n        when(mockBlobStore.getBlobMeta(key1)).thenReturn(rbm);\r\n        when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(1));\r\n        File user1Dir = localizer.getLocalUserFileCacheDir(user1);\r\n        assertTrue(user1Dir.mkdirs(), \"failed to create user dir\");\r\n        LocalAssignment topo1Assignment = constructLocalAssignment(topo1, user1, Collections.emptyList());\r\n        PortAndAssignment topo1Pna = new PortAndAssignmentImpl(1, topo1Assignment);\r\n        LocalizedResource lrsrc = localizer.getBlob(new LocalResource(key1, false, false), topo1Pna, null);\r\n        String expectedUserDir = joinPath(tmp.getPath(), USERCACHE, user1);\r\n        String expectedFileDir = joinPath(expectedUserDir, LocalizedResource.FILECACHE, LocalizedResource.FILESDIR);\r\n        assertTrue(new File(expectedFileDir).exists(), \"user filecache dir not created\");\r\n        Path keyVersionFile = Paths.get(expectedFileDir, key1 + \".version\");\r\n        File keyFileCurrentSymlink = new File(expectedFileDir, key1 + LocalizedResource.CURRENT_BLOB_SUFFIX);\r\n        assertTrue(keyFileCurrentSymlink.exists(), \"blob not created\");\r\n        File versionFile = new File(expectedFileDir, key1 + LocalizedResource.BLOB_VERSION_SUFFIX);\r\n        assertTrue(versionFile.exists(), \"blob version file not created\");\r\n        assertEquals(1, LocalizedResource.localVersionOfBlob(keyVersionFile), \"blob version not correct\");\r\n        ConcurrentMap<String, LocalizedResource> lrsrcSet = localizer.getUserFiles().get(user1);\r\n        assertEquals(1, lrsrcSet.size(), \"local resource set size wrong\");\r\n        // test another topology getting blob with updated version - it should update version now\r\n        rbm.set_version(2);\r\n        when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(2));\r\n        LocalAssignment topo2Assignment = constructLocalAssignment(topo2, user1, Collections.emptyList());\r\n        PortAndAssignment topo2Pna = new PortAndAssignmentImpl(1, topo2Assignment);\r\n        localizer.getBlob(new LocalResource(key1, false, false), topo2Pna, null);\r\n        assertTrue(versionFile.exists(), \"blob version file not created\");\r\n        assertEquals(2, LocalizedResource.localVersionOfBlob(keyVersionFile), \"blob version not correct\");\r\n        assertTrue(new File(expectedFileDir, key1 + \".2\").exists(), \"blob file with version 2 not created\");\r\n        // now test regular updateBlob\r\n        rbm.set_version(3);\r\n        when(mockBlobStore.getBlob(key1)).thenReturn(new TestInputStreamWithMeta(3));\r\n        ArrayList<LocalResource> arr = new ArrayList<>();\r\n        arr.add(new LocalResource(key1, false, false));\r\n        localizer.updateBlobs();\r\n        assertTrue(versionFile.exists(), \"blob version file not created\");\r\n        assertEquals(3, LocalizedResource.localVersionOfBlob(keyVersionFile), \"blob version not correct\");\r\n        assertTrue(new File(expectedFileDir, key1 + \".3\").exists(), \"blob file with version 3 not created\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\AsyncLocalizerTest.java",
  "methodName" : "validatePNAImplementationsMatch",
  "sourceCode" : "@Test\r\npublic void validatePNAImplementationsMatch() {\r\n    LocalAssignment la = new LocalAssignment(\"Topology1\", null);\r\n    PortAndAssignment pna = new PortAndAssignmentImpl(1, la);\r\n    PortAndAssignment tpna = new TimePortAndAssignment(pna, new Timer());\r\n    assertEquals(pna, tpna);\r\n    assertEquals(tpna, pna);\r\n    assertEquals(pna.hashCode(), tpna.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocalizedResourceRetentionSetTest.java",
  "methodName" : "testAddResources",
  "sourceCode" : "@Test\r\npublic void testAddResources() {\r\n    PortAndAssignment pna1 = new PortAndAssignmentImpl(1, new LocalAssignment(\"topo1\", Collections.emptyList()));\r\n    PortAndAssignment pna2 = new PortAndAssignmentImpl(1, new LocalAssignment(\"topo2\", Collections.emptyList()));\r\n    String user = \"user\";\r\n    Map<String, Object> conf = new HashMap<>();\r\n    IAdvancedFSOps ops = mock(IAdvancedFSOps.class);\r\n    LocalizedResourceRetentionSet lrretset = new LocalizedResourceRetentionSet(10);\r\n    ConcurrentMap<String, LocalizedResource> lrset = new ConcurrentHashMap<>();\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    LocalizedResource localresource1 = new LocalizedResource(\"key1\", Paths.get(\"testfile1\"), false, ops, conf, user, metricsRegistry);\r\n    localresource1.addReference(pna1, null);\r\n    LocalizedResource localresource2 = new LocalizedResource(\"key2\", Paths.get(\"testfile2\"), false, ops, conf, user, metricsRegistry);\r\n    localresource2.addReference(pna1, null);\r\n    // check adding reference to local resource with topology of same name\r\n    localresource2.addReference(pna2, null);\r\n    lrset.put(\"key1\", localresource1);\r\n    lrset.put(\"key2\", localresource2);\r\n    lrretset.addResources(lrset);\r\n    assertEquals(0, lrretset.getSizeWithNoReferences(), \"number to clean is not 0 \" + lrretset.noReferences);\r\n    assertTrue(localresource1.removeReference(pna1));\r\n    lrretset = new LocalizedResourceRetentionSet(10);\r\n    lrretset.addResources(lrset);\r\n    assertEquals(1, lrretset.getSizeWithNoReferences(), \"number to clean is not 1 \" + lrretset.noReferences);\r\n    assertTrue(localresource2.removeReference(pna1));\r\n    lrretset = new LocalizedResourceRetentionSet(10);\r\n    lrretset.addResources(lrset);\r\n    assertEquals(1, lrretset.getSizeWithNoReferences(), \"number to clean is not 1  \" + lrretset.noReferences);\r\n    assertTrue(localresource2.removeReference(pna2));\r\n    lrretset = new LocalizedResourceRetentionSet(10);\r\n    lrretset.addResources(lrset);\r\n    assertEquals(2, lrretset.getSizeWithNoReferences(), \"number to clean is not 2 \" + lrretset.noReferences);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocalizedResourceRetentionSetTest.java",
  "methodName" : "testRemoveEquivalent",
  "sourceCode" : "@Test\r\npublic void testRemoveEquivalent() {\r\n    ExecutorInfo a = new ExecutorInfo(1, 1);\r\n    ExecutorInfo b = new ExecutorInfo(2, 2);\r\n    ExecutorInfo c = new ExecutorInfo(3, 3);\r\n    List<ExecutorInfo> origList = new ArrayList<>();\r\n    origList.add(a);\r\n    origList.add(b);\r\n    List<ExecutorInfo> equivList = new ArrayList<>();\r\n    equivList.add(b);\r\n    equivList.add(a);\r\n    List<ExecutorInfo> differentList = new ArrayList<>();\r\n    differentList.add(a);\r\n    differentList.add(c);\r\n    LocalAssignment laOrig = new LocalAssignment(\"topo1\", origList);\r\n    LocalAssignment laEquiv = new LocalAssignment(\"topo1\", equivList);\r\n    LocalAssignment laDifferent = new LocalAssignment(\"topo1\", differentList);\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(laOrig, laEquiv));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(laOrig, laDifferent));\r\n    PortAndAssignment pnaOrig = new PortAndAssignmentImpl(1, laOrig);\r\n    PortAndAssignment pnaEquiv = new PortAndAssignmentImpl(1, laEquiv);\r\n    PortAndAssignment pnaDifferent = new PortAndAssignmentImpl(1, laDifferent);\r\n    assertTrue(pnaOrig.isEquivalentTo(pnaEquiv));\r\n    assertFalse(pnaOrig.isEquivalentTo(pnaDifferent));\r\n    LocalizedResource localresource = new LocalizedResource(\"key1\", Paths.get(\"testfile1\"), false, mock(IAdvancedFSOps.class), new HashMap<>(), \"user\", new StormMetricsRegistry());\r\n    localresource.addReference(pnaOrig, null);\r\n    assertFalse(localresource.removeReference(pnaDifferent));\r\n    assertTrue(localresource.removeReference(pnaEquiv));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocalizedResourceRetentionSetTest.java",
  "methodName" : "testCleanup",
  "sourceCode" : "@Test\r\npublic void testCleanup() throws Exception {\r\n    ClientBlobStore mockBlobstore = mock(ClientBlobStore.class);\r\n    when(mockBlobstore.getBlobMeta(any())).thenReturn(new ReadableBlobMeta(new SettableBlobMeta(), 1));\r\n    PortAndAssignment pna1 = new PortAndAssignmentImpl(1, new LocalAssignment(\"topo1\", Collections.emptyList()));\r\n    String user = \"user\";\r\n    Map<String, Object> conf = new HashMap<>();\r\n    IAdvancedFSOps ops = mock(IAdvancedFSOps.class);\r\n    LocalizedResourceRetentionSet lrretset = spy(new LocalizedResourceRetentionSet(10));\r\n    ConcurrentMap<String, LocalizedResource> lrFiles = new ConcurrentHashMap<>();\r\n    ConcurrentMap<String, LocalizedResource> lrArchives = new ConcurrentHashMap<>();\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    // no reference to key1\r\n    LocalizedResource localresource1 = new LocalizedResource(\"key1\", Paths.get(\"./target/TESTING/testfile1\"), false, ops, conf, user, metricsRegistry);\r\n    localresource1.setSize(10);\r\n    // no reference to archive1\r\n    LocalizedResource archiveresource1 = new LocalizedResource(\"archive1\", Paths.get(\"./target/TESTING/testarchive1\"), true, ops, conf, user, metricsRegistry);\r\n    archiveresource1.setSize(20);\r\n    // reference to key2\r\n    LocalizedResource localresource2 = new LocalizedResource(\"key2\", Paths.get(\"./target/TESTING/testfile2\"), false, ops, conf, user, metricsRegistry);\r\n    localresource2.addReference(pna1, null);\r\n    // check adding reference to local resource with topology of same name\r\n    localresource2.addReference(pna1, null);\r\n    localresource2.setSize(10);\r\n    lrFiles.put(\"key1\", localresource1);\r\n    lrFiles.put(\"key2\", localresource2);\r\n    lrArchives.put(\"archive1\", archiveresource1);\r\n    lrretset.addResources(lrFiles);\r\n    lrretset.addResources(lrArchives);\r\n    assertEquals(2, lrretset.getSizeWithNoReferences(), \"number to clean is not 2\");\r\n    lrretset.cleanup(mockBlobstore);\r\n    assertEquals(0, lrretset.getSizeWithNoReferences(), \"resource not cleaned up\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocallyCachedBlobTest.java",
  "methodName" : "testNotUsed",
  "sourceCode" : "@Test\r\npublic void testNotUsed() throws KeyNotFoundException, AuthorizationException {\r\n    LocallyCachedBlob blob = new LocalizedResource(\"key\", Paths.get(\"/bogus\"), false, AdvancedFSOps.make(conf), conf, \"user1\", new StormMetricsRegistry());\r\n    assertFalse(blob.isUsed());\r\n    assertFalse(blob.requiresUpdate(blobStore, -1L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocallyCachedBlobTest.java",
  "methodName" : "testNotDownloaded",
  "sourceCode" : "@Test\r\npublic void testNotDownloaded() throws KeyNotFoundException, AuthorizationException {\r\n    LocallyCachedBlob blob = new LocalizedResource(\"key\", Paths.get(\"/bogus\"), false, AdvancedFSOps.make(conf), conf, \"user1\", new StormMetricsRegistry());\r\n    blob.addReference(pna, null);\r\n    assertTrue(blob.isUsed());\r\n    assertFalse(blob.isFullyDownloaded());\r\n    assertTrue(blob.requiresUpdate(blobStore, -1L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\localizer\\LocallyCachedBlobTest.java",
  "methodName" : "testOutOfDate",
  "sourceCode" : "@Test\r\npublic void testOutOfDate() throws KeyNotFoundException, AuthorizationException {\r\n    TestableBlob blob = new TestableBlob(\"key\", Paths.get(\"/bogus\"), false, AdvancedFSOps.make(conf), conf, \"user1\", new StormMetricsRegistry());\r\n    blob.addReference(pna, null);\r\n    assertTrue(blob.isUsed());\r\n    assertTrue(blob.isFullyDownloaded());\r\n    // validate blob needs update due to version mismatch\r\n    assertTrue(blob.requiresUpdate(blobStore, -1L));\r\n    // when blob update time matches remote blobstore update time, validate blob\r\n    // will skip looking at remote version and assume it's up to date\r\n    blob.localUpdateTime = 101L;\r\n    assertFalse(blob.requiresUpdate(blobStore, 101L));\r\n    // now when the update time on the remote blobstore differs, we should again see that the\r\n    // blob version differs from the remote blobstore\r\n    assertTrue(blob.requiresUpdate(blobStore, 102L));\r\n    // now validate we don't need any update as versions match, regardless of remote blobstore update time\r\n    blob.localVersion = blob.getRemoteVersion(blobStore);\r\n    assertFalse(blob.requiresUpdate(blobStore, -1L));\r\n    assertFalse(blob.requiresUpdate(blobStore, 101L));\r\n    assertFalse(blob.requiresUpdate(blobStore, 102L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\LocalStateTest.java",
  "methodName" : "testLocalState",
  "sourceCode" : "@Test\r\npublic void testLocalState() throws Exception {\r\n    try (TmpPath dir1_tmp = new TmpPath();\r\n        TmpPath dir2_tmp = new TmpPath()) {\r\n        GlobalStreamId globalStreamId_a = new GlobalStreamId(\"a\", \"a\");\r\n        GlobalStreamId globalStreamId_b = new GlobalStreamId(\"b\", \"b\");\r\n        GlobalStreamId globalStreamId_c = new GlobalStreamId(\"c\", \"c\");\r\n        GlobalStreamId globalStreamId_d = new GlobalStreamId(\"d\", \"d\");\r\n        LocalState ls1 = new LocalState(dir1_tmp.getPath(), true);\r\n        LocalState ls2 = new LocalState(dir2_tmp.getPath(), true);\r\n        assertTrue(ls1.snapshot().isEmpty());\r\n        ls1.put(\"a\", globalStreamId_a);\r\n        ls1.put(\"b\", globalStreamId_b);\r\n        Map<String, GlobalStreamId> expected = new HashMap<>();\r\n        expected.put(\"a\", globalStreamId_a);\r\n        expected.put(\"b\", globalStreamId_b);\r\n        assertEquals(expected, ls1.snapshot());\r\n        assertEquals(expected, new LocalState(dir1_tmp.getPath(), true).snapshot());\r\n        assertTrue(ls2.snapshot().isEmpty());\r\n        ls2.put(\"b\", globalStreamId_a);\r\n        ls2.put(\"b\", globalStreamId_b);\r\n        ls2.put(\"b\", globalStreamId_c);\r\n        ls2.put(\"b\", globalStreamId_d);\r\n        assertEquals(globalStreamId_d, ls2.get(\"b\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\LocalStateTest.java",
  "methodName" : "testEmptyState",
  "sourceCode" : "@Test\r\npublic void testEmptyState() throws IOException {\r\n    try (TmpPath tmp_dir = new TmpPath()) {\r\n        GlobalStreamId globalStreamId_a = new GlobalStreamId(\"a\", \"a\");\r\n        String dir = tmp_dir.getPath();\r\n        LocalState ls = new LocalState(dir, true);\r\n        FileUtils.touch(new File(dir, \"12345\"));\r\n        FileUtils.touch(new File(dir, \"12345.version\"));\r\n        assertNull(ls.get(\"c\"));\r\n        ls.put(\"a\", globalStreamId_a);\r\n        assertEquals(globalStreamId_a, ls.get(\"a\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\LocalStateTest.java",
  "methodName" : "testAllNulState",
  "sourceCode" : "@Test\r\npublic void testAllNulState() throws IOException {\r\n    try (TmpPath tmp_dir = new TmpPath()) {\r\n        GlobalStreamId globalStreamId_a = new GlobalStreamId(\"a\", \"a\");\r\n        String dir = tmp_dir.getPath();\r\n        LocalState ls = new LocalState(dir, true);\r\n        FileUtils.touch(new File(dir, \"12345.version\"));\r\n        try (FileOutputStream data = FileUtils.openOutputStream(new File(dir, \"12345\"))) {\r\n            assertNull(ls.get(\"c\"));\r\n            data.write(new byte[100]);\r\n            ls.put(\"a\", globalStreamId_a);\r\n            assertEquals(globalStreamId_a, ls.get(\"a\"));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\MessagingTest.java",
  "methodName" : "testLocalTransport",
  "sourceCode" : "@Test\r\npublic void testLocalTransport() throws Exception {\r\n    Config topoConf = new Config();\r\n    topoConf.put(Config.TOPOLOGY_WORKERS, 2);\r\n    topoConf.put(Config.STORM_MESSAGING_TRANSPORT, \"org.apache.storm.messaging.netty.Context\");\r\n    try (ILocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withSupervisors(1).withPortsPerSupervisor(2).withDaemonConf(topoConf).build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestWordSpout(true), 2);\r\n        builder.setBolt(\"2\", new TestGlobalCount(), 6).shuffleGrouping(\"1\");\r\n        StormTopology stormTopology = builder.createTopology();\r\n        List<FixedTuple> fixedTuples = new ArrayList<>();\r\n        for (int i = 0; i < 12; i++) {\r\n            fixedTuples.add(new FixedTuple(Collections.singletonList(\"a\")));\r\n            fixedTuples.add(new FixedTuple(Collections.singletonList(\"b\")));\r\n        }\r\n        Map<String, List<FixedTuple>> data = new HashMap<>();\r\n        data.put(\"1\", fixedTuples);\r\n        MockedSources mockedSources = new MockedSources(data);\r\n        CompleteTopologyParam completeTopologyParam = new CompleteTopologyParam();\r\n        completeTopologyParam.setMockedSources(mockedSources);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, stormTopology, completeTopologyParam);\r\n        assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\MessagingTest.java",
  "methodName" : "testRemoteTransportWithManyTasksInReceivingExecutor",
  "sourceCode" : "@Test\r\npublic void testRemoteTransportWithManyTasksInReceivingExecutor() throws Exception {\r\n    //STORM-3141 regression test\r\n    //Verify that remote worker can handle many tasks in one executor\r\n    Config topoConf = new Config();\r\n    topoConf.put(Config.TOPOLOGY_WORKERS, 2);\r\n    topoConf.put(Config.STORM_MESSAGING_TRANSPORT, \"org.apache.storm.messaging.netty.Context\");\r\n    try (ILocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().withSupervisors(1).withPortsPerSupervisor(2).withDaemonConf(topoConf).build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"1\", new TestWordSpout(true), 1);\r\n        builder.setBolt(\"2\", new TestGlobalCount(), 1).setNumTasks(10).shuffleGrouping(\"1\");\r\n        StormTopology stormTopology = builder.createTopology();\r\n        List<FixedTuple> fixedTuples = new ArrayList<>();\r\n        for (int i = 0; i < 12; i++) {\r\n            fixedTuples.add(new FixedTuple(Collections.singletonList(\"a\")));\r\n            fixedTuples.add(new FixedTuple(Collections.singletonList(\"b\")));\r\n        }\r\n        Map<String, List<FixedTuple>> data = new HashMap<>();\r\n        data.put(\"1\", fixedTuples);\r\n        MockedSources mockedSources = new MockedSources(data);\r\n        CompleteTopologyParam completeTopologyParam = new CompleteTopologyParam();\r\n        completeTopologyParam.setMockedSources(mockedSources);\r\n        Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, stormTopology, completeTopologyParam);\r\n        assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metric\\ClusterMetricsConsumerExecutorTest.java",
  "methodName" : "testPrepareDoesNotThrowExceptionWhenInitializingClusterMetricsConsumerIsFailing",
  "sourceCode" : "@Test\r\npublic void testPrepareDoesNotThrowExceptionWhenInitializingClusterMetricsConsumerIsFailing() {\r\n    ClusterMetricsConsumerExecutor sut = new ClusterMetricsConsumerExecutor(MockFailingClusterMetricsConsumer.class.getName(), 2);\r\n    // it shouldn't propagate any exceptions\r\n    sut.prepare();\r\n    sut.prepare();\r\n    assertEquals(2, MockFailingClusterMetricsConsumer.getPrepareCallCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metric\\ClusterMetricsConsumerExecutorTest.java",
  "methodName" : "testHandleDataPointsWithClusterMetricsShouldSkipHandlingMetricsIfFailedBefore",
  "sourceCode" : "@Test\r\npublic void testHandleDataPointsWithClusterMetricsShouldSkipHandlingMetricsIfFailedBefore() {\r\n    ClusterMetricsConsumerExecutor sut = new ClusterMetricsConsumerExecutor(MockFailingClusterMetricsConsumer.class.getName(), 2);\r\n    // below calls shouldn't propagate any exceptions\r\n    sut.prepare();\r\n    // no specific reason to mock... this is one of the easiest ways to make dummy instance\r\n    sut.handleDataPoints(mock(IClusterMetricsConsumer.ClusterInfo.class), Collections.emptyList());\r\n    assertEquals(1, MockFailingClusterMetricsConsumer.getPrepareCallCount());\r\n    assertEquals(0, MockFailingClusterMetricsConsumer.getHandleDataPointsWithClusterInfoCallCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metric\\ClusterMetricsConsumerExecutorTest.java",
  "methodName" : "testHandleDataPointsWithSupervisorMetricsShouldRetryInitializingClusterMetricsConsumerIfFailedBefore",
  "sourceCode" : "@Test\r\npublic void testHandleDataPointsWithSupervisorMetricsShouldRetryInitializingClusterMetricsConsumerIfFailedBefore() {\r\n    ClusterMetricsConsumerExecutor sut = new ClusterMetricsConsumerExecutor(MockFailingClusterMetricsConsumer.class.getName(), 2);\r\n    // below calls shouldn't propagate any exceptions\r\n    sut.prepare();\r\n    // no specific reason to mock... this is one of the easiest ways to make dummy instance\r\n    sut.handleDataPoints(mock(IClusterMetricsConsumer.SupervisorInfo.class), Collections.emptyList());\r\n    assertEquals(1, MockFailingClusterMetricsConsumer.getPrepareCallCount());\r\n    assertEquals(0, MockFailingClusterMetricsConsumer.getHandleDataPointsWithSupervisorInfoCallCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbKeyTest.java",
  "methodName" : "testConstructors",
  "sourceCode" : "@Test\r\npublic void testConstructors() {\r\n    byte[] raw = new byte[RocksDbKey.KEY_SIZE];\r\n    raw[0] = KeyType.COMPONENT_STRING.getValue();\r\n    raw[2] = 0x01;\r\n    raw[3] = 0x02;\r\n    raw[4] = 0x03;\r\n    raw[5] = 0x04;\r\n    RocksDbKey rawKey = new RocksDbKey(raw);\r\n    RocksDbKey metadataKey = new RocksDbKey(KeyType.COMPONENT_STRING, 0x01020304);\r\n    assertEquals(0, metadataKey.compareTo(rawKey));\r\n    assertEquals(KeyType.COMPONENT_STRING, metadataKey.getType());\r\n    metadataKey = new RocksDbKey(KeyType.TOPOLOGY_STRING, 0x01020304);\r\n    assertTrue(metadataKey.compareTo(rawKey) < 0);\r\n    assertEquals(KeyType.TOPOLOGY_STRING, metadataKey.getType());\r\n    metadataKey = new RocksDbKey(KeyType.COMPONENT_STRING, 0x01020305);\r\n    assertTrue(metadataKey.compareTo(rawKey) > 0);\r\n    assertEquals(0x01020304, rawKey.getTopologyId());\r\n    assertEquals(KeyType.COMPONENT_STRING, rawKey.getType());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbKeyTest.java",
  "methodName" : "testMetricKey",
  "sourceCode" : "@Test\r\npublic void testMetricKey() {\r\n    AggLevel aggLevel = AggLevel.AGG_LEVEL_10_MIN;\r\n    int topologyId = 0x45665;\r\n    long timestamp = System.currentTimeMillis();\r\n    int metricId = 0xF3916034;\r\n    int componentId = 0x82915031;\r\n    int executorId = 0x434738;\r\n    int hostId = 0x4348394;\r\n    int port = 3456;\r\n    int streamId = 0x84221956;\r\n    RocksDbKey key = RocksDbKey.createMetricKey(aggLevel, topologyId, timestamp, metricId, componentId, executorId, hostId, port, streamId);\r\n    assertEquals(topologyId, key.getTopologyId());\r\n    assertEquals(timestamp, key.getTimestamp());\r\n    assertEquals(metricId, key.getMetricId());\r\n    assertEquals(componentId, key.getComponentId());\r\n    assertEquals(executorId, key.getExecutorId());\r\n    assertEquals(hostId, key.getHostnameId());\r\n    assertEquals(port, key.getPort());\r\n    assertEquals(streamId, key.getStreamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbStoreTest.java",
  "methodName" : "testAggregation",
  "sourceCode" : "@Test\r\npublic void testAggregation() throws Exception {\r\n    double sum0 = 0.0;\r\n    double sum1 = 0.0;\r\n    double sum10 = 0.0;\r\n    double sum60 = 0.0;\r\n    Metric toPopulate = null;\r\n    for (int i = 0; i < 20; i++) {\r\n        double value = 5 + i;\r\n        long timestamp = 1L + i * 60 * 1000;\r\n        Metric m = new Metric(\"cpu\", timestamp, \"myTopologyId123\", value, \"componentId1\", \"executorId1\", \"hostname1\", \"streamid1\", 7777, AggLevel.AGG_LEVEL_NONE);\r\n        toPopulate = new Metric(m);\r\n        store.insert(m);\r\n        if (timestamp < 60 * 1000) {\r\n            sum0 += value;\r\n            sum1 += value;\r\n            sum10 += value;\r\n            sum60 += value;\r\n        } else if (timestamp < 600 * 1000) {\r\n            sum10 += value;\r\n            sum60 += value;\r\n        } else {\r\n            sum60 += value;\r\n        }\r\n    }\r\n    waitForInsertFinish(toPopulate);\r\n    toPopulate.setTimestamp(1L);\r\n    toPopulate.setAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    boolean res = store.populateValue(toPopulate);\r\n    assertTrue(res);\r\n    assertEquals(sum0, toPopulate.getSum(), 0.001);\r\n    assertEquals(sum0, toPopulate.getValue(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMin(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMax(), 0.001);\r\n    assertEquals(1, toPopulate.getCount());\r\n    toPopulate.setTimestamp(0L);\r\n    toPopulate.setAggLevel(AggLevel.AGG_LEVEL_1_MIN);\r\n    res = store.populateValue(toPopulate);\r\n    assertTrue(res);\r\n    assertEquals(sum1, toPopulate.getSum(), 0.001);\r\n    assertEquals(sum1, toPopulate.getValue(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMin(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMax(), 0.001);\r\n    assertEquals(1, toPopulate.getCount());\r\n    toPopulate.setTimestamp(0L);\r\n    toPopulate.setAggLevel(AggLevel.AGG_LEVEL_10_MIN);\r\n    res = store.populateValue(toPopulate);\r\n    assertTrue(res);\r\n    assertEquals(sum10, toPopulate.getSum(), 0.001);\r\n    assertEquals(sum10 / 10.0, toPopulate.getValue(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMin(), 0.001);\r\n    assertEquals(14.0, toPopulate.getMax(), 0.001);\r\n    assertEquals(10, toPopulate.getCount());\r\n    toPopulate.setTimestamp(0L);\r\n    toPopulate.setAggLevel(AggLevel.AGG_LEVEL_60_MIN);\r\n    res = store.populateValue(toPopulate);\r\n    assertTrue(res);\r\n    assertEquals(sum60, toPopulate.getSum(), 0.001);\r\n    assertEquals(sum60 / 20.0, toPopulate.getValue(), 0.001);\r\n    assertEquals(5.0, toPopulate.getMin(), 0.001);\r\n    assertEquals(24.0, toPopulate.getMax(), 0.001);\r\n    assertEquals(20, toPopulate.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbStoreTest.java",
  "methodName" : "testPopulateFailure",
  "sourceCode" : "@Test\r\npublic void testPopulateFailure() throws Exception {\r\n    Metric m = new Metric(\"cpu\", 3000L, \"myTopologyId456\", 1.0, \"componentId2\", \"executorId2\", \"hostname2\", \"streamid2\", 7778, AggLevel.AGG_LEVEL_NONE);\r\n    store.insert(m);\r\n    waitForInsertFinish(m);\r\n    Metric toFind = new Metric(m);\r\n    toFind.setTopologyId(\"somethingBogus\");\r\n    boolean res = store.populateValue(toFind);\r\n    assertFalse(res);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbStoreTest.java",
  "methodName" : "testScan",
  "sourceCode" : "@Test\r\npublic void testScan() throws Exception {\r\n    FilterOptions filter;\r\n    List<Metric> list;\r\n    Metric m1 = new Metric(\"metricType1\", 50000000L, \"Topo-m1\", 1.0, \"component-1\", \"executor-2\", \"hostname-1\", \"stream-1\", 1, AggLevel.AGG_LEVEL_NONE);\r\n    Metric m2 = new Metric(\"metricType2\", 50030000L, \"Topo-m1\", 1.0, \"component-1\", \"executor-1\", \"hostname-2\", \"stream-2\", 1, AggLevel.AGG_LEVEL_NONE);\r\n    Metric m3 = new Metric(\"metricType3\", 50200000L, \"Topo-m1\", 1.0, \"component-2\", \"executor-1\", \"hostname-1\", \"stream-3\", 1, AggLevel.AGG_LEVEL_NONE);\r\n    Metric m4 = new Metric(\"metricType4\", 50200000L, \"Topo-m2\", 1.0, \"component-2\", \"executor-1\", \"hostname-2\", \"stream-4\", 2, AggLevel.AGG_LEVEL_NONE);\r\n    store.insert(m1);\r\n    store.insert(m2);\r\n    store.insert(m3);\r\n    store.insert(m4);\r\n    waitForInsertFinish(m4);\r\n    // validate search by time\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setStartTime(50000000L);\r\n    filter.setEndTime(50130000L);\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(2, list.size());\r\n    assertTrue(list.contains(m1));\r\n    assertTrue(list.contains(m2));\r\n    // validate search by topology id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setTopologyId(\"Topo-m2\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(1, list.size());\r\n    assertTrue(list.contains(m4));\r\n    // validate search by metric id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setMetricName(\"metricType2\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(1, list.size());\r\n    assertTrue(list.contains(m2));\r\n    // validate search by component id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setComponentId(\"component-2\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(2, list.size());\r\n    assertTrue(list.contains(m3));\r\n    assertTrue(list.contains(m4));\r\n    // validate search by executor id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setExecutorId(\"executor-1\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(3, list.size());\r\n    assertTrue(list.contains(m2));\r\n    assertTrue(list.contains(m3));\r\n    assertTrue(list.contains(m4));\r\n    // validate search by executor id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setExecutorId(\"executor-1\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(3, list.size());\r\n    assertTrue(list.contains(m2));\r\n    assertTrue(list.contains(m3));\r\n    assertTrue(list.contains(m4));\r\n    // validate search by host id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setHostId(\"hostname-2\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(2, list.size());\r\n    assertTrue(list.contains(m2));\r\n    assertTrue(list.contains(m4));\r\n    // validate search by port\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setPort(1);\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(3, list.size());\r\n    assertTrue(list.contains(m1));\r\n    assertTrue(list.contains(m2));\r\n    assertTrue(list.contains(m3));\r\n    // validate search by stream id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setStreamId(\"stream-4\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(1, list.size());\r\n    assertTrue(list.contains(m4));\r\n    // validate 4 metrics (aggregations) found for m4 for all agglevels when searching by port\r\n    filter = new FilterOptions();\r\n    filter.setPort(2);\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(4, list.size());\r\n    assertTrue(list.contains(m4));\r\n    assertFalse(list.contains(m1));\r\n    assertFalse(list.contains(m2));\r\n    assertFalse(list.contains(m3));\r\n    // validate search by topology id and executor id\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    filter.setTopologyId(\"Topo-m1\");\r\n    filter.setExecutorId(\"executor-1\");\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(2, list.size());\r\n    assertTrue(list.contains(m2));\r\n    assertTrue(list.contains(m3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbStoreTest.java",
  "methodName" : "testMetricCleanup",
  "sourceCode" : "@Test\r\npublic void testMetricCleanup() throws Exception {\r\n    FilterOptions filter;\r\n    List<Metric> list;\r\n    // Share some common metadata strings to validate they do not get deleted\r\n    String commonTopologyId = \"topology-cleanup-2\";\r\n    String commonStreamId = \"stream-cleanup-5\";\r\n    String defaultS = \"default\";\r\n    Metric m1 = new Metric(defaultS, 40000000L, commonTopologyId, 1.0, \"component-1\", defaultS, \"hostname-1\", commonStreamId, 1, AggLevel.AGG_LEVEL_NONE);\r\n    Metric m2 = new Metric(defaultS, System.currentTimeMillis(), commonTopologyId, 1.0, \"component-1\", \"executor-1\", defaultS, commonStreamId, 1, AggLevel.AGG_LEVEL_NONE);\r\n    store.insert(m1);\r\n    store.insert(m2);\r\n    waitForInsertFinish(m2);\r\n    // validate at least two agg level none metrics exist\r\n    filter = new FilterOptions();\r\n    filter.addAggLevel(AggLevel.AGG_LEVEL_NONE);\r\n    list = getMetricsFromScan(filter);\r\n    assertTrue(list.size() >= 2);\r\n    // delete anything older than an hour\r\n    MetricsCleaner cleaner = new MetricsCleaner((RocksDbStore) store, 1, 1, null, new StormMetricsRegistry());\r\n    cleaner.purgeMetrics();\r\n    list = getMetricsFromScan(filter);\r\n    assertEquals(1, list.size());\r\n    assertTrue(list.contains(m2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbValueTest.java",
  "methodName" : "testMetadataConstructor",
  "sourceCode" : "@Test\r\npublic void testMetadataConstructor() {\r\n    long timestamp = System.currentTimeMillis();\r\n    String s = \"MyTopology123\";\r\n    RocksDbValue value = new RocksDbValue(timestamp, s);\r\n    assertEquals(timestamp, value.getLastTimestamp());\r\n    assertEquals(s, value.getMetdataString());\r\n    RocksDbValue value2 = new RocksDbValue(value.getRaw());\r\n    assertEquals(timestamp, value2.getLastTimestamp());\r\n    assertEquals(s, value2.getMetdataString());\r\n    int stringId = 0x509;\r\n    RocksDbKey key = new RocksDbKey(KeyType.EXEC_ID_STRING, stringId);\r\n    StringMetadata metadata = value2.getStringMetadata(key);\r\n    assertEquals(stringId, metadata.getStringId());\r\n    assertEquals(timestamp, metadata.getLastTimestamp());\r\n    assertEquals(1, metadata.getMetadataTypes().size());\r\n    assertEquals(KeyType.EXEC_ID_STRING, metadata.getMetadataTypes().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\RocksDbValueTest.java",
  "methodName" : "testMetricConstructor",
  "sourceCode" : "@Test\r\npublic void testMetricConstructor() throws MetricException {\r\n    Metric m = new Metric(\"cpu\", 1L, \"myTopologyId123\", 1, \"componentId1\", \"executorId1\", \"hostname1\", \"streamid1\", 7777, AggLevel.AGG_LEVEL_NONE);\r\n    Metric m2 = new Metric(m);\r\n    Metric m3 = new Metric(m);\r\n    m.addValue(238);\r\n    RocksDbValue value = new RocksDbValue(m);\r\n    value.populateMetric(m2);\r\n    assertEquals(m.getValue(), m2.getValue(), 0x001);\r\n    assertEquals(m.getCount(), m2.getCount(), 0x001);\r\n    assertEquals(m.getSum(), m2.getSum(), 0x001);\r\n    assertEquals(m.getMin(), m2.getMin(), 0x001);\r\n    assertEquals(m.getMax(), m2.getMax(), 0x001);\r\n    RocksDbValue value2 = new RocksDbValue(value.getRaw());\r\n    value2.populateMetric(m3);\r\n    assertEquals(m.getValue(), m3.getValue(), 0x001);\r\n    assertEquals(m.getCount(), m3.getCount(), 0x001);\r\n    assertEquals(m.getSum(), m3.getSum(), 0x001);\r\n    assertEquals(m.getMin(), m3.getMin(), 0x001);\r\n    assertEquals(m.getMax(), m3.getMax(), 0x001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\StringMetadataCacheTest.java",
  "methodName" : "validateEviction",
  "sourceCode" : "@Test\r\npublic void validateEviction() throws MetricException {\r\n    TestDbWriter writer = new TestDbWriter();\r\n    StringMetadataCache.init(writer, 2);\r\n    WritableStringMetadataCache wCache = StringMetadataCache.getWritableStringMetadataCache();\r\n    ReadOnlyStringMetadataCache rCache = StringMetadataCache.getReadOnlyStringMetadataCache();\r\n    String s1 = \"string1\";\r\n    Integer s1Id = 1;\r\n    long s1Timestamp = 1L;\r\n    StringMetadata metadata1 = new StringMetadata(KeyType.STREAM_ID_STRING, s1Id, s1Timestamp);\r\n    wCache.put(s1, metadata1, false);\r\n    assertEquals(metadata1, rCache.get(s1));\r\n    assertTrue(rCache.contains(s1Id));\r\n    assertEquals(s1, rCache.getMetadataString(s1Id));\r\n    String s2 = \"string2\";\r\n    Integer s2Id = 2;\r\n    long s2Timestamp = 2L;\r\n    StringMetadata metadata2 = new StringMetadata(KeyType.EXEC_ID_STRING, s2Id, s2Timestamp);\r\n    wCache.put(s2, metadata2, false);\r\n    assertEquals(metadata2, rCache.get(s2));\r\n    assertTrue(rCache.contains(s2Id));\r\n    assertEquals(s2, rCache.getMetadataString(s2Id));\r\n    assertEquals(false, writer.evictCalled);\r\n    // read s1 last....  This should cause s2 to be evicted on next put\r\n    rCache.get(s1);\r\n    String s3 = \"string3\";\r\n    Integer s3Id = 3;\r\n    long s3Timestamp = 3L;\r\n    StringMetadata metadata3 = new StringMetadata(KeyType.TOPOLOGY_STRING, s3Id, s3Timestamp);\r\n    wCache.put(s3, metadata3, false);\r\n    assertEquals(true, writer.evictCalled);\r\n    assertEquals(metadata3, rCache.get(s3));\r\n    assertTrue(rCache.contains(s3Id));\r\n    assertEquals(s3, rCache.getMetadataString(s3Id));\r\n    // since s2 read last, it should be evicted, s1 and s3 should exist\r\n    assertEquals(null, rCache.get(s2));\r\n    assertFalse(rCache.contains(s2Id));\r\n    assertEquals(metadata1, rCache.get(s1));\r\n    assertTrue(rCache.contains(s1Id));\r\n    assertEquals(s1, rCache.getMetadataString(s1Id));\r\n    StringMetadataCache.cleanUp();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\metricstore\\rocksdb\\StringMetadataCacheTest.java",
  "methodName" : "validateMultipleKeyTypes",
  "sourceCode" : "@Test\r\npublic void validateMultipleKeyTypes() throws MetricException {\r\n    TestDbWriter writer = new TestDbWriter();\r\n    StringMetadataCache.init(writer, 2);\r\n    WritableStringMetadataCache wCache = StringMetadataCache.getWritableStringMetadataCache();\r\n    StringMetadata metadata = new StringMetadata(KeyType.STREAM_ID_STRING, 1, 1L);\r\n    wCache.put(\"default\", metadata, false);\r\n    metadata = wCache.get(\"default\");\r\n    metadata.update(3L, KeyType.COMPONENT_STRING);\r\n    metadata = wCache.get(\"default\");\r\n    metadata.update(2L, KeyType.STREAM_ID_STRING);\r\n    metadata = wCache.get(\"default\");\r\n    assertEquals(2, metadata.getMetadataTypes().size());\r\n    assertTrue(metadata.getMetadataTypes().contains(KeyType.STREAM_ID_STRING));\r\n    assertTrue(metadata.getMetadataTypes().contains(KeyType.COMPONENT_STRING));\r\n    assertEquals(3L, metadata.getLastTimestamp());\r\n    StringMetadataCache.cleanUp();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\nimbus\\LocalNimbusTest.java",
  "methodName" : "testSubmitTopologyToLocalNimbus",
  "sourceCode" : "@Test\r\npublic void testSubmitTopologyToLocalNimbus() throws Exception {\r\n    int port = Utils.getAvailablePort();\r\n    try (ILocalCluster localCluster = new LocalCluster.Builder().withNimbusDaemon(true).withDaemonConf(Config.NIMBUS_THRIFT_PORT, port).build()) {\r\n        Config topoConf = new Config();\r\n        topoConf.putAll(Utils.readDefaultConfig());\r\n        topoConf.setDebug(true);\r\n        // default is aways \"distributed\" but here local cluster is being used.\r\n        topoConf.put(\"storm.cluster.mode\", \"local\");\r\n        topoConf.put(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN, InmemoryTopologySubmitterHook.class.getName());\r\n        topoConf.put(Config.NIMBUS_THRIFT_PORT, port);\r\n        List<TopologyDetails> topologyNames = new ArrayList<>();\r\n        for (int i = 0; i < 4; i++) {\r\n            final String topologyName = \"word-count-\" + UUID.randomUUID();\r\n            final StormTopology stormTopology = createTestTopology();\r\n            topologyNames.add(new TopologyDetails(topologyName, stormTopology));\r\n            localCluster.submitTopology(topologyName, topoConf, stormTopology);\r\n        }\r\n        assertEquals(InmemoryTopologySubmitterHook.submittedTopologies, topologyNames);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerCreatePath",
  "sourceCode" : "@Test\r\npublic void testServerCreatePath() {\r\n    messageWithRandId(HBServerMessageType.CREATE_PATH, HBMessageData.path(\"/testpath\"));\r\n    HBMessage response = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.CREATE_PATH_RESPONSE, response.get_type());\r\n    assertNull(response.get_data());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerExistsFalse",
  "sourceCode" : "@Test\r\npublic void testServerExistsFalse() {\r\n    messageWithRandId(HBServerMessageType.EXISTS, HBMessageData.path(\"/testpath\"));\r\n    HBMessage badResponse = handler.handleMessage(hbMessage, false);\r\n    HBMessage goodResponse = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.EXISTS_RESPONSE, goodResponse.get_type());\r\n    assertFalse(goodResponse.get_data().get_boolval());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerExistsTrue",
  "sourceCode" : "@Test\r\npublic void testServerExistsTrue() {\r\n    String path = \"/exists_path\";\r\n    String dataString = \"pulse data\";\r\n    HBPulse hbPulse = new HBPulse();\r\n    hbPulse.set_id(path);\r\n    hbPulse.set_details(Utils.javaSerialize(dataString));\r\n    messageWithRandId(HBServerMessageType.SEND_PULSE, HBMessageData.pulse(hbPulse));\r\n    handler.handleMessage(hbMessage, true);\r\n    messageWithRandId(HBServerMessageType.EXISTS, HBMessageData.path(path));\r\n    HBMessage badResponse = handler.handleMessage(hbMessage, false);\r\n    HBMessage goodResponse = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.EXISTS_RESPONSE, goodResponse.get_type());\r\n    assertTrue(goodResponse.get_data().get_boolval());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerSendPulseGetPulse",
  "sourceCode" : "@Test\r\npublic void testServerSendPulseGetPulse() {\r\n    String path = \"/pulsepath\";\r\n    String dataString = \"pulse data\";\r\n    HBPulse hbPulse = new HBPulse();\r\n    hbPulse.set_id(path);\r\n    hbPulse.set_details(dataString.getBytes(StandardCharsets.UTF_8));\r\n    messageWithRandId(HBServerMessageType.SEND_PULSE, HBMessageData.pulse(hbPulse));\r\n    HBMessage sendResponse = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, sendResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.SEND_PULSE_RESPONSE, sendResponse.get_type());\r\n    assertNull(sendResponse.get_data());\r\n    messageWithRandId(HBServerMessageType.GET_PULSE, HBMessageData.path(path));\r\n    HBMessage response = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_PULSE_RESPONSE, response.get_type());\r\n    assertEquals(dataString, new String(response.get_data().get_pulse().get_details(), StandardCharsets.UTF_8));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerGetAllPulseForPath",
  "sourceCode" : "@Test\r\npublic void testServerGetAllPulseForPath() {\r\n    messageWithRandId(HBServerMessageType.GET_ALL_PULSE_FOR_PATH, HBMessageData.path(\"/testpath\"));\r\n    HBMessage badResponse = handler.handleMessage(hbMessage, false);\r\n    HBMessage goodResponse = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_ALL_PULSE_FOR_PATH_RESPONSE, goodResponse.get_type());\r\n    assertNull(goodResponse.get_data());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerGetAllNodesForPath",
  "sourceCode" : "@Test\r\npublic void testServerGetAllNodesForPath() throws UnsupportedEncodingException {\r\n    makeNode(handler, \"/some-root-path/foo\");\r\n    makeNode(handler, \"/some-root-path/bar\");\r\n    makeNode(handler, \"/some-root-path/baz\");\r\n    makeNode(handler, \"/some-root-path/boo\");\r\n    messageWithRandId(HBServerMessageType.GET_ALL_NODES_FOR_PATH, HBMessageData.path(\"/some-root-path\"));\r\n    HBMessage badResponse = handler.handleMessage(hbMessage, false);\r\n    HBMessage goodResponse = handler.handleMessage(hbMessage, true);\r\n    List<String> pulseIds = goodResponse.get_data().get_nodes().get_pulseIds();\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, goodResponse.get_type());\r\n    assertTrue(pulseIds.contains(\"foo\"));\r\n    assertTrue(pulseIds.contains(\"bar\"));\r\n    assertTrue(pulseIds.contains(\"baz\"));\r\n    assertTrue(pulseIds.contains(\"boo\"));\r\n    makeNode(handler, \"/some/deeper/path/foo\");\r\n    makeNode(handler, \"/some/deeper/path/bar\");\r\n    makeNode(handler, \"/some/deeper/path/baz\");\r\n    messageWithRandId(HBServerMessageType.GET_ALL_NODES_FOR_PATH, HBMessageData.path(\"/some/deeper/path\"));\r\n    badResponse = handler.handleMessage(hbMessage, false);\r\n    goodResponse = handler.handleMessage(hbMessage, true);\r\n    pulseIds = goodResponse.get_data().get_nodes().get_pulseIds();\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, goodResponse.get_type());\r\n    assertTrue(pulseIds.contains(\"foo\"));\r\n    assertTrue(pulseIds.contains(\"bar\"));\r\n    assertTrue(pulseIds.contains(\"baz\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerGetPulse",
  "sourceCode" : "@Test\r\npublic void testServerGetPulse() throws UnsupportedEncodingException {\r\n    makeNode(handler, \"/some-root/GET_PULSE\");\r\n    messageWithRandId(HBServerMessageType.GET_PULSE, HBMessageData.path(\"/some-root/GET_PULSE\"));\r\n    HBMessage badResponse = handler.handleMessage(hbMessage, false);\r\n    HBMessage goodResponse = handler.handleMessage(hbMessage, true);\r\n    HBPulse goodPulse = goodResponse.get_data().get_pulse();\r\n    assertEquals(mid, badResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.NOT_AUTHORIZED, badResponse.get_type());\r\n    assertNull(badResponse.get_data());\r\n    assertEquals(mid, goodResponse.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_PULSE_RESPONSE, goodResponse.get_type());\r\n    assertEquals(\"/some-root/GET_PULSE\", goodPulse.get_id());\r\n    assertEquals(\"nothing\", new String(goodPulse.get_details(), StandardCharsets.UTF_8));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerDeletePath",
  "sourceCode" : "@Test\r\npublic void testServerDeletePath() throws UnsupportedEncodingException {\r\n    makeNode(handler, \"/some-root/DELETE_PATH/foo\");\r\n    makeNode(handler, \"/some-root/DELETE_PATH/bar\");\r\n    makeNode(handler, \"/some-root/DELETE_PATH/baz\");\r\n    makeNode(handler, \"/some-root/DELETE_PATH/boo\");\r\n    messageWithRandId(HBServerMessageType.DELETE_PATH, HBMessageData.path(\"/some-root/DELETE_PATH\"));\r\n    HBMessage response = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.DELETE_PATH_RESPONSE, response.get_type());\r\n    assertNull(response.get_data());\r\n    messageWithRandId(HBServerMessageType.GET_ALL_NODES_FOR_PATH, HBMessageData.path(\"/some-root/DELETE_PATH\"));\r\n    response = handler.handleMessage(hbMessage, true);\r\n    List<String> pulseIds = response.get_data().get_nodes().get_pulseIds();\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, response.get_type());\r\n    assertTrue(pulseIds.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\PacemakerTest.java",
  "methodName" : "testServerDeletePulseId",
  "sourceCode" : "@Test\r\npublic void testServerDeletePulseId() throws UnsupportedEncodingException {\r\n    makeNode(handler, \"/some-root/DELETE_PULSE_ID/foo\");\r\n    makeNode(handler, \"/some-root/DELETE_PULSE_ID/bar\");\r\n    makeNode(handler, \"/some-root/DELETE_PULSE_ID/baz\");\r\n    makeNode(handler, \"/some-root/DELETE_PULSE_ID/boo\");\r\n    messageWithRandId(HBServerMessageType.DELETE_PULSE_ID, HBMessageData.path(\"/some-root/DELETE_PULSE_ID/foo\"));\r\n    HBMessage response = handler.handleMessage(hbMessage, true);\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.DELETE_PULSE_ID_RESPONSE, response.get_type());\r\n    assertNull(response.get_data());\r\n    messageWithRandId(HBServerMessageType.GET_ALL_NODES_FOR_PATH, HBMessageData.path(\"/some-root/DELETE_PULSE_ID\"));\r\n    response = handler.handleMessage(hbMessage, true);\r\n    List<String> pulseIds = response.get_data().get_nodes().get_pulseIds();\r\n    assertEquals(mid, response.get_message_id());\r\n    assertEquals(HBServerMessageType.GET_ALL_NODES_FOR_PATH_RESPONSE, response.get_type());\r\n    assertFalse(pulseIds.contains(\"foo\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "testBlacklistResumeWhenAckersWontFit",
  "sourceCode" : "@Test\r\npublic void testBlacklistResumeWhenAckersWontFit() throws InvalidTopologyException {\r\n    // 3 supervisors exist with 4 slots, 2 are blacklisted\r\n    // topology with given worker heap size would fit in 4 slots if ignoring ackers, needs 5 slots with ackers.\r\n    // verify that one of the supervisors will be resumed and topology will schedule.\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 300);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 3);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 1800);\r\n    config.put(DaemonConfig.STORM_WORKER_MIN_CPU_PCORE_PERCENT, 100);\r\n    config.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 10);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_ASSUME_SUPERVISOR_BAD_BASED_ON_BAD_SLOT, false);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 3);\r\n    config.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 128);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_STRATEGY, \"org.apache.storm.scheduler.blacklist.strategies.RasBlacklistStrategy\");\r\n    config.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, 1);\r\n    config.setNumWorkers(1);\r\n    config.put(Config.TOPOLOGY_ACKER_EXECUTORS, 4);\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, \"org.apache.storm.scheduler.resource.strategies.scheduling.GenericResourceAwareStrategy\");\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 512);\r\n    config.put(Config.WORKER_HEAP_MEMORY_MB, 768);\r\n    config.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4, 400.0d, 4096.0d);\r\n    Topologies noTopologies = new Topologies();\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<>(), noTopologies, config);\r\n    scheduler = new BlacklistScheduler(new ResourceAwareScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    scheduler.schedule(noTopologies, cluster);\r\n    Map<String, SupervisorDetails> removedSup0 = TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\");\r\n    Map<String, SupervisorDetails> removedSup0Sup1 = TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(removedSup0, \"sup-1\");\r\n    cluster = new Cluster(iNimbus, resourceMetrics, removedSup0Sup1, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), noTopologies, config);\r\n    scheduler.schedule(noTopologies, cluster);\r\n    scheduler.schedule(noTopologies, cluster);\r\n    scheduler.schedule(noTopologies, cluster);\r\n    // 2 supervisors blacklisted at this point.  Let's schedule the topology.\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = createResourceTopo(config);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    // for scheduling debug\r\n    boolean enableTraceLogging = false;\r\n    if (enableTraceLogging) {\r\n        Configurator.setAllLevels(LogManager.getRootLogger().getName(), Level.TRACE);\r\n    }\r\n    scheduler.schedule(topologies, cluster);\r\n    // topology should be fully scheduled with 1 host remaining blacklisted\r\n    String topoScheduleStatus = cluster.getStatus(\"testTopology-id\");\r\n    assertTrue(topoScheduleStatus.contains(\"Running - Fully Scheduled\"));\r\n    assertEquals(1, cluster.getBlacklistedHosts().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestBadSupervisor",
  "sourceCode" : "@Test\r\npublic void TestBadSupervisor() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<>(), topologies, config);\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestBadSlot",
  "sourceCode" : "@ParameterizedTest\r\n@ValueSource(booleans = { true, false })\r\npublic void TestBadSlot(boolean blacklistOnBadSlot) {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_ASSUME_SUPERVISOR_BAD_BASED_ON_BAD_SLOT, blacklistOnBadSlot);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removePortFromSupervisors(supMap, \"sup-0\", 0), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removePortFromSupervisors(supMap, \"sup-0\", 0), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    if (blacklistOnBadSlot) {\r\n        assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n    } else {\r\n        assertEquals(Collections.emptySet(), cluster.getBlacklistedHosts(), \"blacklist\");\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestResumeBlacklist",
  "sourceCode" : "@Test\r\npublic void TestResumeBlacklist() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n    for (int i = 0; i < 300 / 10 - 2; i++) {\r\n        scheduler.schedule(topologies, cluster);\r\n    }\r\n    assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n    scheduler.schedule(topologies, cluster);\r\n    assertEquals(Collections.emptySet(), cluster.getBlacklistedHosts(), \"blacklist\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestReleaseBlacklist",
  "sourceCode" : "@Test\r\npublic void TestReleaseBlacklist() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    TopologyDetails topo2 = TestUtilsForBlacklistScheduler.getTopology(\"topo-2\", config, 5, 15, 1, 1, currentTime - 8, true);\r\n    TopologyDetails topo3 = TestUtilsForBlacklistScheduler.getTopology(\"topo-3\", config, 5, 15, 1, 1, currentTime - 16, true);\r\n    TopologyDetails topo4 = TestUtilsForBlacklistScheduler.getTopology(\"topo-4\", config, 5, 15, 1, 1, currentTime - 32, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n    topoMap.put(topo2.getId(), topo2);\r\n    topoMap.put(topo3.getId(), topo3);\r\n    topoMap.put(topo4.getId(), topo4);\r\n    topologies = new Topologies(topoMap);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n    assertEquals(Collections.emptySet(), cluster.getBlacklistedHosts(), \"blacklist\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestGreylist",
  "sourceCode" : "@Test\r\npublic void TestGreylist() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(2, 3);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    config.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 0.0);\r\n    config.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 0);\r\n    config.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 0);\r\n    config.put(Config.TOPOLOGY_RAS_ONE_EXECUTOR_PER_WORKER, true);\r\n    Class[] strategyClasses = { DefaultResourceAwareStrategy.class, DefaultResourceAwareStrategyOld.class, RoundRobinResourceAwareStrategy.class, GenericResourceAwareStrategy.class, GenericResourceAwareStrategyOld.class };\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, strategyClassName);\r\n        {\r\n            Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n            TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 1, 1, 1, 1, currentTime - 2, true);\r\n            TopologyDetails topo2 = TestUtilsForBlacklistScheduler.getTopology(\"topo-2\", config, 1, 1, 1, 1, currentTime - 8, true);\r\n            Topologies topologies = new Topologies(topoMap);\r\n            StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n            ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n            Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n            scheduler = new BlacklistScheduler(new ResourceAwareScheduler());\r\n            scheduler.prepare(config, metricsRegistry);\r\n            scheduler.schedule(topologies, cluster);\r\n            cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n            scheduler.schedule(topologies, cluster);\r\n            cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n            scheduler.schedule(topologies, cluster);\r\n            cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n            scheduler.schedule(topologies, cluster);\r\n            assertEquals(Collections.singleton(\"host-0\"), cluster.getBlacklistedHosts(), \"blacklist\");\r\n            topoMap.put(topo1.getId(), topo1);\r\n            topoMap.put(topo2.getId(), topo2);\r\n            topologies = new Topologies(topoMap);\r\n            cluster = new Cluster(iNimbus, resourceMetrics, supMap, TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n            scheduler.schedule(topologies, cluster);\r\n            assertEquals(Collections.emptySet(), cluster.getBlacklistedHosts(), \"blacklist using \" + strategyClassName);\r\n            assertEquals(Collections.singletonList(\"sup-0\"), cluster.getGreyListedSupervisors(), \"greylist using\" + strategyClassName);\r\n            LOG.debug(\"{}: Now only these slots remain available: {}\", strategyClassName, cluster.getAvailableSlots());\r\n            if (strategyClass == RoundRobinResourceAwareStrategy.class) {\r\n                // available slots will be across supervisors\r\n                assertFalse(cluster.getAvailableSlots(supMap.get(\"sup-0\")).containsAll(cluster.getAvailableSlots()), \"using \" + strategyClassName);\r\n            } else {\r\n                assertTrue(cluster.getAvailableSlots(supMap.get(\"sup-0\")).containsAll(cluster.getAvailableSlots()), \"using \" + strategyClassName);\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "TestList",
  "sourceCode" : "@Test\r\npublic void TestList() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    TopologyDetails topo2 = TestUtilsForBlacklistScheduler.getTopology(\"topo-2\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    topoMap.put(topo2.getId(), topo2);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    List<Map<Integer, List<Integer>>> faultList = new ArrayList<>();\r\n    faultList.add(new HashMap<>());\r\n    faultList.add(ImmutableMap.of(0, ImmutableList.of(0, 1)));\r\n    faultList.add(ImmutableMap.of(0, new ArrayList<>()));\r\n    for (int i = 0; i < 17; i++) {\r\n        faultList.add(new HashMap<>());\r\n    }\r\n    faultList.add(ImmutableMap.of(0, ImmutableList.of(0, 1)));\r\n    faultList.add(ImmutableMap.of(1, ImmutableList.of(1)));\r\n    for (int i = 0; i < 8; i++) {\r\n        faultList.add(new HashMap<>());\r\n    }\r\n    faultList.add(ImmutableMap.of(0, ImmutableList.of(1)));\r\n    faultList.add(ImmutableMap.of(1, ImmutableList.of(1)));\r\n    for (int i = 0; i < 30; i++) {\r\n        faultList.add(new HashMap<>());\r\n    }\r\n    List<Map<String, SupervisorDetails>> supervisorsList = FaultGenerateUtils.getSupervisorsList(3, 4, faultList);\r\n    Cluster cluster = null;\r\n    int count = 0;\r\n    for (Map<String, SupervisorDetails> supervisors : supervisorsList) {\r\n        cluster = FaultGenerateUtils.nextCluster(cluster, supervisors, iNimbus, config, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        if (count == 0) {\r\n            Set<String> hosts = new HashSet<>();\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 2) {\r\n            Set<String> hosts = new HashSet<>();\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 3) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 30) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 31) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            hosts.add(\"host-1\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 32) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            hosts.add(\"host-1\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 60) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            hosts.add(\"host-1\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 61) {\r\n            Set<String> hosts = new HashSet<>();\r\n            hosts.add(\"host-0\");\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        } else if (count == 62) {\r\n            Set<String> hosts = new HashSet<>();\r\n            assertEquals(hosts, cluster.getBlacklistedHosts(), \"blacklist\");\r\n        }\r\n        count++;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "removeLongTimeDisappearFromCache",
  "sourceCode" : "@Test\r\npublic void removeLongTimeDisappearFromCache() {\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 200);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    BlacklistScheduler bs = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler = bs;\r\n    bs.prepare(config, metricsRegistry);\r\n    bs.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removeSupervisorFromSupervisors(supMap, \"sup-0\"), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    for (int i = 0; i < 20; i++) {\r\n        bs.schedule(topologies, cluster);\r\n    }\r\n    Set<String> cached = new HashSet<>();\r\n    cached.add(\"sup-1\");\r\n    cached.add(\"sup-2\");\r\n    assertEquals(cached, bs.cachedSupervisors.keySet());\r\n    cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    bs.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removePortFromSupervisors(supMap, \"sup-0\", 0), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    for (int i = 0; i < 20; i++) {\r\n        bs.schedule(topologies, cluster);\r\n    }\r\n    Set<Integer> cachedPorts = Sets.newHashSet(1, 2, 3);\r\n    assertEquals(cachedPorts, bs.cachedSupervisors.get(\"sup-0\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\blacklist\\TestBlacklistScheduler.java",
  "methodName" : "blacklistSupervisorWithAddedPort",
  "sourceCode" : "@Test\r\npublic void blacklistSupervisorWithAddedPort() {\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_TIME, 10);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_TOLERANCE_COUNT, 2);\r\n    config.put(DaemonConfig.BLACKLIST_SCHEDULER_RESUME_TIME, 300);\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    scheduler = new BlacklistScheduler(new DefaultScheduler());\r\n    scheduler.prepare(config, metricsRegistry);\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    TopologyDetails topo1 = TestUtilsForBlacklistScheduler.getTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, true);\r\n    topoMap.put(topo1.getId(), topo1);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    INimbus iNimbus = new TestUtilsForBlacklistScheduler.INimbusTest();\r\n    ResourceMetrics resourceMetrics = new ResourceMetrics(metricsRegistry);\r\n    Map<String, SupervisorDetails> supMap = TestUtilsForBlacklistScheduler.genSupervisors(3, 4);\r\n    Cluster cluster = new Cluster(iNimbus, resourceMetrics, supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n    // allow blacklist scheduler to cache the supervisor\r\n    scheduler.schedule(topologies, cluster);\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.addPortToSupervisors(supMap, \"sup-0\", 4), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    // allow blacklist scheduler to cache the supervisor with an added port\r\n    scheduler.schedule(topologies, cluster);\r\n    // remove the port from the supervisor and make sure the blacklist scheduler can remove the port without\r\n    // throwing an exception\r\n    cluster = new Cluster(iNimbus, resourceMetrics, TestUtilsForBlacklistScheduler.removePortFromSupervisors(supMap, \"sup-0\", 4), TestUtilsForBlacklistScheduler.assignmentMapToImpl(cluster.getAssignments()), topologies, config);\r\n    scheduler.schedule(topologies, cluster);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_allNull",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_allNull() {\r\n    Map<String, Object> topConf = getEmptyConfig();\r\n    assertEquals(TOPOLOGY_WORKER_DEFAULT_MEMORY_ALLOCATION, Cluster.getAssignedMemoryForSlot(topConf), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_topologyWorkerGcChildopts",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_topologyWorkerGcChildopts() {\r\n    singleValueTest(Config.TOPOLOGY_WORKER_GC_CHILDOPTS, \"-Xmx128m\", 128.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_workerGcChildopts",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_workerGcChildopts() {\r\n    singleValueTest(Config.WORKER_GC_CHILDOPTS, \"-Xmx256m\", 256.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_topologyWorkerChildopts",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_topologyWorkerChildopts() {\r\n    singleValueTest(Config.TOPOLOGY_WORKER_CHILDOPTS, \"-Xmx512m\", 512.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_workerChildopts",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_workerChildopts() {\r\n    singleValueTest(Config.WORKER_CHILDOPTS, \"-Xmx768m\", 768.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_workerHeapMemoryMb",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_workerHeapMemoryMb() {\r\n    Map<String, Object> topConf = getConfig(Config.WORKER_HEAP_MEMORY_MB, 1024);\r\n    assertEquals(1024.0, Cluster.getAssignedMemoryForSlot(topConf), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_topologyWorkerLwChildopts",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_topologyWorkerLwChildopts() {\r\n    singleValueTest(Config.TOPOLOGY_WORKER_LOGWRITER_CHILDOPTS, \"-Xmx64m\", TOPOLOGY_WORKER_DEFAULT_MEMORY_ALLOCATION + 64.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\ClusterTest.java",
  "methodName" : "getAssignedMemoryForSlot_all",
  "sourceCode" : "@Test\r\npublic void getAssignedMemoryForSlot_all() {\r\n    Map<String, Object> topConf = getPopulatedConfig();\r\n    assertEquals(128.0 + 64.0, Cluster.getAssignedMemoryForSlot(topConf), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourceOfferTest.java",
  "methodName" : "testNodeOverExtendedCpu",
  "sourceCode" : "@Test\r\npublic void testNodeOverExtendedCpu() {\r\n    NormalizedResourceOffer availableResources = createOffer(100.0, 0.0);\r\n    NormalizedResourceOffer scheduledResources = createOffer(110.0, 0.0);\r\n    availableResources.remove(scheduledResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    assertEquals(0.0, availableResources.getTotalCpu(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourceOfferTest.java",
  "methodName" : "testNodeOverExtendedMemory",
  "sourceCode" : "@Test\r\npublic void testNodeOverExtendedMemory() {\r\n    NormalizedResourceOffer availableResources = createOffer(0.0, 5.0);\r\n    NormalizedResourceOffer scheduledResources = createOffer(0.0, 10.0);\r\n    availableResources.remove(scheduledResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    assertEquals(0.0, availableResources.getTotalMemoryMb(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourceRequestTest.java",
  "methodName" : "testAckerCPUSetting",
  "sourceCode" : "@Test\r\npublic void testAckerCPUSetting() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    topoConf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 40);\r\n    topoConf.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 50);\r\n    NormalizedResourceRequest request = new NormalizedResourceRequest(topoConf, Acker.ACKER_COMPONENT_ID);\r\n    Map<String, Double> normalizedMap = request.toNormalizedMap();\r\n    Double cpu = normalizedMap.get(Constants.COMMON_CPU_RESOURCE_NAME);\r\n    assertNotNull(cpu);\r\n    assertEquals(40, cpu, 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourceRequestTest.java",
  "methodName" : "testNonAckerCPUSetting",
  "sourceCode" : "@Test\r\npublic void testNonAckerCPUSetting() {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    topoConf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 40);\r\n    topoConf.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 50);\r\n    NormalizedResourceRequest request = new NormalizedResourceRequest(topoConf, \"notAnAckerComponent\");\r\n    Map<String, Double> normalizedMap = request.toNormalizedMap();\r\n    Double cpu = normalizedMap.get(Constants.COMMON_CPU_RESOURCE_NAME);\r\n    assertNotNull(cpu);\r\n    assertEquals(50, cpu, 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testAddCpu",
  "sourceCode" : "@Test\r\npublic void testAddCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    NormalizedResources addedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    resources.add(addedResources);\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(Constants.COMMON_CPU_RESOURCE_NAME), is(2.0));\r\n    assertThat(resources.getTotalCpu(), is(2.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testAddToExistingResource",
  "sourceCode" : "@Test\r\npublic void testAddToExistingResource() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    NormalizedResources addedResources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    resources.add(addedResources);\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(gpuResourceName), is(2.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testAddWhenOtherHasMoreResourcesThanThis",
  "sourceCode" : "@Test\r\npublic void testAddWhenOtherHasMoreResourcesThanThis() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    NormalizedResources addedResources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    resources.add(addedResources);\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(gpuResourceName), is(1.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testAddWhenOtherHasDifferentResourceThanThis",
  "sourceCode" : "@Test\r\npublic void testAddWhenOtherHasDifferentResourceThanThis() {\r\n    String disks = \"disks\";\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(disks, 23)));\r\n    NormalizedResources addedResources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    resources.add(addedResources);\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(disks), is(23.0));\r\n    assertThat(normalizedMap.get(gpuResourceName), is(1.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testRemoveZeroesWhenResourcesBecomeNegative",
  "sourceCode" : "@Test\r\npublic void testRemoveZeroesWhenResourcesBecomeNegative() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    NormalizedResources removedResources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 2)));\r\n    resources.remove(removedResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(gpuResourceName), is(0.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testRemoveZeroesWhenCpuBecomesNegative",
  "sourceCode" : "@Test\r\npublic void testRemoveZeroesWhenCpuBecomesNegative() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    NormalizedResources removedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    resources.remove(removedResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    assertThat(resources.getTotalCpu(), is(0.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testRemoveFromCpu",
  "sourceCode" : "@Test\r\npublic void testRemoveFromCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources removedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    resources.remove(removedResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(Constants.COMMON_CPU_RESOURCE_NAME), is(1.0));\r\n    assertThat(resources.getTotalCpu(), is(1.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testRemoveFromExistingResources",
  "sourceCode" : "@Test\r\npublic void testRemoveFromExistingResources() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 15)));\r\n    NormalizedResources removedResources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    resources.remove(removedResources, new ResourceMetrics(new StormMetricsRegistry()));\r\n    Map<String, Double> normalizedMap = resources.toNormalizedMap();\r\n    assertThat(normalizedMap.get(gpuResourceName), is(14.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCouldHoldWithTooFewCpus",
  "sourceCode" : "@Test\r\npublic void testCouldHoldWithTooFewCpus() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    NormalizedResources resourcesToCheck = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    boolean couldHold = resources.couldHoldIgnoringSharedMemory(resourcesToCheck, 100, 1);\r\n    assertThat(couldHold, is(false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCouldHoldWithTooFewResource",
  "sourceCode" : "@Test\r\npublic void testCouldHoldWithTooFewResource() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    NormalizedResources resourcesToCheck = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 2)));\r\n    boolean couldHold = resources.couldHoldIgnoringSharedMemory(resourcesToCheck, 100, 1);\r\n    assertThat(couldHold, is(false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCouldHoldWithTooLittleMemory",
  "sourceCode" : "@Test\r\npublic void testCouldHoldWithTooLittleMemory() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    NormalizedResources resourcesToCheck = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    boolean couldHold = resources.couldHoldIgnoringSharedMemory(resourcesToCheck, 100, 200);\r\n    assertThat(couldHold, is(false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCouldHoldWithMissingResource",
  "sourceCode" : "@Test\r\npublic void testCouldHoldWithMissingResource() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    NormalizedResources resourcesToCheck = new NormalizedResources(normalize(Collections.singletonMap(gpuResourceName, 1)));\r\n    boolean couldHold = resources.couldHoldIgnoringSharedMemory(resourcesToCheck, 100, 1);\r\n    assertThat(couldHold, is(false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCouldHoldWithEnoughResources",
  "sourceCode" : "@Test\r\npublic void testCouldHoldWithEnoughResources() {\r\n    Map<String, Double> allResources = new HashMap<>();\r\n    allResources.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResources.put(gpuResourceName, 2.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResources));\r\n    NormalizedResources resourcesToCheck = new NormalizedResources(normalize(allResources));\r\n    boolean couldHold = resources.couldHoldIgnoringSharedMemory(resourcesToCheck, 100, 100);\r\n    assertThat(couldHold, is(true));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgUsageWithNoResourcesInTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgUsageWithNoResourcesInTotal() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    double avg = resources.calculateAveragePercentageUsedBy(usedResources, 0, 0);\r\n    assertThat(avg, is(100.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithOnlyCpu",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithOnlyCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    double avg = resources.calculateAveragePercentageUsedBy(usedResources, 0, 0);\r\n    assertThat(avg, is(50.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithCpuAndMem",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithCpuAndMem() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    double avg = resources.calculateAveragePercentageUsedBy(usedResources, 4, 1);\r\n    assertThat(avg, is((50.0 + 25.0) / 2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithCpuMemAndGenericResource",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithCpuMemAndGenericResource() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 10.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    double avg = resources.calculateAveragePercentageUsedBy(usedResources, 4, 1);\r\n    assertThat(avg, is((50.0 + 25.0 + 10.0) / 3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithUnusedResource",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithUnusedResource() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 10.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    double avg = resources.calculateAveragePercentageUsedBy(usedResources, 4, 1);\r\n    //The resource that is not used should count as if it is being used 0%\r\n    assertThat(avg, is((50.0 + 25.0) / 3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgThrowsIfTotalIsMissingCpu",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgThrowsIfTotalIsMissingCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 5)));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateAveragePercentageUsedBy(usedResources, 0, 0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgThrowsIfTotalIsMissingMemory",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgThrowsIfTotalIsMissingMemory() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateAveragePercentageUsedBy(usedResources, 100, 500));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithResourceMissingFromTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithResourceMissingFromTotal() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateAveragePercentageUsedBy(usedResources, 4, 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateAvgWithTooLittleResourceInTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateAvgWithTooLittleResourceInTotal() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 5.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateAveragePercentageUsedBy(usedResources, 4, 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinUsageWithNoResourcesInTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateMinUsageWithNoResourcesInTotal() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.emptyMap()));\r\n    double min = resources.calculateMinPercentageUsedBy(usedResources, 0, 0);\r\n    assertThat(min, is(100.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithOnlyCpu",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithOnlyCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    double min = resources.calculateMinPercentageUsedBy(usedResources, 0, 0);\r\n    assertThat(min, is(50.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithCpuAndMem",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithCpuAndMem() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    double min = resources.calculateMinPercentageUsedBy(usedResources, 4, 1);\r\n    assertThat(min, is(25.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithCpuMemAndGenericResource",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithCpuMemAndGenericResource() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 10.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    double min = resources.calculateMinPercentageUsedBy(usedResources, 4, 1);\r\n    assertThat(min, is(10.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithUnusedResource",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithUnusedResource() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 10.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    double min = resources.calculateMinPercentageUsedBy(usedResources, 4, 1);\r\n    //The resource that is not used should count as if it is being used 0%\r\n    assertThat(min, is(0.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinThrowsIfTotalIsMissingCpu",
  "sourceCode" : "@Test\r\npublic void testCalculateMinThrowsIfTotalIsMissingCpu() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 5)));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateMinPercentageUsedBy(usedResources, 0, 0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinThrowsIfTotalIsMissingMemory",
  "sourceCode" : "@Test\r\npublic void testCalculateMinThrowsIfTotalIsMissingMemory() {\r\n    NormalizedResources resources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 2)));\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(Collections.singletonMap(Constants.COMMON_CPU_RESOURCE_NAME, 1)));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateMinPercentageUsedBy(usedResources, 100, 500));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithResourceMissingFromTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithResourceMissingFromTotal() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateMinPercentageUsedBy(usedResources, 4, 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\NormalizedResourcesTest.java",
  "methodName" : "testCalculateMinWithTooLittleResourceInTotal",
  "sourceCode" : "@Test\r\npublic void testCalculateMinWithTooLittleResourceInTotal() {\r\n    Map<String, Double> allResourcesMap = new HashMap<>();\r\n    allResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 2.0);\r\n    allResourcesMap.put(gpuResourceName, 1.0);\r\n    NormalizedResources resources = new NormalizedResources(normalize(allResourcesMap));\r\n    Map<String, Double> usedResourcesMap = new HashMap<>();\r\n    usedResourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, 1.0);\r\n    usedResourcesMap.put(gpuResourceName, 5.0);\r\n    NormalizedResources usedResources = new NormalizedResources(normalize(usedResourcesMap));\r\n    assertThrows(IllegalArgumentException.class, () -> resources.calculateMinPercentageUsedBy(usedResources, 4, 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\normalization\\ResourceMapArrayBridgeTest.java",
  "methodName" : "testCanTranslateBackAndForthBetweenMapAndArrayConsistently",
  "sourceCode" : "@Test\r\npublic void testCanTranslateBackAndForthBetweenMapAndArrayConsistently() {\r\n    ResourceMapArrayBridge bridge = new ResourceMapArrayBridge();\r\n    Map<String, Double> allResources = new HashMap<>();\r\n    allResources.put(gpuResourceName, 2.0);\r\n    allResources.put(disksResourceName, 64.0);\r\n    Map<String, Double> normalizedResources = normalize(allResources);\r\n    double[] resources = bridge.translateToResourceArray(normalizedResources);\r\n    Map<String, Integer> resourceNamesToArrayIndex = bridge.getResourceNamesToArrayIndex();\r\n    assertThat(resourceNamesToArrayIndex.size(), is(2));\r\n    int gpuIndex = resourceNamesToArrayIndex.get(gpuResourceName);\r\n    int disksIndex = resourceNamesToArrayIndex.get(disksResourceName);\r\n    assertThat(resources.length, is(2));\r\n    assertThat(resources[gpuIndex], is(2.0));\r\n    assertThat(resources[disksIndex], is(64.0));\r\n    Map<String, Double> roundTrippedResources = bridge.translateFromResourceArray(resources);\r\n    assertThat(roundTrippedResources, is(normalizedResources));\r\n    double[] roundTrippedResourceArray = bridge.translateToResourceArray(roundTrippedResources);\r\n    assertThat(roundTrippedResourceArray, equalTo(resources));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\eviction\\TestDefaultEvictionStrategy.java",
  "methodName" : "testEviction",
  "sourceCode" : "/**\r\n * The resources in the cluster are limited. In the first round of scheduling, all resources in the cluster is used.\r\n * User jerry submits another topology.  Since user jerry has his resource guarantees satisfied, and user bobby\r\n * has exceeded his resource guarantee, topo-3 from user bobby should be evicted.\r\n */\r\n@Test\r\npublic void testEviction() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 200, 2000), userRes(\"bobby\", 100, 1000), userRes(\"derek\", 200, 2000));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"), genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 20, \"bobby\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-3\", \"topo-4\");\r\n        //user jerry submits another topology\r\n        topologies = addTopologies(topologies, genTopology(\"topo-6\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        //topo-3 evicted (lowest priority)\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-4\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-3\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\eviction\\TestDefaultEvictionStrategy.java",
  "methodName" : "testEvictMultipleTopologies",
  "sourceCode" : "@Test\r\npublic void testEvictMultipleTopologies() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 200, 2000), userRes(\"derek\", 100, 1000));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 20, \"bobby\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 2 to 5...\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\", \"topo-4\", \"topo-5\");\r\n        //user jerry submits another topology\r\n        topologies = addTopologies(topologies, genTopology(\"topo-1\", config, 2, 0, 1, 0, currentTime - 2, 10, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1 to 5\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone scheduling...\");\r\n        //bobby has no guarantee so topo-2 and topo-3 evicted\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-4\", \"topo-5\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\eviction\\TestDefaultEvictionStrategy.java",
  "methodName" : "testEvictMultipleTopologiesFromMultipleUsersInCorrectOrder",
  "sourceCode" : "@Test\r\npublic void testEvictMultipleTopologiesFromMultipleUsersInCorrectOrder() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 300, 3000), userRes(\"derek\", 100, 1000));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 20, \"bobby\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 15, 29, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\", \"topo-4\", \"topo-5\");\r\n        //user jerry submits another topology\r\n        topologies = addTopologies(topologies, genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        //topo-3 evicted since user bobby don't have any resource guarantees and topo-3 is the lowest priority for user bobby\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-4\", \"topo-5\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-3\");\r\n        topologies = addTopologies(topologies, genTopology(\"topo-6\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        //topo-2 evicted since user bobby don't have any resource guarantees and topo-2 is the next lowest priority for user bobby\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-4\", \"topo-5\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\");\r\n        topologies = addTopologies(topologies, genTopology(\"topo-7\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        // since user derek has exceeded his resource guarantee while user jerry has not topo-5 or topo-4 could be evicted because they have the same priority\r\n        // but topo-4 was submitted earlier thus we choose that one to evict (somewhat arbitrary)\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-5\", \"topo-7\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\", \"topo-4\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\eviction\\TestDefaultEvictionStrategy.java",
  "methodName" : "testEvictTopologyFromItself",
  "sourceCode" : "/**\r\n * If topologies from other users cannot be evicted to make space\r\n * check if there is a topology with lower priority that can be evicted from the current user\r\n */\r\n@Test\r\npublic void testEvictTopologyFromItself() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 200, 2000), userRes(\"bobby\", 100, 1000), userRes(\"derek\", 100, 1000));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-6\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1,2,5,6\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone Scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-5\", \"topo-6\");\r\n        //user jerry submits another topology into a full cluster\r\n        // topo3 should not be able to scheduled\r\n        topologies = addTopologies(topologies, genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 29, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1,2,3,5,6\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone Scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-5\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-3\");\r\n        //user jerry submits another topology but this one should be scheduled since it has higher priority than the\r\n        //rest of jerry's running topologies\r\n        topologies = addTopologies(topologies, genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1-6\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone Scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-4\", \"topo-5\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\eviction\\TestDefaultEvictionStrategy.java",
  "methodName" : "testOverGuaranteeEviction",
  "sourceCode" : "/**\r\n * If users are above his or her guarantee, check if topology eviction works correctly\r\n */\r\n@Test\r\npublic void testOverGuaranteeEviction() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 70, 700), userRes(\"bobby\", 100, 1000), userRes(\"derek\", 25, 250));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1,3,4,5\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-3\", \"topo-4\", \"topo-5\");\r\n        //user derek submits another topology into a full cluster\r\n        //topo6 should not be able to scheduled initially, but since topo6 has higher priority than topo5\r\n        //topo5 will be evicted so that topo6 can be scheduled\r\n        topologies = addTopologies(topologies, genTopology(\"topo-6\", config, 1, 0, 1, 0, currentTime - 2, 10, \"derek\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1,3,4,5,6\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-3\", \"topo-4\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-5\");\r\n        //user jerry submits topo2\r\n        topologies = addTopologies(topologies, genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"));\r\n        cluster = new Cluster(cluster, topologies);\r\n        LOG.info(\"\\n\\n\\t\\tScheduling topos 1-6\");\r\n        scheduler.schedule(topologies, cluster);\r\n        LOG.info(\"\\n\\n\\t\\tDone scheduling...\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-3\", \"topo-4\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\", \"topo-5\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\priority\\TestFIFOSchedulingPriorityStrategy.java",
  "methodName" : "testFIFOEvictionStrategy",
  "sourceCode" : "@Test\r\npublic void testFIFOEvictionStrategy() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        try (Time.SimulatedTime sim = new Time.SimulatedTime()) {\r\n            INimbus iNimbus = new INimbusTest();\r\n            Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100.0, 1000.0);\r\n            Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 200.0, 2000.0));\r\n            Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n            config.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY, FIFOSchedulingPriorityStrategy.class.getName());\r\n            Topologies topologies = new Topologies(genTopology(\"topo-1-jerry\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 250, 20, \"jerry\"), genTopology(\"topo-2-bobby\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 200, 10, \"bobby\"), genTopology(\"topo-3-bobby\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 300, 20, \"bobby\"), genTopology(\"topo-4-derek\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 201, 29, \"derek\"));\r\n            Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n            ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n            rs.prepare(config, new StormMetricsRegistry());\r\n            try {\r\n                rs.schedule(topologies, cluster);\r\n                assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1-jerry\", \"topo-2-bobby\", \"topo-3-bobby\", \"topo-4-derek\");\r\n                LOG.info(\"\\n\\n\\t\\tINSERTING topo-5\");\r\n                //new topology needs to be scheduled\r\n                //topo-3 should be evicted since it's been up the longest\r\n                topologies = addTopologies(topologies, genTopology(\"topo-5-derek\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 15, 29, \"derek\"));\r\n                cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n                rs.schedule(topologies, cluster);\r\n                assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1-jerry\", \"topo-2-bobby\", \"topo-4-derek\", \"topo-5-derek\");\r\n                assertTopologiesNotScheduled(cluster, strategyClass, \"topo-3-bobby\");\r\n                LOG.info(\"\\n\\n\\t\\tINSERTING topo-6\");\r\n                //new topology needs to be scheduled.  topo-4 should be evicted. Even though topo-1 from user jerry is older, topo-1 will not be evicted\r\n                //since user jerry has enough resource guarantee\r\n                topologies = addTopologies(topologies, genTopology(\"topo-6-bobby\", config, 1, 0, 1, 0, Time.currentTimeSecs() - 10, 29, \"bobby\"));\r\n                cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n                rs.schedule(topologies, cluster);\r\n                assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1-jerry\", \"topo-2-bobby\", \"topo-5-derek\", \"topo-6-bobby\");\r\n                assertTopologiesNotScheduled(cluster, strategyClass, \"topo-3-bobby\", \"topo-4-derek\");\r\n            } finally {\r\n                rs.cleanup();\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\priority\\TestGenericResourceAwareSchedulingPriorityStrategy.java",
  "methodName" : "testDefaultSchedulingPriorityStrategyNotEvicting",
  "sourceCode" : "/*\r\n     * DefaultSchedulingPriorityStrategy will not evict topo as long as the resources request can be met\r\n     *\r\n     *  Ethan asks for heavy cpu and memory while Rui asks for little cpu and memory but heavy generic resource\r\n     *  Since Rui's all types of resources request can be met, no eviction will happen.\r\n    */\r\n@Test\r\npublic void testDefaultSchedulingPriorityStrategyNotEvicting() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        Map<String, Double> requestedgenericResourcesMap = new HashMap<>();\r\n        requestedgenericResourcesMap.put(\"generic.resource.1\", 40.0);\r\n        // Use full memory and cpu of the cluster capacity\r\n        Config ruiConf = createGrasClusterConfig(strategyClass, 20, 50, 50, null, requestedgenericResourcesMap);\r\n        Config ethanConf = createGrasClusterConfig(strategyClass, 80, 400, 500, null, Collections.emptyMap());\r\n        Topologies topologies = new Topologies(genTopology(\"ethan-topo-1\", ethanConf, 1, 0, 1, 0, currentTime - 2, 10, \"ethan\"), genTopology(\"ethan-topo-2\", ethanConf, 1, 0, 1, 0, currentTime - 2, 20, \"ethan\"), genTopology(\"ethan-topo-3\", ethanConf, 1, 0, 1, 0, currentTime - 2, 28, \"ethan\"), genTopology(\"ethan-topo-4\", ethanConf, 1, 0, 1, 0, currentTime - 2, 29, \"ethan\"));\r\n        Topologies withNewTopo = addTopologies(topologies, genTopology(\"rui-topo-1\", ruiConf, 1, 0, 4, 0, currentTime - 2, 10, \"rui\"));\r\n        Config config = mkClusterConfig(strategyClass, DefaultSchedulingPriorityStrategy.class.getName());\r\n        Cluster cluster = mkTestCluster(topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        cluster = new Cluster(cluster, withNewTopo);\r\n        scheduler.schedule(withNewTopo, cluster);\r\n        Map<String, Set<String>> evictedTopos = ((ResourceAwareScheduler) scheduler).getEvictedTopologiesMap();\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesNotBeenEvicted(cluster, strategyClass, collectMapValues(evictedTopos), \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"rui-topo-1\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\priority\\TestGenericResourceAwareSchedulingPriorityStrategy.java",
  "methodName" : "testDefaultSchedulingPriorityStrategyEvicting",
  "sourceCode" : "/*\r\n     * DefaultSchedulingPriorityStrategy does not take generic resources into account when calculating score\r\n     * So even if a user is requesting a lot of generic resources other than CPU and memory, scheduler will still score it very low and kick out other topologies\r\n     *\r\n     *  Ethan asks for medium cpu and memory while Rui asks for little cpu and memory but heavy generic resource\r\n     *  However, Rui's generic request can not be met and default scoring system is not taking generic resources into account,\r\n     *  so the score of Rui's new topo will be much lower than all Ethan's topos'.\r\n     *  Then all Ethan's topo will be evicted in trying to make rooms for Rui.\r\n     */\r\n@Test\r\npublic void testDefaultSchedulingPriorityStrategyEvicting() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        Map<String, Double> requestedgenericResourcesMap = new HashMap<>();\r\n        requestedgenericResourcesMap.put(\"generic.resource.1\", 40.0);\r\n        Config ruiConf = createGrasClusterConfig(strategyClass, 10, 10, 10, null, requestedgenericResourcesMap);\r\n        Config ethanConf = createGrasClusterConfig(strategyClass, 60, 200, 300, null, Collections.emptyMap());\r\n        Topologies topologies = new Topologies(genTopology(\"ethan-topo-1\", ethanConf, 1, 0, 1, 0, currentTime - 2, 10, \"ethan\"), genTopology(\"ethan-topo-2\", ethanConf, 1, 0, 1, 0, currentTime - 2, 20, \"ethan\"), genTopology(\"ethan-topo-3\", ethanConf, 1, 0, 1, 0, currentTime - 2, 28, \"ethan\"), genTopology(\"ethan-topo-4\", ethanConf, 1, 0, 1, 0, currentTime - 2, 29, \"ethan\"));\r\n        Topologies withNewTopo = addTopologies(topologies, genTopology(\"rui-topo-1\", ruiConf, 1, 0, 5, 0, currentTime - 2, 10, \"rui\"));\r\n        Config config = mkClusterConfig(strategyClass, DefaultSchedulingPriorityStrategy.class.getName());\r\n        Cluster cluster = mkTestCluster(topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        cluster = new Cluster(cluster, withNewTopo);\r\n        scheduler.schedule(withNewTopo, cluster);\r\n        Map<String, Set<String>> evictedTopos = ((ResourceAwareScheduler) scheduler).getEvictedTopologiesMap();\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesBeenEvicted(cluster, strategyClass, collectMapValues(evictedTopos), \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"rui-topo-1\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\priority\\TestGenericResourceAwareSchedulingPriorityStrategy.java",
  "methodName" : "testGenericSchedulingPriorityStrategyEvicting",
  "sourceCode" : "/*\r\n     * GenericResourceAwareSchedulingPriorityStrategy extend scoring formula to accommodate generic resources\r\n     *\r\n     *   Same setting as testDefaultSchedulingPriorityStrategyEvicting, but this time, new scoring system is taking generic resources into account,\r\n     *   the score of rui's new topo will be higher than all Ethan's topos' due to its crazy generic request.\r\n     *   At the end, all Ethan's topo will not be evicted as expected.\r\n     */\r\n@Test\r\npublic void testGenericSchedulingPriorityStrategyEvicting() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        Map<String, Double> requestedgenericResourcesMap = new HashMap<>();\r\n        requestedgenericResourcesMap.put(\"generic.resource.1\", 40.0);\r\n        Config ruiConf = createGrasClusterConfig(strategyClass, 10, 10, 10, null, requestedgenericResourcesMap);\r\n        Config ethanConf = createGrasClusterConfig(strategyClass, 60, 200, 300, null, Collections.emptyMap());\r\n        Topologies topologies = new Topologies(genTopology(\"ethan-topo-1\", ethanConf, 1, 0, 1, 0, currentTime - 2, 10, \"ethan\"), genTopology(\"ethan-topo-2\", ethanConf, 1, 0, 1, 0, currentTime - 2, 20, \"ethan\"), genTopology(\"ethan-topo-3\", ethanConf, 1, 0, 1, 0, currentTime - 2, 28, \"ethan\"), genTopology(\"ethan-topo-4\", ethanConf, 1, 0, 1, 0, currentTime - 2, 29, \"ethan\"));\r\n        Topologies withNewTopo = addTopologies(topologies, genTopology(\"rui-topo-1\", ruiConf, 1, 0, 5, 0, currentTime - 2, 10, \"rui\"));\r\n        Config config = mkClusterConfig(strategyClass, GenericResourceAwareSchedulingPriorityStrategy.class.getName());\r\n        Cluster cluster = mkTestCluster(topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        cluster = new Cluster(cluster, withNewTopo);\r\n        scheduler.schedule(withNewTopo, cluster);\r\n        Map<String, Set<String>> evictedTopos = ((ResourceAwareScheduler) scheduler).getEvictedTopologiesMap();\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesNotBeenEvicted(cluster, strategyClass, collectMapValues(evictedTopos), \"ethan-topo-1\", \"ethan-topo-2\", \"ethan-topo-3\", \"ethan-topo-4\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"rui-topo-1\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacks",
  "sourceCode" : "/**\r\n * Test whether strategy will choose correct rack.\r\n */\r\n@Test\r\npublic void testMultipleRacks() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate another rack of supervisors with less resources\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some supervisors that are depleted of one resource\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of memory but little of cpu\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 10, 8000 * 2 + 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of cpu but little of memory\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400 + 200 + 10, 1000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //Generate some that have neither resource, to verify that the strategy will prioritize this last\r\n    //Also put a generic resource with 0 value in the resources list, to verify that it doesn't affect the sorting\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack5 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0.0, 0.0, Collections.singletonMap(\"gpu.count\", 0.0), numaResourceMultiplier);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    supMap.putAll(supMapRack5);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4, supMapRack5);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    List<String> supHostnames = new LinkedList<>();\r\n    for (SupervisorDetails sup : supMap.values()) {\r\n        supHostnames.add(sup.getHost());\r\n    }\r\n    Map<String, List<String>> rackToHosts = testDNSToSwitchMapping.getRackToHosts();\r\n    cluster.setNetworkTopography(rackToHosts);\r\n    NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, topo1, BaseResourceAwareStrategy.NodeSortType.DEFAULT_RAS);\r\n    nodeSorter.prepare(null);\r\n    List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n    String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n    assertEquals(6, sortedRacks.size(), rackSummaries + \"\\n# of racks sorted\");\r\n    Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n    assertEquals(\"rack-000\", it.next().id, rackSummaries + \"\\nrack-000 should be ordered first since it has the most balanced set of resources\");\r\n    assertEquals(\"rack-001\", it.next().id, rackSummaries + \"\\nrack-001 should be ordered second since it has a balanced set of resources but less than rack-000\");\r\n    assertEquals(\"rack-004\", it.next().id, rackSummaries + \"\\nrack-004 should be ordered third since it has a lot of cpu but not a lot of memory\");\r\n    assertEquals(\"rack-003\", it.next().id, rackSummaries + \"\\nrack-003 should be ordered fourth since it has a lot of memory but not cpu\");\r\n    assertEquals(\"rack-002\", it.next().id, rackSummaries + \"\\nrack-002 should be ordered fifth since it has not cpu resources\");\r\n    assertEquals(\"rack-005\", it.next().id, rackSummaries + \"\\nRack-005 should be ordered sixth since it has neither CPU nor memory available\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksWithFavoritism",
  "sourceCode" : "/**\r\n * Test whether strategy will choose correct rack.\r\n */\r\n@Test\r\npublic void testMultipleRacksWithFavoritism() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 2;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000, Collections.emptyMap(), 1.0);\r\n    //generate another rack of supervisors with less resources\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 4000, Collections.emptyMap(), 1.0);\r\n    //generate some supervisors that are depleted of one resource\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0, 8000, Collections.emptyMap(), 1.0);\r\n    //generate some that has a lot of memory but little of cpu\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 10, 8000 * 2 + 4000, Collections.emptyMap(), 1.0);\r\n    //generate some that has a lot of cpu but little of memory\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400 + 200 + 10, 1000, Collections.emptyMap(), 1.0);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4);\r\n    Config t1Conf = new Config();\r\n    t1Conf.putAll(config);\r\n    final List<String> t1FavoredHostNames = Arrays.asList(\"host-41\", \"host-42\", \"host-43\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, t1FavoredHostNames);\r\n    final List<String> t1UnfavoredHostIds = Arrays.asList(\"host-1\", \"host-2\", \"host-3\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, t1UnfavoredHostIds);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", t1Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Config t2Conf = new Config();\r\n    t2Conf.putAll(config);\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, Arrays.asList(\"host-31\", \"host-32\", \"host-33\"));\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, Arrays.asList(\"host-11\", \"host-12\", \"host-13\"));\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", t2Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    List<String> supHostnames = new LinkedList<>();\r\n    for (SupervisorDetails sup : supMap.values()) {\r\n        supHostnames.add(sup.getHost());\r\n    }\r\n    Map<String, List<String>> rackToHosts = testDNSToSwitchMapping.getRackToHosts();\r\n    cluster.setNetworkTopography(rackToHosts);\r\n    NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, topo1, BaseResourceAwareStrategy.NodeSortType.DEFAULT_RAS);\r\n    nodeSorter.prepare(null);\r\n    List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n    String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n    Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n    // Ranked first since rack-000 has the most balanced set of resources\r\n    assertEquals(\"rack-000\", it.next().id, \"rack-000 should be ordered first\");\r\n    // Ranked second since rack-1 has a balanced set of resources but less than rack-0\r\n    assertEquals(\"rack-001\", it.next().id, \"rack-001 should be ordered second\");\r\n    // Ranked third since rack-4 has a lot of cpu but not a lot of memory\r\n    assertEquals(\"rack-004\", it.next().id, \"rack-004 should be ordered third\");\r\n    // Ranked fourth since rack-3 has alot of memory but not cpu\r\n    assertEquals(\"rack-003\", it.next().id, \"rack-003 should be ordered fourth\");\r\n    //Ranked last since rack-2 has not cpu resources\r\n    assertEquals(\"rack-002\", it.next().id, \"rack-00s2 should be ordered fifth\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksWithHostProximity",
  "sourceCode" : "/**\r\n * Test if hosts are presented together regardless of resource availability.\r\n * Supervisors are created with multiple Numa zones in such a manner that resources on two numa zones on the same host\r\n * differ widely in resource availability.\r\n */\r\n@Test\r\npublic void testMultipleRacksWithHostProximity() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 12;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 3;\r\n    final double numaResourceMultiplier = 0.4;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate another rack of supervisors with less resources\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some supervisors that are depleted of one resource\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of memory but little of cpu\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 10, 8000 * 2 + 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of cpu but little of memory\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400 + 200 + 10, 1000, Collections.emptyMap(), numaResourceMultiplier);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4);\r\n    Config t1Conf = new Config();\r\n    t1Conf.putAll(config);\r\n    final List<String> t1FavoredHostNames = Arrays.asList(\"host-41\", \"host-42\", \"host-43\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, t1FavoredHostNames);\r\n    final List<String> t1UnfavoredHostIds = Arrays.asList(\"host-1\", \"host-2\", \"host-3\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, t1UnfavoredHostIds);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", t1Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Config t2Conf = new Config();\r\n    t2Conf.putAll(config);\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, Arrays.asList(\"host-31\", \"host-32\", \"host-33\"));\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, Arrays.asList(\"host-11\", \"host-12\", \"host-13\"));\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", t2Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    INodeSorter nodeSorter = new NodeSorterHostProximity(cluster, topo1);\r\n    nodeSorter.prepare(null);\r\n    Set<String> seenHosts = new HashSet<>();\r\n    String prevHost = null;\r\n    List<String> errLines = new ArrayList();\r\n    Map<String, String> nodeToHost = new RasNodes(cluster).getNodeIdToHostname();\r\n    for (String nodeId : nodeSorter.sortAllNodes()) {\r\n        String host = nodeToHost.getOrDefault(nodeId, \"no-host-for-node-\" + nodeId);\r\n        errLines.add(String.format(\"\\tnodeId:%s, host:%s\", nodeId, host));\r\n        if (!host.equals(prevHost) && seenHosts.contains(host)) {\r\n            String err = String.format(\"Host %s for node %s is out of order:\\n\\t%s\", host, nodeId, String.join(\"\\n\\t\", errLines));\r\n            fail(err);\r\n        }\r\n        seenHosts.add(host);\r\n        prevHost = host;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksOrderedByCapacity",
  "sourceCode" : "/**\r\n * Racks should be returned in order of decreasing capacity.\r\n */\r\n@Test\r\npublic void testMultipleRacksOrderedByCapacity() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 600, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 500, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 300, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    // too small to hold topology\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack5 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 100, 8000 - rackStartNum, Collections.singletonMap(\"gpu.count\", 0.0), numaResourceMultiplier);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    supMap.putAll(supMapRack5);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4, supMapRack5);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, topo1);\r\n    nodeSorter.prepare(null);\r\n    List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n    String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n    NormalizedResourceRequest topoResourceRequest = topo1.getApproximateTotalResources();\r\n    String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", topo1.getId(), topoResourceRequest.toString());\r\n    Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n    assertEquals(\"rack-000\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nRack-000 should be ordered first since it has the largest capacity\");\r\n    assertEquals(\"rack-001\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-001 should be ordered second since it smaller than rack-000\");\r\n    assertEquals(\"rack-002\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-002 should be ordered third since it is smaller than rack-001\");\r\n    assertEquals(\"rack-003\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-003 should be ordered fourth since it since it is smaller than rack-002\");\r\n    assertEquals(\"rack-004\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-004 should be ordered fifth since it since it is smaller than rack-003\");\r\n    assertEquals(\"rack-005\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-005 should be ordered last since it since it is has smallest capacity\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testAntiAffinityWithMultipleTopologies",
  "sourceCode" : "/**\r\n * Schedule two topologies, once with special resources and another without.\r\n * There are enough special resources to hold one topology with special resource (\"my.gpu\").\r\n * If the sort order is incorrect, scheduling will not succeed.\r\n */\r\n@Test\r\npublic void testAntiAffinityWithMultipleTopologies() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(1, 40, 66, 0, 0, 4700, 226200, new HashMap<>());\r\n    HashMap<String, Double> extraResources = new HashMap<>();\r\n    extraResources.put(\"my.gpu\", 1.0);\r\n    supMap.putAll(genSupervisorsWithRacks(1, 40, 66, 1, 0, 4700, 226200, extraResources));\r\n    Config config = new Config();\r\n    config.putAll(createGrasClusterConfig(88, 775, 25, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails tdSimple = genTopology(\"topology-simple\", config, 1, 5, 100, 300, 0, 0, \"user\", 8192);\r\n    //Schedule the simple topology first\r\n    Topologies topologies = new Topologies(tdSimple);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    {\r\n        NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, tdSimple);\r\n        for (ExecutorDetails exec : tdSimple.getExecutors()) {\r\n            nodeSorter.prepare(exec);\r\n            List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n            String rackSummaries = StreamSupport.stream(sortedRacks.spliterator(), false).map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n            NormalizedResourceRequest topoResourceRequest = tdSimple.getApproximateTotalResources();\r\n            String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", tdSimple.getId(), topoResourceRequest.toString());\r\n            assertEquals(2, sortedRacks.size(), rackSummaries + \"\\n# of racks sorted\");\r\n            assertEquals(\"rack-000\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted\");\r\n            assertEquals(\"rack-001\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted\");\r\n        }\r\n    }\r\n    scheduler.schedule(topologies, cluster);\r\n    TopologyBuilder builder = topologyBuilder(1, 5, 100, 300);\r\n    builder.setBolt(\"gpu-bolt\", new TestBolt(), 40).addResource(\"my.gpu\", 1.0).shuffleGrouping(\"spout-0\");\r\n    TopologyDetails tdGpu = topoToTopologyDetails(\"topology-gpu\", config, builder.createTopology(), 0, 0, \"user\", 8192);\r\n    //Now schedule GPU but with the simple topology in place.\r\n    topologies = new Topologies(tdSimple, tdGpu);\r\n    cluster = new Cluster(cluster, topologies);\r\n    {\r\n        NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, tdGpu);\r\n        for (ExecutorDetails exec : tdGpu.getExecutors()) {\r\n            String comp = tdGpu.getComponentFromExecutor(exec);\r\n            nodeSorter.prepare(exec);\r\n            List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n            String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n            NormalizedResourceRequest topoResourceRequest = tdSimple.getApproximateTotalResources();\r\n            String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", tdSimple.getId(), topoResourceRequest.toString());\r\n            assertEquals(2, sortedRacks.size(), rackSummaries + \"\\n# of racks sorted\");\r\n            if (comp.equals(\"gpu-bolt\")) {\r\n                assertEquals(\"rack-001\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted for \" + comp);\r\n                assertEquals(\"rack-000\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted for \" + comp);\r\n            } else {\r\n                assertEquals(\"rack-000\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted for \" + comp);\r\n                assertEquals(\"rack-001\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted for \" + comp);\r\n            }\r\n        }\r\n    }\r\n    scheduler.schedule(topologies, cluster);\r\n    Map<String, SchedulerAssignment> assignments = new TreeMap<>(cluster.getAssignments());\r\n    assertEquals(2, assignments.size());\r\n    Map<String, Map<String, AtomicLong>> topoPerRackCount = new HashMap<>();\r\n    for (Map.Entry<String, SchedulerAssignment> entry : assignments.entrySet()) {\r\n        SchedulerAssignment sa = entry.getValue();\r\n        Map<String, AtomicLong> slotsPerRack = new TreeMap<>();\r\n        for (WorkerSlot slot : sa.getSlots()) {\r\n            String nodeId = slot.getNodeId();\r\n            String rack = supervisorIdToRackName(nodeId);\r\n            slotsPerRack.computeIfAbsent(rack, (r) -> new AtomicLong(0)).incrementAndGet();\r\n        }\r\n        LOG.info(\"{} => {}\", entry.getKey(), slotsPerRack);\r\n        topoPerRackCount.put(entry.getKey(), slotsPerRack);\r\n    }\r\n    Map<String, AtomicLong> simpleCount = topoPerRackCount.get(\"topology-simple-0\");\r\n    assertNotNull(simpleCount);\r\n    //Because the simple topology was scheduled first we want to be sure that it didn't put anything on\r\n    // the GPU nodes.\r\n    //Only 1 rack is in use\r\n    assertEquals(1, simpleCount.size());\r\n    //r001 is the second rack with GPUs\r\n    assertFalse(simpleCount.containsKey(\"r001\"));\r\n    //r000 is the first rack with no GPUs\r\n    assertTrue(simpleCount.containsKey(\"r000\"));\r\n    //We don't really care too much about the scheduling of topology-gpu-0, because it was scheduled.\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testFillUpRackAndSpilloverToNextRack",
  "sourceCode" : "/**\r\n * If the topology is too large for one rack, it should be partially scheduled onto the next rack (and next rack only).\r\n */\r\n@Test\r\npublic void testFillUpRackAndSpilloverToNextRack() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numRacks = 3;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 6;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    // not enough for topo1\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism) * 4 / 5;\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum, supStartNum, cpuPerSuper, memPerSuper, Collections.emptyMap(), numaResourceMultiplier);\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createGrasClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, GenericResourceAwareStrategy.class.getName());\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    //Schedule the topo1 topology and ensure it fits on 2 racks\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    scheduler.schedule(topologies, cluster);\r\n    Set<String> assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(2, assignedRacks.size(), \"Racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testPreferRackWithTopoExecutors",
  "sourceCode" : "/**\r\n * Rack with low resources should be used to schedule an executor if it has other executors for the same topology.\r\n * <li>Schedule topo1 on one rack</li>\r\n * <li>unassign some executors</li>\r\n * <li>schedule another topology to partially fill up rack1</li>\r\n * <li>Add another rack and schedule topology 1 remaining executors again</li>\r\n * <li>scheduling should utilize all resources on rack1 before before trying next rack</li>\r\n */\r\n@Test\r\npublic void testPreferRackWithTopoExecutors() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    int topo2NumSpouts = 1;\r\n    int topo2NumBolts = 5;\r\n    int topo2SpoutParallelism = 10;\r\n    int topo2BoltParallelism = 20;\r\n    final int numRacks = 3;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 6;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + // enough for topo1 but not topo1+topo2\r\n    topo2NumSpouts * topo2SpoutParallelism);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    double topo2MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    final String topoName2 = \"topology2\";\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum, supStartNum, cpuPerSuper, memPerSuper, Collections.emptyMap(), numaResourceMultiplier);\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createGrasClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, GenericResourceAwareStrategy.class.getName());\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    //Schedule the topo1 topology and ensure it fits on 1 rack\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    scheduler.schedule(topologies, cluster);\r\n    Set<String> assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(1, assignedRacks.size(), \"Racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    TopologyBuilder builder = topologyBuilder(topo2NumSpouts, topo2NumBolts, topo2SpoutParallelism, topo2BoltParallelism);\r\n    TopologyDetails td2 = topoToTopologyDetails(topoName2, config, builder.createTopology(), 0, 0, \"user\", topo2MaxHeapSize);\r\n    //Now schedule GPU but with the simple topology in place.\r\n    topologies = new Topologies(td1, td2);\r\n    cluster = new Cluster(cluster, topologies);\r\n    scheduler.schedule(topologies, cluster);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId(), td2.getId());\r\n    assertEquals(2, assignedRacks.size(), \"Racks for topologies=\" + td1.getId() + \"/\" + td2.getId() + \" is \" + assignedRacks);\r\n    // topo2 gets scheduled on its own rack because it is empty and available\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(1, assignedRacks.size(), \"Racks for topologies=\" + td2.getId() + \" is \" + assignedRacks);\r\n    // now unassign topo2, expect only one rack to be in use; free some slots and reschedule topo1 some topo1 executors\r\n    cluster.unassign(td2.getId());\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(0, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td2.getId() + \" is \" + assignedRacks);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(1, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    assertFalse(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should be fully assigned before freeing slots\");\r\n    freeSomeWorkerSlots(cluster);\r\n    assertTrue(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should need scheduling after freeing slots\");\r\n    // then reschedule executors\r\n    scheduler.schedule(topologies, cluster);\r\n    // only one rack should be in use by topology1\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(1, assignedRacks.size(), \"After reassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testWithImpairedClusterNetworkTopography",
  "sourceCode" : "/**\r\n * Assign and then clear out a rack to host list mapping in cluster.networkTopography.\r\n * Expected behavior is that:\r\n *  <li>the rack without hosts does not show up in {@link NodeSorterHostProximity#getSortedRacks()}</li>\r\n *  <li>all the supervisor nodes still get returned in {@link NodeSorterHostProximity#sortAllNodes()} ()}</li>\r\n *  <li>supervisors on cleared rack show up under {@link DNSToSwitchMapping#DEFAULT_RACK}</li>\r\n *\r\n *  <p>\r\n *      Force an usual condition, where one of the racks is still passed to LazyNodeSortingIterator with\r\n *      an empty list and then ensure that code is resilient.\r\n *  </p>\r\n */\r\n@Test\r\nvoid testWithImpairedClusterNetworkTopography() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 66;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + 10);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    int numRacks = 3;\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(numRacks, numSupersPerRack, numPortsPerSuper, 0, 0, cpuPerSuper, memPerSuper, new HashMap<>());\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createGrasClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, GenericResourceAwareStrategy.class.getName());\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    Map<String, List<String>> networkTopography = cluster.getNetworkTopography();\r\n    assertEquals(numRacks, networkTopography.size(), \"Expecting \" + numRacks + \" racks found \" + networkTopography.size());\r\n    assertTrue(networkTopography.size() >= 3, \"Expecting racks count to be >= 3, found \" + networkTopography.size());\r\n    // Impair cluster.networkTopography and set one rack to have zero hosts, getSortedRacks should exclude this rack.\r\n    // Keep, the supervisorDetails unchanged - confirm that these nodes are not lost even with incomplete networkTopography\r\n    String rackIdToZero = networkTopography.keySet().stream().findFirst().get();\r\n    impairClusterRack(cluster, rackIdToZero, true, false);\r\n    NodeSorterHostProximity nodeSorterHostProximity = new NodeSorterHostProximity(cluster, td1);\r\n    nodeSorterHostProximity.getSortedRacks().forEach(x -> assertNotEquals(x.id, rackIdToZero));\r\n    // confirm that the above action has not lost the hosts and that they appear under the DEFAULT rack\r\n    {\r\n        Set<String> seenRacks = new HashSet<>();\r\n        nodeSorterHostProximity.getSortedRacks().forEach(x -> seenRacks.add(x.id));\r\n        assertEquals(numRacks, seenRacks.size(), \"Expecting rack cnt to be still \" + numRacks);\r\n        assertTrue(seenRacks.contains(DNSToSwitchMapping.DEFAULT_RACK), \"Expecting to see default-rack=\" + DNSToSwitchMapping.DEFAULT_RACK + \" in sortedRacks\");\r\n    }\r\n    // now check if node/supervisor is missing when sorting all nodes\r\n    Set<String> expectedNodes = supMap.keySet();\r\n    Set<String> seenNodes = new HashSet<>();\r\n    nodeSorterHostProximity.prepare(null);\r\n    nodeSorterHostProximity.sortAllNodes().forEach(n -> seenNodes.add(n));\r\n    assertEquals(expectedNodes, seenNodes, \"Expecting see all supervisors \");\r\n    // Now fully impair the cluster - confirm no default rack\r\n    {\r\n        cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        cluster.setNetworkTopography(new TestDNSToSwitchMapping(supMap.values()).getRackToHosts());\r\n        impairClusterRack(cluster, rackIdToZero, true, true);\r\n        Set<String> seenRacks = new HashSet<>();\r\n        NodeSorterHostProximity nodeSorterHostProximity2 = new NodeSorterHostProximity(cluster, td1);\r\n        nodeSorterHostProximity2.getSortedRacks().forEach(x -> seenRacks.add(x.id));\r\n        Map<String, Set<String>> rackIdToHosts = nodeSorterHostProximity2.getRackIdToHosts();\r\n        String dumpOfRacks = rackIdToHosts.entrySet().stream().map(x -> String.format(\"rack %s -> hosts [%s]\", x.getKey(), String.join(\",\", x.getValue()))).collect(Collectors.joining(\"\\n\\t\"));\r\n        assertEquals(numRacks - 1, seenRacks.size(), \"Expecting rack cnt to be \" + (numRacks - 1) + \" but found \" + seenRacks.size() + \"\\n\\t\" + dumpOfRacks);\r\n        assertFalse(seenRacks.contains(DNSToSwitchMapping.DEFAULT_RACK), \"Found default-rack=\" + DNSToSwitchMapping.DEFAULT_RACK + \" in \\n\\t\" + dumpOfRacks);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestNodeSorterHostProximity.java",
  "methodName" : "testWithBlackListedHosts",
  "sourceCode" : "/**\r\n * Black list all nodes for a rack before sorting nodes.\r\n * Confirm that {@link NodeSorterHostProximity#sortAllNodes()} still works.\r\n */\r\n@Test\r\nvoid testWithBlackListedHosts() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 66;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + 10);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    int numRacks = 3;\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(numRacks, numSupersPerRack, numPortsPerSuper, 0, 0, cpuPerSuper, memPerSuper, new HashMap<>());\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createGrasClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, GenericResourceAwareStrategy.class.getName());\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    Map<String, List<String>> networkTopography = cluster.getNetworkTopography();\r\n    assertEquals(numRacks, networkTopography.size(), \"Expecting \" + numRacks + \" racks found \" + networkTopography.size());\r\n    assertTrue(networkTopography.size() >= 3, \"Expecting racks count to be >= 3, found \" + networkTopography.size());\r\n    Set<String> blackListedHosts = new HashSet<>();\r\n    List<SupervisorDetails> supArray = new ArrayList<>(supMap.values());\r\n    for (int i = 0; i < numSupersPerRack; i++) {\r\n        blackListedHosts.add(supArray.get(i).getHost());\r\n    }\r\n    blacklistHostsAndSortNodes(blackListedHosts, supMap.values(), cluster, td1);\r\n    String rackToClear = cluster.getNetworkTopography().keySet().stream().findFirst().get();\r\n    blackListedHosts = new HashSet<>(cluster.getNetworkTopography().get(rackToClear));\r\n    blacklistHostsAndSortNodes(blackListedHosts, supMap.values(), cluster, td1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksWithFavoritism",
  "sourceCode" : "/**\r\n * Test whether strategy will choose correct rack.\r\n */\r\n@Test\r\npublic void testMultipleRacksWithFavoritism() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 2;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000, Collections.emptyMap(), 1.0);\r\n    //generate another rack of supervisors with less resources\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 4000, Collections.emptyMap(), 1.0);\r\n    //generate some supervisors that are depleted of one resource\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0, 8000, Collections.emptyMap(), 1.0);\r\n    //generate some that has a lot of memory but little of cpu\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 10, 8000 * 2 + 4000, Collections.emptyMap(), 1.0);\r\n    //generate some that has a lot of cpu but little of memory\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400 + 200 + 10, 1000, Collections.emptyMap(), 1.0);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4);\r\n    Config t1Conf = new Config();\r\n    t1Conf.putAll(config);\r\n    final List<String> t1FavoredHostNames = Arrays.asList(\"host-41\", \"host-42\", \"host-43\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, t1FavoredHostNames);\r\n    final List<String> t1UnfavoredHostIds = Arrays.asList(\"host-1\", \"host-2\", \"host-3\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, t1UnfavoredHostIds);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", t1Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Config t2Conf = new Config();\r\n    t2Conf.putAll(config);\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, Arrays.asList(\"host-31\", \"host-32\", \"host-33\"));\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, Arrays.asList(\"host-11\", \"host-12\", \"host-13\"));\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", t2Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    List<String> supHostnames = new LinkedList<>();\r\n    for (SupervisorDetails sup : supMap.values()) {\r\n        supHostnames.add(sup.getHost());\r\n    }\r\n    Map<String, List<String>> rackToHosts = testDNSToSwitchMapping.getRackToHosts();\r\n    cluster.setNetworkTopography(rackToHosts);\r\n    NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, topo1, BaseResourceAwareStrategy.NodeSortType.COMMON);\r\n    nodeSorter.prepare(null);\r\n    List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n    String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n    Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n    // Ranked first since rack-000 has the most balanced set of resources\r\n    assertEquals(\"rack-004\", it.next().id, \"rack-004 should be ordered first\\n\\t\" + rackSummaries);\r\n    // Ranked second since rack-1 has a balanced set of resources but less than rack-0\r\n    assertEquals(\"rack-000\", it.next().id, \"rack-000 should be ordered second\\n\\t\" + rackSummaries);\r\n    // Ranked third since rack-4 has a lot of cpu but not a lot of memory\r\n    assertEquals(\"rack-003\", it.next().id, \"rack-003 should be ordered\\n\\t\" + rackSummaries);\r\n    // Ranked fourth since rack-3 has alot of memory but not cpu\r\n    assertEquals(\"rack-001\", it.next().id, \"rack-001 should be ordered fourth\\n\\t\" + rackSummaries);\r\n    //Ranked last since rack-2 has not cpu resources\r\n    assertEquals(\"rack-002\", it.next().id, \"rack-002 should be ordered fifth\\n\\t\" + rackSummaries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksWithHostProximity",
  "sourceCode" : "/**\r\n * Test if hosts are presented together regardless of resource availability.\r\n * Supervisors are created with multiple Numa zones in such a manner that resources on two numa zones on the same host\r\n * differ widely in resource availability.\r\n */\r\n@Test\r\npublic void testMultipleRacksWithHostProximity() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 12;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 3;\r\n    final double numaResourceMultiplier = 0.4;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate another rack of supervisors with less resources\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some supervisors that are depleted of one resource\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 0, 8000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of memory but little of cpu\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 10, 8000 * 2 + 4000, Collections.emptyMap(), numaResourceMultiplier);\r\n    //generate some that has a lot of cpu but little of memory\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400 + 200 + 10, 1000, Collections.emptyMap(), numaResourceMultiplier);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4);\r\n    Config t1Conf = new Config();\r\n    t1Conf.putAll(config);\r\n    final List<String> t1FavoredHostNames = Arrays.asList(\"host-41\", \"host-42\", \"host-43\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, t1FavoredHostNames);\r\n    final List<String> t1UnfavoredHostIds = Arrays.asList(\"host-1\", \"host-2\", \"host-3\");\r\n    t1Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, t1UnfavoredHostIds);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", t1Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Config t2Conf = new Config();\r\n    t2Conf.putAll(config);\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, Arrays.asList(\"host-31\", \"host-32\", \"host-33\"));\r\n    t2Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, Arrays.asList(\"host-11\", \"host-12\", \"host-13\"));\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", t2Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    INodeSorter nodeSorter = new NodeSorterHostProximity(cluster, topo1);\r\n    nodeSorter.prepare(null);\r\n    Set<String> seenHosts = new HashSet<>();\r\n    String prevHost = null;\r\n    List<String> errLines = new ArrayList();\r\n    Map<String, String> nodeToHost = new RasNodes(cluster).getNodeIdToHostname();\r\n    for (String nodeId : nodeSorter.sortAllNodes()) {\r\n        String host = nodeToHost.getOrDefault(nodeId, \"no-host-for-node-\" + nodeId);\r\n        errLines.add(String.format(\"\\tnodeId:%s, host:%s\", nodeId, host));\r\n        if (!host.equals(prevHost) && seenHosts.contains(host)) {\r\n            String err = String.format(\"Host %s for node %s is out of order:\\n\\t%s\", host, nodeId, String.join(\"\\n\\t\", errLines));\r\n            fail(err);\r\n        }\r\n        seenHosts.add(host);\r\n        prevHost = host;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testMultipleRacksOrderedByCapacity",
  "sourceCode" : "/**\r\n * Racks should be returned in order of decreasing capacity.\r\n */\r\n@Test\r\npublic void testMultipleRacksOrderedByCapacity() {\r\n    final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n    final int numRacks = 1;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 4;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    final Map<String, SupervisorDetails> supMapRack0 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 600, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack1 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 500, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack2 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 400, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack3 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 300, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack4 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 200, 8000 - rackStartNum, Collections.emptyMap(), numaResourceMultiplier);\r\n    // too small to hold topology\r\n    supStartNum += numSupersPerRack;\r\n    final Map<String, SupervisorDetails> supMapRack5 = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum++, supStartNum, 100, 8000 - rackStartNum, Collections.singletonMap(\"gpu.count\", 0.0), numaResourceMultiplier);\r\n    supMap.putAll(supMapRack0);\r\n    supMap.putAll(supMapRack1);\r\n    supMap.putAll(supMapRack2);\r\n    supMap.putAll(supMapRack3);\r\n    supMap.putAll(supMapRack4);\r\n    supMap.putAll(supMapRack5);\r\n    Config config = createClusterConfig(100, 500, 500, null);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n    INimbus iNimbus = new INimbusTest();\r\n    //create test DNSToSwitchMapping plugin\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4, supMapRack5);\r\n    //generate topologies\r\n    TopologyDetails topo1 = genTopology(\"topo-1\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    TopologyDetails topo2 = genTopology(\"topo-2\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n    Topologies topologies = new Topologies(topo1, topo2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, topo1);\r\n    nodeSorter.prepare(null);\r\n    List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n    String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n    NormalizedResourceRequest topoResourceRequest = topo1.getApproximateTotalResources();\r\n    String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", topo1.getId(), topoResourceRequest.toString());\r\n    Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n    assertEquals(\"rack-000\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nRack-000 should be ordered first since it has the largest capacity\");\r\n    assertEquals(\"rack-001\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-001 should be ordered second since it smaller than rack-000\");\r\n    assertEquals(\"rack-002\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-002 should be ordered third since it is smaller than rack-001\");\r\n    assertEquals(\"rack-003\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-003 should be ordered fourth since it since it is smaller than rack-002\");\r\n    assertEquals(\"rack-004\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-004 should be ordered fifth since it since it is smaller than rack-003\");\r\n    assertEquals(\"rack-005\", it.next().id, topoRequest + \"\\n\\t\" + rackSummaries + \"\\nrack-005 should be ordered last since it since it is has smallest capacity\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testAntiAffinityWithMultipleTopologies",
  "sourceCode" : "/**\r\n * Schedule two topologies, once with special resources and another without.\r\n * There are enough special resources to hold one topology with special resource (\"my.gpu\").\r\n * When using Round Robin scheduling, only one topology will be scheduled.\r\n */\r\n@Test\r\npublic void testAntiAffinityWithMultipleTopologies() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(1, 40, 66, 0, 0, 4700, 226200, new HashMap<>());\r\n    HashMap<String, Double> extraResources = new HashMap<>();\r\n    extraResources.put(\"my.gpu\", 1.0);\r\n    supMap.putAll(genSupervisorsWithRacks(1, 40, 66, 1, 0, 4700, 226200, extraResources));\r\n    Config config = new Config();\r\n    config.putAll(createRoundRobinClusterConfig(88, 775, 25, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails tdSimple = genTopology(\"topology-simple\", config, 1, 5, 100, 300, 0, 0, \"user\", 8192);\r\n    //Schedule the simple topology first\r\n    Topologies topologies = new Topologies(tdSimple);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    {\r\n        NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, tdSimple);\r\n        for (ExecutorDetails exec : tdSimple.getExecutors()) {\r\n            nodeSorter.prepare(exec);\r\n            List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n            String rackSummaries = StreamSupport.stream(sortedRacks.spliterator(), false).map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n            NormalizedResourceRequest topoResourceRequest = tdSimple.getApproximateTotalResources();\r\n            String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", tdSimple.getId(), topoResourceRequest.toString());\r\n            assertEquals(2, sortedRacks.size(), rackSummaries + \"\\n# of racks sorted\");\r\n            assertEquals(\"rack-000\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted\");\r\n            assertEquals(\"rack-001\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted\");\r\n        }\r\n    }\r\n    scheduler.schedule(topologies, cluster);\r\n    Map<String, SchedulerAssignment> assignments = new TreeMap<>(cluster.getAssignments());\r\n    assertEquals(1, assignments.size());\r\n    TopologyBuilder builder = topologyBuilder(1, 5, 100, 300);\r\n    builder.setBolt(\"gpu-bolt\", new TestBolt(), 40).addResource(\"my.gpu\", 1.0).shuffleGrouping(\"spout-0\");\r\n    TopologyDetails tdGpu = topoToTopologyDetails(\"topology-gpu\", config, builder.createTopology(), 0, 0, \"user\", 8192);\r\n    //Now schedule GPU but with the simple topology in place.\r\n    topologies = new Topologies(tdSimple, tdGpu);\r\n    cluster = new Cluster(cluster, topologies);\r\n    {\r\n        NodeSorterHostProximity nodeSorter = new NodeSorterHostProximity(cluster, tdGpu);\r\n        for (ExecutorDetails exec : tdGpu.getExecutors()) {\r\n            String comp = tdGpu.getComponentFromExecutor(exec);\r\n            nodeSorter.prepare(exec);\r\n            List<ObjectResourcesItem> sortedRacks = StreamSupport.stream(nodeSorter.getSortedRacks().spliterator(), false).collect(Collectors.toList());\r\n            String rackSummaries = sortedRacks.stream().map(x -> String.format(\"Rack %s -> scheduled-cnt %d, min-avail %f, avg-avail %f, cpu %f, mem %f\", x.id, nodeSorter.getScheduledExecCntByRackId().getOrDefault(x.id, new AtomicInteger(-1)).get(), x.minResourcePercent, x.avgResourcePercent, x.availableResources.getTotalCpu(), x.availableResources.getTotalMemoryMb())).collect(Collectors.joining(\"\\n\\t\"));\r\n            NormalizedResourceRequest topoResourceRequest = tdSimple.getApproximateTotalResources();\r\n            String topoRequest = String.format(\"Topo %s, approx-requested-resources %s\", tdSimple.getId(), topoResourceRequest.toString());\r\n            assertEquals(2, sortedRacks.size(), rackSummaries + \"\\n# of racks sorted\");\r\n            if (comp.equals(\"gpu-bolt\")) {\r\n                assertEquals(\"rack-001\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted for \" + comp);\r\n                assertEquals(\"rack-000\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted for \" + comp);\r\n            } else {\r\n                assertEquals(\"rack-000\", sortedRacks.get(0).id, rackSummaries + \"\\nFirst rack sorted for \" + comp);\r\n                assertEquals(\"rack-001\", sortedRacks.get(1).id, rackSummaries + \"\\nSecond rack sorted for \" + comp);\r\n            }\r\n        }\r\n    }\r\n    scheduler.schedule(topologies, cluster);\r\n    assignments = new TreeMap<>(cluster.getAssignments());\r\n    // second topology is not expected to be assigned\r\n    assertEquals(1, assignments.size());\r\n    Map<String, Map<String, AtomicLong>> topoPerRackCount = new HashMap<>();\r\n    for (Map.Entry<String, SchedulerAssignment> entry : assignments.entrySet()) {\r\n        SchedulerAssignment sa = entry.getValue();\r\n        Map<String, AtomicLong> slotsPerRack = new TreeMap<>();\r\n        for (WorkerSlot slot : sa.getSlots()) {\r\n            String nodeId = slot.getNodeId();\r\n            String rack = supervisorIdToRackName(nodeId);\r\n            slotsPerRack.computeIfAbsent(rack, (r) -> new AtomicLong(0)).incrementAndGet();\r\n        }\r\n        LOG.info(\"{} => {}\", entry.getKey(), slotsPerRack);\r\n        topoPerRackCount.put(entry.getKey(), slotsPerRack);\r\n    }\r\n    Map<String, AtomicLong> simpleCount = topoPerRackCount.get(\"topology-simple-0\");\r\n    assertNotNull(simpleCount);\r\n    //Because the simple topology was scheduled first we want to be sure that it didn't put anything on\r\n    // the GPU nodes.\r\n    //Both racks are in use\r\n    assertEquals(2, simpleCount.size());\r\n    //r001 is the second rack with GPUs\r\n    assertTrue(simpleCount.containsKey(\"r001\"));\r\n    //r000 is the first rack with no GPUs\r\n    assertTrue(simpleCount.containsKey(\"r000\"));\r\n    //We don't really care too much about the scheduling of topology-gpu-0, because it was scheduled.\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testDistributeOverRacks",
  "sourceCode" : "/**\r\n * If the topology should be scheduled across all available racks instead of\r\n * filling first rack and spilling on to the next rack.\r\n */\r\n@Test\r\npublic void testDistributeOverRacks() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numRacks = 3;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 6;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    // not enough for topo1\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism) * 4 / 5;\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum, supStartNum, cpuPerSuper, memPerSuper, Collections.emptyMap(), numaResourceMultiplier);\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createRoundRobinClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    //Schedule the topo1 topology and ensure it fits on 2 racks\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    scheduler.schedule(topologies, cluster);\r\n    Set<String> assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"Racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testDistributeAcrossRacks",
  "sourceCode" : "/**\r\n * Racks are equally likely to be selected, rather than those with low resources already running components\r\n * for the same topology.\r\n * .\r\n * <li>Schedule topo1 on one rack</li>\r\n * <li>unassign some executors</li>\r\n * <li>schedule another topology - cannot be scheduled since topo1 occupies all slots</li>\r\n * <li>unassign topo2, kill workers and reschedule</li>\r\n * <li>topo1 should utilize all all racks</li>\r\n */\r\n@Test\r\npublic void testDistributeAcrossRacks() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    int topo2NumSpouts = 1;\r\n    int topo2NumBolts = 5;\r\n    int topo2SpoutParallelism = 10;\r\n    int topo2BoltParallelism = 20;\r\n    final int numRacks = 3;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 6;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + // enough for topo1 but not topo1+topo2\r\n    topo2NumSpouts * topo2SpoutParallelism);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    double topo2MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    final String topoName2 = \"topology2\";\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum, supStartNum, cpuPerSuper, memPerSuper, Collections.emptyMap(), numaResourceMultiplier);\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createRoundRobinClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    //Schedule the topo1 topology and ensure it fits on 1 rack\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    scheduler.schedule(topologies, cluster);\r\n    Set<String> assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"Racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    TopologyBuilder builder = topologyBuilder(topo2NumSpouts, topo2NumBolts, topo2SpoutParallelism, topo2BoltParallelism);\r\n    TopologyDetails td2 = topoToTopologyDetails(topoName2, config, builder.createTopology(), 0, 0, \"user\", topo2MaxHeapSize);\r\n    //Now schedule GPU but with the simple topology in place.\r\n    topologies = new Topologies(td1, td2);\r\n    cluster = new Cluster(cluster, topologies);\r\n    scheduler.schedule(topologies, cluster);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId(), td2.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"Racks for topologies=\" + td1.getId() + \"/\" + td2.getId() + \" is \" + assignedRacks);\r\n    // topo2 will not get scheduled as topo1 will occupy all racks\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(0, assignedRacks.size(), \"Racks for topologies=\" + td2.getId() + \" is \" + assignedRacks);\r\n    // now unassign topo2, expect all racks to be in use; free some slots and reschedule topo1 some topo1 executors\r\n    cluster.unassign(td2.getId());\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(0, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td2.getId() + \" is \" + assignedRacks);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    assertFalse(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should be fully assigned before freeing slots\");\r\n    freeSomeWorkerSlots(cluster);\r\n    assertTrue(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should need scheduling after freeing slots\");\r\n    // then reschedule executors\r\n    scheduler.schedule(topologies, cluster);\r\n    // all racks should be in use by topology1\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"After reassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testWithImpairedClusterNetworkTopography",
  "sourceCode" : "/**\r\n * Assign and then clear out a rack to host list mapping in cluster.networkTopography.\r\n * Expected behavior is that:\r\n *  <li>the rack without hosts does not show up in {@link NodeSorterHostProximity#getSortedRacks()}</li>\r\n *  <li>all the supervisor nodes still get returned in {@link NodeSorterHostProximity#sortAllNodes()} ()}</li>\r\n *  <li>supervisors on cleared rack show up under {@link DNSToSwitchMapping#DEFAULT_RACK}</li>\r\n *\r\n *  <p>\r\n *      Force an usual condition, where one of the racks is still passed to LazyNodeSortingIterator with\r\n *      an empty list and then ensure that code is resilient.\r\n *  </p>\r\n */\r\n@Test\r\nvoid testWithImpairedClusterNetworkTopography() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 66;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + 10);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    int numRacks = 3;\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(numRacks, numSupersPerRack, numPortsPerSuper, 0, 0, cpuPerSuper, memPerSuper, new HashMap<>());\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createRoundRobinClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    Map<String, List<String>> networkTopography = cluster.getNetworkTopography();\r\n    assertEquals(numRacks, networkTopography.size(), \"Expecting \" + numRacks + \" racks found \" + networkTopography.size());\r\n    assertTrue(networkTopography.size() >= 3, \"Expecting racks count to be >= 3, found \" + networkTopography.size());\r\n    // Impair cluster.networkTopography and set one rack to have zero hosts, getSortedRacks should exclude this rack.\r\n    // Keep, the supervisorDetails unchanged - confirm that these nodes are not lost even with incomplete networkTopography\r\n    String rackIdToZero = networkTopography.keySet().stream().findFirst().get();\r\n    impairClusterRack(cluster, rackIdToZero, true, false);\r\n    NodeSorterHostProximity nodeSorterHostProximity = new NodeSorterHostProximity(cluster, td1);\r\n    nodeSorterHostProximity.getSortedRacks().forEach(x -> assertNotEquals(x.id, rackIdToZero));\r\n    // confirm that the above action has not lost the hosts and that they appear under the DEFAULT rack\r\n    {\r\n        Set<String> seenRacks = new HashSet<>();\r\n        nodeSorterHostProximity.getSortedRacks().forEach(x -> seenRacks.add(x.id));\r\n        assertEquals(numRacks, seenRacks.size(), \"Expecting rack cnt to be still \" + numRacks);\r\n        assertTrue(seenRacks.contains(DNSToSwitchMapping.DEFAULT_RACK), \"Expecting to see default-rack=\" + DNSToSwitchMapping.DEFAULT_RACK + \" in sortedRacks\");\r\n    }\r\n    // now check if node/supervisor is missing when sorting all nodes\r\n    Set<String> expectedNodes = supMap.keySet();\r\n    Set<String> seenNodes = new HashSet<>();\r\n    nodeSorterHostProximity.prepare(null);\r\n    nodeSorterHostProximity.sortAllNodes().forEach(n -> seenNodes.add(n));\r\n    assertEquals(expectedNodes, seenNodes, \"Expecting see all supervisors \");\r\n    // Now fully impair the cluster - confirm no default rack\r\n    {\r\n        cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        cluster.setNetworkTopography(new TestDNSToSwitchMapping(supMap.values()).getRackToHosts());\r\n        impairClusterRack(cluster, rackIdToZero, true, true);\r\n        Set<String> seenRacks = new HashSet<>();\r\n        NodeSorterHostProximity nodeSorterHostProximity2 = new NodeSorterHostProximity(cluster, td1);\r\n        nodeSorterHostProximity2.getSortedRacks().forEach(x -> seenRacks.add(x.id));\r\n        Map<String, Set<String>> rackIdToHosts = nodeSorterHostProximity2.getRackIdToHosts();\r\n        String dumpOfRacks = rackIdToHosts.entrySet().stream().map(x -> String.format(\"rack %s -> hosts [%s]\", x.getKey(), String.join(\",\", x.getValue()))).collect(Collectors.joining(\"\\n\\t\"));\r\n        assertEquals(numRacks - 1, seenRacks.size(), \"Expecting rack cnt to be \" + (numRacks - 1) + \" but found \" + seenRacks.size() + \"\\n\\t\" + dumpOfRacks);\r\n        assertFalse(seenRacks.contains(DNSToSwitchMapping.DEFAULT_RACK), \"Found default-rack=\" + DNSToSwitchMapping.DEFAULT_RACK + \" in \\n\\t\" + dumpOfRacks);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\sorter\\TestRoundRobinNodeSorterHostProximity.java",
  "methodName" : "testWithBlackListedHosts",
  "sourceCode" : "/**\r\n * Black list all nodes for a rack before sorting nodes.\r\n * Confirm that {@link NodeSorterHostProximity#sortAllNodes()} still works.\r\n */\r\n@Test\r\nvoid testWithBlackListedHosts() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int topo1NumSpouts = 1;\r\n    int topo1NumBolts = 5;\r\n    int topo1SpoutParallelism = 100;\r\n    int topo1BoltParallelism = 200;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 66;\r\n    long compPerRack = (topo1NumSpouts * topo1SpoutParallelism + topo1NumBolts * topo1BoltParallelism + 10);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double topo1MaxHeapSize = memPerSuper;\r\n    final String topoName1 = \"topology1\";\r\n    int numRacks = 3;\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(numRacks, numSupersPerRack, numPortsPerSuper, 0, 0, cpuPerSuper, memPerSuper, new HashMap<>());\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config config = new Config();\r\n    config.putAll(createRoundRobinClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    TopologyDetails td1 = genTopology(topoName1, config, topo1NumSpouts, topo1NumBolts, topo1SpoutParallelism, topo1BoltParallelism, 0, 0, \"user\", topo1MaxHeapSize);\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    Map<String, List<String>> networkTopography = cluster.getNetworkTopography();\r\n    assertEquals(numRacks, networkTopography.size(), \"Expecting \" + numRacks + \" racks found \" + networkTopography.size());\r\n    assertTrue(networkTopography.size() >= 3, \"Expecting racks count to be >= 3, found \" + networkTopography.size());\r\n    Set<String> blackListedHosts = new HashSet<>();\r\n    List<SupervisorDetails> supArray = new ArrayList<>(supMap.values());\r\n    for (int i = 0; i < numSupersPerRack; i++) {\r\n        blackListedHosts.add(supArray.get(i).getHost());\r\n    }\r\n    blacklistHostsAndSortNodes(blackListedHosts, supMap.values(), cluster, td1);\r\n    String rackToClear = cluster.getNetworkTopography().keySet().stream().findFirst().get();\r\n    blackListedHosts = new HashSet<>(cluster.getNetworkTopography().get(rackToClear));\r\n    blacklistHostsAndSortNodes(blackListedHosts, supMap.values(), cluster, td1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testMissingConfig",
  "sourceCode" : "/**\r\n * See if constraint configuration can be instantiated with no or partial constraints.\r\n */\r\n@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testMissingConfig(boolean consolidatedConfigFlag) {\r\n    // no configs\r\n    new ConstraintSolverConfig(\"test-topoid-1\", new HashMap<>(), new HashSet<>());\r\n    // with one or more undefined components with partial constraints\r\n    {\r\n        String s = consolidatedConfigFlag ? String.format(\"{ \\\"comp-1\\\": \" + \"                  { \\\"%s\\\": 2, \" + \"                    \\\"%s\\\": [\\\"comp-2\\\", \\\"comp-3\\\" ] }, \" + \"     \\\"comp-2\\\": \" + \"                  { \\\"%s\\\": [ \\\"comp-4\\\" ] }, \" + \"     \\\"comp-3\\\": \" + \"                  { \\\"%s\\\": \\\"comp-5\\\" }, \" + \"     \\\"comp-6\\\": \" + \"                  { \\\"%s\\\": 2 }\" + \"}\", ConstraintSolverConfig.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, ConstraintSolverConfig.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT) : \"[ \" + \"[ \\\"comp-1\\\", \\\"comp-2\\\" ], \" + \"[ \\\"comp-1\\\", \\\"comp-3\\\" ], \" + \"[ \\\"comp-2\\\", \\\"comp-3\\\" ], \" + \"[ \\\"comp-2\\\", \\\"comp-4\\\" ], \" + \"[ \\\"comp-3\\\", \\\"comp-5\\\" ] \" + \"]\";\r\n        Object jsonValue = JSONValue.parse(s);\r\n        Map<String, Object> conf = new HashMap<>();\r\n        conf.put(Config.TOPOLOGY_RAS_CONSTRAINTS, jsonValue);\r\n        new ConstraintSolverConfig(\"test-topoid-2\", conf, new HashSet<>());\r\n        new ConstraintSolverConfig(\"test-topoid-3\", conf, new HashSet<>(Collections.singletonList(\"comp-x\")));\r\n        new ConstraintSolverConfig(\"test-topoid-4\", conf, new HashSet<>(Collections.singletonList(\"comp-1\")));\r\n        new ConstraintSolverConfig(\"test-topoid-5\", conf, new HashSet<>(Collections.singletonList(\"comp-1, comp-x\")));\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testNewConstraintFormat",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testNewConstraintFormat(boolean consolidatedConfigFlag) {\r\n    String s = String.format(\"{ \\\"comp-1\\\": \" + \"                  { \\\"%s\\\": 2, \" + \"                    \\\"%s\\\": [\\\"comp-2\\\", \\\"comp-3\\\" ] }, \" + \"     \\\"comp-2\\\": \" + \"                  { \\\"%s\\\": [ \\\"comp-4\\\" ] }, \" + \"     \\\"comp-3\\\": \" + \"                  { \\\"%s\\\": \\\"comp-5\\\" } \" + \"}\", ConstraintSolverConfig.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS, ConstraintSolverConfig.CONSTRAINT_TYPE_INCOMPATIBLE_COMPONENTS);\r\n    Object jsonValue = JSONValue.parse(s);\r\n    Map<String, Object> config = Utils.readDefaultConfig();\r\n    config.put(Config.TOPOLOGY_RAS_CONSTRAINTS, jsonValue);\r\n    Set<String> allComps = new HashSet<>();\r\n    allComps.addAll(Arrays.asList(\"comp-1\", \"comp-2\", \"comp-3\", \"comp-4\", \"comp-5\"));\r\n    ConstraintSolverConfig constraintSolverConfig = new ConstraintSolverConfig(\"test-topoid-1\", config, allComps);\r\n    Set<String> expectedSetComp1 = new HashSet<>();\r\n    expectedSetComp1.addAll(Arrays.asList(\"comp-2\", \"comp-3\"));\r\n    Set<String> expectedSetComp2 = new HashSet<>();\r\n    expectedSetComp2.addAll(Arrays.asList(\"comp-1\", \"comp-4\"));\r\n    Set<String> expectedSetComp3 = new HashSet<>();\r\n    expectedSetComp3.addAll(Arrays.asList(\"comp-1\", \"comp-5\"));\r\n    assertEquals(expectedSetComp1, constraintSolverConfig.getIncompatibleComponentSets().get(\"comp-1\"), \"comp-1 incompatible components\");\r\n    assertEquals(expectedSetComp2, constraintSolverConfig.getIncompatibleComponentSets().get(\"comp-2\"), \"comp-2 incompatible components\");\r\n    assertEquals(expectedSetComp3, constraintSolverConfig.getIncompatibleComponentSets().get(\"comp-3\"), \"comp-3 incompatible components\");\r\n    assertEquals(2, (int) constraintSolverConfig.getMaxNodeCoLocationCnts().getOrDefault(\"comp-1\", -1), \"comp-1 maxNodeCoLocationCnt\");\r\n    assertNull(constraintSolverConfig.getMaxNodeCoLocationCnts().get(\"comp-2\"), \"comp-2 maxNodeCoLocationCnt\");\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testConstraintSolverForceBacktrackWithSpreadCoLocation",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testConstraintSolverForceBacktrackWithSpreadCoLocation(boolean consolidatedConfigFlag) {\r\n    //The best way to force backtracking is to change the heuristic so the components are reversed, so it is hard\r\n    // to find an answer.\r\n    if (CO_LOCATION_CNT > 1 && !consolidatedConfigFlag) {\r\n        LOG.info(\"INFO: Skipping Test {} with {}={} (required 1), and consolidatedConfigFlag={} (required false)\", \"testConstraintSolverForceBacktrackWithSpreadCoLocation\", ConstraintSolverConfig.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, CO_LOCATION_CNT, consolidatedConfigFlag);\r\n        return;\r\n    }\r\n    ConstraintSolverStrategy cs = new ConstraintSolverStrategy() {\r\n\r\n        @Override\r\n        protected void prepareForScheduling(Cluster cluster, TopologyDetails topologyDetails) {\r\n            super.prepareForScheduling(cluster, topologyDetails);\r\n            // set a reversing execSorter instance\r\n            IExecSorter execSorter = new ExecSorterByConstraintSeverity(cluster, topologyDetails) {\r\n\r\n                @Override\r\n                public List<ExecutorDetails> sortExecutors(Set<ExecutorDetails> unassignedExecutors) {\r\n                    List<ExecutorDetails> tmp = super.sortExecutors(unassignedExecutors);\r\n                    List<ExecutorDetails> reversed = new ArrayList<>();\r\n                    while (!tmp.isEmpty()) {\r\n                        reversed.add(0, tmp.remove(0));\r\n                    }\r\n                    return reversed;\r\n                }\r\n            };\r\n            setExecSorter(execSorter);\r\n        }\r\n    };\r\n    basicUnitTestWithKillAndRecover(cs, BACKTRACK_BOLT_PARALLEL, CO_LOCATION_CNT, consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testConstraintSolver",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testConstraintSolver(boolean consolidatedConfigFlag) {\r\n    basicUnitTestWithKillAndRecover(new ConstraintSolverStrategy(), NORMAL_BOLT_PARALLEL, 1, consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testConstraintSolverWithSpreadCoLocation",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testConstraintSolverWithSpreadCoLocation(boolean consolidatedConfigFlag) {\r\n    if (CO_LOCATION_CNT > 1 && !consolidatedConfigFlag) {\r\n        LOG.info(\"INFO: Skipping Test {} with {}={} (required 1), and consolidatedConfigFlag={} (required false)\", \"testConstraintSolverWithSpreadCoLocation\", ConstraintSolverConfig.CONSTRAINT_TYPE_MAX_NODE_CO_LOCATION_CNT, CO_LOCATION_CNT, consolidatedConfigFlag);\r\n        return;\r\n    }\r\n    basicUnitTestWithKillAndRecover(new ConstraintSolverStrategy(), NORMAL_BOLT_PARALLEL, CO_LOCATION_CNT, consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testTooManyStateTransitions",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testTooManyStateTransitions(boolean consolidatedConfigFlag) {\r\n    basicFailureTest(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 10, new ConstraintSolverStrategy(), consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testTimeout",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testTimeout(boolean consolidatedConfigFlag) {\r\n    try (Time.SimulatedTime simulating = new Time.SimulatedTime()) {\r\n        ConstraintSolverStrategy cs = new ConstraintSolverStrategy() {\r\n\r\n            @Override\r\n            protected SchedulingResult scheduleExecutorsOnNodes(List<ExecutorDetails> orderedExecutors, Iterable<String> sortedNodes) {\r\n                //Each time we try to schedule a new component simulate taking 1 second longer\r\n                Time.advanceTime(1_001);\r\n                return super.scheduleExecutorsOnNodes(orderedExecutors, sortedNodes);\r\n            }\r\n        };\r\n        basicFailureTest(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_TIME_SECS, 1, cs, consolidatedConfigFlag);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testScheduleLargeExecutorConstraintCountSmall",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testScheduleLargeExecutorConstraintCountSmall(boolean consolidatedConfigFlag) {\r\n    testScheduleLargeExecutorConstraintCount(1, consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testScheduleLargeExecutorConstraintCountLarge",
  "sourceCode" : "/*\r\n     * Test scheduling large number of executors and constraints.\r\n     * This test can succeed only with new style config that allows maxCoLocationCnt = parallelismMultiplier.\r\n     * In prior code, this test would succeed because effectively the old code did not properly enforce the\r\n     * SPREAD constraint.\r\n     *\r\n     * Cluster has sufficient resources for scheduling to succeed but can fail due to StackOverflowError.\r\n     */\r\n@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testScheduleLargeExecutorConstraintCountLarge(boolean consolidatedConfigFlag) {\r\n    testScheduleLargeExecutorConstraintCount(20, consolidatedConfigFlag);\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testIntegrationWithRAS",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testIntegrationWithRAS(boolean consolidatedConfigFlag) {\r\n    if (!consolidatedConfigFlag) {\r\n        LOG.info(\"Skipping test since bolt-1 maxCoLocationCnt=10 requires consolidatedConfigFlag=true, current={}\", consolidatedConfigFlag);\r\n        return;\r\n    }\r\n    Map<String, Object> config = Utils.readDefaultConfig();\r\n    config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, ConstraintSolverStrategy.class.getName());\r\n    config.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, MAX_TRAVERSAL_DEPTH);\r\n    config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 100_000);\r\n    config.put(Config.TOPOLOGY_PRIORITY, 1);\r\n    config.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 10);\r\n    config.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 100);\r\n    config.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 0.0);\r\n    List<List<String>> constraints = new LinkedList<>();\r\n    addConstraints(\"spout-0\", \"bolt-0\", constraints);\r\n    addConstraints(\"bolt-1\", \"bolt-1\", constraints);\r\n    addConstraints(\"bolt-1\", \"bolt-2\", constraints);\r\n    Map<String, Integer> spreads = new HashMap<String, Integer>();\r\n    spreads.put(\"spout-0\", 1);\r\n    spreads.put(\"bolt-1\", 10);\r\n    setConstraintConfig(constraints, spreads, config, consolidatedConfigFlag);\r\n    TopologyDetails topo = genTopology(\"testTopo\", config, 2, 3, 30, 300, 0, 0, \"user\");\r\n    Map<String, TopologyDetails> topoMap = new HashMap<>();\r\n    topoMap.put(topo.getId(), topo);\r\n    Topologies topologies = new Topologies(topoMap);\r\n    // Fails with 36 supervisors, works with 37\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(37, 16, 400, 1024 * 4);\r\n    Cluster cluster = makeCluster(topologies, supMap);\r\n    ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n    rs.prepare(config, new StormMetricsRegistry());\r\n    try {\r\n        rs.schedule(topologies, cluster);\r\n        assertStatusSuccess(cluster, topo.getId());\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo).size(), \"topo all executors scheduled?\");\r\n    } finally {\r\n        rs.cleanup();\r\n    }\r\n    //simulate worker loss\r\n    Map<ExecutorDetails, WorkerSlot> newExecToSlot = new HashMap<>();\r\n    Map<ExecutorDetails, WorkerSlot> execToSlot = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();\r\n    Iterator<Map.Entry<ExecutorDetails, WorkerSlot>> it = execToSlot.entrySet().iterator();\r\n    for (int i = 0; i < execToSlot.size() / 2; i++) {\r\n        ExecutorDetails exec = it.next().getKey();\r\n        WorkerSlot ws = it.next().getValue();\r\n        newExecToSlot.put(exec, ws);\r\n    }\r\n    Map<String, SchedulerAssignment> newAssignments = new HashMap<>();\r\n    newAssignments.put(topo.getId(), new SchedulerAssignmentImpl(topo.getId(), newExecToSlot, null, null));\r\n    cluster.setAssignments(newAssignments, false);\r\n    rs.prepare(config, new StormMetricsRegistry());\r\n    try {\r\n        rs.schedule(topologies, cluster);\r\n        assertStatusSuccess(cluster, topo.getId());\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo).size(), \"topo all executors scheduled?\");\r\n    } finally {\r\n        rs.cleanup();\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testZeroExecutorScheduling",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testZeroExecutorScheduling(boolean consolidatedConfigFlag) {\r\n    ConstraintSolverStrategy cs = new ConstraintSolverStrategy();\r\n    cs.prepare(new HashMap<>());\r\n    Map<String, Object> topoConf = Utils.readDefaultConfig();\r\n    topoConf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 1_000);\r\n    topoConf.put(Config.TOPOLOGY_RAS_ONE_EXECUTOR_PER_WORKER, false);\r\n    topoConf.put(Config.TOPOLOGY_RAS_ONE_COMPONENT_PER_WORKER, false);\r\n    TopologyDetails topo = makeTopology(topoConf, 1);\r\n    Cluster cluster = makeCluster(new Topologies(topo));\r\n    cs.schedule(cluster, topo);\r\n    LOG.info(\"********************* Scheduling Zero Unassigned Executors *********************\");\r\n    // reschedule a fully schedule topology\r\n    cs.schedule(cluster, topo);\r\n    LOG.info(\"********************* End of Scheduling Zero Unassigned Executors *********************\");\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestConstraintSolverStrategy.java",
  "methodName" : "testGetMaxStateSearchFromTopoConf",
  "sourceCode" : "@ParameterizedTest\r\n@MethodSource(\"data\")\r\npublic void testGetMaxStateSearchFromTopoConf(boolean consolidatedConfigFlag) {\r\n    Map<String, Object> topoConf = new HashMap<>();\r\n    assertEquals(10_000, ConstraintSolverStrategy.getMaxStateSearchFromTopoConf(topoConf));\r\n    topoConf.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, 40_000);\r\n    assertEquals(40_000, ConstraintSolverStrategy.getMaxStateSearchFromTopoConf(topoConf));\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testMultipleSharedMemoryWithOneExecutorPerWorker",
  "sourceCode" : "/*\r\n     * test assigned memory with shared memory types and oneWorkerPerExecutor\r\n     */\r\n@ParameterizedTest\r\n@EnumSource(SharedMemoryType.class)\r\npublic void testMultipleSharedMemoryWithOneExecutorPerWorker(SharedMemoryType memoryType) {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 4;\r\n        double cpuPercent = 10;\r\n        double memoryOnHeap = 10;\r\n        double memoryOffHeap = 10;\r\n        double sharedOnHeapWithinWorker = 450;\r\n        double sharedOffHeapWithinNode = 600;\r\n        double sharedOffHeapWithinWorker = 400;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        switch(memoryType) {\r\n            case SHARED_OFF_HEAP_NODE:\r\n                builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addSharedMemory(new SharedOffHeapWithinNode(sharedOffHeapWithinNode, \"spout shared off heap within node\"));\r\n                break;\r\n            case SHARED_OFF_HEAP_WORKER:\r\n                builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addSharedMemory(new SharedOffHeapWithinWorker(sharedOffHeapWithinWorker, \"spout shared off heap within worker\"));\r\n                break;\r\n            case SHARED_ON_HEAP_WORKER:\r\n                builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addSharedMemory(new SharedOnHeap(sharedOnHeapWithinWorker, \"spout shared on heap within worker\"));\r\n                break;\r\n        }\r\n        StormTopology stormToplogy = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 500, 1000);\r\n        Config conf = createClusterConfig(strategyClass, cpuPercent, memoryOnHeap, memoryOffHeap, null);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        conf.put(Config.TOPOLOGY_RAS_ONE_EXECUTOR_PER_WORKER, true);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormToplogy, 0, genExecsAndComps(stormToplogy), CURRENT_TIME, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        TopologyResources topologyResources = cluster.getTopologyResourcesMap().get(topo.getId());\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(topo.getId());\r\n        long numNodes = assignment.getSlotToExecutors().keySet().stream().map(WorkerSlot::getNodeId).distinct().count();\r\n        switch(memoryType) {\r\n            case SHARED_OFF_HEAP_NODE:\r\n                // 4 workers on single node. OffHeapNode memory is shared\r\n                assertThat(topologyResources.getAssignedMemOnHeap(), closeTo(spoutParallelism * memoryOnHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedMemOffHeap(), closeTo(spoutParallelism * memoryOffHeap + sharedOffHeapWithinNode, 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOnHeap(), closeTo(0, 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOffHeap(), closeTo(sharedOffHeapWithinNode, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOnHeap(), closeTo(spoutParallelism * memoryOnHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOffHeap(), closeTo(spoutParallelism * memoryOffHeap, 0.01));\r\n                assertThat(numNodes, is(1L));\r\n                assertThat(cluster.getAssignedNumWorkers(topo), is(spoutParallelism));\r\n                break;\r\n            case SHARED_OFF_HEAP_WORKER:\r\n                // 4 workers on 2 nodes. OffHeapWorker memory not shared -- consumed 4x, once for each worker\r\n                assertThat(topologyResources.getAssignedMemOnHeap(), closeTo(spoutParallelism * memoryOnHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedMemOffHeap(), closeTo(spoutParallelism * (memoryOffHeap + sharedOffHeapWithinWorker), 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOnHeap(), closeTo(0, 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOffHeap(), closeTo(spoutParallelism * sharedOffHeapWithinWorker, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOnHeap(), closeTo(spoutParallelism * memoryOnHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOffHeap(), closeTo(spoutParallelism * memoryOffHeap, 0.01));\r\n                assertThat(numNodes, is(2L));\r\n                assertThat(cluster.getAssignedNumWorkers(topo), is(spoutParallelism));\r\n                break;\r\n            case SHARED_ON_HEAP_WORKER:\r\n                // 4 workers on 2 nodes. onHeap memory not shared -- consumed 4x, once for each worker\r\n                assertThat(topologyResources.getAssignedMemOnHeap(), closeTo(spoutParallelism * (memoryOnHeap + sharedOnHeapWithinWorker), 0.01));\r\n                assertThat(topologyResources.getAssignedMemOffHeap(), closeTo(spoutParallelism * memoryOffHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOnHeap(), closeTo(spoutParallelism * sharedOnHeapWithinWorker, 0.01));\r\n                assertThat(topologyResources.getAssignedSharedMemOffHeap(), closeTo(0, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOnHeap(), closeTo(spoutParallelism * memoryOnHeap, 0.01));\r\n                assertThat(topologyResources.getAssignedNonSharedMemOffHeap(), closeTo(spoutParallelism * memoryOffHeap, 0.01));\r\n                assertThat(numNodes, is(2L));\r\n                assertThat(cluster.getAssignedNumWorkers(topo), is(spoutParallelism));\r\n                break;\r\n        }\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testSchedulingNegativeResources",
  "sourceCode" : "/*\r\n     * test scheduling does not cause negative resources\r\n     */\r\n@Test\r\npublic void testSchedulingNegativeResources() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 2;\r\n        int boltParallelism = 2;\r\n        double cpuPercent = 10;\r\n        double memoryOnHeap = 10;\r\n        double memoryOffHeap = 10;\r\n        double sharedOnHeapWithinWorker = 400;\r\n        double sharedOffHeapWithinNode = 700;\r\n        double sharedOffHeapWithinWorker = 500;\r\n        Config conf = createClusterConfig(strategyClass, cpuPercent, memoryOnHeap, memoryOffHeap, null);\r\n        TopologyDetails[] topo = new TopologyDetails[2];\r\n        // 1st topology\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinWorker(sharedOffHeapWithinWorker, \"bolt-1 shared off heap within worker\")).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinNode(sharedOffHeapWithinNode, \"bolt-2 shared off heap within node\")).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOnHeap(sharedOnHeapWithinWorker, \"bolt-3 shared on heap within worker\")).shuffleGrouping(\"bolt-2\");\r\n        StormTopology stormTopology = builder.createTopology();\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 1);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology-0\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        topo[0] = new TopologyDetails(\"testTopology-id-0\", conf, stormTopology, 0, genExecsAndComps(stormTopology), CURRENT_TIME, \"user\");\r\n        // 2nd topology\r\n        builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addSharedMemory(new SharedOffHeapWithinNode(sharedOffHeapWithinNode, \"spout shared off heap within node\"));\r\n        stormTopology = builder.createTopology();\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology-1\");\r\n        topo[1] = new TopologyDetails(\"testTopology-id-1\", conf, stormTopology, 0, genExecsAndComps(stormTopology), CURRENT_TIME, \"user\");\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 4, 500, 2000);\r\n        Topologies topologies = new Topologies(topo[0]);\r\n        Cluster cluster = new Cluster(new INimbusTest(), new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        // schedule 1st topology\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, topo[0].getName());\r\n        // attempt scheduling both topologies.\r\n        // this triggered negative resource event as the second topology incorrectly scheduled with the first in place\r\n        // first topology should get evicted for higher priority (lower value) second topology to successfully schedule\r\n        topologies = new Topologies(topo[0], topo[1]);\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesNotScheduled(cluster, strategyClass, topo[0].getName());\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, topo[1].getName());\r\n        // check negative resource count\r\n        assertThat(cluster.getResourceMetrics().getNegativeResourceEventsMeter().getCount(), is(0L));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testDefaultResourceAwareStrategySharedMemory",
  "sourceCode" : "/**\r\n * test if the scheduling shared memory is correct with/without oneExecutorPerWorker enabled\r\n */\r\n@ParameterizedTest\r\n@EnumSource(WorkerRestrictionType.class)\r\npublic void testDefaultResourceAwareStrategySharedMemory(WorkerRestrictionType schedulingLimitation) {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 2;\r\n        int boltParallelism = 2;\r\n        int numBolts = 3;\r\n        double cpuPercent = 10;\r\n        double memoryOnHeap = 10;\r\n        double memoryOffHeap = 10;\r\n        double sharedOnHeapWithinWorker = 400;\r\n        double sharedOffHeapWithinNode = 700;\r\n        double sharedOffHeapWithinWorker = 600;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinWorker(sharedOffHeapWithinWorker, \"bolt-1 shared off heap within worker\")).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinNode(sharedOffHeapWithinNode, \"bolt-2 shared off heap within node\")).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOnHeap(sharedOnHeapWithinWorker, \"bolt-3 shared on heap within worker\")).shuffleGrouping(\"bolt-2\");\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 500, 2000);\r\n        Config conf = createClusterConfig(strategyClass, cpuPercent, memoryOnHeap, memoryOffHeap, null);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        switch(schedulingLimitation) {\r\n            case WORKER_RESTRICTION_ONE_EXECUTOR:\r\n                conf.put(Config.TOPOLOGY_RAS_ONE_EXECUTOR_PER_WORKER, true);\r\n                break;\r\n            case WORKER_RESTRICTION_ONE_COMPONENT:\r\n                conf.put(Config.TOPOLOGY_RAS_ONE_COMPONENT_PER_WORKER, true);\r\n                break;\r\n        }\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(stormTopology), CURRENT_TIME, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // [3,3] [7,7], [0,0] [2,2] [6,6] [1,1] [5,5] [4,4] sorted executor ordering\r\n        // spout  [0,0] [1,1]\r\n        // bolt-1 [2,2] [3,3]\r\n        // bolt-2 [6,6] [7,7]\r\n        // bolt-3 [4,4] [5,5]\r\n        // WorkerRestrictionType.WORKER_RESTRICTION_NONE\r\n        // expect 1 worker, 1 node\r\n        // WorkerRestrictionType.WORKER_RESTRICTION_ONE_EXECUTOR\r\n        // expect 8 workers, 2 nodes\r\n        // node r000s000 workers: bolt-1 bolt-2 spout bolt-1 (no memory sharing)\r\n        // node r000s001 workers: bolt-2 spout bolt-3 bolt-3 (no memory sharing)\r\n        // WorkerRestrictionType.WORKER_RESTRICTION_ONE_COMPONENT\r\n        // expect 4 workers, 1 node\r\n        for (Entry<String, SupervisorResources> entry : cluster.getSupervisorsResourcesMap().entrySet()) {\r\n            String supervisorId = entry.getKey();\r\n            SupervisorResources resources = entry.getValue();\r\n            assertTrue(resources.getTotalCpu() >= resources.getUsedCpu(), supervisorId);\r\n            assertTrue(resources.getTotalMem() >= resources.getUsedMem(), supervisorId);\r\n        }\r\n        int totalNumberOfTasks = spoutParallelism + boltParallelism * numBolts;\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(topo.getId());\r\n        TopologyResources topologyResources = cluster.getTopologyResourcesMap().get(topo.getId());\r\n        long numNodes = assignment.getSlotToExecutors().keySet().stream().map(WorkerSlot::getNodeId).distinct().count();\r\n        String assignmentString = \"Assignments:\\n\\t\" + assignment.getSlotToExecutors().entrySet().stream().map(x -> String.format(\"Node=%s, components=%s\", x.getKey().getNodeId(), x.getValue().stream().map(topo::getComponentFromExecutor).collect(Collectors.joining(\",\")))).collect(Collectors.joining(\"\\n\\t\"));\r\n        if (schedulingLimitation == WorkerRestrictionType.WORKER_RESTRICTION_NONE) {\r\n            // Everything should fit in a single slot\r\n            double totalExpectedCPU = totalNumberOfTasks * cpuPercent;\r\n            double totalExpectedOnHeap = (totalNumberOfTasks * memoryOnHeap) + sharedOnHeapWithinWorker;\r\n            double totalExpectedWorkerOffHeap = (totalNumberOfTasks * memoryOffHeap) + sharedOffHeapWithinWorker;\r\n            assertThat(assignment.getSlots().size(), is(1));\r\n            WorkerSlot ws = assignment.getSlots().iterator().next();\r\n            String nodeId = ws.getNodeId();\r\n            assertThat(assignment.getNodeIdToTotalSharedOffHeapNodeMemory().size(), is(1));\r\n            assertThat(assignment.getNodeIdToTotalSharedOffHeapNodeMemory().get(nodeId), closeTo(sharedOffHeapWithinNode, 0.01));\r\n            assertThat(assignment.getScheduledResources().size(), is(1));\r\n            WorkerResources resources = assignment.getScheduledResources().get(ws);\r\n            assertThat(resources.get_cpu(), closeTo(totalExpectedCPU, 0.01));\r\n            assertThat(resources.get_mem_on_heap(), closeTo(totalExpectedOnHeap, 0.01));\r\n            assertThat(resources.get_mem_off_heap(), closeTo(totalExpectedWorkerOffHeap, 0.01));\r\n            assertThat(resources.get_shared_mem_on_heap(), closeTo(sharedOnHeapWithinWorker, 0.01));\r\n            assertThat(resources.get_shared_mem_off_heap(), closeTo(sharedOffHeapWithinWorker, 0.01));\r\n        } else if (schedulingLimitation == WorkerRestrictionType.WORKER_RESTRICTION_ONE_EXECUTOR) {\r\n            double expectedMemOnHeap = (totalNumberOfTasks * memoryOnHeap) + 2 * sharedOnHeapWithinWorker;\r\n            double expectedMemOffHeap = (totalNumberOfTasks * memoryOffHeap) + 2 * sharedOffHeapWithinWorker + 2 * sharedOffHeapWithinNode;\r\n            double expectedMemSharedOnHeap = 2 * sharedOnHeapWithinWorker;\r\n            double expectedMemSharedOffHeap = 2 * sharedOffHeapWithinWorker + 2 * sharedOffHeapWithinNode;\r\n            double expectedMemNonSharedOnHeap = totalNumberOfTasks * memoryOnHeap;\r\n            double expectedMemNonSharedOffHeap = totalNumberOfTasks * memoryOffHeap;\r\n            assertThat(topologyResources.getAssignedMemOnHeap(), closeTo(expectedMemOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedMemOffHeap(), closeTo(expectedMemOffHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedSharedMemOnHeap(), closeTo(expectedMemSharedOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedSharedMemOffHeap(), closeTo(expectedMemSharedOffHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedNonSharedMemOnHeap(), closeTo(expectedMemNonSharedOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedNonSharedMemOffHeap(), closeTo(expectedMemNonSharedOffHeap, 0.01));\r\n            double totalExpectedCPU = totalNumberOfTasks * cpuPercent;\r\n            assertThat(topologyResources.getAssignedCpu(), closeTo(totalExpectedCPU, 0.01));\r\n            int numAssignedWorkers = cluster.getAssignedNumWorkers(topo);\r\n            assertThat(numAssignedWorkers, is(8));\r\n            assertThat(assignment.getSlots().size(), is(8));\r\n            assertThat(assignmentString, numNodes, is(2L));\r\n        } else if (schedulingLimitation == WorkerRestrictionType.WORKER_RESTRICTION_ONE_COMPONENT) {\r\n            double expectedMemOnHeap = (totalNumberOfTasks * memoryOnHeap) + sharedOnHeapWithinWorker;\r\n            double expectedMemOffHeap = (totalNumberOfTasks * memoryOffHeap) + sharedOffHeapWithinWorker + sharedOffHeapWithinNode;\r\n            double expectedMemSharedOnHeap = sharedOnHeapWithinWorker;\r\n            double expectedMemSharedOffHeap = sharedOffHeapWithinWorker + sharedOffHeapWithinNode;\r\n            double expectedMemNonSharedOnHeap = totalNumberOfTasks * memoryOnHeap;\r\n            double expectedMemNonSharedOffHeap = totalNumberOfTasks * memoryOffHeap;\r\n            assertThat(topologyResources.getAssignedMemOnHeap(), closeTo(expectedMemOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedMemOffHeap(), closeTo(expectedMemOffHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedSharedMemOnHeap(), closeTo(expectedMemSharedOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedSharedMemOffHeap(), closeTo(expectedMemSharedOffHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedNonSharedMemOnHeap(), closeTo(expectedMemNonSharedOnHeap, 0.01));\r\n            assertThat(topologyResources.getAssignedNonSharedMemOffHeap(), closeTo(expectedMemNonSharedOffHeap, 0.01));\r\n            double totalExpectedCPU = totalNumberOfTasks * cpuPercent;\r\n            assertThat(topologyResources.getAssignedCpu(), closeTo(totalExpectedCPU, 0.01));\r\n            int numAssignedWorkers = cluster.getAssignedNumWorkers(topo);\r\n            assertThat(numAssignedWorkers, is(4));\r\n            assertThat(assignment.getSlots().size(), is(4));\r\n            assertThat(numNodes, is(1L));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testDefaultResourceAwareStrategyWithoutSettingAckerExecutors",
  "sourceCode" : "/**\r\n * test if the scheduling logic for the DefaultResourceAwareStrategy is correct\r\n * when topology.acker.executors.per.worker is set to different values.\r\n *\r\n * If {@link Config#TOPOLOGY_ACKER_EXECUTORS} is not set,\r\n * it will be calculated by Nimbus as (num of estimated worker * topology.acker.executors.per.worker).\r\n * In this test, {@link Config#TOPOLOGY_ACKER_EXECUTORS} is set to 2 (num of estimated workers based on topo resources usage)\r\n *\r\n * For different value for {@link Config#TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER}:\r\n * -1: Note we don't really set it to be -1.\r\n *     It is just a special case in this test that topology.acker.executors.per.worker is unset, nimbus will set to 1 by default.\r\n * 0:  Since {@link Config#TOPOLOGY_ACKER_EXECUTORS} is not set either, acking is disabled.\r\n * 1:  2 ackers in total. Distribute 1 acker per worker. With ackers being added, this topology will now need 3 workers.\r\n *     Then first two worker will get 1 acker and last worker get 0.\r\n * 2:  4 ackers in total. First two workers will get 2 acker per worker respectively.\r\n */\r\n@ParameterizedTest\r\n@ValueSource(ints = { -1, 0, 1, 2 })\r\npublic void testDefaultResourceAwareStrategyWithoutSettingAckerExecutors(int numOfAckersPerWorker) throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\");\r\n        String topoName = \"testTopology\";\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000);\r\n        Config conf = createClusterConfig(strategyClass, 50, 450, 0, null);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, topoName);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        // Topology needs 2 workers (estimated by nimbus based on resources),\r\n        // but with ackers added, probably more worker will be launched.\r\n        // Parameterized test on different numOfAckersPerWorker\r\n        if (numOfAckersPerWorker == -1) {\r\n            // Both Config.TOPOLOGY_ACKER_EXECUTORS and Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER are not set\r\n            // Default will be 2 (estimate num of workers) and 1 respectively\r\n        } else {\r\n            conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numOfAckersPerWorker);\r\n        }\r\n        int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRasTopo(conf, stormTopology);\r\n        Nimbus.setUpAckerExecutorConfigs(topoName, conf, conf, estimatedNumWorker);\r\n        conf.put(Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB, 250);\r\n        conf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 50);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormTopology)), CURRENT_TIME, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // Ordered execs: [[6, 6], [2, 2], [4, 4], [5, 5], [1, 1], [3, 3], [0, 0], [8, 8], [7, 7]]\r\n        // Ackers: [[8, 8], [7, 7]] (+ [[9, 9], [10, 10]] when numOfAckersPerWorker=2)\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        if (numOfAckersPerWorker == -1 || numOfAckersPerWorker == 1) {\r\n            // Setting topology.acker.executors = null and topology.acker.executors.per.worker = null\r\n            // are equivalent to topology.acker.executors = null and topology.acker.executors.per.worker = 1\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n            new ExecutorDetails(6, 6), //bolt-1\r\n            new ExecutorDetails(2, 2), //bolt-2\r\n            new ExecutorDetails(4, 4), //acker\r\n            new ExecutorDetails(8, 8))));\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n            new ExecutorDetails(5, 5), //bolt-1\r\n            new ExecutorDetails(1, 1), //bolt-2\r\n            new ExecutorDetails(3, 3), //acker\r\n            new ExecutorDetails(7, 7))));\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//spout\r\n            new ExecutorDetails(0, 0))));\r\n        } else if (numOfAckersPerWorker == 0) {\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n            new ExecutorDetails(6, 6), //bolt-1\r\n            new ExecutorDetails(2, 2), //bolt-2\r\n            new ExecutorDetails(4, 4), //bolt-3\r\n            new ExecutorDetails(5, 5))));\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//spout\r\n            new ExecutorDetails(0, 0), //bolt-2\r\n            new ExecutorDetails(3, 3), //bolt-1\r\n            new ExecutorDetails(1, 1))));\r\n        } else if (numOfAckersPerWorker == 2) {\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n            new ExecutorDetails(6, 6), //bolt-1\r\n            new ExecutorDetails(2, 2), //acker\r\n            new ExecutorDetails(7, 7), //acker\r\n            new ExecutorDetails(8, 8))));\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-2\r\n            new ExecutorDetails(4, 4), //bolt-3\r\n            new ExecutorDetails(5, 5), //acker\r\n            new ExecutorDetails(9, 9), //acker\r\n            new ExecutorDetails(10, 10))));\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-1\r\n            new ExecutorDetails(1, 1), //bolt-2\r\n            new ExecutorDetails(3, 3), //spout\r\n            new ExecutorDetails(0, 0))));\r\n        }\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testDefaultResourceAwareStrategyWithSettingAckerExecutors",
  "sourceCode" : "/**\r\n * test if the scheduling logic for the DefaultResourceAwareStrategy is correct\r\n * when topology.acker.executors is set.\r\n *\r\n * If yes, topology.acker.executors.per.worker setting will be ignored and calculated as\r\n * Math.ceil(topology.acker.executors / estimate num of workers) by Nimbus\r\n */\r\n@ParameterizedTest\r\n@ValueSource(ints = { -1, 0, 2, 300 })\r\npublic void testDefaultResourceAwareStrategyWithSettingAckerExecutors(int numOfAckersPerWorker) throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\");\r\n        String topoName = \"testTopology\";\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000);\r\n        Config conf = createClusterConfig(strategyClass, 50, 450, 0, null);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, topoName);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, 4);\r\n        conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numOfAckersPerWorker);\r\n        if (numOfAckersPerWorker == -1) {\r\n            // Leave topology.acker.executors.per.worker unset\r\n        } else {\r\n            conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numOfAckersPerWorker);\r\n        }\r\n        int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRasTopo(conf, stormTopology);\r\n        Nimbus.setUpAckerExecutorConfigs(topoName, conf, conf, estimatedNumWorker);\r\n        conf.put(Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB, 250);\r\n        conf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 50);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormTopology)), CURRENT_TIME, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // Sorted execs: [[6, 6], [2, 2], [4, 4], [5, 5], [1, 1], [3, 3], [0, 0], [8, 8], [7, 7], [10, 10], [9, 9]]\r\n        // Ackers: [[8, 8], [7, 7], [10, 10], [9, 9]]\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n        new ExecutorDetails(6, 6), //bolt-1\r\n        new ExecutorDetails(2, 2), //acker\r\n        new ExecutorDetails(7, 7), //acker\r\n        new ExecutorDetails(8, 8))));\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n        new ExecutorDetails(5, 5), //bolt-2\r\n        new ExecutorDetails(4, 4), //acker\r\n        new ExecutorDetails(9, 9), //acker\r\n        new ExecutorDetails(10, 10))));\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//spout\r\n        new ExecutorDetails(0, 0), //bolt-2\r\n        new ExecutorDetails(3, 3), //bolt-1\r\n        new ExecutorDetails(1, 1))));\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testDefaultResourceAwareStrategyInFavorOfShuffle",
  "sourceCode" : "/**\r\n * test if the scheduling logic for the DefaultResourceAwareStrategy (when made by network proximity needs.) is correct\r\n */\r\n@Test\r\npublic void testDefaultResourceAwareStrategyInFavorOfShuffle() throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\");\r\n        StormTopology stormToplogy = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000);\r\n        Config conf = createClusterConfig(strategyClass, 50, 250, 250, null);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        conf.put(Config.TOPOLOGY_RAS_ORDER_EXECUTORS_BY_PROXIMITY_NEEDS, true);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormToplogy, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormToplogy)), CURRENT_TIME, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n        rs.prepare(conf, new StormMetricsRegistry());\r\n        rs.schedule(topologies, cluster);\r\n        // Sorted execs: [[0, 0], [2, 2], [6, 6], [4, 4], [1, 1], [5, 5], [3, 3], [7, 7]]\r\n        // Ackers: [[7, 7]]]\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//spout\r\n        new ExecutorDetails(0, 0), //bolt-2\r\n        new ExecutorDetails(6, 6), //bolt-1\r\n        new ExecutorDetails(2, 2), //acker\r\n        new ExecutorDetails(7, 7))));\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n        new ExecutorDetails(3, 3), //bolt-2\r\n        new ExecutorDetails(5, 5), //bolt-3\r\n        new ExecutorDetails(4, 4), //bolt-1\r\n        new ExecutorDetails(1, 1))));\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testMultipleRacks",
  "sourceCode" : "/**\r\n * Test whether strategy will choose correct rack\r\n */\r\n@Test\r\npublic void testMultipleRacks() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n        final Map<String, SupervisorDetails> supMapRack0 = genSupervisors(10, 4, 0, 400, 8000);\r\n        //generate another rack of supervisors with less resources\r\n        final Map<String, SupervisorDetails> supMapRack1 = genSupervisors(10, 4, 10, 200, 4000);\r\n        //generate some supervisors that are depleted of one resource\r\n        final Map<String, SupervisorDetails> supMapRack2 = genSupervisors(10, 4, 20, 0, 8000);\r\n        //generate some that has alot of memory but little of cpu\r\n        final Map<String, SupervisorDetails> supMapRack3 = genSupervisors(10, 4, 30, 10, 8000 * 2 + 4000);\r\n        //generate some that has alot of cpu but little of memory\r\n        final Map<String, SupervisorDetails> supMapRack4 = genSupervisors(10, 4, 40, 400 + 200 + 10, 1000);\r\n        //Generate some that have neither resource, to verify that the strategy will prioritize this last\r\n        //Also put a generic resource with 0 value in the resources list, to verify that it doesn't affect the sorting\r\n        final Map<String, SupervisorDetails> supMapRack5 = genSupervisors(10, 4, 50, 0.0, 0.0, Collections.singletonMap(\"gpu.count\", 0.0));\r\n        supMap.putAll(supMapRack0);\r\n        supMap.putAll(supMapRack1);\r\n        supMap.putAll(supMapRack2);\r\n        supMap.putAll(supMapRack3);\r\n        supMap.putAll(supMapRack4);\r\n        supMap.putAll(supMapRack5);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n        INimbus iNimbus = new INimbusTest();\r\n        //create test DNSToSwitchMapping plugin\r\n        DNSToSwitchMapping TestNetworkTopographyPlugin = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4, supMapRack5);\r\n        //generate topologies\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n        TopologyDetails topo2 = genTopology(\"topo-2\", config, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n        Topologies topologies = new Topologies(topo1, topo2);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        List<String> supHostnames = new LinkedList<>();\r\n        for (SupervisorDetails sup : supMap.values()) {\r\n            supHostnames.add(sup.getHost());\r\n        }\r\n        Map<String, List<String>> rackToNodes = new HashMap<>();\r\n        Map<String, String> resolvedSuperVisors = TestNetworkTopographyPlugin.resolve(supHostnames);\r\n        for (Map.Entry<String, String> entry : resolvedSuperVisors.entrySet()) {\r\n            String hostName = entry.getKey();\r\n            String rack = entry.getValue();\r\n            rackToNodes.computeIfAbsent(rack, rid -> new ArrayList<>()).add(hostName);\r\n        }\r\n        cluster.setNetworkTopography(rackToNodes);\r\n        DefaultResourceAwareStrategyOld rs = new DefaultResourceAwareStrategyOld();\r\n        rs.prepareForScheduling(cluster, topo1);\r\n        INodeSorter nodeSorter = new NodeSorterHostProximity(cluster, topo1, BaseResourceAwareStrategy.NodeSortType.DEFAULT_RAS);\r\n        nodeSorter.prepare(null);\r\n        Iterable<ObjectResourcesItem> sortedRacks = nodeSorter.getSortedRacks();\r\n        Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n        // Ranked first since rack-0 has the most balanced set of resources\r\n        assertEquals(\"rack-0\", it.next().id, \"rack-0 should be ordered first\");\r\n        // Ranked second since rack-1 has a balanced set of resources but less than rack-0\r\n        assertEquals(\"rack-1\", it.next().id, \"rack-1 should be ordered second\");\r\n        // Ranked third since rack-4 has a lot of cpu but not a lot of memory\r\n        assertEquals(\"rack-4\", it.next().id, \"rack-4 should be ordered third\");\r\n        // Ranked fourth since rack-3 has alot of memory but not cpu\r\n        assertEquals(\"rack-3\", it.next().id, \"rack-3 should be ordered fourth\");\r\n        //Ranked fifth since rack-2 has not cpu resources\r\n        assertEquals(\"rack-2\", it.next().id, \"rack-2 should be ordered fifth\");\r\n        //Ranked last since rack-5 has neither CPU nor memory available\r\n        assertEquals(\"rack-5\", it.next().id, \"Rack-5 should be ordered sixth\");\r\n        SchedulingResult schedulingResult = rs.schedule(cluster, topo1);\r\n        assertTrue(schedulingResult.isSuccess(), \"Scheduling failed\");\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(topo1.getId());\r\n        for (WorkerSlot ws : assignment.getSlotToExecutors().keySet()) {\r\n            //make sure all workers on scheduled in rack-0\r\n            assertEquals(\"rack-0\", resolvedSuperVisors.get(rs.idToNode(ws.getNodeId()).getHostname()), \"assert worker scheduled on rack-0\");\r\n        }\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo1).size(), \"All executors in topo-1 scheduled\");\r\n        //Test if topology is already partially scheduled on one rack\r\n        Iterator<ExecutorDetails> executorIterator = topo2.getExecutors().iterator();\r\n        List<String> nodeHostnames = rackToNodes.get(\"rack-1\");\r\n        for (int i = 0; i < topo2.getExecutors().size() / 2; i++) {\r\n            String nodeHostname = nodeHostnames.get(i % nodeHostnames.size());\r\n            RasNode node = rs.hostnameToNodes(nodeHostname).get(0);\r\n            WorkerSlot targetSlot = node.getFreeSlots().iterator().next();\r\n            ExecutorDetails targetExec = executorIterator.next();\r\n            // to keep track of free slots\r\n            node.assign(targetSlot, topo2, Collections.singletonList(targetExec));\r\n        }\r\n        rs = new DefaultResourceAwareStrategyOld();\r\n        // schedule topo2\r\n        schedulingResult = rs.schedule(cluster, topo2);\r\n        assertTrue(schedulingResult.isSuccess(), \"Scheduling failed\");\r\n        assignment = cluster.getAssignmentById(topo2.getId());\r\n        for (WorkerSlot ws : assignment.getSlotToExecutors().keySet()) {\r\n            //make sure all workers on scheduled in rack-1\r\n            assertEquals(\"rack-1\", resolvedSuperVisors.get(rs.idToNode(ws.getNodeId()).getHostname()), \"assert worker scheduled on rack-1\");\r\n        }\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo2).size(), \"All executors in topo-2 scheduled\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestDefaultResourceAwareStrategy.java",
  "methodName" : "testMultipleRacksWithFavoritism",
  "sourceCode" : "/**\r\n * Test whether strategy will choose correct rack\r\n */\r\n@Test\r\npublic void testMultipleRacksWithFavoritism() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        final Map<String, SupervisorDetails> supMap = new HashMap<>();\r\n        final Map<String, SupervisorDetails> supMapRack0 = genSupervisors(10, 4, 0, 400, 8000);\r\n        //generate another rack of supervisors with less resources\r\n        final Map<String, SupervisorDetails> supMapRack1 = genSupervisors(10, 4, 10, 200, 4000);\r\n        //generate some supervisors that are depleted of one resource\r\n        final Map<String, SupervisorDetails> supMapRack2 = genSupervisors(10, 4, 20, 0, 8000);\r\n        //generate some that has alot of memory but little of cpu\r\n        final Map<String, SupervisorDetails> supMapRack3 = genSupervisors(10, 4, 30, 10, 8000 * 2 + 4000);\r\n        //generate some that has alot of cpu but little of memory\r\n        final Map<String, SupervisorDetails> supMapRack4 = genSupervisors(10, 4, 40, 400 + 200 + 10, 1000);\r\n        supMap.putAll(supMapRack0);\r\n        supMap.putAll(supMapRack1);\r\n        supMap.putAll(supMapRack2);\r\n        supMap.putAll(supMapRack3);\r\n        supMap.putAll(supMapRack4);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        config.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n        INimbus iNimbus = new INimbusTest();\r\n        //create test DNSToSwitchMapping plugin\r\n        DNSToSwitchMapping TestNetworkTopographyPlugin = new TestDNSToSwitchMapping(supMapRack0, supMapRack1, supMapRack2, supMapRack3, supMapRack4);\r\n        Config t1Conf = new Config();\r\n        t1Conf.putAll(config);\r\n        final List<String> t1FavoredHostNames = Arrays.asList(\"host-41\", \"host-42\", \"host-43\");\r\n        t1Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, t1FavoredHostNames);\r\n        final List<String> t1UnfavoredHostIds = Arrays.asList(\"host-1\", \"host-2\", \"host-3\");\r\n        t1Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, t1UnfavoredHostIds);\r\n        //generate topologies\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", t1Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n        Config t2Conf = new Config();\r\n        t2Conf.putAll(config);\r\n        t2Conf.put(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES, Arrays.asList(\"host-31\", \"host-32\", \"host-33\"));\r\n        t2Conf.put(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES, Arrays.asList(\"host-11\", \"host-12\", \"host-13\"));\r\n        TopologyDetails topo2 = genTopology(\"topo-2\", t2Conf, 8, 0, 2, 0, CURRENT_TIME - 2, 10, \"user\");\r\n        Topologies topologies = new Topologies(topo1, topo2);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        List<String> supHostnames = new LinkedList<>();\r\n        for (SupervisorDetails sup : supMap.values()) {\r\n            supHostnames.add(sup.getHost());\r\n        }\r\n        Map<String, List<String>> rackToNodes = new HashMap<>();\r\n        Map<String, String> resolvedSuperVisors = TestNetworkTopographyPlugin.resolve(supHostnames);\r\n        for (Map.Entry<String, String> entry : resolvedSuperVisors.entrySet()) {\r\n            String hostName = entry.getKey();\r\n            String rack = entry.getValue();\r\n            List<String> nodesForRack = rackToNodes.get(rack);\r\n            if (nodesForRack == null) {\r\n                nodesForRack = new ArrayList<>();\r\n                rackToNodes.put(rack, nodesForRack);\r\n            }\r\n            nodesForRack.add(hostName);\r\n        }\r\n        cluster.setNetworkTopography(rackToNodes);\r\n        DefaultResourceAwareStrategyOld rs = new DefaultResourceAwareStrategyOld();\r\n        rs.prepareForScheduling(cluster, topo1);\r\n        INodeSorter nodeSorter = new NodeSorterHostProximity(cluster, topo1, BaseResourceAwareStrategy.NodeSortType.DEFAULT_RAS);\r\n        nodeSorter.prepare(null);\r\n        Iterable<ObjectResourcesItem> sortedRacks = nodeSorter.getSortedRacks();\r\n        Iterator<ObjectResourcesItem> it = sortedRacks.iterator();\r\n        // Ranked first since rack-0 has the most balanced set of resources\r\n        assertEquals(\"rack-0\", it.next().id, \"rack-0 should be ordered first\");\r\n        // Ranked second since rack-1 has a balanced set of resources but less than rack-0\r\n        assertEquals(\"rack-1\", it.next().id, \"rack-1 should be ordered second\");\r\n        // Ranked third since rack-4 has a lot of cpu but not a lot of memory\r\n        assertEquals(\"rack-4\", it.next().id, \"rack-4 should be ordered third\");\r\n        // Ranked fourth since rack-3 has alot of memory but not cpu\r\n        assertEquals(\"rack-3\", it.next().id, \"rack-3 should be ordered fourth\");\r\n        //Ranked last since rack-2 has not cpu resources\r\n        assertEquals(\"rack-2\", it.next().id, \"rack-2 should be ordered fifth\");\r\n        SchedulingResult schedulingResult = rs.schedule(cluster, topo1);\r\n        assertTrue(schedulingResult.isSuccess(), \"Scheduling failed\");\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(topo1.getId());\r\n        for (WorkerSlot ws : assignment.getSlotToExecutors().keySet()) {\r\n            String hostName = rs.idToNode(ws.getNodeId()).getHostname();\r\n            String rackId = resolvedSuperVisors.get(hostName);\r\n            assertTrue(t1FavoredHostNames.contains(hostName) || \"rack-0\".equals(rackId), ws + \" is neither on a favored node \" + t1FavoredHostNames + \" nor the highest priority rack (rack-0)\");\r\n            assertFalse(t1UnfavoredHostIds.contains(hostName), ws + \" is a part of an unfavored node \" + t1UnfavoredHostIds);\r\n        }\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo1).size(), \"All executors in topo-1 scheduled\");\r\n        //Test if topology is already partially scheduled on one rack\r\n        Iterator<ExecutorDetails> executorIterator = topo2.getExecutors().iterator();\r\n        List<String> nodeHostnames = rackToNodes.get(\"rack-1\");\r\n        for (int i = 0; i < topo2.getExecutors().size() / 2; i++) {\r\n            String nodeHostname = nodeHostnames.get(i % nodeHostnames.size());\r\n            RasNode node = rs.hostnameToNodes(nodeHostname).get(0);\r\n            WorkerSlot targetSlot = node.getFreeSlots().iterator().next();\r\n            ExecutorDetails targetExec = executorIterator.next();\r\n            // to keep track of free slots\r\n            node.assign(targetSlot, topo2, Collections.singletonList(targetExec));\r\n        }\r\n        rs = new DefaultResourceAwareStrategyOld();\r\n        // schedule topo2\r\n        schedulingResult = rs.schedule(cluster, topo2);\r\n        assertTrue(schedulingResult.isSuccess(), \"Scheduling failed\");\r\n        assignment = cluster.getAssignmentById(topo2.getId());\r\n        for (WorkerSlot ws : assignment.getSlotToExecutors().keySet()) {\r\n            //make sure all workers on scheduled in rack-1\r\n            // The favored nodes would have put it on a different rack, but because that rack does not have free space to run the\r\n            // topology it falls back to this rack\r\n            assertEquals(\"rack-1\", resolvedSuperVisors.get(rs.idToNode(ws.getNodeId()).getHostname()), \"assert worker scheduled on rack-1\");\r\n        }\r\n        assertEquals(0, cluster.getUnassignedExecutors(topo2).size(), \"All executors in topo-2 scheduled\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testGenericResourceAwareStrategySharedMemory",
  "sourceCode" : "/**\r\n * test if the scheduling logic for the GenericResourceAwareStrategy is correct.\r\n */\r\n@Test\r\npublic void testGenericResourceAwareStrategySharedMemory() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 2;\r\n        int boltParallelism = 2;\r\n        int numBolts = 3;\r\n        double cpuPercent = 10;\r\n        double memoryOnHeap = 10;\r\n        double memoryOffHeap = 10;\r\n        double sharedOnHeap = 500;\r\n        double sharedOffHeapNode = 700;\r\n        double sharedOffHeapWorker = 500;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addResource(\"gpu.count\", 1.0);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinWorker(sharedOffHeapWorker, \"bolt-1 shared off heap worker\")).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOffHeapWithinNode(sharedOffHeapNode, \"bolt-2 shared node\")).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).addSharedMemory(new SharedOnHeap(sharedOnHeap, \"bolt-3 shared worker\")).shuffleGrouping(\"bolt-2\");\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Config conf = createGrasClusterConfig(strategyClass, cpuPercent, memoryOnHeap, memoryOffHeap, null, Collections.emptyMap());\r\n        Map<String, Double> genericResourcesMap = new HashMap<>();\r\n        genericResourcesMap.put(\"gpu.count\", 1.0);\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 500, 2000, genericResourcesMap);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(stormTopology), currentTime, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        for (Entry<String, SupervisorResources> entry : cluster.getSupervisorsResourcesMap().entrySet()) {\r\n            String supervisorId = entry.getKey();\r\n            SupervisorResources resources = entry.getValue();\r\n            assertTrue(resources.getTotalCpu() >= resources.getUsedCpu(), supervisorId);\r\n            assertTrue(resources.getTotalMem() >= resources.getUsedMem(), supervisorId);\r\n        }\r\n        // If we didn't take GPUs into account everything would fit under a single slot\r\n        // But because there is only 1 GPU per node, and each of the 2 spouts needs a GPU\r\n        // It has to be scheduled on at least 2 nodes, and hence 2 slots.\r\n        // Because of this, all the bolts will be scheduled on a single slot with one of\r\n        // the spouts and the other spout is on its own slot.  So everything that can be shared is\r\n        // shared.\r\n        int totalNumberOfTasks = (spoutParallelism + (boltParallelism * numBolts));\r\n        double totalExpectedCPU = totalNumberOfTasks * cpuPercent;\r\n        double totalExpectedOnHeap = (totalNumberOfTasks * memoryOnHeap) + sharedOnHeap;\r\n        double totalExpectedWorkerOffHeap = (totalNumberOfTasks * memoryOffHeap) + sharedOffHeapWorker;\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(topo.getId());\r\n        Set<WorkerSlot> slots = assignment.getSlots();\r\n        Map<String, Double> nodeToTotalShared = assignment.getNodeIdToTotalSharedOffHeapNodeMemory();\r\n        LOG.info(\"NODE TO SHARED OFF HEAP {}\", nodeToTotalShared);\r\n        Map<WorkerSlot, WorkerResources> scheduledResources = assignment.getScheduledResources();\r\n        assertEquals(2, slots.size());\r\n        assertEquals(2, nodeToTotalShared.size());\r\n        assertEquals(2, scheduledResources.size());\r\n        double totalFoundCPU = 0.0;\r\n        double totalFoundOnHeap = 0.0;\r\n        double totalFoundWorkerOffHeap = 0.0;\r\n        for (WorkerSlot ws : slots) {\r\n            WorkerResources resources = scheduledResources.get(ws);\r\n            totalFoundCPU += resources.get_cpu();\r\n            totalFoundOnHeap += resources.get_mem_on_heap();\r\n            totalFoundWorkerOffHeap += resources.get_mem_off_heap();\r\n        }\r\n        assertEquals(totalExpectedCPU, totalFoundCPU, 0.01);\r\n        assertEquals(totalExpectedOnHeap, totalFoundOnHeap, 0.01);\r\n        assertEquals(totalExpectedWorkerOffHeap, totalFoundWorkerOffHeap, 0.01);\r\n        assertEquals(sharedOffHeapNode, nodeToTotalShared.values().stream().mapToDouble((d) -> d).sum(), 0.01);\r\n        assertEquals(sharedOnHeap, scheduledResources.values().stream().mapToDouble(WorkerResources::get_shared_mem_on_heap).sum(), 0.01);\r\n        assertEquals(sharedOffHeapWorker, scheduledResources.values().stream().mapToDouble(WorkerResources::get_shared_mem_off_heap).sum(), 0.01);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testGenericResourceAwareStrategyWithoutSettingAckerExecutors",
  "sourceCode" : "/**\r\n * Test if the scheduling logic for the GenericResourceAwareStrategy is correct\r\n * without setting {@link Config#TOPOLOGY_ACKER_EXECUTORS}.\r\n *\r\n * Test details refer to {@link TestDefaultResourceAwareStrategy#testDefaultResourceAwareStrategyWithoutSettingAckerExecutors(int)}\r\n */\r\n@ParameterizedTest\r\n@ValueSource(ints = { -1, 0, 1, 2 })\r\npublic void testGenericResourceAwareStrategyWithoutSettingAckerExecutors(int numOfAckersPerWorker) throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\").addResource(\"gpu.count\", 1.0);\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\").addResource(\"gpu.count\", 2.0);\r\n        String topoName = \"testTopology\";\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Config conf = createGrasClusterConfig(strategyClass, 50, 500, 0, null, Collections.emptyMap());\r\n        Map<String, Double> genericResourcesMap = new HashMap<>();\r\n        genericResourcesMap.put(\"gpu.count\", 2.0);\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000, genericResourcesMap);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, topoName);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        // Topology needs 2 workers (estimated by nimbus based on resources),\r\n        // but with ackers added, probably more worker will be launched.\r\n        // Parameterized test on different numOfAckersPerWorker\r\n        if (numOfAckersPerWorker == -1) {\r\n            // Both Config.TOPOLOGY_ACKER_EXECUTORS and Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER are not set\r\n            // Default will be 2 (estimate num of workers) and 1 respectively\r\n        } else {\r\n            conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numOfAckersPerWorker);\r\n        }\r\n        int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRasTopo(conf, stormTopology);\r\n        Nimbus.setUpAckerExecutorConfigs(topoName, conf, conf, estimatedNumWorker);\r\n        conf.put(Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB, 250);\r\n        conf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 50);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormTopology)), currentTime, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // We need to have 3 slots on 3 separate hosts. The topology needs 6 GPUs 3500 MB memory and 350% CPU\r\n        // The bolt-3 instances must be on separate nodes because they each need 2 GPUs.\r\n        // The bolt-2 instances must be on the same node as they each need 1 GPU\r\n        // (this assumes that we are packing the components to avoid fragmentation).\r\n        // The bolt-1 and spout instances fill in the rest.\r\n        // Ordered execs: [[6, 6], [2, 2], [4, 4], [5, 5], [1, 1], [3, 3], [0, 0]]\r\n        // Ackers: [[8, 8], [7, 7]] (+ [[9, 9], [10, 10]] when numOfAckersPerWorker=2)\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        if (numOfAckersPerWorker == -1 || numOfAckersPerWorker == 1) {\r\n            expectedScheduling.add(new HashSet<>(Collections.singletonList(//bolt-3 - 500 MB, 50% CPU, 2 GPU\r\n            new ExecutorDetails(3, 3))));\r\n            //Total 500 MB, 50% CPU, 2 - GPU -> this node has 1500 MB, 150% cpu, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(6, 6), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(2, 2), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(5, 5), //acker - 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(8, 8))));\r\n            //Total 1750 MB, 200% CPU, 2 GPU -> this node has 250 MB, 0% CPU, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3 500 MB, 50% cpu, 2 GPU\r\n            new ExecutorDetails(4, 4), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(1, 1), //Spout - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(0, 0), //acker - 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(7, 7))));\r\n            //Total 1750 MB, 200% CPU, 2 GPU -> this node has 250 MB, 0% CPU, 0 GPU left\r\n        } else if (numOfAckersPerWorker == 0) {\r\n            expectedScheduling.add(new HashSet<>(Collections.singletonList(//bolt-3 - 500 MB, 50% CPU, 2 GPU\r\n            new ExecutorDetails(3, 3))));\r\n            //Total 500 MB, 50% CPU, 2 - GPU -> this node has 1500 MB, 150% cpu, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(6, 6), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(2, 2), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(5, 5), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(1, 1))));\r\n            //Total 2000 MB, 200% CPU, 2 GPU -> this node has 0 MB, 0% CPU, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//Spout - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(0, 0), //bolt-3 500 MB, 50% cpu, 2 GPU\r\n            new ExecutorDetails(4, 4))));\r\n            //Total 1000 MB, 100% CPU, 2 GPU -> this node has 1000 MB, 100% CPU, 0 GPU left\r\n        } else if (numOfAckersPerWorker == 2) {\r\n            expectedScheduling.add(new HashSet<>(Collections.singletonList(//bolt-3 - 500 MB, 50% CPU, 2 GPU\r\n            new ExecutorDetails(3, 3))));\r\n            //Total 500 MB, 50% CPU, 2 - GPU -> this node has 1500 MB, 150% cpu, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//acker - 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(7, 7), //acker - 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(8, 8), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(6, 6), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(2, 2))));\r\n            //Total 1500 MB, 200% CPU, 2 GPU -> this node has 500 MB, 0% CPU, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//acker- 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(9, 9), //acker- 250 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(10, 10), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(1, 1), //bolt-3 500 MB, 50% cpu, 2 GPU\r\n            new ExecutorDetails(4, 4))));\r\n            //Total 1500 MB, 200% CPU, 2 GPU -> this node has 500 MB, 0% CPU, 0 GPU left\r\n            expectedScheduling.add(new HashSet<>(Arrays.asList(//Spout - 500 MB, 50% CPU, 0 GPU\r\n            new ExecutorDetails(0, 0), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n            new ExecutorDetails(5, 5))));\r\n            //Total 1000 MB, 100% CPU, 2 GPU -> this node has 1000 MB, 100% CPU, 0 GPU left\r\n        }\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testGenericResourceAwareStrategyWithSettingAckerExecutors",
  "sourceCode" : "/**\r\n * Test if the scheduling logic for the GenericResourceAwareStrategy is correct\r\n * with setting {@link Config#TOPOLOGY_ACKER_EXECUTORS}.\r\n *\r\n * Test details refer to {@link TestDefaultResourceAwareStrategy#testDefaultResourceAwareStrategyWithSettingAckerExecutors(int)}\r\n */\r\n@ParameterizedTest\r\n@ValueSource(ints = { -1, 0, 2, 200 })\r\npublic void testGenericResourceAwareStrategyWithSettingAckerExecutors(int numOfAckersPerWorker) throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\").addResource(\"gpu.count\", 1.0);\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\").addResource(\"gpu.count\", 2.0);\r\n        String topoName = \"testTopology\";\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Config conf = createGrasClusterConfig(strategyClass, 50, 500, 0, null, Collections.emptyMap());\r\n        Map<String, Double> genericResourcesMap = new HashMap<>();\r\n        genericResourcesMap.put(\"gpu.count\", 2.0);\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000, genericResourcesMap);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, topoName);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 2000);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, 4);\r\n        if (numOfAckersPerWorker == -1) {\r\n            // Leave topology.acker.executors.per.worker unset\r\n        } else {\r\n            conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, numOfAckersPerWorker);\r\n        }\r\n        int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRasTopo(conf, stormTopology);\r\n        Nimbus.setUpAckerExecutorConfigs(topoName, conf, conf, estimatedNumWorker);\r\n        conf.put(Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB, 250);\r\n        conf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 50);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormTopology)), currentTime, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // We need to have 3 slots on 3 separate hosts. The topology needs 6 GPUs 3500 MB memory and 350% CPU\r\n        // The bolt-3 instances must be on separate nodes because they each need 2 GPUs.\r\n        // The bolt-2 instances must be on the same node as they each need 1 GPU\r\n        // (this assumes that we are packing the components to avoid fragmentation).\r\n        // The bolt-1 and spout instances fill in the rest.\r\n        // Ordered execs: [[6, 6], [2, 2], [4, 4], [5, 5], [1, 1], [3, 3], [0, 0]]\r\n        // Ackers: [[8, 8], [7, 7]] (+ [[9, 9], [10, 10]] when numOfAckersPerWorker=2)\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        expectedScheduling.add(new HashSet<>(Collections.singletonList(//bolt-3 - 500 MB, 50% CPU, 2 GPU\r\n        new ExecutorDetails(3, 3))));\r\n        //Total 500 MB, 50% CPU, 2 - GPU -> this node has 1500 MB, 150% cpu, 0 GPU left\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//acker - 250 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(7, 7), //acker - 250 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(8, 8), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n        new ExecutorDetails(6, 6), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(2, 2))));\r\n        //Total 1500 MB, 200% CPU, 2 GPU -> this node has 500 MB, 0% CPU, 0 GPU left\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//acker- 250 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(9, 9), //acker- 250 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(10, 10), //bolt-1 - 500 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(1, 1), //bolt-3 500 MB, 50% cpu, 2 GPU\r\n        new ExecutorDetails(4, 4))));\r\n        //Total 1500 MB, 200% CPU, 2 GPU -> this node has 500 MB, 0% CPU, 0 GPU left\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//Spout - 500 MB, 50% CPU, 0 GPU\r\n        new ExecutorDetails(0, 0), //bolt-2 - 500 MB, 50% CPU, 1 GPU\r\n        new ExecutorDetails(5, 5))));\r\n        //Total 1000 MB, 100% CPU, 2 GPU -> this node has 1000 MB, 100% CPU, 0 GPU left\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "ParameterizedTest" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testGrasRequiringEviction",
  "sourceCode" : "/*\r\n     * test requiring eviction until Generic Resource (gpu) is evicted.\r\n     */\r\n@Test\r\npublic void testGrasRequiringEviction() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 3;\r\n        double cpuPercent = 10;\r\n        double memoryOnHeap = 10;\r\n        double memoryOffHeap = 10;\r\n        // Sufficient Cpu/Memory. But insufficient gpu to schedule all topologies (gpu1, noGpu, gpu2).\r\n        // gpu topology (requires 3 gpu's in total)\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism).addResource(\"gpu.count\", 1.0);\r\n        StormTopology stormTopologyWithGpu = builder.createTopology();\r\n        // non-gpu topology\r\n        builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        StormTopology stormTopologyNoGpu = builder.createTopology();\r\n        Config conf = createGrasClusterConfig(strategyClass, cpuPercent, memoryOnHeap, memoryOffHeap, null, Collections.emptyMap());\r\n        // allow 1 round of evictions\r\n        conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_TOPOLOGY_SCHEDULING_ATTEMPTS, 2);\r\n        String gpu1 = \"hasGpu1\";\r\n        String noGpu = \"hasNoGpu\";\r\n        String gpu2 = \"hasGpu2\";\r\n        TopologyDetails[] topo = { createTestStormTopology(stormTopologyWithGpu, 10, gpu1, conf), createTestStormTopology(stormTopologyNoGpu, 10, noGpu, conf), createTestStormTopology(stormTopologyWithGpu, 9, gpu2, conf) };\r\n        Topologies topologies = new Topologies(topo[0], topo[1]);\r\n        Map<String, Double> genericResourcesMap = new HashMap<>();\r\n        genericResourcesMap.put(\"gpu.count\", 1.0);\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 500, 2000, genericResourcesMap);\r\n        Cluster cluster = new Cluster(new INimbusTest(), new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        // should schedule gpu1 and noGpu successfully\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, gpu1);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, noGpu);\r\n        // should evict gpu1 and noGpu topologies in order to schedule gpu2 topology; then fail to reschedule gpu1 topology;\r\n        // then schedule noGpu topology.\r\n        // Scheduling used to ignore gpu resource when deciding when to stop evicting, and gpu2 would fail to schedule.\r\n        topologies = new Topologies(topo[0], topo[1], topo[2]);\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesNotScheduled(cluster, strategyClass, gpu1);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, noGpu);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, gpu2);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testGenericResourceAwareStrategyInFavorOfShuffle",
  "sourceCode" : "/**\r\n * test if the scheduling logic for the GenericResourceAwareStrategy (when in favor of shuffle) is correct.\r\n */\r\n@Test\r\npublic void testGenericResourceAwareStrategyInFavorOfShuffle() throws InvalidTopologyException {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        int boltParallelism = 2;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), boltParallelism).shuffleGrouping(\"spout\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-1\").addResource(\"gpu.count\", 1.0);\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), boltParallelism).shuffleGrouping(\"bolt-2\").addResource(\"gpu.count\", 2.0);\r\n        StormTopology stormTopology = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Config conf = createGrasClusterConfig(strategyClass, 50, 250, 250, null, Collections.emptyMap());\r\n        Map<String, Double> genericResourcesMap = new HashMap<>();\r\n        genericResourcesMap.put(\"gpu.count\", 2.0);\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 200, 2000, genericResourcesMap);\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, \"testTopology\");\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        conf.put(Config.TOPOLOGY_RAS_ORDER_EXECUTORS_BY_PROXIMITY_NEEDS, true);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormTopology, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormTopology)), currentTime, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n        rs.prepare(conf, new StormMetricsRegistry());\r\n        rs.schedule(topologies, cluster);\r\n        // Sorted execs: [[0, 0], [2, 2], [6, 6], [4, 4], [1, 1], [5, 5], [3, 3], [7, 7]]\r\n        // Ackers: [[7, 7]]]\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//spout\r\n        new ExecutorDetails(0, 0), //bolt-1\r\n        new ExecutorDetails(2, 2), //bolt-2\r\n        new ExecutorDetails(6, 6), //acker\r\n        new ExecutorDetails(7, 7))));\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(//bolt-3\r\n        new ExecutorDetails(4, 4), //bolt-1\r\n        new ExecutorDetails(1, 1))));\r\n        //bolt-2\r\n        expectedScheduling.add(new HashSet<>(Collections.singletonList(new ExecutorDetails(5, 5))));\r\n        //bolt-3\r\n        expectedScheduling.add(new HashSet<>(Collections.singletonList(new ExecutorDetails(3, 3))));\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testAntiAffinityWithMultipleTopologies",
  "sourceCode" : "@Test\r\npublic void testAntiAffinityWithMultipleTopologies() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisorsWithRacks(1, 40, 66, 0, 0, 4700, 226200, new HashMap<>());\r\n        HashMap<String, Double> extraResources = new HashMap<>();\r\n        extraResources.put(\"my.gpu\", 1.0);\r\n        supMap.putAll(genSupervisorsWithRacks(1, 40, 66, 1, 0, 4700, 226200, extraResources));\r\n        Config config = new Config();\r\n        config.putAll(createGrasClusterConfig(strategyClass, 88, 775, 25, null, null));\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        TopologyDetails tdSimple = genTopology(\"topology-simple\", config, 1, 5, 100, 300, 0, 0, \"user\", 8192);\r\n        //Schedule the simple topology first\r\n        Topologies topologies = new Topologies(tdSimple);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler.schedule(topologies, cluster);\r\n        TopologyBuilder builder = topologyBuilder(1, 5, 100, 300);\r\n        builder.setBolt(\"gpu-bolt\", new TestBolt(), 40).addResource(\"my.gpu\", 1.0).shuffleGrouping(\"spout-0\");\r\n        TopologyDetails tdGpu = topoToTopologyDetails(\"topology-gpu\", config, builder.createTopology(), 0, 0, \"user\", 8192);\r\n        //Now schedule GPU but with the simple topology in place.\r\n        topologies = new Topologies(tdSimple, tdGpu);\r\n        cluster = new Cluster(cluster, topologies);\r\n        scheduler.schedule(topologies, cluster);\r\n        Map<String, SchedulerAssignment> assignments = new TreeMap<>(cluster.getAssignments());\r\n        assertEquals(2, assignments.size());\r\n        Map<String, Map<String, AtomicLong>> topoPerRackCount = new HashMap<>();\r\n        for (Entry<String, SchedulerAssignment> entry : assignments.entrySet()) {\r\n            SchedulerAssignment sa = entry.getValue();\r\n            Map<String, AtomicLong> slotsPerRack = new TreeMap<>();\r\n            for (WorkerSlot slot : sa.getSlots()) {\r\n                String nodeId = slot.getNodeId();\r\n                String rack = supervisorIdToRackName(nodeId);\r\n                slotsPerRack.computeIfAbsent(rack, (r) -> new AtomicLong(0)).incrementAndGet();\r\n            }\r\n            LOG.info(\"{} => {}\", entry.getKey(), slotsPerRack);\r\n            topoPerRackCount.put(entry.getKey(), slotsPerRack);\r\n        }\r\n        Map<String, AtomicLong> simpleCount = topoPerRackCount.get(\"topology-simple-0\");\r\n        assertNotNull(simpleCount);\r\n        //Because the simple topology was scheduled first we want to be sure that it didn't put anything on\r\n        // the GPU nodes.\r\n        //Only 1 rack is in use\r\n        assertEquals(1, simpleCount.size());\r\n        //r001 is the second rack with GPUs\r\n        assertFalse(simpleCount.containsKey(\"r001\"));\r\n        //r000 is the first rack with no GPUs\r\n        assertTrue(simpleCount.containsKey(\"r000\"));\r\n        //We don't really care too much about the scheduling of topology-gpu-0, because it was scheduled.\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestGenericResourceAwareStrategy.java",
  "methodName" : "testScheduleLeftOverAckers",
  "sourceCode" : "@Test\r\npublic void testScheduleLeftOverAckers() throws Exception {\r\n    for (Class strategyClass : strategyClasses) {\r\n        int spoutParallelism = 1;\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout\", new TestSpout(), spoutParallelism);\r\n        String topoName = \"testTopology\";\r\n        StormTopology stormToplogy = builder.createTopology();\r\n        INimbus iNimbus = new INimbusTest();\r\n        Config conf = createGrasClusterConfig(strategyClass, 50, 400, 0, null, Collections.emptyMap());\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 1, 100, 1100);\r\n        Map<String, SupervisorDetails> tmpSupMap = genSupervisors(2, 1, 100, 400);\r\n        supMap.put(\"r000s001\", tmpSupMap.get(\"r000s001\"));\r\n        LOG.info(\"{}\", tmpSupMap.get(\"r000s001\"));\r\n        conf.put(Config.TOPOLOGY_PRIORITY, 0);\r\n        conf.put(Config.TOPOLOGY_NAME, topoName);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 1000);\r\n        conf.put(Config.TOPOLOGY_SUBMITTER_USER, \"user\");\r\n        conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, 2);\r\n        conf.put(Config.TOPOLOGY_RAS_ACKER_EXECUTORS_PER_WORKER, 1);\r\n        conf.put(Config.TOPOLOGY_ACKER_RESOURCES_ONHEAP_MEMORY_MB, 500);\r\n        conf.put(Config.TOPOLOGY_ACKER_CPU_PCORE_PERCENT, 0);\r\n        TopologyDetails topo = new TopologyDetails(\"testTopology-id\", conf, stormToplogy, 0, genExecsAndComps(StormCommon.systemTopology(conf, stormToplogy)), currentTime, \"user\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, conf);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(conf, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // First it tries to schedule spout [0, 0] with a bound acker [1, 1] to sup1 r000s000.\r\n        // However, sup2 r000s001 only has 400 on-heap mem which can not fit the left over acker [2, 2]\r\n        // So it backtrack to [0, 0] and put it to sup2 r000s001.\r\n        // Then put two ackers both as left-over ackers to sup1 r000s000.\r\n        HashSet<HashSet<ExecutorDetails>> expectedScheduling = new HashSet<>();\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(// spout\r\n        new ExecutorDetails(0, 0))));\r\n        expectedScheduling.add(new HashSet<>(Arrays.asList(// acker\r\n        new ExecutorDetails(1, 1), // acker\r\n        new ExecutorDetails(2, 2))));\r\n        HashSet<HashSet<ExecutorDetails>> foundScheduling = new HashSet<>();\r\n        SchedulerAssignment assignment = cluster.getAssignmentById(\"testTopology-id\");\r\n        for (Collection<ExecutorDetails> execs : assignment.getSlotToExecutors().values()) {\r\n            foundScheduling.add(new HashSet<>(execs));\r\n        }\r\n        assertEquals(expectedScheduling, foundScheduling);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestLargeCluster.java",
  "methodName" : "testReadSerializedTopologiesAndConfigs",
  "sourceCode" : "/**\r\n * Check if the files in the resource directory are matched, can be read properly, and code/config files occur\r\n * in matched pairs.\r\n *\r\n * @throws Exception showing bad and unmatched resource files.\r\n */\r\n@Test\r\npublic void testReadSerializedTopologiesAndConfigs() throws Exception {\r\n    for (TEST_CLUSTER_NAME testClusterName : TEST_CLUSTER_NAME.values()) {\r\n        String resourcePath = testClusterName.getResourcePath();\r\n        List<String> resources = getResourceFiles(resourcePath);\r\n        assertFalse(resources.isEmpty(), \"No resource files found in \" + resourcePath);\r\n        createTopoDetailsArray(resourcePath, true);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestLargeCluster.java",
  "methodName" : "testLargeCluster",
  "sourceCode" : "/**\r\n * Create a large cluster, read topologies and configuration from resource directory and schedule.\r\n *\r\n * @throws Exception upon error.\r\n */\r\n@Test\r\npublic void testLargeCluster() throws Exception {\r\n    for (Class strategyClass : strategyClasses) {\r\n        for (TEST_CLUSTER_NAME testClusterName : TEST_CLUSTER_NAME.values()) {\r\n            LOG.info(\"********************************************\");\r\n            LOG.info(\"testLargeCluster: Start Processing cluster {} using \", testClusterName.getClusterName(), strategyClass.getName());\r\n            String resourcePath = testClusterName.getResourcePath();\r\n            Map<String, SupervisorDetails> supervisors = createSupervisors(testClusterName, 0);\r\n            TopologyDetails[] topoDetailsArray = createTopoDetailsArray(resourcePath, false);\r\n            assertTrue(topoDetailsArray.length > 0, \"No topologies found for cluster \" + testClusterName.getClusterName());\r\n            Topologies topologies = new Topologies(topoDetailsArray);\r\n            Config confWithDefaultStrategy = new Config();\r\n            confWithDefaultStrategy.putAll(topoDetailsArray[0].getConf());\r\n            confWithDefaultStrategy.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, strategyClass.getName());\r\n            confWithDefaultStrategy.put(Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN, TestUtilsForResourceAwareScheduler.GenSupervisorsDnsToSwitchMapping.class.getName());\r\n            INimbus iNimbus = new INimbusTest();\r\n            Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supervisors, new HashMap<>(), topologies, confWithDefaultStrategy);\r\n            scheduler = new ResourceAwareScheduler();\r\n            List<Class> classesToDebug = Arrays.asList(DefaultResourceAwareStrategy.class, GenericResourceAwareStrategy.class, ResourceAwareScheduler.class, Cluster.class);\r\n            // switch to Level.DEBUG for verbose otherwise Level.INFO\r\n            Level logLevel = Level.INFO;\r\n            classesToDebug.forEach(x -> Configurator.setLevel(x.getName(), logLevel));\r\n            long startTime = System.currentTimeMillis();\r\n            scheduler.prepare(confWithDefaultStrategy, new StormMetricsRegistry());\r\n            scheduler.schedule(topologies, cluster);\r\n            long endTime = System.currentTimeMillis();\r\n            LOG.info(\"Cluster={} Scheduling Time: {} topologies in {} seconds\", testClusterName.getClusterName(), topoDetailsArray.length, (endTime - startTime) / 1000.0);\r\n            for (TopologyDetails td : topoDetailsArray) {\r\n                TestUtilsForResourceAwareScheduler.assertTopologiesFullyScheduled(cluster, strategyClass, td.getName());\r\n            }\r\n            // Remove topology and reschedule it\r\n            for (int i = 0; i < topoDetailsArray.length; i++) {\r\n                startTime = System.currentTimeMillis();\r\n                TopologyDetails topoDetails = topoDetailsArray[i];\r\n                cluster.unassign(topoDetails.getId());\r\n                LOG.info(\"Cluster={},  ({}) Removed topology {}\", testClusterName.getClusterName(), i, topoDetails.getName());\r\n                IScheduler rescheduler = new ResourceAwareScheduler();\r\n                rescheduler.prepare(confWithDefaultStrategy, new StormMetricsRegistry());\r\n                rescheduler.schedule(topologies, cluster);\r\n                TestUtilsForResourceAwareScheduler.assertTopologiesFullyScheduled(cluster, strategyClass, topoDetails.getName());\r\n                endTime = System.currentTimeMillis();\r\n                LOG.info(\"Cluster={}, ({}) Scheduling Time: Removed topology {} and rescheduled in {} seconds\", testClusterName.getClusterName(), i, topoDetails.getName(), (endTime - startTime) / 1000.0);\r\n            }\r\n            classesToDebug.forEach(x -> Configurator.setLevel(x.getName(), Level.INFO));\r\n            LOG.info(\"testLargeCluster: End Processing cluster {}\", testClusterName.getClusterName());\r\n            LOG.info(\"********************************************\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\strategies\\scheduling\\TestRoundRobinNodeSorterHostIsolation.java",
  "methodName" : "testTopologyIsolation",
  "sourceCode" : "/**\r\n * Test whether number of nodes is limited by {@link Config#TOPOLOGY_ISOLATED_MACHINES} by scheduling\r\n * two topologies and verifying the number of nodes that each one occupies and are not overlapping.\r\n */\r\n@Test\r\nvoid testTopologyIsolation() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    double compPcore = 100;\r\n    double compOnHeap = 775;\r\n    double compOffHeap = 25;\r\n    int[] topoNumSpouts = { 1, 1 };\r\n    int[] topoNumBolts = { 1, 1 };\r\n    int[] topoSpoutParallelism = { 100, 100 };\r\n    int[] topoBoltParallelism = { 200, 200 };\r\n    final int numRacks = 3;\r\n    final int numSupersPerRack = 10;\r\n    final int numPortsPerSuper = 6;\r\n    final int numZonesPerHost = 1;\r\n    final double numaResourceMultiplier = 1.0;\r\n    int rackStartNum = 0;\r\n    int supStartNum = 0;\r\n    long compPerRack = (topoNumSpouts[0] * topoSpoutParallelism[0] + topoNumBolts[0] * topoBoltParallelism[0] + // enough for topo1 but not topo1+topo2\r\n    topoNumSpouts[1] * topoSpoutParallelism[1]);\r\n    long compPerSuper = compPerRack / numSupersPerRack;\r\n    double cpuPerSuper = compPcore * compPerSuper;\r\n    double memPerSuper = (compOnHeap + compOffHeap) * compPerSuper;\r\n    double[] topoMaxHeapSize = { memPerSuper, memPerSuper };\r\n    final String[] topoNames = { \"topology1\", \"topology2\" };\r\n    int[] maxNodes = { 15, 13 };\r\n    Map<String, SupervisorDetails> supMap = genSupervisorsWithRacksAndNuma(numRacks, numSupersPerRack, numZonesPerHost, numPortsPerSuper, rackStartNum, supStartNum, cpuPerSuper, memPerSuper, Collections.emptyMap(), numaResourceMultiplier);\r\n    TestDNSToSwitchMapping testDNSToSwitchMapping = new TestDNSToSwitchMapping(supMap.values());\r\n    Config[] configs = new Config[topoNames.length];\r\n    TopologyDetails[] topos = new TopologyDetails[topoNames.length];\r\n    for (int i = 0; i < topoNames.length; i++) {\r\n        configs[i] = new Config();\r\n        configs[i].putAll(createRoundRobinClusterConfig(compPcore, compOnHeap, compOffHeap, null, null));\r\n        configs[i].put(Config.TOPOLOGY_ISOLATED_MACHINES, maxNodes[i]);\r\n        topos[i] = genTopology(topoNames[i], configs[i], topoNumSpouts[i], topoNumBolts[i], topoSpoutParallelism[i], topoBoltParallelism[i], 0, 0, \"user\", topoMaxHeapSize[i]);\r\n    }\r\n    TopologyDetails td1 = topos[0];\r\n    TopologyDetails td2 = topos[1];\r\n    IScheduler scheduler = new ResourceAwareScheduler();\r\n    scheduler.prepare(configs[0], new StormMetricsRegistry());\r\n    //Schedule the topo1 topology and ensure it uses limited number of nodes\r\n    Topologies topologies = new Topologies(td1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, configs[0]);\r\n    cluster.setNetworkTopography(testDNSToSwitchMapping.getRackToHosts());\r\n    scheduler.schedule(topologies, cluster);\r\n    Set<String> assignedRacks = cluster.getAssignedRacks(topos[0].getId());\r\n    assertEquals(2, assignedRacks.size(), \"Racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    //Now schedule GPU but with the simple topology in place.\r\n    topologies = new Topologies(td1, td2);\r\n    cluster = new Cluster(cluster, topologies);\r\n    scheduler.schedule(topologies, cluster);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId(), td2.getId());\r\n    assertEquals(numRacks, assignedRacks.size(), \"Racks for topologies=\" + td1.getId() + \"/\" + td2.getId() + \" is \" + assignedRacks);\r\n    SchedulerAssignment[] assignments = new SchedulerAssignment[topoNames.length];\r\n    Collection<String>[] assignmentNodes = new Collection[topoNames.length];\r\n    for (int i = 0; i < topoNames.length; i++) {\r\n        assignments[i] = cluster.getAssignmentById(topos[i].getId());\r\n        if (assignments[i] == null) {\r\n            fail(\"Topology \" + topoNames[i] + \" cannot be scheduled\");\r\n        }\r\n        assignmentNodes[i] = assignments[i].getSlots().stream().map(WorkerSlot::getNodeId).collect(Collectors.toList());\r\n        assertEquals(maxNodes[i], assignmentNodes[i].size(), \"Max Nodes for \" + topoNames[i] + \" assignment\");\r\n    }\r\n    // confirm no overlap in nodes\r\n    Set<String> nodes1 = new HashSet<>(assignmentNodes[0]);\r\n    Set<String> nodes2 = new HashSet<>(assignmentNodes[1]);\r\n    Set<String> dupNodes = Sets.intersection(nodes1, nodes2);\r\n    if (dupNodes.size() > 0) {\r\n        List<String> lines = new ArrayList<>();\r\n        lines.add(\"Topologies shared nodes when not expected to\");\r\n        lines.add(\"Duplicated nodes are \" + String.join(\",\", dupNodes));\r\n        fail(String.join(\"\\n\\t\", lines));\r\n    }\r\n    nodes2.removeAll(nodes1);\r\n    // topo2 gets scheduled on across the two racks even if there is one rack with enough capacity\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(numRacks - 1, assignedRacks.size(), \"Racks for topologies=\" + td2.getId() + \" is \" + assignedRacks);\r\n    // now unassign topo2, expect only two of three racks to be in use; free some slots and reschedule topo1 some topo1 executors\r\n    cluster.unassign(td2.getId());\r\n    assignedRacks = cluster.getAssignedRacks(td2.getId());\r\n    assertEquals(0, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td2.getId() + \" is \" + assignedRacks);\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks - 1, assignedRacks.size(), \"After unassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n    assertFalse(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should be fully assigned before freeing slots\");\r\n    freeSomeWorkerSlots(cluster);\r\n    assertTrue(cluster.needsSchedulingRas(td1), \"Topology \" + td1.getId() + \" should need scheduling after freeing slots\");\r\n    // then reschedule executors\r\n    scheduler.schedule(topologies, cluster);\r\n    // only two of three racks should be in use still\r\n    assignedRacks = cluster.getAssignedRacks(td1.getId());\r\n    assertEquals(numRacks - 1, assignedRacks.size(), \"After reassigning topology \" + td2.getId() + \", racks for topology=\" + td1.getId() + \" is \" + assignedRacks);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testRASNodeSlotAssign",
  "sourceCode" : "@Test\r\npublic void testRASNodeSlotAssign() {\r\n    Config config = new Config();\r\n    config.putAll(defaultTopologyConf);\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(5, 4, 400, 2000);\r\n    TopologyDetails topology1 = genTopology(\"topology1\", config, 1, 0, 2, 0, 0, 0, \"user\");\r\n    TopologyDetails topology2 = genTopology(\"topology2\", config, 1, 0, 2, 0, 0, 0, \"user\");\r\n    Topologies topologies = new Topologies(topology1, topology2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    Map<String, RasNode> nodes = RasNodes.getAllNodesFrom(cluster);\r\n    assertEquals(5, nodes.size());\r\n    RasNode node = nodes.get(\"r000s000\");\r\n    assertEquals(\"r000s000\", node.getId());\r\n    assertTrue(node.isAlive());\r\n    assertEquals(0, node.getRunningTopologies().size());\r\n    assertTrue(node.isTotallyFree());\r\n    assertEquals(4, node.totalSlotsFree());\r\n    assertEquals(0, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n    List<ExecutorDetails> executors11 = new ArrayList<>();\r\n    executors11.add(new ExecutorDetails(1, 1));\r\n    node.assign(node.getFreeSlots().iterator().next(), topology1, executors11);\r\n    assertEquals(1, node.getRunningTopologies().size());\r\n    assertFalse(node.isTotallyFree());\r\n    assertEquals(3, node.totalSlotsFree());\r\n    assertEquals(1, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n    List<ExecutorDetails> executors12 = new ArrayList<>();\r\n    executors12.add(new ExecutorDetails(2, 2));\r\n    node.assign(node.getFreeSlots().iterator().next(), topology1, executors12);\r\n    assertEquals(1, node.getRunningTopologies().size());\r\n    assertFalse(node.isTotallyFree());\r\n    assertEquals(2, node.totalSlotsFree());\r\n    assertEquals(2, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n    List<ExecutorDetails> executors21 = new ArrayList<>();\r\n    executors21.add(new ExecutorDetails(1, 1));\r\n    node.assign(node.getFreeSlots().iterator().next(), topology2, executors21);\r\n    assertEquals(2, node.getRunningTopologies().size());\r\n    assertFalse(node.isTotallyFree());\r\n    assertEquals(1, node.totalSlotsFree());\r\n    assertEquals(3, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n    List<ExecutorDetails> executors22 = new ArrayList<>();\r\n    executors22.add(new ExecutorDetails(2, 2));\r\n    node.assign(node.getFreeSlots().iterator().next(), topology2, executors22);\r\n    assertEquals(2, node.getRunningTopologies().size());\r\n    assertFalse(node.isTotallyFree());\r\n    assertEquals(0, node.totalSlotsFree());\r\n    assertEquals(4, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n    node.freeAllSlots();\r\n    assertEquals(0, node.getRunningTopologies().size());\r\n    assertTrue(node.isTotallyFree());\r\n    assertEquals(4, node.totalSlotsFree());\r\n    assertEquals(0, node.totalSlotsUsed());\r\n    assertEquals(4, node.totalSlots());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "sanityTestOfScheduling",
  "sourceCode" : "@Test\r\npublic void sanityTestOfScheduling() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(1, 2, 400, 2000);\r\n    Config config = new Config();\r\n    config.putAll(defaultTopologyConf);\r\n    scheduler = new ResourceAwareScheduler();\r\n    TopologyDetails topology1 = genTopology(\"topology1\", config, 1, 1, 1, 1, 0, 0, \"user\");\r\n    Topologies topologies = new Topologies(topology1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    scheduler.schedule(topologies, cluster);\r\n    SchedulerAssignment assignment = cluster.getAssignmentById(topology1.getId());\r\n    Set<WorkerSlot> assignedSlots = assignment.getSlots();\r\n    Set<String> nodesIDs = new HashSet<>();\r\n    for (WorkerSlot slot : assignedSlots) {\r\n        nodesIDs.add(slot.getNodeId());\r\n    }\r\n    Collection<ExecutorDetails> executors = assignment.getExecutors();\r\n    assertEquals(1, assignedSlots.size());\r\n    assertEquals(1, nodesIDs.size());\r\n    assertEquals(2, executors.size());\r\n    assertFalse(cluster.needsSchedulingRas(topology1));\r\n    assertTrue(cluster.getStatusMap().get(topology1.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testTopologyWithMultipleSpouts",
  "sourceCode" : "@Test\r\npublic void testTopologyWithMultipleSpouts() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(2, 4, 400, 2000);\r\n    // a topology with multiple spouts\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout1\", new TestWordSpout(), 1);\r\n    builder1.setSpout(\"wordSpout2\", new TestWordSpout(), 1);\r\n    builder1.setBolt(\"wordCountBolt1\", new TestWordCounter(), 1).shuffleGrouping(\"wordSpout1\").shuffleGrouping(\"wordSpout2\");\r\n    builder1.setBolt(\"wordCountBolt2\", new TestWordCounter(), 1).shuffleGrouping(\"wordCountBolt1\");\r\n    builder1.setBolt(\"wordCountBolt3\", new TestWordCounter(), 1).shuffleGrouping(\"wordCountBolt1\");\r\n    builder1.setBolt(\"wordCountBolt4\", new TestWordCounter(), 1).shuffleGrouping(\"wordCountBolt2\");\r\n    builder1.setBolt(\"wordCountBolt5\", new TestWordCounter(), 1).shuffleGrouping(\"wordSpout2\");\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config = new Config();\r\n    config.putAll(defaultTopologyConf);\r\n    Map<ExecutorDetails, String> executorMap1 = genExecsAndComps(stormTopology1);\r\n    TopologyDetails topology1 = new TopologyDetails(\"topology1\", config, stormTopology1, 0, executorMap1, 0, \"user\");\r\n    // a topology with two unconnected partitions\r\n    TopologyBuilder builder2 = new TopologyBuilder();\r\n    builder2.setSpout(\"wordSpoutX\", new TestWordSpout(), 1);\r\n    builder2.setSpout(\"wordSpoutY\", new TestWordSpout(), 1);\r\n    StormTopology stormTopology2 = builder2.createTopology();\r\n    Map<ExecutorDetails, String> executorMap2 = genExecsAndComps(stormTopology2);\r\n    TopologyDetails topology2 = new TopologyDetails(\"topology2\", config, stormTopology2, 0, executorMap2, 0, \"user\");\r\n    scheduler = new ResourceAwareScheduler();\r\n    Topologies topologies = new Topologies(topology1, topology2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    scheduler.prepare(config, new StormMetricsRegistry());\r\n    scheduler.schedule(topologies, cluster);\r\n    SchedulerAssignment assignment1 = cluster.getAssignmentById(topology1.getId());\r\n    Set<WorkerSlot> assignedSlots1 = assignment1.getSlots();\r\n    Set<String> nodesIDs1 = new HashSet<>();\r\n    for (WorkerSlot slot : assignedSlots1) {\r\n        nodesIDs1.add(slot.getNodeId());\r\n    }\r\n    Collection<ExecutorDetails> executors1 = assignment1.getExecutors();\r\n    assertEquals(1, assignedSlots1.size());\r\n    assertEquals(1, nodesIDs1.size());\r\n    assertEquals(7, executors1.size());\r\n    assertFalse(cluster.needsSchedulingRas(topology1));\r\n    assertTrue(cluster.getStatusMap().get(topology1.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n    SchedulerAssignment assignment2 = cluster.getAssignmentById(topology2.getId());\r\n    Set<WorkerSlot> assignedSlots2 = assignment2.getSlots();\r\n    Set<String> nodesIDs2 = new HashSet<>();\r\n    for (WorkerSlot slot : assignedSlots2) {\r\n        nodesIDs2.add(slot.getNodeId());\r\n    }\r\n    Collection<ExecutorDetails> executors2 = assignment2.getExecutors();\r\n    assertEquals(1, assignedSlots2.size());\r\n    assertEquals(1, nodesIDs2.size());\r\n    assertEquals(2, executors2.size());\r\n    assertFalse(cluster.needsSchedulingRas(topology2));\r\n    assertTrue(cluster.getStatusMap().get(topology2.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testTopologySetCpuAndMemLoad",
  "sourceCode" : "@Test\r\npublic void testTopologySetCpuAndMemLoad() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(2, 2, 400, 2000);\r\n    // a topology with multiple spouts\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout\", new TestWordSpout(), 1).setCPULoad(20.0).setMemoryLoad(200.0);\r\n    builder1.setBolt(\"wordCountBolt\", new TestWordCounter(), 1).shuffleGrouping(\"wordSpout\").setCPULoad(20.0).setMemoryLoad(200.0);\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config = new Config();\r\n    config.putAll(defaultTopologyConf);\r\n    Map<ExecutorDetails, String> executorMap1 = genExecsAndComps(stormTopology1);\r\n    TopologyDetails topology1 = new TopologyDetails(\"topology1\", config, stormTopology1, 0, executorMap1, 0, \"user\");\r\n    ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n    scheduler = rs;\r\n    Topologies topologies = new Topologies(topology1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    rs.prepare(config, new StormMetricsRegistry());\r\n    rs.schedule(topologies, cluster);\r\n    SchedulerAssignment assignment1 = cluster.getAssignmentById(topology1.getId());\r\n    Map<WorkerSlot, WorkerResources> assignedSlots1 = assignment1.getScheduledResources();\r\n    double assignedMemory = 0.0;\r\n    double assignedCpu = 0.0;\r\n    Set<String> nodesIDs1 = new HashSet<>();\r\n    for (Entry<WorkerSlot, WorkerResources> entry : assignedSlots1.entrySet()) {\r\n        WorkerResources wr = entry.getValue();\r\n        nodesIDs1.add(entry.getKey().getNodeId());\r\n        assignedMemory += wr.get_mem_on_heap() + wr.get_mem_off_heap();\r\n        assignedCpu += wr.get_cpu();\r\n    }\r\n    Collection<ExecutorDetails> executors1 = assignment1.getExecutors();\r\n    assertEquals(1, assignedSlots1.size());\r\n    assertEquals(1, nodesIDs1.size());\r\n    assertEquals(2, executors1.size());\r\n    assertEquals(400.0, assignedMemory, 0.001);\r\n    assertEquals(40.0, assignedCpu, 0.001);\r\n    assertFalse(cluster.needsSchedulingRas(topology1));\r\n    String expectedStatusPrefix = \"Running - Fully Scheduled by DefaultResourceAwareStrategy\";\r\n    assertTrue(cluster.getStatusMap().get(topology1.getId()).startsWith(expectedStatusPrefix));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testResourceLimitation",
  "sourceCode" : "@Test\r\npublic void testResourceLimitation() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(2, 2, 400, 2000);\r\n    // a topology with multiple spouts\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout\", new TestWordSpout(), 2).setCPULoad(250.0).setMemoryLoad(1000.0, 200.0);\r\n    builder1.setBolt(\"wordCountBolt\", new TestWordCounter(), 1).shuffleGrouping(\"wordSpout\").setCPULoad(100.0).setMemoryLoad(500.0, 100.0);\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config = new Config();\r\n    config.putAll(defaultTopologyConf);\r\n    Map<ExecutorDetails, String> executorMap1 = genExecsAndComps(stormTopology1);\r\n    TopologyDetails topology1 = new TopologyDetails(\"topology1\", config, stormTopology1, 2, executorMap1, 0, \"user\");\r\n    ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n    scheduler = rs;\r\n    Topologies topologies = new Topologies(topology1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n    rs.prepare(config, new StormMetricsRegistry());\r\n    rs.schedule(topologies, cluster);\r\n    SchedulerAssignment assignment1 = cluster.getAssignmentById(topology1.getId());\r\n    Set<WorkerSlot> assignedSlots1 = assignment1.getSlots();\r\n    Set<String> nodesIDs1 = new HashSet<>();\r\n    for (WorkerSlot slot : assignedSlots1) {\r\n        nodesIDs1.add(slot.getNodeId());\r\n    }\r\n    Collection<ExecutorDetails> executors1 = assignment1.getExecutors();\r\n    List<Double> assignedExecutorMemory = new ArrayList<>();\r\n    List<Double> assignedExecutorCpu = new ArrayList<>();\r\n    for (ExecutorDetails executor : executors1) {\r\n        assignedExecutorMemory.add(topology1.getTotalMemReqTask(executor));\r\n        assignedExecutorCpu.add(topology1.getTotalCpuReqTask(executor));\r\n    }\r\n    Collections.sort(assignedExecutorCpu);\r\n    Collections.sort(assignedExecutorMemory);\r\n    Map<ExecutorDetails, SupervisorDetails> executorToSupervisor = new HashMap<>();\r\n    Map<SupervisorDetails, List<ExecutorDetails>> supervisorToExecutors = new HashMap<>();\r\n    Map<Double, Double> cpuAvailableToUsed = new HashMap<>();\r\n    Map<Double, Double> memoryAvailableToUsed = new HashMap<>();\r\n    for (Map.Entry<ExecutorDetails, WorkerSlot> entry : assignment1.getExecutorToSlot().entrySet()) {\r\n        executorToSupervisor.put(entry.getKey(), cluster.getSupervisorById(entry.getValue().getNodeId()));\r\n    }\r\n    for (Map.Entry<ExecutorDetails, SupervisorDetails> entry : executorToSupervisor.entrySet()) {\r\n        supervisorToExecutors.computeIfAbsent(entry.getValue(), k -> new ArrayList<>()).add(entry.getKey());\r\n    }\r\n    for (Map.Entry<SupervisorDetails, List<ExecutorDetails>> entry : supervisorToExecutors.entrySet()) {\r\n        Double supervisorTotalCpu = entry.getKey().getTotalCpu();\r\n        Double supervisorTotalMemory = entry.getKey().getTotalMemory();\r\n        Double supervisorUsedCpu = 0.0;\r\n        Double supervisorUsedMemory = 0.0;\r\n        for (ExecutorDetails executor : entry.getValue()) {\r\n            supervisorUsedMemory += topology1.getTotalCpuReqTask(executor);\r\n            supervisorTotalCpu += topology1.getTotalMemReqTask(executor);\r\n        }\r\n        cpuAvailableToUsed.put(supervisorTotalCpu, supervisorUsedCpu);\r\n        memoryAvailableToUsed.put(supervisorTotalMemory, supervisorUsedMemory);\r\n    }\r\n    // executor0 resides one one worker (on one), executor1 and executor2 on another worker (on the other node)\r\n    assertEquals(2, assignedSlots1.size());\r\n    assertEquals(2, nodesIDs1.size());\r\n    assertEquals(3, executors1.size());\r\n    assertEquals(100.0, assignedExecutorCpu.get(0), 0.001);\r\n    assertEquals(250.0, assignedExecutorCpu.get(1), 0.001);\r\n    assertEquals(250.0, assignedExecutorCpu.get(2), 0.001);\r\n    assertEquals(600.0, assignedExecutorMemory.get(0), 0.001);\r\n    assertEquals(1200.0, assignedExecutorMemory.get(1), 0.001);\r\n    assertEquals(1200.0, assignedExecutorMemory.get(2), 0.001);\r\n    for (Map.Entry<Double, Double> entry : memoryAvailableToUsed.entrySet()) {\r\n        assertTrue(entry.getKey() - entry.getValue() >= 0);\r\n    }\r\n    for (Map.Entry<Double, Double> entry : cpuAvailableToUsed.entrySet()) {\r\n        assertTrue(entry.getKey() - entry.getValue() >= 0);\r\n    }\r\n    assertFalse(cluster.needsSchedulingRas(topology1));\r\n    assertTrue(cluster.getStatusMap().get(topology1.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testScheduleResilience",
  "sourceCode" : "@Test\r\npublic void testScheduleResilience() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(2, 2, 400, 2000);\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout1\", new TestWordSpout(), 3);\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config1 = new Config();\r\n    config1.putAll(defaultTopologyConf);\r\n    Map<ExecutorDetails, String> executorMap1 = genExecsAndComps(stormTopology1);\r\n    TopologyDetails topology1 = new TopologyDetails(\"topology1\", config1, stormTopology1, 3, executorMap1, 0, \"user\");\r\n    TopologyBuilder builder2 = new TopologyBuilder();\r\n    builder2.setSpout(\"wordSpout2\", new TestWordSpout(), 2);\r\n    StormTopology stormTopology2 = builder2.createTopology();\r\n    Config config2 = new Config();\r\n    config2.putAll(defaultTopologyConf);\r\n    // memory requirement is large enough so that two executors can not be fully assigned to one node\r\n    config2.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 1280.0);\r\n    Map<ExecutorDetails, String> executorMap2 = genExecsAndComps(stormTopology2);\r\n    TopologyDetails topology2 = new TopologyDetails(\"topology2\", config2, stormTopology2, 2, executorMap2, 0, \"user\");\r\n    // Test1: When a worker fails, RAS does not alter existing assignments on healthy workers\r\n    scheduler = new ResourceAwareScheduler();\r\n    Topologies topologies = new Topologies(topology2);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config1);\r\n    scheduler.prepare(config1, new StormMetricsRegistry());\r\n    scheduler.schedule(topologies, cluster);\r\n    SchedulerAssignment assignment = cluster.getAssignmentById(topology2.getId());\r\n    // pick a worker to mock as failed\r\n    WorkerSlot failedWorker = new ArrayList<>(assignment.getSlots()).get(0);\r\n    Map<ExecutorDetails, WorkerSlot> executorToSlot = assignment.getExecutorToSlot();\r\n    List<ExecutorDetails> failedExecutors = new ArrayList<>();\r\n    for (Map.Entry<ExecutorDetails, WorkerSlot> entry : executorToSlot.entrySet()) {\r\n        if (entry.getValue().equals(failedWorker)) {\r\n            failedExecutors.add(entry.getKey());\r\n        }\r\n    }\r\n    for (ExecutorDetails executor : failedExecutors) {\r\n        // remove executor details assigned to the failed worker\r\n        executorToSlot.remove(executor);\r\n    }\r\n    Map<ExecutorDetails, WorkerSlot> copyOfOldMapping = new HashMap<>(executorToSlot);\r\n    Set<ExecutorDetails> healthyExecutors = copyOfOldMapping.keySet();\r\n    scheduler.schedule(topologies, cluster);\r\n    SchedulerAssignment newAssignment = cluster.getAssignmentById(topology2.getId());\r\n    Map<ExecutorDetails, WorkerSlot> newExecutorToSlot = newAssignment.getExecutorToSlot();\r\n    for (ExecutorDetails executor : healthyExecutors) {\r\n        assertEquals(copyOfOldMapping.get(executor), newExecutorToSlot.get(executor));\r\n    }\r\n    assertFalse(cluster.needsSchedulingRas(topology2));\r\n    assertTrue(cluster.getStatusMap().get(topology2.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n    // end of Test1\r\n    // Test2: When a supervisor fails, RAS does not alter existing assignments\r\n    executorToSlot = new HashMap<>();\r\n    executorToSlot.put(new ExecutorDetails(0, 0), new WorkerSlot(\"r000s000\", 0));\r\n    executorToSlot.put(new ExecutorDetails(1, 1), new WorkerSlot(\"r000s000\", 1));\r\n    executorToSlot.put(new ExecutorDetails(2, 2), new WorkerSlot(\"r000s001\", 1));\r\n    Map<String, SchedulerAssignment> existingAssignments = new HashMap<>();\r\n    assignment = new SchedulerAssignmentImpl(topology1.getId(), executorToSlot, null, null);\r\n    existingAssignments.put(topology1.getId(), assignment);\r\n    copyOfOldMapping = new HashMap<>(executorToSlot);\r\n    Set<ExecutorDetails> existingExecutors = copyOfOldMapping.keySet();\r\n    Map<String, SupervisorDetails> supMap1 = new HashMap<>(supMap);\r\n    // mock the supervisor r000s000 as a failed supervisor\r\n    supMap1.remove(\"r000s000\");\r\n    topologies = new Topologies(topology1);\r\n    Cluster cluster1 = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap1, existingAssignments, topologies, config1);\r\n    scheduler.schedule(topologies, cluster1);\r\n    newAssignment = cluster1.getAssignmentById(topology1.getId());\r\n    newExecutorToSlot = newAssignment.getExecutorToSlot();\r\n    for (ExecutorDetails executor : existingExecutors) {\r\n        assertEquals(copyOfOldMapping.get(executor), newExecutorToSlot.get(executor));\r\n    }\r\n    assertEquals(\"Fully Scheduled\", cluster1.getStatusMap().get(topology1.getId()));\r\n    // end of Test2\r\n    // Test3: When a supervisor and a worker on it fails, RAS does not alter existing assignments\r\n    executorToSlot = new HashMap<>();\r\n    // the worker to orphan\r\n    executorToSlot.put(new ExecutorDetails(0, 0), new WorkerSlot(\"r000s000\", 1));\r\n    // the worker that fails\r\n    executorToSlot.put(new ExecutorDetails(1, 1), new WorkerSlot(\"r000s000\", 2));\r\n    // the healthy worker\r\n    executorToSlot.put(new ExecutorDetails(2, 2), new WorkerSlot(\"r000s001\", 1));\r\n    existingAssignments = new HashMap<>();\r\n    assignment = new SchedulerAssignmentImpl(topology1.getId(), executorToSlot, null, null);\r\n    existingAssignments.put(topology1.getId(), assignment);\r\n    // delete one worker of r000s000 (failed) from topo1 assignment to enable actual schedule for testing\r\n    executorToSlot.remove(new ExecutorDetails(1, 1));\r\n    copyOfOldMapping = new HashMap<>(executorToSlot);\r\n    // namely the two eds on the orphaned worker and the healthy worker\r\n    existingExecutors = copyOfOldMapping.keySet();\r\n    supMap1 = new HashMap<>(supMap);\r\n    // mock the supervisor r000s000 as a failed supervisor\r\n    supMap1.remove(\"r000s000\");\r\n    topologies = new Topologies(topology1);\r\n    cluster1 = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap1, existingAssignments, topologies, config1);\r\n    scheduler.schedule(topologies, cluster1);\r\n    newAssignment = cluster1.getAssignmentById(topology1.getId());\r\n    newExecutorToSlot = newAssignment.getExecutorToSlot();\r\n    for (ExecutorDetails executor : existingExecutors) {\r\n        assertEquals(copyOfOldMapping.get(executor), newExecutorToSlot.get(executor));\r\n    }\r\n    assertFalse(cluster1.needsSchedulingRas(topology1));\r\n    assertEquals(\"Fully Scheduled\", cluster1.getStatusMap().get(topology1.getId()));\r\n    // end of Test3\r\n    // Test4: Scheduling a new topology does not disturb other assignments unnecessarily\r\n    topologies = new Topologies(topology1);\r\n    cluster1 = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config1);\r\n    scheduler.schedule(topologies, cluster1);\r\n    assignment = cluster1.getAssignmentById(topology1.getId());\r\n    executorToSlot = assignment.getExecutorToSlot();\r\n    copyOfOldMapping = new HashMap<>(executorToSlot);\r\n    topologies = addTopologies(topologies, topology2);\r\n    cluster1 = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config1);\r\n    scheduler.schedule(topologies, cluster1);\r\n    newAssignment = cluster1.getAssignmentById(topology1.getId());\r\n    newExecutorToSlot = newAssignment.getExecutorToSlot();\r\n    for (ExecutorDetails executor : copyOfOldMapping.keySet()) {\r\n        assertEquals(copyOfOldMapping.get(executor), newExecutorToSlot.get(executor));\r\n    }\r\n    assertFalse(cluster1.needsSchedulingRas(topology1));\r\n    assertFalse(cluster1.needsSchedulingRas(topology2));\r\n    String expectedStatusPrefix = \"Running - Fully Scheduled by DefaultResourceAwareStrategy\";\r\n    assertTrue(cluster1.getStatusMap().get(topology1.getId()).startsWith(expectedStatusPrefix));\r\n    assertTrue(cluster1.getStatusMap().get(topology2.getId()).startsWith(expectedStatusPrefix));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testHeterogeneousClusterwithDefaultRas",
  "sourceCode" : "@Test\r\npublic void testHeterogeneousClusterwithDefaultRas() {\r\n    testHeterogeneousCluster(defaultTopologyConf, DefaultResourceAwareStrategy.class.getSimpleName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testHeterogeneousClusterwithGras",
  "sourceCode" : "@Test\r\npublic void testHeterogeneousClusterwithGras() {\r\n    Config grasClusterConfig = (Config) defaultTopologyConf.clone();\r\n    grasClusterConfig.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, GenericResourceAwareStrategy.class.getName());\r\n    testHeterogeneousCluster(grasClusterConfig, GenericResourceAwareStrategy.class.getSimpleName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testTopologyWorkerMaxHeapSize",
  "sourceCode" : "@Test\r\npublic void testTopologyWorkerMaxHeapSize() {\r\n    // Test1: If RAS spreads executors across multiple workers based on the set limit for a worker used by the topology\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(2, 2, 400, 2000);\r\n    TopologyBuilder builder1 = new TopologyBuilder();\r\n    builder1.setSpout(\"wordSpout1\", new TestWordSpout(), 4);\r\n    StormTopology stormTopology1 = builder1.createTopology();\r\n    Config config1 = new Config();\r\n    config1.putAll(defaultTopologyConf);\r\n    config1.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 128.0);\r\n    Map<ExecutorDetails, String> executorMap1 = genExecsAndComps(stormTopology1);\r\n    TopologyDetails topology1 = new TopologyDetails(\"topology1\", config1, stormTopology1, 1, executorMap1, 0, \"user\");\r\n    ResourceAwareScheduler rs = new ResourceAwareScheduler();\r\n    Topologies topologies = new Topologies(topology1);\r\n    Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config1);\r\n    rs.prepare(config1, new StormMetricsRegistry());\r\n    try {\r\n        rs.schedule(topologies, cluster);\r\n        assertFalse(cluster.needsSchedulingRas(topology1));\r\n        assertTrue(cluster.getStatusMap().get(topology1.getId()).startsWith(\"Running - Fully Scheduled by DefaultResourceAwareStrategy\"));\r\n        assertEquals(4, cluster.getAssignedNumWorkers(topology1));\r\n    } finally {\r\n        rs.cleanup();\r\n    }\r\n    // Test2: test when no more workers are available due to topology worker max heap size limit but there is memory is still available\r\n    // wordSpout2 is going to contain 5 executors that needs scheduling. Each of those executors has a memory requirement of 128.0 MB\r\n    // The cluster contains 4 free WorkerSlots. For this topolology each worker is limited to a max heap size of 128.0\r\n    // Thus, one executor not going to be able to get scheduled thus failing the scheduling of this topology and no executors of this\r\n    // topology will be scheduled\r\n    TopologyBuilder builder2 = new TopologyBuilder();\r\n    builder2.setSpout(\"wordSpout2\", new TestWordSpout(), 5);\r\n    StormTopology stormTopology2 = builder2.createTopology();\r\n    Config config2 = new Config();\r\n    config2.putAll(defaultTopologyConf);\r\n    config2.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 128.0);\r\n    Map<ExecutorDetails, String> executorMap2 = genExecsAndComps(stormTopology2);\r\n    TopologyDetails topology2 = new TopologyDetails(\"topology2\", config2, stormTopology2, 1, executorMap2, 0, \"user\");\r\n    topologies = new Topologies(topology2);\r\n    cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config2);\r\n    rs.prepare(config2, new StormMetricsRegistry());\r\n    try {\r\n        rs.schedule(topologies, cluster);\r\n        assertTrue(cluster.needsSchedulingRas(topology2));\r\n        String status = cluster.getStatusMap().get(topology2.getId());\r\n        String expectedStatusPrefix = \"Not enough resources to schedule\";\r\n        assertTrue(status.startsWith(expectedStatusPrefix), \"Expected status to start with \\\"\" + expectedStatusPrefix + \"\\\" but status is: \" + status);\r\n        assertEquals(5, cluster.getUnassignedExecutors(topology2).size());\r\n    } finally {\r\n        rs.cleanup();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testReadInResourceAwareSchedulerUserPools",
  "sourceCode" : "@Test\r\npublic void testReadInResourceAwareSchedulerUserPools() {\r\n    Map<String, Object> fromFile = Utils.findAndReadConfigFile(\"user-resource-pools.yaml\", false);\r\n    LOG.info(\"fromFile: {}\", fromFile);\r\n    ConfigValidation.validateFields(fromFile);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testSubmitUsersWithNoGuarantees",
  "sourceCode" : "@Test\r\npublic void testSubmitUsersWithNoGuarantees() {\r\n    INimbus iNimbus = new INimbusTest();\r\n    Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n    Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 200, 2000));\r\n    for (Class strategyClass : strategyClasses) {\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 10, \"jerry\"), genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 2, 20, \"bobby\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-3\", \"topo-4\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-5\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testMultipleUsers",
  "sourceCode" : "@Test\r\npublic void testMultipleUsers() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        if (strategyClass.getName().equals(RoundRobinResourceAwareStrategy.class.getName())) {\r\n            // exclude RoundRbin from this test\r\n            continue;\r\n        }\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(20, 4, 1000, 1024 * 10);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 1_000, 8_192), userRes(\"bobby\", 10_000, 32_768), userRes(\"derek\", 5_000, 16_384));\r\n        Config config = createClusterConfig(strategyClass, 10, 128, 0, resourceUserPool);\r\n        TopologyDetails[] topos = { genTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-2\", config, 5, 15, 1, 1, currentTime - 8, 29, \"jerry\"), genTopology(\"topo-3\", config, 5, 15, 1, 1, currentTime - 16, 29, \"jerry\"), genTopology(\"topo-4\", config, 5, 15, 1, 1, currentTime - 16, 20, \"jerry\"), genTopology(\"topo-5\", config, 5, 15, 1, 1, currentTime - 24, 29, \"jerry\"), genTopology(\"topo-6\", config, 5, 15, 1, 1, currentTime - 2, 20, \"bobby\"), genTopology(\"topo-7\", config, 5, 15, 1, 1, currentTime - 8, 29, \"bobby\"), genTopology(\"topo-8\", config, 5, 15, 1, 1, currentTime - 16, 29, \"bobby\"), genTopology(\"topo-9\", config, 5, 15, 1, 1, currentTime - 16, 20, \"bobby\"), genTopology(\"topo-10\", config, 5, 15, 1, 1, currentTime - 24, 29, \"bobby\"), genTopology(\"topo-11\", config, 5, 15, 1, 1, currentTime - 2, 20, \"derek\"), genTopology(\"topo-12\", config, 5, 15, 1, 1, currentTime - 8, 29, \"derek\"), genTopology(\"topo-13\", config, 5, 15, 1, 1, currentTime - 16, 29, \"derek\"), genTopology(\"topo-14\", config, 5, 15, 1, 1, currentTime - 16, 20, \"derek\"), genTopology(\"topo-15\", config, 5, 15, 1, 1, currentTime - 24, 29, \"derek\") };\r\n        Topologies topologies = new Topologies(topos);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, topos.length);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testHandlingClusterSubscription",
  "sourceCode" : "@Test\r\npublic void testHandlingClusterSubscription() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 4, 200, 1024 * 10);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 1_000, 8_192), userRes(\"bobby\", 10_000, 32_768), userRes(\"derek\", 5_000, 16_384));\r\n        Config config = createClusterConfig(strategyClass, 10, 128, 0, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 5, 15, 1, 1, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-2\", config, 5, 15, 1, 1, currentTime - 8, 29, \"jerry\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-2\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testFaultTolerance",
  "sourceCode" : "/**\r\n * Test correct behavior when a supervisor dies.  Check if the scheduler handles it correctly and evicts the correct\r\n * topology when rescheduling the executors from the died supervisor\r\n */\r\n@Test\r\npublic void testFaultTolerance() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(6, 4, 100, 1000);\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"jerry\", 50, 500), userRes(\"bobby\", 200, 2_000), userRes(\"derek\", 100, 1_000));\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, resourceUserPool);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 1, 0, currentTime - 2, 21, \"jerry\"), genTopology(\"topo-2\", config, 1, 0, 1, 0, currentTime - 2, 20, \"jerry\"), genTopology(\"topo-3\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-4\", config, 1, 0, 1, 0, currentTime - 2, 10, \"bobby\"), genTopology(\"topo-5\", config, 1, 0, 1, 0, currentTime - 2, 29, \"derek\"), genTopology(\"topo-6\", config, 1, 0, 1, 0, currentTime - 2, 10, \"derek\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-1\", \"topo-2\", \"topo-3\", \"topo-4\", \"topo-5\", \"topo-6\");\r\n        //fail supervisor\r\n        SupervisorDetails supFailed = cluster.getSupervisors().values().iterator().next();\r\n        LOG.info(\"/***** failing supervisor: {} ****/\", supFailed.getHost());\r\n        supMap.remove(supFailed.getId());\r\n        Map<String, SchedulerAssignmentImpl> newAssignments = new HashMap<>();\r\n        for (Map.Entry<String, SchedulerAssignment> topoToAssignment : cluster.getAssignments().entrySet()) {\r\n            String topoId = topoToAssignment.getKey();\r\n            SchedulerAssignment assignment = topoToAssignment.getValue();\r\n            Map<ExecutorDetails, WorkerSlot> executorToSlots = new HashMap<>();\r\n            for (Map.Entry<ExecutorDetails, WorkerSlot> execToWorker : assignment.getExecutorToSlot().entrySet()) {\r\n                ExecutorDetails exec = execToWorker.getKey();\r\n                WorkerSlot ws = execToWorker.getValue();\r\n                if (!ws.getNodeId().equals(supFailed.getId())) {\r\n                    executorToSlots.put(exec, ws);\r\n                }\r\n            }\r\n            newAssignments.put(topoId, new SchedulerAssignmentImpl(topoId, executorToSlots, null, null));\r\n        }\r\n        Map<String, String> statusMap = cluster.getStatusMap();\r\n        LOG.warn(\"Rescheduling with removed Supervisor....\");\r\n        cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, newAssignments, topologies, config);\r\n        cluster.setStatusMap(statusMap);\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, \"topo-2\", \"topo-3\", \"topo-4\", \"topo-5\", \"topo-6\");\r\n        assertTopologiesNotScheduled(cluster, strategyClass, \"topo-1\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testNodeFreeSlot",
  "sourceCode" : "/**\r\n * test if free slots on nodes work correctly\r\n */\r\n@Test\r\npublic void testNodeFreeSlot() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        Topologies topologies = new Topologies(genTopology(\"topo-1\", config, 1, 0, 2, 0, currentTime - 2, 29, \"user\"), genTopology(\"topo-2\", config, 1, 0, 2, 0, currentTime - 2, 10, \"user\"));\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        Map<String, RasNode> nodes = RasNodes.getAllNodesFrom(cluster);\r\n        for (SchedulerAssignment assignment : cluster.getAssignments().values()) {\r\n            for (Entry<WorkerSlot, WorkerResources> entry : new HashMap<>(assignment.getScheduledResources()).entrySet()) {\r\n                WorkerSlot ws = entry.getKey();\r\n                WorkerResources wr = entry.getValue();\r\n                double memoryBefore = nodes.get(ws.getNodeId()).getAvailableMemoryResources();\r\n                double cpuBefore = nodes.get(ws.getNodeId()).getAvailableCpuResources();\r\n                double memoryUsedByWorker = wr.get_mem_on_heap() + wr.get_mem_off_heap();\r\n                assertEquals(1000.0, memoryUsedByWorker, 0.001, \"Check if memory used by worker is calculated correctly\");\r\n                double cpuUsedByWorker = wr.get_cpu();\r\n                assertEquals(100.0, cpuUsedByWorker, 0.001, \"Check if CPU used by worker is calculated correctly\");\r\n                nodes.get(ws.getNodeId()).free(ws);\r\n                double memoryAfter = nodes.get(ws.getNodeId()).getAvailableMemoryResources();\r\n                double cpuAfter = nodes.get(ws.getNodeId()).getAvailableCpuResources();\r\n                assertEquals(memoryBefore + memoryUsedByWorker, memoryAfter, 0.001, \"Check if free correctly frees amount of memory\");\r\n                assertEquals(cpuBefore + cpuUsedByWorker, cpuAfter, 0.001, \"Check if free correctly frees amount of memory\");\r\n                assertFalse(assignment.getSlotToExecutors().containsKey(ws), \"Check if worker was removed from assignments\");\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testSchedulingAfterFailedScheduling",
  "sourceCode" : "/**\r\n * When the first topology failed to be scheduled make sure subsequent schedulings can still succeed\r\n */\r\n@Test\r\npublic void testSchedulingAfterFailedScheduling() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(8, 4, 100, 1000);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 8, 0, 2, 0, currentTime - 2, 10, \"jerry\");\r\n        TopologyDetails topo2 = genTopology(\"topo-2\", config, 2, 0, 2, 0, currentTime - 2, 20, \"jerry\");\r\n        TopologyDetails topo3 = genTopology(\"topo-3\", config, 1, 2, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        Topologies topologies = new Topologies(topo1, topo2, topo3);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertFalse(cluster.getAssignmentById(topo1.getId()) != null, \"Topo-1 unscheduled?\");\r\n        assertTrue(cluster.getAssignmentById(topo2.getId()) != null, \"Topo-2 scheduled?\");\r\n        assertEquals(4, cluster.getAssignmentById(topo2.getId()).getExecutorToSlot().size(), \"Topo-2 all executors scheduled?\");\r\n        assertTrue(cluster.getAssignmentById(topo3.getId()) != null, \"Topo-3 scheduled?\");\r\n        assertEquals(3, cluster.getAssignmentById(topo3.getId()).getExecutorToSlot().size(), \"Topo-3 all executors scheduled?\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "minCpuWorkerJustFits",
  "sourceCode" : "/**\r\n * Min CPU for worker set to 50%.  1 supervisor with 100% CPU.\r\n * A topology with 10 10% components should schedule.\r\n */\r\n@Test\r\npublic void minCpuWorkerJustFits() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 4, 100, 60000);\r\n        Config config = createClusterConfig(strategyClass, 10, 500, 500, null);\r\n        config.put(DaemonConfig.STORM_WORKER_MIN_CPU_PCORE_PERCENT, 50.0);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 10, 0, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        Topologies topologies = new Topologies(topo1);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertFalse(cluster.needsSchedulingRas(topo1));\r\n        assertTrue(cluster.getAssignmentById(topo1.getId()) != null, \"Topo-1 scheduled?\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "minCpuPreventsThirdTopo",
  "sourceCode" : "/**\r\n * Min CPU for worker set to 40%.  1 supervisor with 100% CPU.\r\n * 2 topologies with 2 10% components should schedule.  A third topology should then fail scheduling due to lack of CPU.\r\n */\r\n@Test\r\npublic void minCpuPreventsThirdTopo() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        if (strategyClass.getName().equals(RoundRobinResourceAwareStrategy.class.getName())) {\r\n            // exclude RoundRbin from this test\r\n            continue;\r\n        }\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 4, 100, 60000);\r\n        Config config = createClusterConfig(strategyClass, 10, 500, 500, null);\r\n        config.put(DaemonConfig.STORM_WORKER_MIN_CPU_PCORE_PERCENT, 40.0);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 2, 0, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        TopologyDetails topo2 = genTopology(\"topo-2\", config, 2, 0, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        TopologyDetails topo3 = genTopology(\"topo-3\", config, 2, 0, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        Topologies topologies = new Topologies(topo1, topo2, topo3);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertFalse(cluster.needsSchedulingRas(topo1), \"using \" + strategyClass);\r\n        assertFalse(cluster.needsSchedulingRas(topo2), \"using \" + strategyClass);\r\n        assertTrue(cluster.needsSchedulingRas(topo3), \"using \" + strategyClass);\r\n        assertTrue(cluster.getAssignmentById(topo1.getId()) != null, \"topo-1 scheduled? using \" + strategyClass);\r\n        assertTrue(cluster.getAssignmentById(topo2.getId()) != null, \"topo-2 scheduled? using \" + strategyClass);\r\n        assertFalse(cluster.getAssignmentById(topo3.getId()) != null, \"topo-3 unscheduled? using \" + strategyClass);\r\n        SchedulerAssignment assignment1 = cluster.getAssignmentById(topo1.getId());\r\n        assertEquals(1, assignment1.getSlots().size());\r\n        Map<WorkerSlot, WorkerResources> assignedSlots1 = assignment1.getScheduledResources();\r\n        double assignedCpu = 0.0;\r\n        for (Entry<WorkerSlot, WorkerResources> entry : assignedSlots1.entrySet()) {\r\n            WorkerResources wr = entry.getValue();\r\n            assignedCpu += wr.get_cpu();\r\n        }\r\n        assertEquals(40.0, assignedCpu, 0.001);\r\n        SchedulerAssignment assignment2 = cluster.getAssignmentById(topo2.getId());\r\n        assertEquals(1, assignment2.getSlots().size());\r\n        Map<WorkerSlot, WorkerResources> assignedSlots2 = assignment2.getScheduledResources();\r\n        assignedCpu = 0.0;\r\n        for (Entry<WorkerSlot, WorkerResources> entry : assignedSlots2.entrySet()) {\r\n            WorkerResources wr = entry.getValue();\r\n            assignedCpu += wr.get_cpu();\r\n        }\r\n        assertEquals(40.0, assignedCpu, 0.001);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testMinCpuMaxMultipleSupervisors",
  "sourceCode" : "@Test\r\npublic void testMinCpuMaxMultipleSupervisors() {\r\n    int topoCnt = 10;\r\n    for (Class strategyClass : strategyClasses) {\r\n        if (strategyClass.getName().equals(RoundRobinResourceAwareStrategy.class.getName())) {\r\n            // exclude RoundRbin from this test\r\n            continue;\r\n        }\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(3, 4, 300, 60000);\r\n        Config config = createClusterConfig(strategyClass, 5, 50, 50, null);\r\n        config.put(DaemonConfig.STORM_WORKER_MIN_CPU_PCORE_PERCENT, 100.0);\r\n        TopologyDetails[] topos = new TopologyDetails[topoCnt];\r\n        for (int i = 0; i < topoCnt; i++) {\r\n            String topoName = \"topo-\" + i;\r\n            topos[i] = genTopology(topoName, config, 4, 5, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        }\r\n        Topologies topologies = new Topologies(topos);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        // topo-9 will not be scheduled\r\n        assertTopologiesFullyScheduled(cluster, strategyClass, topoCnt - 1);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "minCpuWorkerSplitFails",
  "sourceCode" : "/**\r\n * Min CPU for worker set to 50%.  1 supervisor with 100% CPU.\r\n * A topology with 3 workers should fail scheduling even if under CPU.\r\n */\r\n@Test\r\npublic void minCpuWorkerSplitFails() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(1, 4, 100, 60000);\r\n        Config config = createClusterConfig(strategyClass, 10, 500, 500, null);\r\n        config.put(DaemonConfig.STORM_WORKER_MIN_CPU_PCORE_PERCENT, 50.0);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 10, 0, 1, 1, currentTime - 2, 20, \"jerry\", 2000.0);\r\n        Topologies topologies = new Topologies(topo1);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTrue(cluster.needsSchedulingRas(topo1));\r\n        assertFalse(cluster.getAssignmentById(topo1.getId()) != null, \"Topo-1 unscheduled?\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "TestLargeFragmentedClusterScheduling",
  "sourceCode" : "/**\r\n * Check time to schedule a fragmented cluster using different strategies\r\n *\r\n * Simulate scheduling on a large production cluster. Find the ratio of time to schedule a set of topologies when\r\n * the cluster is empty and when the cluster is nearly full. While the cluster has sufficient resources to schedule\r\n * all topologies, when nearly full the cluster becomes fragmented and some topologies fail to schedule.\r\n */\r\n@Test\r\npublic void TestLargeFragmentedClusterScheduling() {\r\n    /*\r\n        Without fragmentation, the cluster would be able to schedule both topologies on each node. Let's call each node\r\n        with both topologies scheduled as 100% scheduled.\r\n\r\n        We schedule the cluster in 3 blocks of topologies, measuring the time to schedule the blocks. The first, middle\r\n        and last blocks attempt to schedule the following 0-10%, 10%-90%, 90%-100%. The last block has a number of\r\n        scheduling failures due to cluster fragmentation and its time is dominated by attempting to evict topologies.\r\n\r\n        Timing results for scheduling are noisy. As a result, we do multiple runs and use median values for FirstBlock\r\n        and LastBlock times. (somewhere a statistician is crying). The ratio of LastBlock / FirstBlock remains fairly constant.\r\n\r\n\r\n        TestLargeFragmentedClusterScheduling took 91118 ms\r\n        DefaultResourceAwareStrategy, FirstBlock 249.0, LastBlock 1734.0 ratio 6.963855421686747\r\n        GenericResourceAwareStrategy, FirstBlock 215.0, LastBlock 1673.0 ratio 7.78139534883721\r\n        ConstraintSolverStrategy, FirstBlock 279.0, LastBlock 2200.0 ratio 7.885304659498208\r\n\r\n        TestLargeFragmentedClusterScheduling took 98455 ms\r\n        DefaultResourceAwareStrategy, FirstBlock 266.0, LastBlock 1812.0 ratio 6.81203007518797\r\n        GenericResourceAwareStrategy, FirstBlock 235.0, LastBlock 1802.0 ratio 7.6680851063829785\r\n        ConstraintSolverStrategy, FirstBlock 304.0, LastBlock 2320.0 ratio 7.631578947368421\r\n\r\n        TestLargeFragmentedClusterScheduling took 97268 ms\r\n        DefaultResourceAwareStrategy, FirstBlock 251.0, LastBlock 1826.0 ratio 7.274900398406374\r\n        GenericResourceAwareStrategy, FirstBlock 220.0, LastBlock 1719.0 ratio 7.8136363636363635\r\n        ConstraintSolverStrategy, FirstBlock 296.0, LastBlock 2469.0 ratio 8.341216216216216\r\n\r\n        TestLargeFragmentedClusterScheduling took 97963 ms\r\n        DefaultResourceAwareStrategy, FirstBlock 249.0, LastBlock 1788.0 ratio 7.180722891566265\r\n        GenericResourceAwareStrategy, FirstBlock 240.0, LastBlock 1796.0 ratio 7.483333333333333\r\n        ConstraintSolverStrategy, FirstBlock 328.0, LastBlock 2544.0 ratio 7.7560975609756095\r\n\r\n        TestLargeFragmentedClusterScheduling took 93106 ms\r\n        DefaultResourceAwareStrategy, FirstBlock 258.0, LastBlock 1714.0 ratio 6.6434108527131785\r\n        GenericResourceAwareStrategy, FirstBlock 215.0, LastBlock 1692.0 ratio 7.869767441860465\r\n        ConstraintSolverStrategy, FirstBlock 309.0, LastBlock 2342.0 ratio 7.5792880258899675\r\n\r\n        Choose the median value of the values above\r\n        DefaultResourceAwareStrategy    6.96\r\n        GenericResourceAwareStrategy    7.78\r\n        ConstraintSolverStrategy        7.75\r\n        */\r\n    final int numNodes = 500;\r\n    final int numRuns = 5;\r\n    Map<String, Config> strategyToConfigs = new HashMap<>();\r\n    // AcceptedBlockTimeRatios obtained by empirical testing (see comment block above)\r\n    Map<String, Double> strategyToAcceptedBlockTimeRatios = new HashMap<>();\r\n    for (Class strategyClass : strategyClasses) {\r\n        strategyToConfigs.put(strategyClass.getName(), createClusterConfig(strategyClass, 10, 10, 0, null));\r\n        strategyToAcceptedBlockTimeRatios.put(strategyClass.getName(), 6.96);\r\n    }\r\n    strategyToAcceptedBlockTimeRatios.put(DefaultResourceAwareStrategy.class.getName(), 6.96);\r\n    strategyToAcceptedBlockTimeRatios.put(GenericResourceAwareStrategy.class.getName(), 7.78);\r\n    strategyToConfigs.put(ConstraintSolverStrategy.class.getName(), createCSSClusterConfig(10, 10, 0, null));\r\n    strategyToAcceptedBlockTimeRatios.put(ConstraintSolverStrategy.class.getName(), 7.75);\r\n    Map<String, TimeBlockResult> strategyToTimeBlockResults = new HashMap<>();\r\n    // Get first and last block times for multiple runs and strategies\r\n    long startTime = Time.currentTimeMillis();\r\n    for (Entry<String, Config> strategyConfig : strategyToConfigs.entrySet()) {\r\n        TimeBlockResult strategyTimeBlockResult = strategyToTimeBlockResults.computeIfAbsent(strategyConfig.getKey(), (k) -> new TimeBlockResult());\r\n        for (int run = 0; run < numRuns; ++run) {\r\n            TimeBlockResult result = testLargeClusterSchedulingTiming(numNodes, strategyConfig.getValue());\r\n            strategyTimeBlockResult.append(result);\r\n        }\r\n    }\r\n    // Log median ratios for different strategies\r\n    LOG.info(\"TestLargeFragmentedClusterScheduling took {} ms\", Time.currentTimeMillis() - startTime);\r\n    for (Entry<String, TimeBlockResult> strategyResult : strategyToTimeBlockResults.entrySet()) {\r\n        TimeBlockResult strategyTimeBlockResult = strategyResult.getValue();\r\n        double medianFirstBlockTime = getMedianValue(strategyTimeBlockResult.firstBlockTime);\r\n        double medianLastBlockTime = getMedianValue(strategyTimeBlockResult.lastBlockTime);\r\n        double ratio = medianLastBlockTime / medianFirstBlockTime;\r\n        LOG.info(\"{}, FirstBlock {}, LastBlock {} ratio {}\", strategyResult.getKey(), medianFirstBlockTime, medianLastBlockTime, ratio);\r\n    }\r\n    // Check last block scheduling time does not get significantly slower\r\n    for (Entry<String, TimeBlockResult> strategyResult : strategyToTimeBlockResults.entrySet()) {\r\n        TimeBlockResult strategyTimeBlockResult = strategyResult.getValue();\r\n        double medianFirstBlockTime = getMedianValue(strategyTimeBlockResult.firstBlockTime);\r\n        double medianLastBlockTime = getMedianValue(strategyTimeBlockResult.lastBlockTime);\r\n        double ratio = medianLastBlockTime / medianFirstBlockTime;\r\n        double slowSchedulingThreshold = 1.5;\r\n        String msg = \"Strategy \" + strategyResult.getKey() + \" scheduling is significantly slower for mostly full fragmented cluster\\n\";\r\n        double ratioAccepted = strategyToAcceptedBlockTimeRatios.get(strategyResult.getKey());\r\n        msg += String.format(\"Ratio was %.2f (high/low=%.2f/%.2f), max allowed is %.2f (%.2f * %.2f)\", ratio, medianLastBlockTime, medianFirstBlockTime, ratioAccepted * slowSchedulingThreshold, ratioAccepted, slowSchedulingThreshold);\r\n        assertTrue(ratio < slowSchedulingThreshold * ratioAccepted, msg);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testMultipleSpoutsAndCyclicTopologies",
  "sourceCode" : "/**\r\n * Test multiple spouts and cyclic topologies\r\n */\r\n@Test\r\npublic void testMultipleSpoutsAndCyclicTopologies() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        builder.setSpout(\"spout-1\", new TestSpout(), 5);\r\n        builder.setSpout(\"spout-2\", new TestSpout(), 5);\r\n        builder.setBolt(\"bolt-1\", new TestBolt(), 5).shuffleGrouping(\"spout-1\").shuffleGrouping(\"bolt-3\");\r\n        builder.setBolt(\"bolt-2\", new TestBolt(), 5).shuffleGrouping(\"bolt-1\");\r\n        builder.setBolt(\"bolt-3\", new TestBolt(), 5).shuffleGrouping(\"bolt-2\").shuffleGrouping(\"spout-2\");\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(25, 1, 100, 1000);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        StormTopology stormTopology = builder.createTopology();\r\n        config.put(Config.TOPOLOGY_SUBMITTER_USER, \"jerry\");\r\n        TopologyDetails topo = new TopologyDetails(\"topo-1\", config, stormTopology, 0, genExecsAndComps(stormTopology), 0, \"jerry\");\r\n        Topologies topologies = new Topologies(topo);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<String, SchedulerAssignmentImpl>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertTrue(cluster.getAssignmentById(topo.getId()) != null, \"Topo scheduled?\");\r\n        assertEquals(25, cluster.getAssignmentById(topo.getId()).getExecutorToSlot().size(), \"Topo all executors scheduled?\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testSchedulerStrategyWhitelist",
  "sourceCode" : "@Test\r\npublic void testSchedulerStrategyWhitelist() {\r\n    Map<String, Object> config = ConfigUtils.readStormConfig();\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        String allowed = strategyClassName;\r\n        config.put(Config.NIMBUS_SCHEDULER_STRATEGY_CLASS_WHITELIST, Arrays.asList(allowed));\r\n        Object sched = ReflectionUtils.newSchedulerStrategyInstance(allowed, config);\r\n        assertEquals(sched.getClass().getName(), allowed);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testSchedulerStrategyWhitelistException",
  "sourceCode" : "@Test\r\npublic void testSchedulerStrategyWhitelistException() {\r\n    Map<String, Object> config = ConfigUtils.readStormConfig();\r\n    String allowed = \"org.apache.storm.scheduler.resource.strategies.scheduling.SomeNonExistantStrategy\";\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        String notAllowed = strategyClassName;\r\n        config.put(Config.NIMBUS_SCHEDULER_STRATEGY_CLASS_WHITELIST, Arrays.asList(allowed));\r\n        Assertions.assertThrows(DisallowedStrategyException.class, () -> ReflectionUtils.newSchedulerStrategyInstance(notAllowed, config));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testSchedulerStrategyEmptyWhitelist",
  "sourceCode" : "@Test\r\npublic void testSchedulerStrategyEmptyWhitelist() {\r\n    Map<String, Object> config = ConfigUtils.readStormConfig();\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        String allowed = strategyClassName;\r\n        Object sched = ReflectionUtils.newSchedulerStrategyInstance(allowed, config);\r\n        assertEquals(sched.getClass().getName(), allowed);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testLargeTopologiesOnLargeClusters",
  "sourceCode" : "@PerformanceTest\r\n@Test\r\npublic void testLargeTopologiesOnLargeClusters() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        String strategyClassName = strategyClass.getName();\r\n        Assertions.assertTimeoutPreemptively(Duration.ofSeconds(30), () -> testLargeTopologiesCommon(strategyClassName, false, 1));\r\n    }\r\n}",
  "annotations" : [ "PerformanceTest", "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testLargeTopologiesOnLargeClustersGras",
  "sourceCode" : "@PerformanceTest\r\n@Test\r\npublic void testLargeTopologiesOnLargeClustersGras() {\r\n    Assertions.assertTimeoutPreemptively(Duration.ofSeconds(75), () -> testLargeTopologiesCommon(GenericResourceAwareStrategy.class.getName(), true, 1));\r\n}",
  "annotations" : [ "PerformanceTest", "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testLargeTopologiesOnLargeClustersRoundRobin",
  "sourceCode" : "@PerformanceTest\r\n@Test\r\npublic void testLargeTopologiesOnLargeClustersRoundRobin() {\r\n    Assertions.assertTimeoutPreemptively(Duration.ofSeconds(30), () -> testLargeTopologiesCommon(RoundRobinResourceAwareStrategy.class.getName(), true, 1));\r\n}",
  "annotations" : [ "PerformanceTest", "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestResourceAwareScheduler.java",
  "methodName" : "testStrategyTakingTooLong",
  "sourceCode" : "@Test\r\npublic void testStrategyTakingTooLong() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(8, 4, 100, 1000);\r\n        Config config = createClusterConfig(strategyClass, 100, 500, 500, null);\r\n        List<String> allowedSchedulerStrategies = new ArrayList<>();\r\n        allowedSchedulerStrategies.add(DefaultResourceAwareStrategy.class.getName());\r\n        allowedSchedulerStrategies.add(DefaultResourceAwareStrategyOld.class.getName());\r\n        allowedSchedulerStrategies.add(GenericResourceAwareStrategy.class.getName());\r\n        allowedSchedulerStrategies.add(GenericResourceAwareStrategyOld.class.getName());\r\n        allowedSchedulerStrategies.add(RoundRobinResourceAwareStrategy.class.getName());\r\n        allowedSchedulerStrategies.add(NeverEndingSchedulingStrategy.class.getName());\r\n        config.put(Config.NIMBUS_SCHEDULER_STRATEGY_CLASS_WHITELIST, allowedSchedulerStrategies);\r\n        config.put(DaemonConfig.SCHEDULING_TIMEOUT_SECONDS_PER_TOPOLOGY, 30);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 1, 0, 2, 0, currentTime - 2, 10, \"jerry\");\r\n        TopologyDetails topo3 = genTopology(\"topo-3\", config, 1, 2, 1, 1, currentTime - 2, 20, \"jerry\");\r\n        config.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, NeverEndingSchedulingStrategy.class.getName());\r\n        TopologyDetails topo2 = genTopology(\"topo-2\", config, 2, 0, 2, 0, currentTime - 2, 20, \"jerry\");\r\n        Topologies topologies = new Topologies(topo1, topo2, topo3);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        scheduler = new ResourceAwareScheduler();\r\n        scheduler.prepare(config, new StormMetricsRegistry());\r\n        scheduler.schedule(topologies, cluster);\r\n        assertFalse(cluster.needsSchedulingRas(topo1));\r\n        assertTrue(cluster.needsSchedulingRas(topo2));\r\n        assertFalse(cluster.needsSchedulingRas(topo3));\r\n        assertTrue(cluster.getAssignmentById(topo1.getId()) != null, \"Topo-1 scheduled?\");\r\n        assertEquals(2, cluster.getAssignmentById(topo1.getId()).getExecutorToSlot().size(), \"Topo-1 all executors scheduled?\");\r\n        assertTrue(cluster.getAssignmentById(topo2.getId()) == null, \"Topo-2 not scheduled\");\r\n        assertEquals(\"Scheduling took too long for \" + topo2.getId() + \" using strategy \" + NeverEndingSchedulingStrategy.class.getName() + \" timeout after 30 seconds using config scheduling.timeout.seconds.per.topology.\", cluster.getStatusMap().get(topo2.getId()));\r\n        assertTrue(cluster.getAssignmentById(topo3.getId()) != null, \"Topo-3 scheduled?\");\r\n        assertEquals(3, cluster.getAssignmentById(topo3.getId()).getExecutorToSlot().size(), \"Topo-3 all executors scheduled?\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\resource\\TestUser.java",
  "methodName" : "testResourcePoolUtilization",
  "sourceCode" : "@Test\r\npublic void testResourcePoolUtilization() {\r\n    for (Class strategyClass : strategyClasses) {\r\n        INimbus iNimbus = new INimbusTest();\r\n        Map<String, SupervisorDetails> supMap = genSupervisors(4, 4, 100, 1000);\r\n        double cpuGuarantee = 400.0;\r\n        double memoryGuarantee = 1000.0;\r\n        Map<String, Map<String, Number>> resourceUserPool = userResourcePool(userRes(\"user1\", cpuGuarantee, memoryGuarantee));\r\n        Config config = createClusterConfig(strategyClass, 100, 200, 200, resourceUserPool);\r\n        TopologyDetails topo1 = genTopology(\"topo-1\", config, 1, 1, 2, 1, Time.currentTimeSecs() - 24, 9, \"user1\");\r\n        Topologies topologies = new Topologies(topo1);\r\n        Cluster cluster = new Cluster(iNimbus, new ResourceMetrics(new StormMetricsRegistry()), supMap, new HashMap<>(), topologies, config);\r\n        User user1 = new User(\"user1\", toDouble(resourceUserPool.get(\"user1\")));\r\n        WorkerSlot slot = cluster.getAvailableSlots().get(0);\r\n        cluster.assign(slot, topo1.getId(), topo1.getExecutors());\r\n        assertEquals(cpuGuarantee, user1.getCpuResourceGuaranteed(), 0.001, \"check cpu resource guarantee\");\r\n        assertEquals(memoryGuarantee, user1.getMemoryResourceGuaranteed(), 0.001, \"check memory resource guarantee\");\r\n        assertEquals(((100.0 * 3.0) / cpuGuarantee), user1.getCpuResourcePoolUtilization(cluster), 0.001, \"check cpu resource pool utilization\");\r\n        assertEquals(((200.0 + 200.0) * 3.0) / memoryGuarantee, user1.getMemoryResourcePoolUtilization(cluster), 0.001, \"check memory resource pool utilization\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\ArtifactoryConfigLoaderTest.java",
  "methodName" : "testInvalidConfig",
  "sourceCode" : "@Test\r\npublic void testInvalidConfig() {\r\n    Config conf = new Config();\r\n    ArtifactoryConfigLoaderMock loaderMock = new ArtifactoryConfigLoaderMock(conf);\r\n    Map<String, Object> ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNull(ret, \"Unexpectedly returned not null\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\ArtifactoryConfigLoaderTest.java",
  "methodName" : "testPointingAtDirectory",
  "sourceCode" : "@Test\r\npublic void testPointingAtDirectory() {\r\n    // This is a test where we are configured to point right at an artifact dir\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, ARTIFACTORY_HTTP_SCHEME_PREFIX + \"bogushost.yahoo.com:9999/location/of/this/dir\");\r\n    conf.put(Config.STORM_LOCAL_DIR, tmpDirPath.toString());\r\n    ArtifactoryConfigLoaderMock loaderMock = new ArtifactoryConfigLoaderMock(conf);\r\n    loaderMock.setData(\"Anything\", \"/location/of/this/dir\", \"{\\\"children\\\" : [ { \\\"uri\\\" : \\\"/20160621204337.yaml\\\", \\\"folder\\\" : false }]}\");\r\n    loaderMock.setData(null, null, \"{ \\\"\" + DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS + \"\\\": {one: 1, two: 2, three: 3, four : 4}}\");\r\n    Map<String, Object> ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNotNull(ret, \"Unexpectedly returned null\");\r\n    assertEquals(1, ret.get(\"one\"));\r\n    assertEquals(2, ret.get(\"two\"));\r\n    assertEquals(3, ret.get(\"three\"));\r\n    assertEquals(4, ret.get(\"four\"));\r\n    // Now let's load w/o setting up gets and we should still get valid map back\r\n    ArtifactoryConfigLoaderMock tc2 = new ArtifactoryConfigLoaderMock(conf);\r\n    Map<String, Object> ret2 = tc2.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNotNull(ret2, \"Unexpectedly returned null\");\r\n    assertEquals(1, ret2.get(\"one\"));\r\n    assertEquals(2, ret2.get(\"two\"));\r\n    assertEquals(3, ret2.get(\"three\"));\r\n    assertEquals(4, ret2.get(\"four\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\ArtifactoryConfigLoaderTest.java",
  "methodName" : "testArtifactUpdate",
  "sourceCode" : "@Test\r\npublic void testArtifactUpdate() {\r\n    // This is a test where we are configured to point right at an artifact dir\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, ARTIFACTORY_HTTP_SCHEME_PREFIX + \"bogushost.yahoo.com:9999/location/of/test/dir\");\r\n    conf.put(Config.STORM_LOCAL_DIR, tmpDirPath.toString());\r\n    try (SimulatedTime ignored = new SimulatedTime()) {\r\n        ArtifactoryConfigLoaderMock loaderMock = new ArtifactoryConfigLoaderMock(conf);\r\n        loaderMock.setData(\"Anything\", \"/location/of/test/dir\", \"{\\\"children\\\" : [ { \\\"uri\\\" : \\\"/20160621204337.yaml\\\", \\\"folder\\\" : false }]}\");\r\n        loaderMock.setData(null, null, \"{ \\\"\" + DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS + \"\\\": {one: 1, two: 2, three: 3}}\");\r\n        Map<String, Object> ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret.get(\"one\"));\r\n        assertEquals(2, ret.get(\"two\"));\r\n        assertEquals(3, ret.get(\"three\"));\r\n        assertNull(ret.get(\"four\"), \"Unexpectedly contained \\\"four\\\"\");\r\n        // Now let's load w/o setting up gets, and we should still get valid map back\r\n        ArtifactoryConfigLoaderMock tc2 = new ArtifactoryConfigLoaderMock(conf);\r\n        Map<String, Object> ret2 = tc2.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret2, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret2.get(\"one\"));\r\n        assertEquals(2, ret2.get(\"two\"));\r\n        assertEquals(3, ret2.get(\"three\"));\r\n        assertNull(ret2.get(\"four\"), \"Unexpectedly did not return null\");\r\n        // Now let's update it, but not advance time.  Should get old map again.\r\n        loaderMock.setData(\"Anything\", \"/location/of/test/dir\", \"{\\\"children\\\" : [ { \\\"uri\\\" : \\\"/20160621204999.yaml\\\", \\\"folder\\\" : false }]}\");\r\n        loaderMock.setData(null, null, \"{ \\\"\" + DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS + \"\\\": {one: 1, two: 2, three: 3, four : 4}}\");\r\n        ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret.get(\"one\"));\r\n        assertEquals(2, ret.get(\"two\"));\r\n        assertEquals(3, ret.get(\"three\"));\r\n        assertNull(ret.get(\"four\"), \"Unexpectedly did not return null, not enough time passed!\");\r\n        // Re-load from cached' file.\r\n        ret2 = tc2.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret2, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret2.get(\"one\"));\r\n        assertEquals(2, ret2.get(\"two\"));\r\n        assertEquals(3, ret2.get(\"three\"));\r\n        assertNull(ret2.get(\"four\"), \"Unexpectedly did not return null, last cached result should not have \\\"four\\\"\");\r\n        // Now, let's advance time.\r\n        Time.advanceTime(11 * 60 * 1000);\r\n        ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret.get(\"one\"));\r\n        assertEquals(2, ret.get(\"two\"));\r\n        assertEquals(3, ret.get(\"three\"));\r\n        assertEquals(4, ret.get(\"four\"));\r\n        // Re-load from cached' file.\r\n        ret2 = tc2.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n        assertNotNull(ret2, \"Unexpectedly returned null\");\r\n        assertEquals(1, ret2.get(\"one\"));\r\n        assertEquals(2, ret2.get(\"two\"));\r\n        assertEquals(3, ret2.get(\"three\"));\r\n        assertEquals(4, ret2.get(\"four\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\ArtifactoryConfigLoaderTest.java",
  "methodName" : "testPointingAtSpecificArtifact",
  "sourceCode" : "@Test\r\npublic void testPointingAtSpecificArtifact() {\r\n    // This is a test where we are configured to point right at a single artifact\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, ARTIFACTORY_HTTP_SCHEME_PREFIX + \"bogushost.yahoo.com:9999/location/of/this/artifact\");\r\n    conf.put(Config.STORM_LOCAL_DIR, tmpDirPath.toString());\r\n    ArtifactoryConfigLoaderMock loaderMock = new ArtifactoryConfigLoaderMock(conf);\r\n    loaderMock.setData(\"Anything\", \"/location/of/this/artifact\", \"{ \\\"downloadUri\\\": \\\"anything\\\"}\");\r\n    loaderMock.setData(null, null, \"{ \\\"\" + DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS + \"\\\": {one: 1, two: 2, three: 3}}\");\r\n    Map<String, Object> ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNotNull(ret, \"Unexpectedly returned null\");\r\n    assertEquals(1, ret.get(\"one\"));\r\n    assertEquals(2, ret.get(\"two\"));\r\n    assertEquals(3, ret.get(\"three\"));\r\n    // Now let's load w/o setting up gets and we should still get valid map back\r\n    ArtifactoryConfigLoaderMock tc2 = new ArtifactoryConfigLoaderMock(conf);\r\n    Map<String, Object> ret2 = tc2.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNotNull(ret2, \"Unexpectedly returned null\");\r\n    assertEquals(1, ret2.get(\"one\"));\r\n    assertEquals(2, ret2.get(\"two\"));\r\n    assertEquals(3, ret2.get(\"three\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\ArtifactoryConfigLoaderTest.java",
  "methodName" : "testMalformedYaml",
  "sourceCode" : "@Test\r\npublic void testMalformedYaml() {\r\n    // This is a test where we are configured to point right at a single artifact\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, ARTIFACTORY_HTTP_SCHEME_PREFIX + \"bogushost.yahoo.com:9999/location/of/this/artifact\");\r\n    conf.put(Config.STORM_LOCAL_DIR, tmpDirPath.toString());\r\n    ArtifactoryConfigLoaderMock loaderMock = new ArtifactoryConfigLoaderMock(conf);\r\n    loaderMock.setData(\"Anything\", \"/location/of/this/artifact\", \"{ \\\"downloadUri\\\": \\\"anything\\\"}\");\r\n    loaderMock.setData(null, null, \"ThisIsNotValidYaml\");\r\n    Map<String, Object> ret = loaderMock.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNull(ret, \"Unexpectedly returned a map\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\FileConfigLoaderTest.java",
  "methodName" : "testFileNotThere",
  "sourceCode" : "@Test\r\npublic void testFileNotThere() {\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, FILE_SCHEME_PREFIX + \"/file/not/exist/\");\r\n    FileConfigLoader testLoader = new FileConfigLoader(conf);\r\n    Map<String, Object> result = testLoader.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNull(result, \"Unexpectedly returned a map\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\FileConfigLoaderTest.java",
  "methodName" : "testInvalidConfig",
  "sourceCode" : "@Test\r\npublic void testInvalidConfig() {\r\n    Config conf = new Config();\r\n    FileConfigLoader testLoader = new FileConfigLoader(conf);\r\n    Map<String, Object> result = testLoader.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNull(result, \"Unexpectedly returned a map\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\FileConfigLoaderTest.java",
  "methodName" : "testMalformedYaml",
  "sourceCode" : "@Test\r\npublic void testMalformedYaml() throws Exception {\r\n    File temp = Files.createTempFile(\"FileLoader\", \".yaml\").toFile();\r\n    temp.deleteOnExit();\r\n    FileWriter fw = new FileWriter(temp);\r\n    String outputData = \"ThisIsNotValidYaml\";\r\n    fw.write(outputData, 0, outputData.length());\r\n    fw.flush();\r\n    fw.close();\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, FILE_SCHEME_PREFIX + temp.getCanonicalPath());\r\n    FileConfigLoader testLoader = new FileConfigLoader(conf);\r\n    Map<String, Object> result = testLoader.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNull(result, \"Unexpectedly returned a map\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\scheduler\\utils\\FileConfigLoaderTest.java",
  "methodName" : "testValidFile",
  "sourceCode" : "@Test\r\npublic void testValidFile() throws Exception {\r\n    File temp = Files.createTempFile(\"FileLoader\", \".yaml\").toFile();\r\n    temp.deleteOnExit();\r\n    Map<String, Integer> testMap = new HashMap<>();\r\n    testMap.put(\"a\", 1);\r\n    testMap.put(\"b\", 2);\r\n    testMap.put(\"c\", 3);\r\n    testMap.put(\"d\", 4);\r\n    testMap.put(\"e\", 5);\r\n    Map<String, Map<String, Integer>> confMap = new HashMap<>();\r\n    confMap.put(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS, testMap);\r\n    Yaml yaml = new Yaml();\r\n    FileWriter fw = new FileWriter(temp);\r\n    yaml.dump(confMap, fw);\r\n    fw.flush();\r\n    fw.close();\r\n    Config conf = new Config();\r\n    conf.put(DaemonConfig.SCHEDULER_CONFIG_LOADER_URI, FILE_SCHEME_PREFIX + temp.getCanonicalPath());\r\n    FileConfigLoader loader = new FileConfigLoader(conf);\r\n    Map<String, Object> result = loader.load(DaemonConfig.MULTITENANT_SCHEDULER_USER_POOLS);\r\n    assertNotNull(result, \"Unexpectedly returned null\");\r\n    assertEquals(testMap.keySet().size(), result.keySet().size(), \"Maps are a different size\");\r\n    for (String key : testMap.keySet()) {\r\n        Integer expectedValue = testMap.get(key);\r\n        Integer returnedValue = (Integer) result.get(key);\r\n        assertEquals(expectedValue, returnedValue, \"Bad value for key=\" + key);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "kerbToLocalTest",
  "sourceCode" : "@Test\r\npublic void kerbToLocalTest() {\r\n    KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();\r\n    kptol.prepare(Collections.emptyMap());\r\n    assertEquals(\"me\", kptol.toLocal(mkPrincipal(\"me@realm\")));\r\n    assertEquals(\"simple\", kptol.toLocal(mkPrincipal(\"simple\")));\r\n    assertEquals(\"someone\", kptol.toLocal(mkPrincipal(\"someone/host@realm\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "simpleAuthTest",
  "sourceCode" : "@Test\r\npublic void simpleAuthTest() throws Exception {\r\n    Nimbus.Iface impl = mock(Nimbus.Iface.class);\r\n    withServer(SimpleTransportPlugin.class, impl, (ThriftServer server, Map<String, Object> conf) -> {\r\n        try (NimbusClient client = NimbusClient.Builder.withConf(conf).withTimeout(NIMBUS_TIMEOUT).buildWithNimbusHostPort(\"localhost\", server.getPort())) {\r\n            client.getClient().activate(\"security_auth_test_topology\");\r\n        }\r\n        //Verify digest is rejected...\r\n        Map<String, Object> badConf = new HashMap<>(conf);\r\n        badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());\r\n        badConf.put(\"java.security.auth.login.config\", DIGEST_JAAS_CONF);\r\n        badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);\r\n        try (NimbusClient client = NimbusClient.Builder.withConf(badConf).withTimeout(NIMBUS_TIMEOUT).buildWithNimbusHostPort(\"localhost\", server.getPort())) {\r\n            client.getClient().activate(\"bad_security_auth_test_topology\");\r\n            fail(\"An exception should have been thrown trying to connect.\");\r\n        } catch (Exception e) {\r\n            LOG.info(\"Got Exception...\", e);\r\n            if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\r\n                throw new AssertionError(\"Expecting TTransportException but got \" + e.getClass().getName(), e);\r\n            }\r\n        }\r\n    });\r\n    verify(impl).activate(\"security_auth_test_topology\");\r\n    verify(impl, never()).activate(\"bad_security_auth_test_topology\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "digestAuthTest",
  "sourceCode" : "@Test\r\npublic void digestAuthTest() throws Exception {\r\n    Nimbus.Iface impl = mock(Nimbus.Iface.class);\r\n    final AtomicReference<ReqContext> user = new AtomicReference<>();\r\n    doAnswer((invocation) -> {\r\n        user.set(new ReqContext(ReqContext.context()));\r\n        return null;\r\n    }).when(impl).activate(anyString());\r\n    withServer(DIGEST_JAAS_CONF, DigestSaslTransportPlugin.class, impl, (ThriftServer server, Map<String, Object> conf) -> {\r\n        try (NimbusClient client = NimbusClient.Builder.withConf(conf).withTimeout(NIMBUS_TIMEOUT).buildWithNimbusHostPort(\"localhost\", server.getPort())) {\r\n            client.getClient().activate(\"security_auth_test_topology\");\r\n        }\r\n        conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);\r\n        //Verify simple is rejected...\r\n        Map<String, Object> badTransport = new HashMap<>(conf);\r\n        badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());\r\n        try (NimbusClient client = NimbusClient.Builder.withConf(badTransport).withTimeout(NIMBUS_TIMEOUT).buildWithNimbusHostPort(\"localhost\", server.getPort())) {\r\n            client.getClient().activate(\"bad_security_auth_test_topology\");\r\n            fail(\"An exception should have been thrown trying to connect.\");\r\n        } catch (Exception e) {\r\n            LOG.info(\"Got Exception...\", e);\r\n            if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\r\n                throw new AssertionError(\"Expecting TTransportException but got \" + e.getClass().getName(), e);\r\n            }\r\n        }\r\n        //The user here from the jaas conf is bob.  No impersonation is done, so verify that\r\n        ReqContext found = user.get();\r\n        assertNotNull(found);\r\n        assertEquals(\"bob\", found.principal().getName());\r\n        assertFalse(found.isImpersonating());\r\n        user.set(null);\r\n        verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);\r\n        verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);\r\n        verifyIncorrectJaasConf(server, conf, \"./nonexistent.conf\", RuntimeException.class);\r\n        verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);\r\n    });\r\n    verify(impl).activate(\"security_auth_test_topology\");\r\n    verify(impl, never()).activate(\"bad_auth_test_topology\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "workerTokenDigestAuthTest",
  "sourceCode" : "@Test\r\npublic void workerTokenDigestAuthTest() throws Exception {\r\n    LOG.info(\"\\n\\n\\t\\tworkerTokenDigestAuthTest - START\\n\\n\");\r\n    Nimbus.Iface impl = mock(Nimbus.Iface.class);\r\n    final AtomicReference<ReqContext> user = new AtomicReference<>();\r\n    doAnswer((invocation) -> {\r\n        user.set(new ReqContext(ReqContext.context()));\r\n        return null;\r\n    }).when(impl).activate(anyString());\r\n    Map<String, Object> extraConfs = new HashMap<>();\r\n    //Let worker tokens work on insecure ZK...\r\n    extraConfs.put(\"TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS\", true);\r\n    try (InProcessZookeeper zk = new InProcessZookeeper()) {\r\n        withServer(MISSING_CLIENT, DigestSaslTransportPlugin.class, impl, zk, extraConfs, (ThriftServer server, Map<String, Object> conf) -> {\r\n            try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {\r\n                conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);\r\n                //We cannot connect if there is no client section in the jaas conf...\r\n                try (NimbusClient client = NimbusClient.Builder.withConf(conf).withTimeout(NIMBUS_TIMEOUT).buildWithNimbusHostPort(\"localhost\", server.getPort())) {\r\n                    client.getClient().activate(\"bad_auth_test_topology\");\r\n                    fail(\"We should not be able to connect without a token...\");\r\n                } catch (Exception e) {\r\n                    if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {\r\n                        throw new AssertionError(\"Expecting IOException but got \" + e.getClass().getName(), e);\r\n                    }\r\n                }\r\n                //Now let's create a token and verify that we can connect...\r\n                IStormClusterState state = ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));\r\n                WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);\r\n                Subject bob = testConnectWithTokenFor(wtMan, conf, server, \"bob\", \"topo-bob\");\r\n                verifyUserIs(user, \"bob\");\r\n                Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));\r\n                //Alice has no digest jaas section at all...\r\n                Subject alice = testConnectWithTokenFor(wtMan, conf, server, \"alice\", \"topo-alice\");\r\n                verifyUserIs(user, \"alice\");\r\n                Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));\r\n                //Verify that bob's token has expired\r\n                try {\r\n                    tryConnectAs(conf, server, bob, \"bad_auth_test_topology\");\r\n                    fail(\"We should not be able to connect with bad auth\");\r\n                } catch (Exception e) {\r\n                    if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\r\n                        throw new AssertionError(\"Expecting TTransportException but got \" + e.getClass().getName(), e);\r\n                    }\r\n                }\r\n                tryConnectAs(conf, server, alice, \"topo-alice\");\r\n                verifyUserIs(user, \"alice\");\r\n                //Now see if we can create a new token for bob and try again.\r\n                bob = testConnectWithTokenFor(wtMan, conf, server, \"bob\", \"topo-bob\");\r\n                verifyUserIs(user, \"bob\");\r\n                tryConnectAs(conf, server, alice, \"topo-alice\");\r\n                verifyUserIs(user, \"alice\");\r\n            }\r\n        });\r\n    }\r\n    verify(impl, times(2)).activate(\"topo-bob\");\r\n    verify(impl, times(3)).activate(\"topo-alice\");\r\n    verify(impl, never()).activate(\"bad_auth_test_topology\");\r\n    LOG.info(\"\\n\\n\\t\\tworkerTokenDigestAuthTest - END\\n\\n\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "negativeWhitelistAuthroizationTest",
  "sourceCode" : "@Test\r\npublic void negativeWhitelistAuthroizationTest() {\r\n    SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    auth.prepare(conf);\r\n    ReqContext context = new ReqContext(mkSubject(\"user\"));\r\n    assertFalse(auth.permit(context, \"activate\", conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "positiveWhitelistAuthroizationTest",
  "sourceCode" : "@Test\r\npublic void positiveWhitelistAuthroizationTest() {\r\n    SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList(\"user\"));\r\n    auth.prepare(conf);\r\n    ReqContext context = new ReqContext(mkSubject(\"user\"));\r\n    assertTrue(auth.permit(context, \"activate\", conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "simpleAclUserAuthTest",
  "sourceCode" : "@Test\r\npublic void simpleAclUserAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList(\"admin\"));\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList(\"supervisor\"));\r\n    ReqContext admin = new ReqContext(mkSubject(\"admin\"));\r\n    ReqContext supervisor = new ReqContext(mkSubject(\"supervisor\"));\r\n    ReqContext userA = new ReqContext(mkSubject(\"user-a\"));\r\n    ReqContext userB = new ReqContext(mkSubject(\"user-b\"));\r\n    final Map<String, Object> empty = Collections.emptyMap();\r\n    final Map<String, Object> aAllowed = new HashMap<>();\r\n    aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList(\"user-a\"));\r\n    SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(userA, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(userB, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(admin, \"submitTopology\", empty));\r\n    assertFalse(authorizer.permit(supervisor, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(userA, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(userB, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(admin, \"fileUpload\", null));\r\n    assertFalse(authorizer.permit(supervisor, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(userA, \"getNimbusConf\", null));\r\n    assertTrue(authorizer.permit(userB, \"getNimbusConf\", null));\r\n    assertTrue(authorizer.permit(admin, \"getNimbusConf\", null));\r\n    assertFalse(authorizer.permit(supervisor, \"getNimbusConf\", null));\r\n    assertTrue(authorizer.permit(userA, \"getClusterInfo\", null));\r\n    assertTrue(authorizer.permit(userB, \"getClusterInfo\", null));\r\n    assertTrue(authorizer.permit(admin, \"getClusterInfo\", null));\r\n    assertFalse(authorizer.permit(supervisor, \"getClusterInfo\", null));\r\n    assertFalse(authorizer.permit(userA, \"fileDownload\", null));\r\n    assertFalse(authorizer.permit(userB, \"fileDownload\", null));\r\n    assertTrue(authorizer.permit(admin, \"fileDownload\", null));\r\n    assertTrue(authorizer.permit(supervisor, \"fileDownload\", null));\r\n    assertTrue(authorizer.permit(userA, \"killTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"killTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"killTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"killTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"uploadNewCredentials\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"uploadNewCredentials\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"uploadNewCredentials\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"uploadNewCredentials\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"rebalance\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"rebalance\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"rebalance\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"rebalance\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"activate\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"activate\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"activate\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"activate\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"deactivate\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"deactivate\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"deactivate\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"deactivate\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"getTopologyConf\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"getTopologyConf\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopologyConf\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"getTopologyConf\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"getTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"getTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"getTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"getUserTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"getUserTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getUserTopology\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"getUserTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(userA, \"getTopologyInfo\", aAllowed));\r\n    assertFalse(authorizer.permit(userB, \"getTopologyInfo\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopologyInfo\", aAllowed));\r\n    assertFalse(authorizer.permit(supervisor, \"getTopologyInfo\", aAllowed));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "simpleAclNimbusUsersAuthTest",
  "sourceCode" : "@Test\r\npublic void simpleAclNimbusUsersAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList(\"admin\"));\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList(\"supervisor\"));\r\n    clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList(\"user-a\"));\r\n    ReqContext admin = new ReqContext(mkSubject(\"admin\"));\r\n    ReqContext supervisor = new ReqContext(mkSubject(\"supervisor\"));\r\n    ReqContext userA = new ReqContext(mkSubject(\"user-a\"));\r\n    ReqContext userB = new ReqContext(mkSubject(\"user-b\"));\r\n    final Map<String, Object> empty = Collections.emptyMap();\r\n    SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(userA, \"submitTopology\", empty));\r\n    assertFalse(authorizer.permit(userB, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(admin, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(supervisor, \"fileDownload\", null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "simpleAclNimbusGroupsAuthTest",
  "sourceCode" : "@Test\r\npublic void simpleAclNimbusGroupsAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList(\"admin-group\"));\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList(\"supervisor\"));\r\n    clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList(\"user-a\"));\r\n    clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());\r\n    Map<String, Object> groups = new HashMap<>();\r\n    groups.put(\"admin\", Collections.singleton(\"admin-group\"));\r\n    groups.put(\"not-admin\", Collections.singleton(\"not-admin-group\"));\r\n    Map<String, Object> groupsParams = new HashMap<>();\r\n    groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);\r\n    clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);\r\n    ReqContext admin = new ReqContext(mkSubject(\"admin\"));\r\n    ReqContext notAdmin = new ReqContext(mkSubject(\"not-admin\"));\r\n    ReqContext supervisor = new ReqContext(mkSubject(\"supervisor\"));\r\n    ReqContext userA = new ReqContext(mkSubject(\"user-a\"));\r\n    ReqContext userB = new ReqContext(mkSubject(\"user-b\"));\r\n    final Map<String, Object> empty = Collections.emptyMap();\r\n    SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(userA, \"submitTopology\", empty));\r\n    assertFalse(authorizer.permit(userB, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(admin, \"fileUpload\", null));\r\n    assertFalse(authorizer.permit(notAdmin, \"fileUpload\", null));\r\n    assertFalse(authorizer.permit(userB, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(supervisor, \"fileDownload\", null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "simpleAclSameUserAuthTest",
  "sourceCode" : "@Test\r\npublic void simpleAclSameUserAuthTest() {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList(\"admin\"));\r\n    clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList(\"admin\"));\r\n    ReqContext admin = new ReqContext(mkSubject(\"admin\"));\r\n    final Map<String, Object> empty = Collections.emptyMap();\r\n    final Map<String, Object> aAllowed = new HashMap<>();\r\n    aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList(\"user-a\"));\r\n    SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    assertTrue(authorizer.permit(admin, \"submitTopology\", empty));\r\n    assertTrue(authorizer.permit(admin, \"fileUpload\", null));\r\n    assertTrue(authorizer.permit(admin, \"getNimbusConf\", null));\r\n    assertTrue(authorizer.permit(admin, \"getClusterInfo\", null));\r\n    assertTrue(authorizer.permit(admin, \"fileDownload\", null));\r\n    assertTrue(authorizer.permit(admin, \"killTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"uploadNewCredentials\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"rebalance\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"activate\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopologyConf\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getUserTopology\", aAllowed));\r\n    assertTrue(authorizer.permit(admin, \"getTopologyInfo\", aAllowed));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "shellBaseGroupsMappingTest",
  "sourceCode" : "@Test\r\npublic void shellBaseGroupsMappingTest() throws Exception {\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();\r\n    groups.prepare(clusterConf);\r\n    String userName = System.getProperty(\"user.name\");\r\n    assertTrue(groups.getGroups(userName).size() >= 0);\r\n    assertEquals(0, groups.getGroups(\"userDoesNotExist\").size());\r\n    assertEquals(0, groups.getGroups(null).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "getTransportPluginThrowsRunimeTest",
  "sourceCode" : "@Test\r\npublic void getTransportPluginThrowsRunimeTest() {\r\n    Map<String, Object> conf = ConfigUtils.readStormConfig();\r\n    conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, \"null.invalid\");\r\n    assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\AuthTest.java",
  "methodName" : "impersonationAuthorizerTest",
  "sourceCode" : "@Test\r\npublic void impersonationAuthorizerTest() throws Exception {\r\n    final String impersonatingUser = \"admin\";\r\n    final String userBeingImpersonated = System.getProperty(\"user.name\");\r\n    Map<String, Object> clusterConf = ConfigUtils.readStormConfig();\r\n    ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();\r\n    groupMapper.prepare(clusterConf);\r\n    Set<String> groups = groupMapper.getGroups(userBeingImpersonated);\r\n    InetAddress localHost = InetAddress.getLocalHost();\r\n    Map<String, Object> acl = new HashMap<>();\r\n    Map<String, Object> aclConf = new HashMap<>();\r\n    aclConf.put(\"hosts\", Collections.singletonList(localHost.getHostName()));\r\n    aclConf.put(\"groups\", groups);\r\n    acl.put(impersonatingUser, aclConf);\r\n    clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);\r\n    InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString(\"10.10.10.10\");\r\n    ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();\r\n    authorizer.prepare(clusterConf);\r\n    //non impersonating request, should be permitted.\r\n    assertTrue(authorizer.permit(new ReqContext(mkSubject(\"anyuser\")), \"fileUplaod\", null));\r\n    //user with no impersonation acl should be rejected\r\n    assertFalse(authorizer.permit(mkImpersonatingReqContext(\"user-with-no-acl\", userBeingImpersonated, localHost), \"someOperation\", null));\r\n    //request from hosts that are not authorized should be rejected\r\n    assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost), \"someOperation\", null));\r\n    //request to impersonate users from unauthorized groups should be rejected.\r\n    assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, \"unauthorized-user\", localHost), \"someOperation\", null));\r\n    //request from authorized hosts and group should be allowed.\r\n    assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost), \"someOperation\", null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\DefaultHttpCredentialsPluginTest.java",
  "methodName" : "test_getUserName",
  "sourceCode" : "@Test\r\npublic void test_getUserName() {\r\n    DefaultHttpCredentialsPlugin handler = new DefaultHttpCredentialsPlugin();\r\n    handler.prepare(new HashMap<>());\r\n    assertNull(handler.getUserName((HttpServletRequest) null), \"Should return null when request is null\");\r\n    assertNull(handler.getUserName(Mockito.mock(HttpServletRequest.class)), \"Should return null when user principal is null\");\r\n    HttpServletRequest mockRequest = Mockito.mock(HttpServletRequest.class);\r\n    Mockito.when(mockRequest.getUserPrincipal()).thenReturn(new SingleUserPrincipal(\"\"));\r\n    assertNull(handler.getUserName(mockRequest), \"Should return null when user is blank\");\r\n    String expName = \"Alice\";\r\n    mockRequest = Mockito.mock(HttpServletRequest.class);\r\n    Mockito.when(mockRequest.getUserPrincipal()).thenReturn(new SingleUserPrincipal(expName));\r\n    assertEquals(expName, handler.getUserName(mockRequest), \"Should return correct user from requests principal\");\r\n    try {\r\n        String doAsUserName = \"Bob\";\r\n        mockRequest = Mockito.mock(HttpServletRequest.class);\r\n        Mockito.when(mockRequest.getUserPrincipal()).thenReturn(new SingleUserPrincipal(expName));\r\n        Mockito.when(mockRequest.getHeader(\"doAsUser\")).thenReturn(doAsUserName);\r\n        ReqContext context = handler.populateContext(ReqContext.context(), mockRequest);\r\n        assertTrue(context.isImpersonating());\r\n        assertEquals(expName, context.realPrincipal().getName());\r\n        assertEquals(doAsUserName, context.principal().getName());\r\n    } finally {\r\n        ReqContext.reset();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\DefaultHttpCredentialsPluginTest.java",
  "methodName" : "test_populate_req_context_on_null_user",
  "sourceCode" : "@Test\r\npublic void test_populate_req_context_on_null_user() {\r\n    try {\r\n        DefaultHttpCredentialsPlugin handler = new DefaultHttpCredentialsPlugin();\r\n        handler.prepare(new HashMap<>());\r\n        Subject subject = new Subject(false, ImmutableSet.<Principal>of(new SingleUserPrincipal(\"test\")), new HashSet<>(), new HashSet<>());\r\n        ReqContext context = new ReqContext(subject);\r\n        assertEquals(0, handler.populateContext(context, Mockito.mock(HttpServletRequest.class)).subject().getPrincipals().size());\r\n    } finally {\r\n        ReqContext.reset();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\ServerAuthUtilsTest.java",
  "methodName" : "uiHttpCredentialsPluginTest",
  "sourceCode" : "@Test\r\npublic void uiHttpCredentialsPluginTest() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    conf.put(DaemonConfig.UI_HTTP_CREDS_PLUGIN, AuthUtilsTestMock.class.getName());\r\n    conf.put(DaemonConfig.DRPC_HTTP_CREDS_PLUGIN, AuthUtilsTestMock.class.getName());\r\n    assertSame(ServerAuthUtils.getUiHttpCredentialsPlugin(conf).getClass(), AuthUtilsTestMock.class);\r\n    assertSame(ServerAuthUtils.getDrpcHttpCredentialsPlugin(conf).getClass(), AuthUtilsTestMock.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\workertoken\\WorkerTokenTest.java",
  "methodName" : "testBasicGenerateAndAuthorize",
  "sourceCode" : "@Test\r\npublic void testBasicGenerateAndAuthorize() {\r\n    final AtomicReference<PrivateWorkerKey> privateKey = new AtomicReference<>();\r\n    final String topoId = \"topo-1\";\r\n    final String userName = \"user\";\r\n    final WorkerTokenServiceType type = WorkerTokenServiceType.NIMBUS;\r\n    final long versionNumber = 0L;\r\n    //Simulate time starts out at 0, so we are going to just leave it here.\r\n    try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {\r\n        IStormClusterState mockState = mock(IStormClusterState.class);\r\n        Map<String, Object> conf = new HashMap<>();\r\n        WorkerTokenManager wtm = new WorkerTokenManager(conf, mockState);\r\n        when(mockState.getNextPrivateWorkerKeyVersion(type, topoId)).thenReturn(versionNumber);\r\n        doAnswer((invocation) -> {\r\n            //Save the private worker key away so we can test it too.\r\n            privateKey.set(invocation.getArgument(3));\r\n            return null;\r\n        }).when(mockState).addPrivateWorkerKey(eq(type), eq(topoId), eq(versionNumber), any(PrivateWorkerKey.class));\r\n        //Answer when we ask for a private key...\r\n        when(mockState.getPrivateWorkerKey(type, topoId, versionNumber)).thenAnswer((invocation) -> privateKey.get());\r\n        WorkerToken wt = wtm.createOrUpdateTokenFor(type, userName, topoId);\r\n        verify(mockState).addPrivateWorkerKey(eq(type), eq(topoId), eq(versionNumber), any(PrivateWorkerKey.class));\r\n        assertTrue(wt.is_set_serviceType());\r\n        assertEquals(type, wt.get_serviceType());\r\n        assertTrue(wt.is_set_info());\r\n        assertTrue(wt.is_set_signature());\r\n        PrivateWorkerKey pwk = privateKey.get();\r\n        assertNotNull(pwk);\r\n        assertTrue(pwk.is_set_expirationTimeMillis());\r\n        assertEquals(ONE_DAY_MILLIS, pwk.get_expirationTimeMillis());\r\n        WorkerTokenInfo info = ClientAuthUtils.getWorkerTokenInfo(wt);\r\n        assertTrue(info.is_set_topologyId());\r\n        assertTrue(info.is_set_userName());\r\n        assertTrue(info.is_set_expirationTimeMillis());\r\n        assertTrue(info.is_set_secretVersion());\r\n        assertEquals(topoId, info.get_topologyId());\r\n        assertEquals(userName, info.get_userName());\r\n        assertEquals(ONE_DAY_MILLIS, info.get_expirationTimeMillis());\r\n        assertEquals(versionNumber, info.get_secretVersion());\r\n        try (WorkerTokenAuthorizer wta = new WorkerTokenAuthorizer(type, mockState)) {\r\n            //Verify the signature...\r\n            byte[] signature = wta.getSignedPasswordFor(wt.get_info(), info);\r\n            assertArrayEquals(wt.get_signature(), signature);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\security\\auth\\workertoken\\WorkerTokenTest.java",
  "methodName" : "testExpiration",
  "sourceCode" : "@Test\r\npublic void testExpiration() {\r\n    final AtomicReference<PrivateWorkerKey> privateKey = new AtomicReference<>();\r\n    final String topoId = \"topo-1\";\r\n    final String userName = \"user\";\r\n    final WorkerTokenServiceType type = WorkerTokenServiceType.NIMBUS;\r\n    final long versionNumber = 5L;\r\n    //Simulate time starts out at 0, so we are going to just leave it here.\r\n    try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {\r\n        IStormClusterState mockState = mock(IStormClusterState.class);\r\n        Map<String, Object> conf = new HashMap<>();\r\n        WorkerTokenManager wtm = new WorkerTokenManager(conf, mockState);\r\n        when(mockState.getNextPrivateWorkerKeyVersion(type, topoId)).thenReturn(versionNumber);\r\n        doAnswer((invocation) -> {\r\n            //Save the private worker key away so we can test it too.\r\n            privateKey.set(invocation.getArgument(3));\r\n            return null;\r\n        }).when(mockState).addPrivateWorkerKey(eq(type), eq(topoId), eq(versionNumber), any(PrivateWorkerKey.class));\r\n        //Answer when we ask for a private key...\r\n        when(mockState.getPrivateWorkerKey(type, topoId, versionNumber)).thenAnswer((invocation) -> privateKey.get());\r\n        WorkerToken wt = wtm.createOrUpdateTokenFor(type, userName, topoId);\r\n        verify(mockState).addPrivateWorkerKey(eq(type), eq(topoId), eq(versionNumber), any(PrivateWorkerKey.class));\r\n        assertTrue(wt.is_set_serviceType());\r\n        assertEquals(type, wt.get_serviceType());\r\n        assertTrue(wt.is_set_info());\r\n        assertTrue(wt.is_set_signature());\r\n        PrivateWorkerKey pwk = privateKey.get();\r\n        assertNotNull(pwk);\r\n        assertTrue(pwk.is_set_expirationTimeMillis());\r\n        assertEquals(ONE_DAY_MILLIS, pwk.get_expirationTimeMillis());\r\n        WorkerTokenInfo info = ClientAuthUtils.getWorkerTokenInfo(wt);\r\n        assertTrue(info.is_set_topologyId());\r\n        assertTrue(info.is_set_userName());\r\n        assertTrue(info.is_set_expirationTimeMillis());\r\n        assertTrue(info.is_set_secretVersion());\r\n        assertEquals(topoId, info.get_topologyId());\r\n        assertEquals(userName, info.get_userName());\r\n        assertEquals(ONE_DAY_MILLIS, info.get_expirationTimeMillis());\r\n        assertEquals(versionNumber, info.get_secretVersion());\r\n        //Expire the token\r\n        Time.advanceTime(ONE_DAY_MILLIS + 1);\r\n        try (WorkerTokenAuthorizer wta = new WorkerTokenAuthorizer(type, mockState)) {\r\n            try {\r\n                //Verify the signature...\r\n                wta.getSignedPasswordFor(wt.get_info(), info);\r\n                fail(\"Expected an expired token to not be signed!!!\");\r\n            } catch (IllegalArgumentException ia) {\r\n                //What we want...\r\n            }\r\n        }\r\n        //Verify if WorkerTokenManager recognizes the expired WorkerToken.\r\n        Map<String, String> creds = new HashMap<>();\r\n        ClientAuthUtils.setWorkerToken(creds, wt);\r\n        assertTrue(wtm.shouldRenewWorkerToken(creds, type), \"Expired WorkerToken should be eligible for renewal\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestCgroups.java",
  "methodName" : "testSetupAndTearDown",
  "sourceCode" : "/**\r\n * Test whether cgroups are setup up correctly for use.  Also tests whether Cgroups produces the right command to\r\n * start a worker and cleans up correctly after the worker is shutdown\r\n */\r\n@Test\r\npublic void testSetupAndTearDown() throws IOException {\r\n    Config config = new Config();\r\n    config.putAll(Utils.readDefaultConfig());\r\n    //We don't want to run the test is CGroups are not setup\r\n    assumeTrue(((boolean) config.get(DaemonConfig.STORM_RESOURCE_ISOLATION_PLUGIN_ENABLE)) == true, \"Check if CGroups are setup\");\r\n    assertTrue(stormCgroupHierarchyExists(config), \"Check if STORM_CGROUP_HIERARCHY_DIR exists\");\r\n    assertTrue(stormCgroupSupervisorRootDirExists(config), \"Check if STORM_SUPERVISOR_CGROUP_ROOTDIR exists\");\r\n    CgroupManager manager = new CgroupManager();\r\n    manager.prepare(config);\r\n    String workerId = UUID.randomUUID().toString();\r\n    manager.reserveResourcesForWorker(workerId, 1024, 200, null);\r\n    List<String> commandList = manager.getLaunchCommand(workerId, new ArrayList<>());\r\n    StringBuilder command = new StringBuilder();\r\n    for (String entry : commandList) {\r\n        command.append(entry).append(\" \");\r\n    }\r\n    String correctCommand1 = config.get(DaemonConfig.STORM_CGROUP_CGEXEC_CMD) + \" -g memory,cpu:/\" + config.get(DaemonConfig.STORM_SUPERVISOR_CGROUP_ROOTDIR) + \"/\" + workerId + \" \";\r\n    String correctCommand2 = config.get(DaemonConfig.STORM_CGROUP_CGEXEC_CMD) + \" -g cpu,memory:/\" + config.get(DaemonConfig.STORM_SUPERVISOR_CGROUP_ROOTDIR) + \"/\" + workerId + \" \";\r\n    assertTrue(command.toString().equals(correctCommand1) || command.toString().equals(correctCommand2), \"Check if cgroup launch command is correct\");\r\n    String pathToWorkerCgroupDir = config.get(Config.STORM_CGROUP_HIERARCHY_DIR) + \"/\" + config.get(DaemonConfig.STORM_SUPERVISOR_CGROUP_ROOTDIR) + \"/\" + workerId;\r\n    assertTrue(dirExists(pathToWorkerCgroupDir), \"Check if cgroup directory exists for worker\");\r\n    /* validate cpu settings */\r\n    String pathToCpuShares = pathToWorkerCgroupDir + \"/cpu.shares\";\r\n    assertTrue(fileExists(pathToCpuShares), \"Check if cpu.shares file exists\");\r\n    assertEquals(\"200\", readFileAll(pathToCpuShares), \"Check if the correct value is written into cpu.shares\");\r\n    /* validate memory settings */\r\n    String pathTomemoryLimitInBytes = pathToWorkerCgroupDir + \"/memory.limit_in_bytes\";\r\n    assertTrue(fileExists(pathTomemoryLimitInBytes), \"Check if memory.limit_in_bytes file exists\");\r\n    assertEquals(String.valueOf(1024 * 1024 * 1024), readFileAll(pathTomemoryLimitInBytes), \"Check if the correct value is written into memory.limit_in_bytes\");\r\n    String user = \"dummy-user\";\r\n    int dummyPort = 0;\r\n    manager.cleanup(user, workerId, dummyPort);\r\n    assertFalse(dirExists(pathToWorkerCgroupDir), \"Make sure cgroup was removed properly\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestDaemonConfigValidate.java",
  "methodName" : "testSupervisorSchedulerMetaIsStringMap",
  "sourceCode" : "@Test\r\npublic void testSupervisorSchedulerMetaIsStringMap() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    Map<String, Object> schedulerMeta = new HashMap<>();\r\n    conf.put(DaemonConfig.SUPERVISOR_SCHEDULER_META, schedulerMeta);\r\n    ConfigValidation.validateFields(conf);\r\n    schedulerMeta.put(\"foo\", \"bar\");\r\n    conf.put(DaemonConfig.SUPERVISOR_SCHEDULER_META, schedulerMeta);\r\n    ConfigValidation.validateFields(conf);\r\n    schedulerMeta.put(\"baz\", true);\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf), \"Expected Exception not Thrown\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestDaemonConfigValidate.java",
  "methodName" : "testIsolationSchedulerMachinesIsMap",
  "sourceCode" : "@Test\r\npublic void testIsolationSchedulerMachinesIsMap() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    Map<String, Integer> isolationMap = new HashMap<>();\r\n    conf.put(DaemonConfig.ISOLATION_SCHEDULER_MACHINES, isolationMap);\r\n    ConfigValidation.validateFields(conf);\r\n    isolationMap.put(\"host0\", 1);\r\n    isolationMap.put(\"host1\", 2);\r\n    conf.put(DaemonConfig.ISOLATION_SCHEDULER_MACHINES, isolationMap);\r\n    ConfigValidation.validateFields(conf);\r\n    conf.put(DaemonConfig.ISOLATION_SCHEDULER_MACHINES, 42);\r\n    assertThrows(IllegalArgumentException.class, () -> ConfigValidation.validateFields(conf), \"Expected Exception not Thrown\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestDaemonConfigValidate.java",
  "methodName" : "testSupervisorSlotsPorts",
  "sourceCode" : "@Test\r\npublic void testSupervisorSlotsPorts() {\r\n    Map<String, Object> conf = new HashMap<>();\r\n    Collection<Object> passCases = new LinkedList<>();\r\n    Collection<Object> failCases = new LinkedList<>();\r\n    Integer[] test1 = { 1233, 1234, 1235 };\r\n    Integer[] test2 = { 1233 };\r\n    passCases.add(Arrays.asList(test1));\r\n    passCases.add(Arrays.asList(test2));\r\n    String[] test3 = { \"1233\", \"1234\", \"1235\" };\r\n    //duplicate case\r\n    Integer[] test4 = { 1233, 1233, 1235 };\r\n    failCases.add(test3);\r\n    failCases.add(test4);\r\n    failCases.add(null);\r\n    failCases.add(\"1234\");\r\n    failCases.add(1234);\r\n    for (Object value : passCases) {\r\n        conf.put(DaemonConfig.SUPERVISOR_SLOTS_PORTS, value);\r\n        ConfigValidation.validateFields(conf);\r\n    }\r\n    for (Object value : failCases) {\r\n        assertThrows(IllegalArgumentException.class, () -> {\r\n            conf.put(DaemonConfig.SUPERVISOR_SLOTS_PORTS, value);\r\n            ConfigValidation.validateFields(conf);\r\n        }, \"Expected Exception not Thrown for value: \" + value);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestingTest.java",
  "methodName" : "testCompleteTopologyNettySimulated",
  "sourceCode" : "@Test\r\npublic void testCompleteTopologyNettySimulated() throws Exception {\r\n    Config daemonConf = new Config();\r\n    daemonConf.put(Config.STORM_LOCAL_MODE_ZMQ, true);\r\n    MkClusterParam param = new MkClusterParam();\r\n    param.setSupervisors(4);\r\n    param.setDaemonConf(daemonConf);\r\n    Testing.withSimulatedTimeLocalCluster(param, COMPLETE_TOPOLOGY_TESTJOB);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestingTest.java",
  "methodName" : "testCompleteTopologyNetty",
  "sourceCode" : "@Test\r\npublic void testCompleteTopologyNetty() throws Exception {\r\n    Config daemonConf = new Config();\r\n    daemonConf.put(Config.STORM_LOCAL_MODE_ZMQ, true);\r\n    MkClusterParam param = new MkClusterParam();\r\n    param.setSupervisors(4);\r\n    param.setDaemonConf(daemonConf);\r\n    Testing.withLocalCluster(param, COMPLETE_TOPOLOGY_TESTJOB);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestingTest.java",
  "methodName" : "testCompleteTopologyLocalSimulated",
  "sourceCode" : "@Test\r\npublic void testCompleteTopologyLocalSimulated() throws Exception {\r\n    MkClusterParam param = new MkClusterParam();\r\n    param.setSupervisors(4);\r\n    Testing.withSimulatedTimeLocalCluster(param, COMPLETE_TOPOLOGY_TESTJOB);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestingTest.java",
  "methodName" : "testCompleteTopologyLocal",
  "sourceCode" : "@Test\r\npublic void testCompleteTopologyLocal() throws Exception {\r\n    MkClusterParam param = new MkClusterParam();\r\n    param.setSupervisors(4);\r\n    Testing.withLocalCluster(param, COMPLETE_TOPOLOGY_TESTJOB);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TestRebalance.java",
  "methodName" : "testRebalanceTopologyResourcesAndConfigs",
  "sourceCode" : "@Test\r\npublic void testRebalanceTopologyResourcesAndConfigs() throws Exception {\r\n    for (Class strategyClass : strategyClasses) {\r\n        LOG.info(\"Starting local cluster...using \", strategyClass.getName());\r\n        Config conf = new Config();\r\n        conf.put(DaemonConfig.STORM_SCHEDULER, ResourceAwareScheduler.class.getName());\r\n        conf.put(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY, DefaultSchedulingPriorityStrategy.class.getName());\r\n        conf.put(Config.TOPOLOGY_SCHEDULER_STRATEGY, strategyClass.getName());\r\n        conf.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 10.0);\r\n        conf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 10.0);\r\n        conf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 100.0);\r\n        conf.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, Double.MAX_VALUE);\r\n        Map<String, Double> resourcesMap = new HashMap();\r\n        resourcesMap.put(\"gpu.count\", 5.0);\r\n        conf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);\r\n        try (ILocalCluster cluster = new LocalCluster.Builder().withDaemonConf(conf).build()) {\r\n            TopologyBuilder builder = new TopologyBuilder();\r\n            SpoutDeclarer s1 = builder.setSpout(\"spout-1\", new TestUtilsForResourceAwareScheduler.TestSpout(), 2);\r\n            BoltDeclarer b1 = builder.setBolt(\"bolt-1\", new TestUtilsForResourceAwareScheduler.TestBolt(), 2).shuffleGrouping(\"spout-1\");\r\n            BoltDeclarer b2 = builder.setBolt(\"bolt-2\", new TestUtilsForResourceAwareScheduler.TestBolt(), 2).shuffleGrouping(\"bolt-1\");\r\n            StormTopology stormTopology = builder.createTopology();\r\n            LOG.info(\"submitting topologies....\");\r\n            String topoName = \"topo1\";\r\n            cluster.submitTopology(topoName, new HashMap<>(), stormTopology);\r\n            waitTopologyScheduled(topoName, cluster, 20);\r\n            RebalanceOptions opts = new RebalanceOptions();\r\n            Map<String, Map<String, Double>> resources = new HashMap<String, Map<String, Double>>();\r\n            resources.put(\"spout-1\", new HashMap<String, Double>());\r\n            resources.get(\"spout-1\").put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 120.0);\r\n            resources.get(\"spout-1\").put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, 25.0);\r\n            resources.get(\"spout-1\").put(\"gpu.count\", 5.0);\r\n            opts.set_topology_resources_overrides(resources);\r\n            opts.set_wait_secs(0);\r\n            JSONObject jsonObject = new JSONObject();\r\n            jsonObject.put(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB, 768.0);\r\n            opts.set_topology_conf_overrides(jsonObject.toJSONString());\r\n            LOG.info(\"rebalancing....\");\r\n            cluster.rebalance(\"topo1\", opts);\r\n            waitTopologyScheduled(topoName, cluster, 10);\r\n            boolean topologyUpdated = false;\r\n            JSONParser parser = new JSONParser();\r\n            for (int i = 0; i < 5; i++) {\r\n                Utils.sleep(SLEEP_TIME_BETWEEN_RETRY);\r\n                String confRaw = cluster.getTopologyConf(topoNameToId(topoName, cluster));\r\n                JSONObject readConf = (JSONObject) parser.parse(confRaw);\r\n                if (768.0 == (double) readConf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB)) {\r\n                    topologyUpdated = true;\r\n                    break;\r\n                }\r\n            }\r\n            StormTopology readStormTopology = cluster.getTopology(topoNameToId(topoName, cluster));\r\n            String componentConfRaw = readStormTopology.get_spouts().get(\"spout-1\").get_common().get_json_conf();\r\n            JSONObject readTopologyConf = (JSONObject) parser.parse(componentConfRaw);\r\n            Map<String, Double> componentResources = (Map<String, Double>) readTopologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);\r\n            assertTrue(topologyUpdated, \"Topology has been updated\");\r\n            assertEquals(25.0, componentResources.get(Constants.COMMON_CPU_RESOURCE_NAME), 0.001, \"Updated CPU correct\");\r\n            assertEquals(120.0, componentResources.get(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME), 0.001, \"Updated Memory correct\");\r\n            assertEquals(5.0, componentResources.get(\"gpu.count\"), 0.001, \"Updated Generic resource correct\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\TickTupleTest.java",
  "methodName" : "testTickTupleWorksWithSystemBolt",
  "sourceCode" : "@Test\r\npublic void testTickTupleWorksWithSystemBolt() throws Exception {\r\n    try (ILocalCluster cluster = new LocalCluster.Builder().withSimulatedTime().build()) {\r\n        TopologyBuilder builder = new TopologyBuilder();\r\n        FeederSpout feeder = new FeederSpout(new Fields(\"field1\"));\r\n        AckFailMapTracker tracker = new AckFailMapTracker();\r\n        feeder.setAckFailDelegate(tracker);\r\n        builder.setSpout(\"Spout\", feeder);\r\n        builder.setBolt(\"Bolt\", new NoopBolt()).shuffleGrouping(\"Spout\");\r\n        Config topoConf = new Config();\r\n        topoConf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, TICK_INTERVAL_SECS);\r\n        try (ILocalTopology ignored = cluster.submitTopology(\"test\", topoConf, builder.createTopology())) {\r\n            //Use a bootstrap tuple to wait for topology to be running\r\n            feeder.feed(new Values(\"val\"), 1);\r\n            AssertLoop.assertAcked(tracker, 1);\r\n            /*\r\n                 * Verify that some ticks are received. The interval between ticks is validated by the bolt.\r\n                 * Too few and the checks will time out. Too many and the bolt may crash (not reliably, but the test should become flaky).\r\n                 */\r\n            try {\r\n                cluster.advanceClusterTime(TICK_INTERVAL_SECS);\r\n                waitForTicks(1);\r\n                cluster.advanceClusterTime(TICK_INTERVAL_SECS);\r\n                waitForTicks(2);\r\n                cluster.advanceClusterTime(TICK_INTERVAL_SECS);\r\n                waitForTicks(3);\r\n            } catch (ConditionTimeoutException e) {\r\n                throw new AssertionError(e.getMessage());\r\n            }\r\n            assertNull(nonTickTuple.get(), \"The bolt got a tuple that is not a tick tuple \" + nonTickTuple.get());\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\EquivalenceUtilsTest.java",
  "methodName" : "testWorkerResourceEquality",
  "sourceCode" : "@Test\r\npublic void testWorkerResourceEquality() {\r\n    WorkerResources resourcesRNull = mkWorkerResources(100.0, 100.0, 100.0, null);\r\n    WorkerResources resourcesREmpty = mkWorkerResources(100.0, 100.0, 100.0, Maps.newHashMap());\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesRNull, resourcesREmpty));\r\n    Map<String, Double> resources = new HashMap<>();\r\n    resources.put(\"network.resource.units\", 0.0);\r\n    WorkerResources resourcesRNetwork = mkWorkerResources(100.0, 100.0, 100.0, resources);\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesREmpty, resourcesRNetwork));\r\n    Map<String, Double> resourcesNetwork = new HashMap<>();\r\n    resourcesNetwork.put(\"network.resource.units\", 50.0);\r\n    WorkerResources resourcesRNetworkNonZero = mkWorkerResources(100.0, 100.0, 100.0, resourcesNetwork);\r\n    assertFalse(EquivalenceUtils.customWorkerResourcesEquality(resourcesREmpty, resourcesRNetworkNonZero));\r\n    Map<String, Double> resourcesNetworkOne = new HashMap<>();\r\n    resourcesNetworkOne.put(\"network.resource.units\", 50.0);\r\n    WorkerResources resourcesRNetworkOne = mkWorkerResources(100.0, 100.0, 100.0, resourcesNetworkOne);\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesRNetworkOne, resourcesRNetworkNonZero));\r\n    Map<String, Double> resourcesNetworkTwo = new HashMap<>();\r\n    resourcesNetworkTwo.put(\"network.resource.units\", 100.0);\r\n    WorkerResources resourcesRNetworkTwo = mkWorkerResources(100.0, 100.0, 100.0, resourcesNetworkTwo);\r\n    assertFalse(EquivalenceUtils.customWorkerResourcesEquality(resourcesRNetworkOne, resourcesRNetworkTwo));\r\n    WorkerResources resourcesCpuNull = mkWorkerResources(null, 100.0, 100.0);\r\n    WorkerResources resourcesCPUZero = mkWorkerResources(0.0, 100.0, 100.0);\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesCpuNull, resourcesCPUZero));\r\n    WorkerResources resourcesOnHeapMemNull = mkWorkerResources(100.0, null, 100.0);\r\n    WorkerResources resourcesOnHeapMemZero = mkWorkerResources(100.0, 0.0, 100.0);\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesOnHeapMemNull, resourcesOnHeapMemZero));\r\n    WorkerResources resourcesOffHeapMemNull = mkWorkerResources(100.0, 100.0, null);\r\n    WorkerResources resourcesOffHeapMemZero = mkWorkerResources(100.0, 100.0, 0.0);\r\n    assertTrue(EquivalenceUtils.customWorkerResourcesEquality(resourcesOffHeapMemNull, resourcesOffHeapMemZero));\r\n    assertFalse(EquivalenceUtils.customWorkerResourcesEquality(resourcesOffHeapMemNull, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\EquivalenceUtilsTest.java",
  "methodName" : "testEquivalent",
  "sourceCode" : "@Test\r\npublic void testEquivalent() {\r\n    LocalAssignment a = mkLocalAssignment(\"A\", mkExecutorInfoList(1, 2, 3, 4, 5), mkWorkerResources(100.0, 100.0, 100.0));\r\n    LocalAssignment aResized = mkLocalAssignment(\"A\", mkExecutorInfoList(1, 2, 3, 4, 5), mkWorkerResources(100.0, 200.0, 100.0));\r\n    LocalAssignment b = mkLocalAssignment(\"B\", mkExecutorInfoList(1, 2, 3, 4, 5, 6), mkWorkerResources(100.0, 100.0, 100.0));\r\n    LocalAssignment bReordered = mkLocalAssignment(\"B\", mkExecutorInfoList(6, 5, 4, 3, 2, 1), mkWorkerResources(100.0, 100.0, 100.0));\r\n    LocalAssignment c = mkLocalAssignment(\"C\", mkExecutorInfoList(188, 261), mkWorkerResources(400.0, 10000.0, 0.0));\r\n    WorkerResources workerResources = mkWorkerResources(400.0, 10000.0, 0.0);\r\n    Map<String, Double> additionalResources = workerResources.get_resources();\r\n    if (additionalResources == null)\r\n        additionalResources = new HashMap<>();\r\n    additionalResources.put(\"network.resource.units\", 0.0);\r\n    workerResources.set_resources(additionalResources);\r\n    LocalAssignment cReordered = mkLocalAssignment(\"C\", mkExecutorInfoList(188, 261), workerResources);\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(c, cReordered));\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(null, null));\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(a, a));\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(b, bReordered));\r\n    assertTrue(EquivalenceUtils.areLocalAssignmentsEquivalent(bReordered, b));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(a, aResized));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(aResized, a));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(a, null));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(null, b));\r\n    assertFalse(EquivalenceUtils.areLocalAssignmentsEquivalent(a, b));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testExtractZipFileDisallowsPathTraversal",
  "sourceCode" : "@Test\r\npublic void testExtractZipFileDisallowsPathTraversal() throws Exception {\r\n    try (TmpPath path = new TmpPath()) {\r\n        Path testRoot = Paths.get(path.getPath());\r\n        Path extractionDest = testRoot.resolve(\"dest\");\r\n        Files.createDirectories(extractionDest);\r\n        /*\r\n             * Contains good.txt and ../evil.txt. Evil.txt will path outside the target dir, and should not be extracted.\r\n             */\r\n        try (ZipFile zip = new ZipFile(Paths.get(\"src/test/resources/evil-path-traversal.jar\").toFile())) {\r\n            ServerUtils.extractZipFile(zip, extractionDest.toFile(), null);\r\n        }\r\n        assertThat(Files.exists(extractionDest.resolve(\"good.txt\")), is(true));\r\n        assertThat(Files.exists(testRoot.resolve(\"evil.txt\")), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testExtractZipFileDisallowsPathTraversalWhenUsingPrefix",
  "sourceCode" : "@Test\r\npublic void testExtractZipFileDisallowsPathTraversalWhenUsingPrefix() throws Exception {\r\n    try (TmpPath path = new TmpPath()) {\r\n        Path testRoot = Paths.get(path.getPath());\r\n        Path destParent = testRoot.resolve(\"outer\");\r\n        Path extractionDest = destParent.resolve(\"resources\");\r\n        Files.createDirectories(extractionDest);\r\n        /*\r\n             * Contains resources/good.txt and resources/../evil.txt. Evil.txt should not be extracted as it would end\r\n             * up outside the extraction dest.\r\n             */\r\n        try (ZipFile zip = new ZipFile(Paths.get(\"src/test/resources/evil-path-traversal-resources.jar\").toFile())) {\r\n            ServerUtils.extractZipFile(zip, extractionDest.toFile(), \"resources\");\r\n        }\r\n        assertThat(Files.exists(extractionDest.resolve(\"good.txt\")), is(true));\r\n        assertThat(Files.exists(extractionDest.resolve(\"evil.txt\")), is(false));\r\n        assertThat(Files.exists(destParent.resolve(\"evil.txt\")), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testIsProcessAlive",
  "sourceCode" : "@Test\r\npublic void testIsProcessAlive() throws Exception {\r\n    // specific selected process should not be alive for a randomly generated user\r\n    String randomUser = RandomStringUtils.randomAlphanumeric(12);\r\n    // get list of few running processes\r\n    Collection<Long> pids = getRunningProcessIds(null);\r\n    assertFalse(pids.isEmpty());\r\n    for (long pid : pids) {\r\n        boolean status = ServerUtils.isProcessAlive(pid, randomUser);\r\n        assertFalse(status, \"Random user \" + randomUser + \" is not expected to own any process\");\r\n    }\r\n    boolean status = false;\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    for (long pid : pids) {\r\n        // at least one pid will be owned by the current user (doing the testing)\r\n        if (ServerUtils.isProcessAlive(pid, currentUser)) {\r\n            status = true;\r\n            break;\r\n        }\r\n    }\r\n    assertTrue(status, \"Expecting user \" + currentUser + \" to own at least one process\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testIsAnyProcessAlive",
  "sourceCode" : "@Test\r\npublic void testIsAnyProcessAlive() throws Exception {\r\n    // no process should be alive for a randomly generated user\r\n    String randomUser = RandomStringUtils.randomAlphanumeric(12);\r\n    Collection<Long> pids = getRunningProcessIds(null);\r\n    assertFalse(pids.isEmpty());\r\n    boolean status = ServerUtils.isAnyProcessAlive(pids, randomUser);\r\n    assertFalse(status, \"Random user \" + randomUser + \" is not expected to own any process\");\r\n    // at least one pid will be owned by the current user (doing the testing)\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    status = ServerUtils.isAnyProcessAlive(pids, currentUser);\r\n    assertTrue(status, \"Expecting user \" + currentUser + \" to own at least one process\");\r\n    if (!ServerUtils.IS_ON_WINDOWS) {\r\n        // userid test is valid only on Posix platforms\r\n        int inValidUserId = -1;\r\n        status = ServerUtils.isAnyProcessAlive(pids, inValidUserId);\r\n        assertFalse(status, \"Invalid userId \" + randomUser + \" is not expected to own any process\");\r\n        int currentUid = ServerUtils.getUserId(null);\r\n        status = ServerUtils.isAnyProcessAlive(pids, currentUid);\r\n        assertTrue(status, \"Expecting uid \" + currentUid + \" to own at least one process\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testGetUserId",
  "sourceCode" : "@Test\r\npublic void testGetUserId() throws Exception {\r\n    if (ServerUtils.IS_ON_WINDOWS) {\r\n        // trivially succeed on Windows, since this test is not for Windows platform\r\n        return;\r\n    }\r\n    int uid1 = ServerUtils.getUserId(null);\r\n    Path p = Files.createTempFile(\"testGetUser\", \".txt\");\r\n    int uid2 = ServerUtils.getPathOwnerUid(p.toString());\r\n    if (!p.toFile().delete()) {\r\n        LOG.warn(\"Could not delete temporary file {}\", p);\r\n    }\r\n    assertEquals(uid1, uid2, \"User UID \" + uid1 + \" is not same as file \" + p + \" owner UID of \" + uid2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testIsAnyProcessPosixProcessPidDirAlive",
  "sourceCode" : "@Test\r\npublic void testIsAnyProcessPosixProcessPidDirAlive() throws IOException {\r\n    final String testName = \"testIsAnyProcessPosixProcessPidDirAlive\";\r\n    List<String> errors = new ArrayList<>();\r\n    int maxPidCnt = 5;\r\n    if (ServerUtils.IS_ON_WINDOWS) {\r\n        LOG.info(\"{}: test cannot be run on Windows. Marked as successful\", testName);\r\n        return;\r\n    }\r\n    final Path parentDir = Paths.get(\"/proc\");\r\n    if (!parentDir.toFile().exists()) {\r\n        LOG.info(\"{}: test cannot be run on system without process directory {}, os.name={}\", testName, parentDir, System.getProperty(\"os.name\"));\r\n        // check if we can get process id on this Posix system - testing test code, useful on Mac\r\n        String cmd = \"/bin/sleep 10\";\r\n        if (getPidOfPosixProcess(Runtime.getRuntime().exec(cmd), errors) < 0) {\r\n            fail(String.format(\"%s: Cannot obtain process id for executed command \\\"%s\\\"\\n%s\", testName, cmd, String.join(\"\\n\\t\", errors)));\r\n        }\r\n        return;\r\n    }\r\n    // Create processes and wait for their termination\r\n    Set<Long> observables = new HashSet<>();\r\n    for (int i = 0; i < maxPidCnt; i++) {\r\n        String cmd = \"sleep 20000\";\r\n        Process process = Runtime.getRuntime().exec(cmd);\r\n        long pid = getPidOfPosixProcess(process, errors);\r\n        LOG.info(\"{}: ({}) ran process \\\"{}\\\" with pid={}\", testName, i, cmd, pid);\r\n        if (pid < 0) {\r\n            String e = String.format(\"%s: (%d) Cannot obtain process id for executed command \\\"%s\\\"\", testName, i, cmd);\r\n            errors.add(e);\r\n            LOG.error(e);\r\n            continue;\r\n        }\r\n        observables.add(pid);\r\n    }\r\n    String userName = System.getProperty(\"user.name\");\r\n    // now kill processes one by one\r\n    List<Long> pidList = new ArrayList<>(observables);\r\n    final long processKillIntervalMs = 2000;\r\n    for (int i = 0; i < pidList.size(); i++) {\r\n        long pid = pidList.get(i);\r\n        LOG.info(\"{}: ({}) Sleeping for {} milliseconds before kill\", testName, i, processKillIntervalMs);\r\n        if (sleepInterrupted(processKillIntervalMs)) {\r\n            return;\r\n        }\r\n        Runtime.getRuntime().exec(\"kill -9 \" + pid);\r\n        LOG.info(\"{}: ({}) Sleeping for {} milliseconds after kill\", testName, i, processKillIntervalMs);\r\n        if (sleepInterrupted(processKillIntervalMs)) {\r\n            return;\r\n        }\r\n        boolean pidDirsAvailable = ServerUtils.isAnyPosixProcessPidDirAlive(observables, userName);\r\n        if (i < pidList.size() - 1) {\r\n            if (pidDirsAvailable) {\r\n                LOG.info(\"{}: ({}) Found existing process directories before killing last process\", testName, i);\r\n            } else {\r\n                String e = String.format(\"%s: (%d) Found no existing process directories before killing last process\", testName, i);\r\n                errors.add(e);\r\n                LOG.error(e);\r\n            }\r\n        } else {\r\n            if (pidDirsAvailable) {\r\n                String e = String.format(\"%s: (%d) Found existing process directories after killing last process\", testName, i);\r\n                errors.add(e);\r\n                LOG.error(e);\r\n            } else {\r\n                LOG.info(\"{}: ({}) Found no existing process directories after killing last process\", testName, i);\r\n            }\r\n        }\r\n    }\r\n    if (!errors.isEmpty()) {\r\n        fail(String.format(\"There are %d failures in test:\\n\\t%s\", errors.size(), String.join(\"\\n\\t\", errors)));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ServerUtilsTest.java",
  "methodName" : "testIsAnyPosixProcessPidDirAliveMockingFileOwnerUid",
  "sourceCode" : "/**\r\n * Simulate the production scenario where the owner of the process directory is sometimes returned as the\r\n * UID instead of user. This scenario is simulated by calling\r\n * {@link ServerUtils#isAnyPosixProcessPidDirAlive(Collection, String, boolean)} with the last parameter\r\n * set to true as well as false.\r\n *\r\n * @throws Exception on I/O exception\r\n */\r\n@Test\r\npublic void testIsAnyPosixProcessPidDirAliveMockingFileOwnerUid() throws Exception {\r\n    File procDir = new File(\"/proc\");\r\n    if (!procDir.exists()) {\r\n        LOG.info(\"Test testIsAnyPosixProcessPidDirAlive is designed to run on systems with /proc directory only, marking as success\");\r\n        return;\r\n    }\r\n    Collection<Long> allPids = getRunningProcessIds(null);\r\n    Collection<Long> rootPids = getRunningProcessIds(\"root\");\r\n    assertFalse(allPids.isEmpty());\r\n    assertFalse(rootPids.isEmpty());\r\n    String currentUser = System.getProperty(\"user.name\");\r\n    for (boolean mockFileOwnerToUid : Arrays.asList(true, false)) {\r\n        // at least one pid will be owned by the current user (doing the testing)\r\n        boolean status = ServerUtils.isAnyPosixProcessPidDirAlive(allPids, currentUser, mockFileOwnerToUid);\r\n        String err = String.format(\"(mockFileOwnerToUid=%s) Expecting user %s to own at least one process\", mockFileOwnerToUid, currentUser);\r\n        assertTrue(status, err);\r\n    }\r\n    // simulate reassignment of all process id to a different user (root)\r\n    for (boolean mockFileOwnerToUid : Arrays.asList(true, false)) {\r\n        boolean status = ServerUtils.isAnyPosixProcessPidDirAlive(rootPids, currentUser, mockFileOwnerToUid);\r\n        String err = String.format(\"(mockFileOwnerToUid=%s) Expecting user %s to own no process\", mockFileOwnerToUid, currentUser);\r\n        assertFalse(status, err);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ZookeeperServerCnxnFactoryTest.java",
  "methodName" : "test_Exception_In_Constructor_If_Port_Too_Large",
  "sourceCode" : "@Test\r\npublic void test_Exception_In_Constructor_If_Port_Too_Large() {\r\n    assertThrows(RuntimeException.class, () -> new ZookeeperServerCnxnFactory(65536, 42));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-server\\src\\test\\java\\org\\apache\\storm\\utils\\ZookeeperServerCnxnFactoryTest.java",
  "methodName" : "testFactory",
  "sourceCode" : "@Test\r\npublic void testFactory() {\r\n    int arbitraryTestClients = 42;\r\n    ZookeeperServerCnxnFactory zkcfNegative = new ZookeeperServerCnxnFactory(-42, arbitraryTestClients);\r\n    int nextPort = zkcfNegative.port() + 1;\r\n    ZookeeperServerCnxnFactory zkcfNext = new ZookeeperServerCnxnFactory(nextPort, arbitraryTestClients);\r\n    assertEquals(zkcfNext.factory().getMaxClientCnxnsPerHost(), arbitraryTestClients);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-submit-tools\\src\\test\\java\\org\\apache\\storm\\submit\\dependency\\AetherUtilsTest.java",
  "methodName" : "parseDependency",
  "sourceCode" : "@Test\r\npublic void parseDependency() {\r\n    String testDependency = \"testgroup:testartifact:1.0.0^testgroup:testexcartifact^testgroup:*\";\r\n    Dependency dependency = AetherUtils.parseDependency(testDependency);\r\n    assertEquals(\"testgroup\", dependency.getArtifact().getGroupId());\r\n    assertEquals(\"testartifact\", dependency.getArtifact().getArtifactId());\r\n    assertEquals(\"1.0.0\", dependency.getArtifact().getVersion());\r\n    assertEquals(JavaScopes.COMPILE, dependency.getScope());\r\n    assertEquals(2, dependency.getExclusions().size());\r\n    List<Exclusion> exclusions = Lists.newArrayList(dependency.getExclusions());\r\n    Exclusion exclusion = exclusions.get(0);\r\n    assertEquals(\"testgroup\", exclusion.getGroupId());\r\n    assertEquals(\"testexcartifact\", exclusion.getArtifactId());\r\n    assertEquals(JavaScopes.COMPILE, dependency.getScope());\r\n    exclusion = exclusions.get(1);\r\n    assertEquals(\"testgroup\", exclusion.getGroupId());\r\n    assertEquals(\"*\", exclusion.getArtifactId());\r\n    assertEquals(JavaScopes.COMPILE, dependency.getScope());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-submit-tools\\src\\test\\java\\org\\apache\\storm\\submit\\dependency\\AetherUtilsTest.java",
  "methodName" : "createExclusion",
  "sourceCode" : "@Test\r\npublic void createExclusion() {\r\n    String testExclusion = \"group\";\r\n    Exclusion exclusion = AetherUtils.createExclusion(testExclusion);\r\n    assertEquals(\"group\", exclusion.getGroupId());\r\n    assertEquals(\"*\", exclusion.getArtifactId());\r\n    assertEquals(\"*\", exclusion.getClassifier());\r\n    assertEquals(\"*\", exclusion.getExtension());\r\n    testExclusion = \"group:artifact\";\r\n    exclusion = AetherUtils.createExclusion(testExclusion);\r\n    assertEquals(\"group\", exclusion.getGroupId());\r\n    assertEquals(\"artifact\", exclusion.getArtifactId());\r\n    assertEquals(\"*\", exclusion.getClassifier());\r\n    assertEquals(\"*\", exclusion.getExtension());\r\n    testExclusion = \"group:artifact:site\";\r\n    exclusion = AetherUtils.createExclusion(testExclusion);\r\n    assertEquals(\"group\", exclusion.getGroupId());\r\n    assertEquals(\"artifact\", exclusion.getArtifactId());\r\n    assertEquals(\"site\", exclusion.getClassifier());\r\n    assertEquals(\"*\", exclusion.getExtension());\r\n    testExclusion = \"group:artifact:site:jar\";\r\n    exclusion = AetherUtils.createExclusion(testExclusion);\r\n    assertEquals(\"group\", exclusion.getGroupId());\r\n    assertEquals(\"artifact\", exclusion.getArtifactId());\r\n    assertEquals(\"site\", exclusion.getClassifier());\r\n    assertEquals(\"jar\", exclusion.getExtension());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-submit-tools\\src\\test\\java\\org\\apache\\storm\\submit\\dependency\\AetherUtilsTest.java",
  "methodName" : "artifactToString",
  "sourceCode" : "@Test\r\npublic void artifactToString() {\r\n    Artifact testArtifact = new DefaultArtifact(\"org.apache.storm:storm-core:1.0.0\");\r\n    String ret = AetherUtils.artifactToString(testArtifact);\r\n    assertEquals(\"org.apache.storm:storm-core:jar:1.0.0\", ret);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-submit-tools\\src\\test\\java\\org\\apache\\storm\\submit\\dependency\\DependencyResolverTest.java",
  "methodName" : "resolveValid",
  "sourceCode" : "@Test\r\npublic void resolveValid() throws Exception {\r\n    // please pick small artifact which has small transitive dependency\r\n    // and let's mark as Ignore if we want to run test even without internet or maven central is often not stable\r\n    Dependency dependency = new Dependency(new DefaultArtifact(\"org.apache.storm:flux-core:1.0.0\"), JavaScopes.COMPILE);\r\n    List<ArtifactResult> results = sut.resolve(Lists.newArrayList(dependency));\r\n    assertTrue(results.size() > 0);\r\n    // it should be org.apache.storm:flux-core:jar:1.0.0 and commons-cli:commons-cli:jar:1.2\r\n    assertContains(results, \"org.apache.storm\", \"flux-core\", \"1.0.0\");\r\n    assertContains(results, \"commons-cli\", \"commons-cli\", \"1.2\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServer.java",
  "methodName" : "start",
  "sourceCode" : "@VisibleForTesting\r\nvoid start() throws Exception {\r\n    LOG.info(\"Starting Distributed RPC servers...\");\r\n    new Thread(invokeServer::serve).start();\r\n    if (httpServer != null) {\r\n        httpServer.start();\r\n    }\r\n    if (handlerServer != null) {\r\n        handlerServerThread = new Thread(handlerServer::serve);\r\n        handlerServerThread.start();\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServer.java",
  "methodName" : "awaitTermination",
  "sourceCode" : "@VisibleForTesting\r\nvoid awaitTermination() throws InterruptedException {\r\n    if (handlerServerThread != null) {\r\n        handlerServerThread.join();\r\n    } else {\r\n        httpServer.join();\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "substringSearch",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, Object> substringSearch(Path file, String searchString) throws InvalidRequestException {\r\n    return substringSearch(file, searchString, false, 10, 0);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "substringSearch",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, Object> substringSearch(Path file, String searchString, int numMatches) throws InvalidRequestException {\r\n    return substringSearch(file, searchString, false, numMatches, 0);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "substringSearch",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, Object> substringSearch(Path file, String searchString, int numMatches, int startByteOffset) throws InvalidRequestException {\r\n    return substringSearch(file, searchString, false, numMatches, startByteOffset);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "substringSearchDaemonLog",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, Object> substringSearchDaemonLog(Path file, String searchString) throws InvalidRequestException {\r\n    return substringSearch(file, searchString, true, 10, 0);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "logsForPort",
  "sourceCode" : "/**\r\n * Get the filtered, authorized, sorted log files for a port.\r\n */\r\n@VisibleForTesting\r\nList<Path> logsForPort(String user, Path portDir) {\r\n    try {\r\n        List<Path> workerLogs = directoryCleaner.getFilesForDir(portDir).stream().filter(file -> WORKER_LOG_FILENAME_PATTERN.asPredicate().test(file.getFileName().toString())).collect(toList());\r\n        return workerLogs.stream().filter(log -> resourceAuthorizer.isUserAllowedToAccessFile(user, WorkerLogs.getTopologyPortWorkerLog(log))).map(p -> {\r\n            try {\r\n                return Pair.of(p, Files.getLastModifiedTime(p));\r\n            } catch (IOException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        }).sorted(Comparator.comparing((Pair<Path, FileTime> p) -> p.getRight()).reversed()).map(p -> p.getLeft()).collect(toList());\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "findNMatches",
  "sourceCode" : "/**\r\n * Find the first N matches of target string in files.\r\n *\r\n * @param logs all candidate log files to search\r\n * @param numMatches number of matches expected\r\n * @param fileOffset number of log files to skip initially\r\n * @param startByteOffset number of byte to be ignored in each log file\r\n * @param targetStr searched string\r\n * @return all matched results\r\n */\r\n@VisibleForTesting\r\nMatched findNMatches(List<Path> logs, int numMatches, int fileOffset, int startByteOffset, String targetStr) {\r\n    logs = drop(logs, fileOffset);\r\n    LOG.debug(\"{} files to scan\", logs.size());\r\n    List<Map<String, Object>> matches = new ArrayList<>();\r\n    int matchCount = 0;\r\n    int scannedFiles = 0;\r\n    while (true) {\r\n        if (logs.isEmpty()) {\r\n            //fileOffset = one past last scanned file\r\n            break;\r\n        }\r\n        Path firstLog = logs.get(0);\r\n        Map<String, Object> matchInLog;\r\n        try {\r\n            LOG.debug(\"Looking through {}\", firstLog);\r\n            matchInLog = substringSearch(firstLog, targetStr, numMatches - matchCount, startByteOffset);\r\n            scannedFiles++;\r\n        } catch (InvalidRequestException e) {\r\n            LOG.error(\"Can't search past end of file.\", e);\r\n            matchInLog = new HashMap<>();\r\n        }\r\n        String fileName = WorkerLogs.getTopologyPortWorkerLog(firstLog);\r\n        //This section simply put the formatted log filename and corresponding port in the matching.\r\n        final List<Map<String, Object>> newMatches = new ArrayList<>(matches);\r\n        Map<String, Object> currentFileMatch = new HashMap<>(matchInLog);\r\n        currentFileMatch.put(\"fileName\", fileName);\r\n        Path firstLogAbsPath = firstLog.toAbsolutePath().normalize();\r\n        currentFileMatch.put(\"port\", truncatePathToLastElements(firstLogAbsPath, 2).getName(0).toString());\r\n        newMatches.add(currentFileMatch);\r\n        int newCount = matchCount + ((List<?>) matchInLog.getOrDefault(\"matches\", Collections.emptyList())).size();\r\n        if (newCount == matchCount) {\r\n            // matches and matchCount is not changed\r\n            logs = rest(logs);\r\n            startByteOffset = 0;\r\n            fileOffset = fileOffset + 1;\r\n        } else if (newCount >= numMatches) {\r\n            matches = newMatches;\r\n            //fileOffset = the index of last scanned file\r\n            break;\r\n        } else {\r\n            matches = newMatches;\r\n            logs = rest(logs);\r\n            startByteOffset = 0;\r\n            fileOffset = fileOffset + 1;\r\n            matchCount = newCount;\r\n        }\r\n    }\r\n    LOG.debug(\"scanned {} files\", scannedFiles);\r\n    return new Matched(fileOffset, targetStr, matches, scannedFiles);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "urlToMatchCenteredInLogPage",
  "sourceCode" : "@VisibleForTesting\r\nString urlToMatchCenteredInLogPage(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {\r\n    final String host = Utils.hostname();\r\n    final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 3);\r\n    Map<String, Object> parameters = new HashMap<>();\r\n    parameters.put(\"file\", truncatedFilePath.toString());\r\n    parameters.put(\"start\", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));\r\n    parameters.put(\"length\", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);\r\n    return UrlBuilder.build(String.format(this.scheme + \"://%s:%d/api/v1/log\", host, port), parameters);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandler.java",
  "methodName" : "urlToMatchCenteredInLogPageDaemonFile",
  "sourceCode" : "@VisibleForTesting\r\nString urlToMatchCenteredInLogPageDaemonFile(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {\r\n    final String host = Utils.hostname();\r\n    final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 1);\r\n    Map<String, Object> parameters = new HashMap<>();\r\n    parameters.put(\"file\", truncatedFilePath.toString());\r\n    parameters.put(\"start\", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));\r\n    parameters.put(\"length\", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);\r\n    return UrlBuilder.build(String.format(this.scheme + \"://%s:%d/api/v1/daemonlog\", host, port), parameters);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\LogviewerServer.java",
  "methodName" : "start",
  "sourceCode" : "@VisibleForTesting\r\nvoid start() throws Exception {\r\n    LOG.info(\"Starting Logviewer...\");\r\n    if (httpServer != null) {\r\n        httpServer.start();\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\LogviewerServer.java",
  "methodName" : "awaitTermination",
  "sourceCode" : "@VisibleForTesting\r\nvoid awaitTermination() throws InterruptedException {\r\n    httpServer.join();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "perWorkerDirCleanup",
  "sourceCode" : "/**\r\n * Delete the oldest files in each overloaded worker log dir.\r\n */\r\n@VisibleForTesting\r\nList<DeletionMeta> perWorkerDirCleanup(long size) {\r\n    return workerLogs.getAllWorkerDirs().stream().map(dir -> {\r\n        try {\r\n            return directoryCleaner.deleteOldestWhileTooLarge(Collections.singletonList(dir), size, true, null);\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }).collect(toList());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "globalLogCleanup",
  "sourceCode" : "/**\r\n * Delete the oldest files in overloaded worker-artifacts globally.\r\n */\r\n@VisibleForTesting\r\nDeletionMeta globalLogCleanup(long size) throws Exception {\r\n    List<Path> workerDirs = new ArrayList<>(workerLogs.getAllWorkerDirs());\r\n    Set<Path> aliveWorkerDirs = workerLogs.getAliveWorkerDirs();\r\n    return directoryCleaner.deleteOldestWhileTooLarge(workerDirs, size, false, aliveWorkerDirs);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "cleanupEmptyTopoDirectory",
  "sourceCode" : "/**\r\n * Delete the topo dir if it contains zero port dirs.\r\n */\r\n@VisibleForTesting\r\nvoid cleanupEmptyTopoDirectory(Path dir) throws IOException {\r\n    Path topoDir = dir.getParent();\r\n    try (Stream<Path> topoDirContent = Files.list(topoDir)) {\r\n        if (!topoDirContent.findAny().isPresent()) {\r\n            Utils.forceDelete(topoDir.toAbsolutePath().normalize().toString());\r\n        }\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "getDeadWorkerDirs",
  "sourceCode" : "/**\r\n * Return a sorted set of paths that were written by workers that are now dead.\r\n */\r\n@VisibleForTesting\r\nSortedSet<Path> getDeadWorkerDirs(int nowSecs, Set<Path> logDirs) throws Exception {\r\n    if (logDirs.isEmpty()) {\r\n        return new TreeSet<>();\r\n    } else {\r\n        Set<String> aliveIds = workerLogs.getAliveIds(nowSecs);\r\n        return workerLogs.getLogDirs(logDirs, (wid) -> !aliveIds.contains(wid));\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "selectDirsForCleanup",
  "sourceCode" : "@VisibleForTesting\r\nSet<Path> selectDirsForCleanup(long nowMillis) {\r\n    Predicate<Path> fileFilter = mkFileFilterForLogCleanup(nowMillis);\r\n    try (Stream<Path> fileList = Files.list(logRootDir)) {\r\n        return fileList.flatMap(a -> {\r\n            try {\r\n                return Files.list(a);\r\n            } catch (IOException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        }).filter(fileFilter).collect(Collectors.toCollection(TreeSet::new));\r\n    } catch (IOException e) {\r\n        throw Utils.wrapInRuntime(e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "mkFileFilterForLogCleanup",
  "sourceCode" : "@VisibleForTesting\r\nPredicate<Path> mkFileFilterForLogCleanup(long nowMillis) {\r\n    //It seems safer not to follow symlinks, since we don't expect them here\r\n    return file -> Files.isDirectory(file, LinkOption.NOFOLLOW_LINKS) && lastModifiedTimeWorkerLogdir(file) <= cleanupCutoffAgeMillis(nowMillis);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleaner.java",
  "methodName" : "cleanupCutoffAgeMillis",
  "sourceCode" : "@VisibleForTesting\r\nlong cleanupCutoffAgeMillis(long nowMillis) {\r\n    final Integer intervalMins = ObjectReader.getInt(stormConf.get(LOGVIEWER_CLEANUP_AGE_MINS));\r\n    return nowMillis - TimeUnit.MINUTES.toMillis(intervalMins);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\main\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizer.java",
  "methodName" : "getUserGroups",
  "sourceCode" : "@VisibleForTesting\r\nSet<String> getUserGroups(String user) {\r\n    try {\r\n        if (StringUtils.isEmpty(user)) {\r\n            return new HashSet<>();\r\n        } else {\r\n            return groupMappingServiceProvider.getGroups(user);\r\n        }\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServerTest.java",
  "methodName" : "testGoodThrift",
  "sourceCode" : "@Test\r\npublic void testGoodThrift() throws Exception {\r\n    Map<String, Object> conf = getConf(0, 0, null);\r\n    try (DRPCServer server = new DRPCServer(conf, new StormMetricsRegistry())) {\r\n        server.start();\r\n        try (DRPCClient client = new DRPCClient(conf, \"localhost\", server.getDrpcPort());\r\n            DRPCInvocationsClient invoke = new DRPCInvocationsClient(conf, \"localhost\", server.getDrpcInvokePort())) {\r\n            final Future<String> found = exec.submit(() -> client.getClient().execute(\"testing\", \"test\"));\r\n            DRPCRequest request = getNextAvailableRequest(invoke, \"testing\");\r\n            assertNotNull(request);\r\n            assertEquals(\"test\", request.get_func_args());\r\n            assertNotNull(request.get_request_id());\r\n            invoke.result(request.get_request_id(), \"tested\");\r\n            String result = found.get(1000, TimeUnit.MILLISECONDS);\r\n            assertEquals(\"tested\", result);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServerTest.java",
  "methodName" : "testFailedThrift",
  "sourceCode" : "@Test\r\npublic void testFailedThrift() throws Exception {\r\n    Map<String, Object> conf = getConf(0, 0, null);\r\n    try (DRPCServer server = new DRPCServer(conf, new StormMetricsRegistry())) {\r\n        server.start();\r\n        try (DRPCClient client = new DRPCClient(conf, \"localhost\", server.getDrpcPort());\r\n            DRPCInvocationsClient invoke = new DRPCInvocationsClient(conf, \"localhost\", server.getDrpcInvokePort())) {\r\n            Future<String> found = exec.submit(() -> client.getClient().execute(\"testing\", \"test\"));\r\n            DRPCRequest request = getNextAvailableRequest(invoke, \"testing\");\r\n            assertNotNull(request);\r\n            assertEquals(\"test\", request.get_func_args());\r\n            assertNotNull(request.get_request_id());\r\n            invoke.failRequest(request.get_request_id());\r\n            try {\r\n                found.get(1000, TimeUnit.MILLISECONDS);\r\n                fail(\"exec did not throw an exception\");\r\n            } catch (ExecutionException e) {\r\n                Throwable t = e.getCause();\r\n                assertEquals(t.getClass(), DRPCExecutionException.class);\r\n                //Don't know a better way to validate that it failed.\r\n                assertEquals(\"Request failed\", ((DRPCExecutionException) t).get_msg());\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServerTest.java",
  "methodName" : "testGoodHttpGet",
  "sourceCode" : "@Test\r\npublic void testGoodHttpGet() throws Exception {\r\n    LOG.info(\"STARTING HTTP GET TEST...\");\r\n    Map<String, Object> conf = getConf(0, 0, 0);\r\n    try (DRPCServer server = new DRPCServer(conf, new StormMetricsRegistry())) {\r\n        server.start();\r\n        //TODO need a better way to do this\r\n        Thread.sleep(2000);\r\n        try (DRPCInvocationsClient invoke = new DRPCInvocationsClient(conf, \"localhost\", server.getDrpcInvokePort())) {\r\n            final Future<String> found = exec.submit(() -> doGet(server.getHttpServerPort(), \"testing\", \"test\"));\r\n            DRPCRequest request = getNextAvailableRequest(invoke, \"testing\");\r\n            assertNotNull(request);\r\n            assertEquals(\"test\", request.get_func_args());\r\n            assertNotNull(request.get_request_id());\r\n            invoke.result(request.get_request_id(), \"tested\");\r\n            String result = found.get(1000, TimeUnit.MILLISECONDS);\r\n            assertEquals(\"tested\", result);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\drpc\\DRPCServerTest.java",
  "methodName" : "testFailedHttpGet",
  "sourceCode" : "@Test\r\npublic void testFailedHttpGet() throws Exception {\r\n    LOG.info(\"STARTING HTTP GET (FAIL) TEST...\");\r\n    Map<String, Object> conf = getConf(0, 0, 0);\r\n    try (DRPCServer server = new DRPCServer(conf, new StormMetricsRegistry())) {\r\n        server.start();\r\n        //TODO need a better way to do this\r\n        Thread.sleep(2000);\r\n        try (DRPCInvocationsClient invoke = new DRPCInvocationsClient(conf, \"localhost\", server.getDrpcInvokePort())) {\r\n            Future<String> found = exec.submit(() -> doGet(server.getHttpServerPort(), \"testing\", \"test\"));\r\n            DRPCRequest request = getNextAvailableRequest(invoke, \"testing\");\r\n            assertNotNull(request);\r\n            assertEquals(\"test\", request.get_func_args());\r\n            assertNotNull(request.get_request_id());\r\n            invoke.getClient().failRequest(request.get_request_id());\r\n            try {\r\n                found.get(1000, TimeUnit.MILLISECONDS);\r\n                fail(\"exec did not throw an exception\");\r\n            } catch (ExecutionException e) {\r\n                LOG.warn(\"Got Expected Exception\", e);\r\n                //Getting the exact response code is a bit more complex.\r\n                //TODO should use a better client\r\n            }\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogDownloadHandlerTest.java",
  "methodName" : "testDownloadLogFile",
  "sourceCode" : "@Test\r\npublic void testDownloadLogFile() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogDownloadHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.downloadLogFile(\"host\", \"topoA/1111/worker.log\", \"user\");\r\n        Response topoBResponse = handler.downloadLogFile(\"host\", \"topoB/1111/worker.log\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        assertThat(topoAResponse.getEntity(), not(nullValue()));\r\n        String topoAContentDisposition = topoAResponse.getHeaderString(HttpHeaders.CONTENT_DISPOSITION);\r\n        assertThat(topoAContentDisposition, containsString(\"host-topoA-1111-worker.log\"));\r\n        assertThat(topoBResponse.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        assertThat(topoBResponse.getEntity(), not(nullValue()));\r\n        String topoBContentDisposition = topoBResponse.getHeaderString(HttpHeaders.CONTENT_DISPOSITION);\r\n        assertThat(topoBContentDisposition, containsString(\"host-topoB-1111-worker.log\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogDownloadHandlerTest.java",
  "methodName" : "testDownloadLogFileTraversal",
  "sourceCode" : "@Test\r\npublic void testDownloadLogFileTraversal() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogDownloadHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.downloadLogFile(\"host\", \"../nimbus.log\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogDownloadHandlerTest.java",
  "methodName" : "testDownloadDaemonLogFile",
  "sourceCode" : "@Test\r\npublic void testDownloadDaemonLogFile() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogDownloadHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response response = handler.downloadDaemonLogFile(\"host\", \"nimbus.log\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(response.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        assertThat(response.getEntity(), not(nullValue()));\r\n        String contentDisposition = response.getHeaderString(HttpHeaders.CONTENT_DISPOSITION);\r\n        assertThat(contentDisposition, containsString(\"host-nimbus.log\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogDownloadHandlerTest.java",
  "methodName" : "testDownloadDaemonLogFilePathIntoWorkerLogs",
  "sourceCode" : "@Test\r\npublic void testDownloadDaemonLogFilePathIntoWorkerLogs() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogDownloadHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response response = handler.downloadDaemonLogFile(\"host\", \"workers-artifacts/topoA/1111/worker.log\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(response.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogDownloadHandlerTest.java",
  "methodName" : "testDownloadDaemonLogFilePathOutsideLogRoot",
  "sourceCode" : "@Test\r\npublic void testDownloadDaemonLogFilePathOutsideLogRoot() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogDownloadHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response response = handler.downloadDaemonLogFile(\"host\", \"../evil.sh\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(response.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogPageHandlerTest.java",
  "methodName" : "testListLogFiles",
  "sourceCode" : "/**\r\n * list-log-files filter selects the correct log files to return.\r\n */\r\n@Test\r\npublic void testListLogFiles() throws IOException {\r\n    String rootPath = Files.createTempDirectory(\"workers-artifacts\").toFile().getCanonicalPath();\r\n    File file1 = new File(String.join(File.separator, rootPath, \"topoA\", \"1111\"), \"worker.log\");\r\n    File file2 = new File(String.join(File.separator, rootPath, \"topoA\", \"2222\"), \"worker.log\");\r\n    File file3 = new File(String.join(File.separator, rootPath, \"topoB\", \"1111\"), \"worker.log\");\r\n    file1.getParentFile().mkdirs();\r\n    file2.getParentFile().mkdirs();\r\n    file3.getParentFile().mkdirs();\r\n    file1.createNewFile();\r\n    file2.createNewFile();\r\n    file3.createNewFile();\r\n    String origin = \"www.origin.server.net\";\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    StormMetricsRegistry metricsRegistry = new StormMetricsRegistry();\r\n    LogviewerLogPageHandler handler = new LogviewerLogPageHandler(rootPath, rootPath, new WorkerLogs(stormConf, Paths.get(rootPath), metricsRegistry), new ResourceAuthorizer(stormConf), metricsRegistry);\r\n    final Response expectedAll = LogviewerResponseBuilder.buildSuccessJsonResponse(List.of(\"topoA/port1/worker.log\", \"topoA/port2/worker.log\", \"topoB/port1/worker.log\"), null, origin);\r\n    final Response expectedFilterPort = LogviewerResponseBuilder.buildSuccessJsonResponse(List.of(\"topoA/port1/worker.log\", \"topoB/port1/worker.log\"), null, origin);\r\n    final Response expectedFilterTopoId = LogviewerResponseBuilder.buildSuccessJsonResponse(List.of(\"topoB/port1/worker.log\"), null, origin);\r\n    final Response returnedAll = handler.listLogFiles(\"user\", null, null, null, origin);\r\n    final Response returnedFilterPort = handler.listLogFiles(\"user\", 1111, null, null, origin);\r\n    final Response returnedFilterTopoId = handler.listLogFiles(\"user\", null, \"topoB\", null, origin);\r\n    Utils.forceDelete(rootPath);\r\n    assertEqualsJsonResponse(expectedAll, returnedAll, List.class);\r\n    assertEqualsJsonResponse(expectedFilterPort, returnedFilterPort, List.class);\r\n    assertEqualsJsonResponse(expectedFilterTopoId, returnedFilterTopoId, List.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogPageHandlerTest.java",
  "methodName" : "testListLogFilesOutsideLogRoot",
  "sourceCode" : "@Test\r\npublic void testListLogFilesOutsideLogRoot() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        String origin = \"www.origin.server.net\";\r\n        LogviewerLogPageHandler handler = createHandlerForTraversalTests(rootPath.getFile().toPath());\r\n        //The response should be empty, since you should not be able to list files outside the worker log root.\r\n        final Response expected = LogviewerResponseBuilder.buildSuccessJsonResponse(List.of(), null, origin);\r\n        final Response returned = handler.listLogFiles(\"user\", null, \"../\", null, origin);\r\n        assertEqualsJsonResponse(expected, returned, List.class);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogPageHandlerTest.java",
  "methodName" : "testLogPageOutsideLogRoot",
  "sourceCode" : "@Test\r\npublic void testLogPageOutsideLogRoot() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogPageHandler handler = createHandlerForTraversalTests(rootPath.getFile().toPath());\r\n        final Response returned = handler.logPage(\"../nimbus.log\", 0, 100, null, \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        //Should not show files outside worker log root.\r\n        assertThat(returned.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogPageHandlerTest.java",
  "methodName" : "testDaemonLogPageOutsideLogRoot",
  "sourceCode" : "@Test\r\npublic void testDaemonLogPageOutsideLogRoot() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogPageHandler handler = createHandlerForTraversalTests(rootPath.getFile().toPath());\r\n        final Response returned = handler.daemonLogPage(\"../evil.sh\", 0, 100, null, \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        //Should not show files outside daemon log root.\r\n        assertThat(returned.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogPageHandlerTest.java",
  "methodName" : "testDaemonLogPagePathIntoWorkerLogs",
  "sourceCode" : "@Test\r\npublic void testDaemonLogPagePathIntoWorkerLogs() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerLogPageHandler handler = createHandlerForTraversalTests(rootPath.getFile().toPath());\r\n        final Response returned = handler.daemonLogPage(\"workers-artifacts/topoA/worker.log\", 0, 100, null, \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        //Should not show files outside log root.\r\n        assertThat(returned.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testSearchViaRestApiThrowsIfBogusFileIsGiven",
  "sourceCode" : "@Test\r\npublic void testSearchViaRestApiThrowsIfBogusFileIsGiven() throws InvalidRequestException {\r\n    LogviewerLogSearchHandler handler = getSearchHandler();\r\n    assertThrows(RuntimeException.class, () -> handler.substringSearch(null, \"a string\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testLogviewerLinkCentersTheMatchInThePage",
  "sourceCode" : "@Test\r\npublic void testLogviewerLinkCentersTheMatchInThePage() throws UnknownHostException {\r\n    String expectedFname = \"foobar.log\";\r\n    LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        String actualUrl = handler.urlToMatchCenteredInLogPage(new byte[42], new File(expectedFname).toPath(), 27526, 8888);\r\n        assertEquals(\"http://\" + expectedHost + \":\" + expectedPort + \"/api/v1/log?file=\" + expectedFname + \"&start=1947&length=\" + LogviewerConstant.DEFAULT_BYTES_PER_PAGE, actualUrl);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testLogviewerLinkCentersTheMatchInThePageDaemon",
  "sourceCode" : "@Test\r\npublic void testLogviewerLinkCentersTheMatchInThePageDaemon() throws UnknownHostException {\r\n    String expectedFname = \"foobar.log\";\r\n    LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        String actualUrl = handler.urlToMatchCenteredInLogPageDaemonFile(new byte[42], new File(expectedFname).toPath(), 27526, 8888);\r\n        assertEquals(\"http://\" + expectedHost + \":\" + expectedPort + \"/api/v1/daemonlog?file=\" + expectedFname + \"&start=1947&length=\" + LogviewerConstant.DEFAULT_BYTES_PER_PAGE, actualUrl);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testReturnsCorrectBeforeAndAfterContext",
  "sourceCode" : "@SuppressWarnings(\"checkstyle:LineLength\")\r\n@Test\r\npublic void testReturnsCorrectBeforeAndAfterContext() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"logviewer-search-context-tests.log.test\");\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(0, \"\", \" needle000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000needle \", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(7, \"needle \", \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000needle needle\\n\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(127, \"needle needle000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", \" needle\\n\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(134, \" needle000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000needle \", \"\\n\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAreallySmallLogFile",
  "sourceCode" : "@Test\r\npublic void testAreallySmallLogFile() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"small-worker.log.test\");\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(7, \"000000 \", \" 000000\\n\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAreallySmallLogDaemonFile",
  "sourceCode" : "@Test\r\npublic void testAreallySmallLogDaemonFile() throws InvalidRequestException, UnknownHostException {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"small-worker.log.test\");\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"yes\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(7, \"000000 \", \" 000000\\n\", pattern, \"/api/v1/daemonlog?file=\" + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearchDaemonLog(file.toPath(), pattern);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testNoOffsetReturnedWhenFileEndsOnBufferOffset",
  "sourceCode" : "@Test\r\npublic void testNoOffsetReturnedWhenFileEndsOnBufferOffset() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-3072.log.test\");\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(3066, \".\".repeat(128), \"\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern);\r\n        Map<String, Object> searchResult2 = handler.substringSearch(file.toPath(), pattern, 1);\r\n        assertEquals(expected, searchResult);\r\n        assertEquals(expected, searchResult2);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testNextByteOffsetsAreCorrectForEachMatch",
  "sourceCode" : "@SuppressWarnings(\"checkstyle:LineLength\")\r\n@Test\r\npublic void testNextByteOffsetsAreCorrectForEachMatch() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-worker.log.test\");\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        List<Tuple3<Integer, Integer, Integer>> dataAndExpected = new ArrayList<>();\r\n        // numMatchesSought, numMatchesFound, expectedNextByteOffset\r\n        dataAndExpected.add(new Tuple3<>(1, 1, 11));\r\n        dataAndExpected.add(new Tuple3<>(2, 2, 2042));\r\n        dataAndExpected.add(new Tuple3<>(3, 3, 2052));\r\n        dataAndExpected.add(new Tuple3<>(4, 4, 3078));\r\n        dataAndExpected.add(new Tuple3<>(5, 5, 3196));\r\n        dataAndExpected.add(new Tuple3<>(6, 6, 3202));\r\n        dataAndExpected.add(new Tuple3<>(7, 7, 6252));\r\n        dataAndExpected.add(new Tuple3<>(8, 8, 6321));\r\n        dataAndExpected.add(new Tuple3<>(9, 9, 6397));\r\n        dataAndExpected.add(new Tuple3<>(10, 10, 6476));\r\n        dataAndExpected.add(new Tuple3<>(11, 11, 6554));\r\n        dataAndExpected.add(new Tuple3<>(12, 12, null));\r\n        dataAndExpected.add(new Tuple3<>(13, 12, null));\r\n        dataAndExpected.forEach(data -> {\r\n            Map<String, Object> result;\r\n            try {\r\n                result = handler.substringSearch(file.toPath(), pattern, data.value1);\r\n                assertEquals(data.value3, result.get(\"nextByteOffset\"));\r\n                assertEquals(data.value2.intValue(), ((List) result.get(\"matches\")).size());\r\n            } catch (InvalidRequestException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        });\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        expected.put(\"nextByteOffset\", 6252);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(5, \"Test \", \" is near the beginning of the file.\\nThis file assumes a buffer size of 2048 bytes, a max search string size of 1024 bytes, and a\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(2036, \"ng 146\\npadding 147\\npadding 148\\npadding 149\\npadding 150\\npadding 151\\npadding 152\\npadding 153\\nNear the end of a 1024 byte block, a \", \".\\nA needle that straddles a 1024 byte boundary should also be detected.\\n\\npadding 157\\npadding 158\\npadding 159\\npadding 160\\npadding\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(2046, \"ding 147\\npadding 148\\npadding 149\\npadding 150\\npadding 151\\npadding 152\\npadding 153\\nNear the end of a 1024 byte block, a needle.\\nA \", \" that straddles a 1024 byte boundary should also be detected.\\n\\npadding 157\\npadding 158\\npadding 159\\npadding 160\\npadding 161\\npaddi\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(3072, \"adding 226\\npadding 227\\npadding 228\\npadding 229\\npadding 230\\npadding 231\\npadding 232\\npadding 233\\npadding 234\\npadding 235\\n\\n\\nHere a \", \" occurs just after a 1024 byte boundary.  It should have the correct context.\\n\\nText with two adjoining matches: needleneedle\\n\\npa\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(3190, \"\\n\\n\\nHere a needle occurs just after a 1024 byte boundary.  It should have the correct context.\\n\\nText with two adjoining matches: \", \"needle\\n\\npadding 243\\npadding 244\\npadding 245\\npadding 246\\npadding 247\\npadding 248\\npadding 249\\npadding 250\\npadding 251\\npadding 252\\n\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(3196, \"e a needle occurs just after a 1024 byte boundary.  It should have the correct context.\\n\\nText with two adjoining matches: needle\", \"\\n\\npadding 243\\npadding 244\\npadding 245\\npadding 246\\npadding 247\\npadding 248\\npadding 249\\npadding 250\\npadding 251\\npadding 252\\npaddin\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(6246, \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\n\\nHere are four non-ascii 1-byte UTF-8 characters: \\n\\n\", \"\\n\\nHere are four printable 2-byte UTF-8 characters: \\n\\nneedle\\n\\n\\n\\nHere are four printable 3-byte UTF-8 characters: \", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern, 7);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testCorrectMatchOffsetIsReturnedWhenSkippingBytes",
  "sourceCode" : "@SuppressWarnings(\"checkstyle:LineLength\")\r\n@Test\r\npublic void testCorrectMatchOffsetIsReturnedWhenSkippingBytes() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-worker.log.test\");\r\n        int startByteOffset = 3197;\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", startByteOffset);\r\n        expected.put(\"nextByteOffset\", 6252);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(6246, \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\n\\nHere are four non-ascii 1-byte UTF-8 characters: \\n\\n\", \"\\n\\nHere are four printable 2-byte UTF-8 characters: \\n\\nneedle\\n\\n\\n\\nHere are four printable 3-byte UTF-8 characters: \", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern, 1, startByteOffset);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAnotherPatterns1",
  "sourceCode" : "@SuppressWarnings(\"checkstyle:LineLength\")\r\n@Test\r\npublic void testAnotherPatterns1() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-worker.log.test\");\r\n        String pattern = \"X\".repeat(1024);\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        expected.put(\"nextByteOffset\", 6183);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(4075, \"\\n\\nThe following match of 1024 bytes completely fills half the byte buffer.  It is a search substring of the maximum size......\\n\\n\", \"\\nThe following max-size match straddles a 1024 byte buffer.\\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        matches.add(buildMatchData(5159, \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\nThe following max-size match straddles a 1024 byte buffer.\\n\", \"\\n\\nHere are four non-ascii 1-byte UTF-8 characters: \\n\\nneedle\\n\\nHere are four printable 2-byte UTF-8 characters: \", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern, 2);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAnotherPatterns2",
  "sourceCode" : "@SuppressWarnings(\"checkstyle:LineLength\")\r\n@Test\r\npublic void testAnotherPatterns2() throws Exception {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-worker.log.test\");\r\n        String pattern = \"\uD800\uDD00\uD800\uDD01\uD800\uDD02\";\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        expected.put(\"nextByteOffset\", 7176);\r\n        List<Map<String, Object>> matches = new ArrayList<>();\r\n        matches.add(buildMatchData(7164, \"padding 372\\npadding 373\\npadding 374\\npadding 375\\n\\nThe following tests multibyte UTF-8 Characters straddling the byte boundary:   \", \"\\n\\nneedle\", pattern, \"/api/v1/log?file=test\" + encodedFileSeparator() + \"resources\" + encodedFileSeparator() + file.getName() + \"&start=0&length=51200\"));\r\n        expected.put(\"matches\", matches);\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern, 1);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testReturnsZeroMatchesForUnseenPattern",
  "sourceCode" : "@Test\r\npublic void testReturnsZeroMatchesForUnseenPattern() throws UnknownHostException, InvalidRequestException {\r\n    Utils prevUtils = null;\r\n    try {\r\n        Utils mockedUtil = mock(Utils.class);\r\n        prevUtils = Utils.setInstance(mockedUtil);\r\n        String pattern = \"Not There\";\r\n        when(mockedUtil.hostname()).thenReturn(expectedHost);\r\n        final File file = new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"test-worker.log.test\");\r\n        Map<String, Object> expected = new HashMap<>();\r\n        expected.put(\"isDaemon\", \"no\");\r\n        expected.put(\"searchString\", pattern);\r\n        expected.put(\"startByteOffset\", 0);\r\n        expected.put(\"matches\", Collections.emptyList());\r\n        LogviewerLogSearchHandler handler = getSearchHandlerWithPort(expectedPort);\r\n        Map<String, Object> searchResult = handler.substringSearch(file.toPath(), pattern);\r\n        assertEquals(expected, searchResult);\r\n    } finally {\r\n        Utils.setInstance(prevUtils);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testFindNMatches",
  "sourceCode" : "/**\r\n * find-n-matches looks through logs properly.\r\n */\r\n@Test\r\npublic void testFindNMatches() {\r\n    List<Path> files = new ArrayList<>();\r\n    files.add(new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"logviewer-search-context-tests.log.test\").toPath());\r\n    files.add(new File(String.join(File.separator, \"src\", \"test\", \"resources\"), \"logviewer-search-context-tests.log.gz\").toPath());\r\n    final LogviewerLogSearchHandler handler = getSearchHandler();\r\n    final List<Map<String, Object>> matches1 = handler.findNMatches(files, 20, 0, 0, \"needle\").getMatches();\r\n    final List<Map<String, Object>> matches2 = handler.findNMatches(files, 20, 0, 126, \"needle\").getMatches();\r\n    final List<Map<String, Object>> matches3 = handler.findNMatches(files, 20, 1, 0, \"needle\").getMatches();\r\n    assertEquals(2, matches1.size());\r\n    assertEquals(4, ((List) matches1.get(0).get(\"matches\")).size());\r\n    assertEquals(4, ((List) matches1.get(1).get(\"matches\")).size());\r\n    assertEquals(String.join(File.separator, \"test\", \"resources\", \"logviewer-search-context-tests.log.test\"), matches1.get(0).get(\"fileName\"));\r\n    assertEquals(String.join(File.separator, \"test\", \"resources\", \"logviewer-search-context-tests.log.gz\"), matches1.get(1).get(\"fileName\"));\r\n    assertEquals(2, ((List) matches2.get(0).get(\"matches\")).size());\r\n    assertEquals(4, ((List) matches2.get(1).get(\"matches\")).size());\r\n    assertEquals(1, matches3.size());\r\n    assertEquals(4, ((List) matches3.get(0).get(\"matches\")).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAllPortsAndSearchArchivedIsTrue",
  "sourceCode" : "@Test\r\npublic void testAllPortsAndSearchArchivedIsTrue() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"*\", \"20\", \"199\", true, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    verify(handler, times(4)).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler, times(4)).logsForPort(isNull(), any());\r\n    // File offset and byte offset should always be zero when searching multiple workers (multiple ports).\r\n    assertEquals(logFiles, files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(0));\r\n    assertEquals(\"search\", search.getAllValues().get(0));\r\n    assertEquals(logFiles, files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(1));\r\n    assertEquals(\"search\", search.getAllValues().get(1));\r\n    assertEquals(logFiles, files.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(2));\r\n    assertEquals(\"search\", search.getAllValues().get(2));\r\n    assertEquals(logFiles, files.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(3));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(3));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(3));\r\n    assertEquals(\"search\", search.getAllValues().get(3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testAllPortsAndSearchArchivedIsFalse",
  "sourceCode" : "@Test\r\npublic void testAllPortsAndSearchArchivedIsFalse() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", null, \"20\", \"199\", false, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    verify(handler, times(4)).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler, times(4)).logsForPort(isNull(), any());\r\n    // File offset and byte offset should always be zero when searching multiple workers (multiple ports).\r\n    assertEquals(Collections.singletonList(logFiles.get(0)), files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(0));\r\n    assertEquals(\"search\", search.getAllValues().get(0));\r\n    assertEquals(Collections.singletonList(logFiles.get(0)), files.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(1));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(1));\r\n    assertEquals(\"search\", search.getAllValues().get(1));\r\n    assertEquals(Collections.singletonList(logFiles.get(0)), files.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(2));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(2));\r\n    assertEquals(\"search\", search.getAllValues().get(2));\r\n    assertEquals(Collections.singletonList(logFiles.get(0)), files.getAllValues().get(3));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(3));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(3));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(3));\r\n    assertEquals(\"search\", search.getAllValues().get(3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testOnePortAndSearchArchivedIsTrueAndNotFileOffset",
  "sourceCode" : "@Test\r\npublic void testOnePortAndSearchArchivedIsTrueAndNotFileOffset() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"6700\", \"0\", \"0\", true, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    verify(handler, times(1)).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler).logsForPort(isNull(), any());\r\n    assertEquals(logFiles, files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(0));\r\n    assertEquals(\"search\", search.getAllValues().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testOnePortAndSearchArchivedIsTrueAndFileOffsetIs1",
  "sourceCode" : "@Test\r\npublic void testOnePortAndSearchArchivedIsTrueAndFileOffsetIs1() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"6700\", \"1\", \"0\", true, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    verify(handler, times(1)).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler).logsForPort(isNull(), any());\r\n    assertEquals(logFiles, files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(1), fileOffset.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(0));\r\n    assertEquals(\"search\", search.getAllValues().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testOnePortAndSearchArchivedIsFalseAndFileOffsetIs1",
  "sourceCode" : "@Test\r\npublic void testOnePortAndSearchArchivedIsFalseAndFileOffsetIs1() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"6700\", \"1\", \"0\", false, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    verify(handler, times(1)).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler).logsForPort(isNull(), any());\r\n    // File offset should be zero, since search-archived is false.\r\n    assertEquals(Collections.singletonList(logFiles.get(0)), files.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(20), numMatches.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), fileOffset.getAllValues().get(0));\r\n    assertEquals(Integer.valueOf(0), offset.getAllValues().get(0));\r\n    assertEquals(\"search\", search.getAllValues().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testOnePortAndSearchArchivedIsTrueAndFileOffsetIs1AndByteOffsetIs100",
  "sourceCode" : "@Test\r\npublic void testOnePortAndSearchArchivedIsTrueAndFileOffsetIs1AndByteOffsetIs100() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"6700\", \"1\", \"100\", true, null, null);\r\n    verify(handler, times(1)).findNMatches(anyList(), anyInt(), anyInt(), anyInt(), anyString());\r\n    verify(handler, times(1)).logsForPort(isNull(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerLogSearchHandlerTest.java",
  "methodName" : "testBadPortAndSearchArchivedIsFalseAndFileOffsetIs1",
  "sourceCode" : "@Test\r\npublic void testBadPortAndSearchArchivedIsFalseAndFileOffsetIs1() throws IOException {\r\n    LogviewerLogSearchHandler handler = getStubbedSearchHandler();\r\n    handler.deepSearchLogsForTopology(\"\", null, \"search\", \"20\", \"2700\", \"1\", \"0\", false, null, null);\r\n    ArgumentCaptor<List> files = ArgumentCaptor.forClass(List.class);\r\n    ArgumentCaptor<Integer> numMatches = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> fileOffset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<Integer> offset = ArgumentCaptor.forClass(Integer.class);\r\n    ArgumentCaptor<String> search = ArgumentCaptor.forClass(String.class);\r\n    // Called with a bad port (not in the config) No searching should be done.\r\n    verify(handler, never()).findNMatches(files.capture(), numMatches.capture(), fileOffset.capture(), offset.capture(), search.capture());\r\n    verify(handler, never()).logsForPort(anyString(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testListDumpFiles",
  "sourceCode" : "@Test\r\npublic void testListDumpFiles() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.listDumpFiles(\"topoA\", \"localhost:1111\", \"user\");\r\n        Response topoBResponse = handler.listDumpFiles(\"topoB\", \"localhost:1111\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        String contentA = (String) topoAResponse.getEntity();\r\n        assertThat(contentA, containsString(\"worker.jfr\"));\r\n        assertThat(contentA, not(containsString(\"worker.bin\")));\r\n        assertThat(contentA, not(containsString(\"worker.txt\")));\r\n        String contentB = (String) topoBResponse.getEntity();\r\n        assertThat(contentB, containsString(\"worker.txt\"));\r\n        assertThat(contentB, not(containsString(\"worker.jfr\")));\r\n        assertThat(contentB, not(containsString(\"worker.bin\")));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testListDumpFilesTraversalInTopoId",
  "sourceCode" : "@Test\r\npublic void testListDumpFilesTraversalInTopoId() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response response = handler.listDumpFiles(\"../../\", \"localhost:logs\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(response.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testListDumpFilesTraversalInPort",
  "sourceCode" : "@Test\r\npublic void testListDumpFilesTraversalInPort() throws Exception {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response response = handler.listDumpFiles(\"../\", \"localhost:../logs\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(response.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testDownloadDumpFile",
  "sourceCode" : "@Test\r\npublic void testDownloadDumpFile() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.downloadDumpFile(\"topoA\", \"localhost:1111\", \"worker.jfr\", \"user\");\r\n        Response topoBResponse = handler.downloadDumpFile(\"topoB\", \"localhost:1111\", \"worker.txt\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        assertThat(topoAResponse.getEntity(), not(nullValue()));\r\n        String topoAContentDisposition = topoAResponse.getHeaderString(HttpHeaders.CONTENT_DISPOSITION);\r\n        assertThat(topoAContentDisposition, containsString(\"localhost-topoA-1111-worker.jfr\"));\r\n        assertThat(topoBResponse.getStatus(), is(Response.Status.OK.getStatusCode()));\r\n        assertThat(topoBResponse.getEntity(), not(nullValue()));\r\n        String topoBContentDisposition = topoBResponse.getHeaderString(HttpHeaders.CONTENT_DISPOSITION);\r\n        assertThat(topoBContentDisposition, containsString(\"localhost-topoB-1111-worker.txt\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testDownloadDumpFileTraversalInTopoId",
  "sourceCode" : "@Test\r\npublic void testDownloadDumpFileTraversalInTopoId() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.downloadDumpFile(\"../../\", \"localhost:logs\", \"daemon-dump.bin\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\handler\\LogviewerProfileHandlerTest.java",
  "methodName" : "testDownloadDumpFileTraversalInPort",
  "sourceCode" : "@Test\r\npublic void testDownloadDumpFileTraversalInPort() throws IOException {\r\n    try (TmpPath rootPath = new TmpPath()) {\r\n        LogviewerProfileHandler handler = createHandlerTraversalTests(rootPath.getFile().toPath());\r\n        Response topoAResponse = handler.downloadDumpFile(\"../\", \"localhost:../logs\", \"daemon-dump.bin\", \"user\");\r\n        Utils.forceDelete(rootPath.toString());\r\n        assertThat(topoAResponse.getStatus(), is(Response.Status.NOT_FOUND.getStatusCode()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleanerTest.java",
  "methodName" : "testMkFileFilterForLogCleanup",
  "sourceCode" : "/**\r\n * Log file filter selects the correct worker-log dirs for purge.\r\n */\r\n@Test\r\npublic void testMkFileFilterForLogCleanup() throws IOException {\r\n    // this is to read default value for other configurations\r\n    Map<String, Object> conf = Utils.readStormConfig();\r\n    conf.put(LOGVIEWER_CLEANUP_AGE_MINS, 60);\r\n    conf.put(LOGVIEWER_CLEANUP_INTERVAL_SECS, 300);\r\n    StormMetricsRegistry metricRegistry = new StormMetricsRegistry();\r\n    WorkerLogs workerLogs = new WorkerLogs(conf, Paths.get(\"\"), metricRegistry);\r\n    LogCleaner logCleaner = new LogCleaner(conf, workerLogs, new DirectoryCleaner(metricRegistry), null, metricRegistry);\r\n    final long nowMillis = Time.currentTimeMillis();\r\n    final long cutoffMillis = logCleaner.cleanupCutoffAgeMillis(nowMillis);\r\n    final long oldMtimeMillis = cutoffMillis - 500;\r\n    final long newMtimeMillis = cutoffMillis + 500;\r\n    try (TmpPath testDir = new TmpPath()) {\r\n        Files.createDirectories(testDir.getFile().toPath());\r\n        List<Path> matchingFiles = Arrays.asList(createDir(testDir.getFile().toPath(), \"3031\", oldMtimeMillis), createDir(testDir.getFile().toPath(), \"3032\", oldMtimeMillis), createDir(testDir.getFile().toPath(), \"7077\", oldMtimeMillis));\r\n        List<Path> excludedFiles = Arrays.asList(createFile(testDir.getFile().toPath(), \"oldlog-1-2-worker-.log\", oldMtimeMillis), createFile(testDir.getFile().toPath(), \"newlog-1-2-worker-.log\", newMtimeMillis), createFile(testDir.getFile().toPath(), \"some-old-file.txt\", oldMtimeMillis), createFile(testDir.getFile().toPath(), \"olddir-1-2-worker.log\", newMtimeMillis), createFile(testDir.getFile().toPath(), \"metadata\", newMtimeMillis), createFile(testDir.getFile().toPath(), \"newdir\", newMtimeMillis));\r\n        Predicate<Path> fileFilter = logCleaner.mkFileFilterForLogCleanup(nowMillis);\r\n        matchingFiles.forEach(p -> assertTrue(fileFilter.test(p), \"Missing \" + p.getFileName()));\r\n        excludedFiles.forEach(p -> assertFalse(fileFilter.test(p), \"Not excluded \" + p.getFileName()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleanerTest.java",
  "methodName" : "testPerWorkerDirectoryCleanup",
  "sourceCode" : "/**\r\n * cleaner deletes oldest files in each worker dir if files are larger than per-dir quota.\r\n */\r\n@Test\r\npublic void testPerWorkerDirectoryCleanup() throws IOException {\r\n    long nowMillis = Time.currentTimeMillis();\r\n    try (TmpPath testDir = new TmpPath()) {\r\n        Files.createDirectories(testDir.getFile().toPath());\r\n        Path rootDir = createDir(testDir.getFile().toPath(), \"workers-artifacts\");\r\n        Path topo1Dir = createDir(rootDir, \"topo1\");\r\n        Path topo2Dir = createDir(rootDir, \"topo2\");\r\n        Path port1Dir = createDir(topo1Dir, \"port1\");\r\n        Path port2Dir = createDir(topo1Dir, \"port2\");\r\n        Path port3Dir = createDir(topo2Dir, \"port3\");\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port1Dir, \"A\" + idx, nowMillis + 100L * idx, 200));\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port2Dir, \"B\" + idx, nowMillis + 100L * idx, 200));\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port3Dir, \"C\" + idx, nowMillis + 100L * idx, 200));\r\n        Map<String, Object> conf = Utils.readStormConfig();\r\n        StormMetricsRegistry metricRegistry = new StormMetricsRegistry();\r\n        WorkerLogs workerLogs = new WorkerLogs(conf, rootDir, metricRegistry);\r\n        LogCleaner logCleaner = new LogCleaner(conf, workerLogs, new DirectoryCleaner(metricRegistry), rootDir, metricRegistry);\r\n        List<Integer> deletedFiles = logCleaner.perWorkerDirCleanup(1200).stream().map(deletionMeta -> deletionMeta.deletedFiles).collect(toList());\r\n        assertEquals(Integer.valueOf(4), deletedFiles.get(0));\r\n        assertEquals(Integer.valueOf(4), deletedFiles.get(1));\r\n        assertEquals(Integer.valueOf(4), deletedFiles.get(deletedFiles.size() - 1));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleanerTest.java",
  "methodName" : "testGlobalLogCleanup",
  "sourceCode" : "@Test\r\npublic void testGlobalLogCleanup() throws Exception {\r\n    long nowMillis = Time.currentTimeMillis();\r\n    try (TmpPath testDir = new TmpPath()) {\r\n        Files.createDirectories(testDir.getFile().toPath());\r\n        Path rootDir = createDir(testDir.getFile().toPath(), \"workers-artifacts\");\r\n        Path topo1Dir = createDir(rootDir, \"topo1\");\r\n        Path topo2Dir = createDir(rootDir, \"topo2\");\r\n        // note that port1Dir is active worker containing active logs\r\n        Path port1Dir = createDir(topo1Dir, \"port1\");\r\n        Path port2Dir = createDir(topo1Dir, \"port2\");\r\n        Path port3Dir = createDir(topo2Dir, \"port3\");\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port1Dir, \"A\" + idx + \".log\", nowMillis + 100L * idx, 200));\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port2Dir, \"B\" + idx, nowMillis + 100L * idx, 200));\r\n        IntStream.range(0, 10).forEach(idx -> createFile(port3Dir, \"C\" + idx, nowMillis + 100L * idx, 200));\r\n        Map<String, Object> conf = Utils.readStormConfig();\r\n        StormMetricsRegistry metricRegistry = new StormMetricsRegistry();\r\n        WorkerLogs stubbedWorkerLogs = new WorkerLogs(conf, rootDir, metricRegistry) {\r\n\r\n            @Override\r\n            public SortedSet<Path> getAliveWorkerDirs() {\r\n                return new TreeSet<>(Collections.singletonList(port1Dir));\r\n            }\r\n        };\r\n        LogCleaner logCleaner = new LogCleaner(conf, stubbedWorkerLogs, new DirectoryCleaner(metricRegistry), rootDir, metricRegistry);\r\n        int deletedFiles = logCleaner.globalLogCleanup(2400).deletedFiles;\r\n        assertEquals(18, deletedFiles);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleanerTest.java",
  "methodName" : "testGetDeadWorkerDirs",
  "sourceCode" : "/**\r\n * return directories for workers that are not alive.\r\n */\r\n@Test\r\npublic void testGetDeadWorkerDirs() throws Exception {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    stormConf.put(SUPERVISOR_WORKER_TIMEOUT_SECS, 5);\r\n    LSWorkerHeartbeat hb = new LSWorkerHeartbeat();\r\n    hb.set_time_secs(1);\r\n    Map<String, LSWorkerHeartbeat> idToHb = Collections.singletonMap(\"42\", hb);\r\n    int nowSecs = 2;\r\n    try (TmpPath testDir = new TmpPath()) {\r\n        Path unexpectedDir1 = createDir(testDir.getFile().toPath(), \"dir1\");\r\n        Path expectedDir2 = createDir(testDir.getFile().toPath(), \"dir2\");\r\n        Path expectedDir3 = createDir(testDir.getFile().toPath(), \"dir3\");\r\n        Set<Path> logDirs = Sets.newSet(unexpectedDir1, expectedDir2, expectedDir3);\r\n        SupervisorUtils mockedSupervisorUtils = mock(SupervisorUtils.class);\r\n        SupervisorUtils.setInstance(mockedSupervisorUtils);\r\n        Map<String, Object> conf = Utils.readStormConfig();\r\n        StormMetricsRegistry metricRegistry = new StormMetricsRegistry();\r\n        WorkerLogs stubbedWorkerLogs = new WorkerLogs(conf, Paths.get(\"\"), metricRegistry) {\r\n\r\n            @Override\r\n            public SortedSet<Path> getLogDirs(Set<Path> logDirs, Predicate<String> predicate) {\r\n                TreeSet<Path> ret = new TreeSet<>();\r\n                if (predicate.test(\"42\")) {\r\n                    ret.add(unexpectedDir1);\r\n                }\r\n                if (predicate.test(\"007\")) {\r\n                    ret.add(expectedDir2);\r\n                }\r\n                if (predicate.test(\"\")) {\r\n                    ret.add(expectedDir3);\r\n                }\r\n                return ret;\r\n            }\r\n        };\r\n        LogCleaner logCleaner = new LogCleaner(conf, stubbedWorkerLogs, new DirectoryCleaner(metricRegistry), null, metricRegistry);\r\n        when(mockedSupervisorUtils.readWorkerHeartbeatsImpl(anyMap())).thenReturn(idToHb);\r\n        assertEquals(Sets.newSet(expectedDir2, expectedDir3), logCleaner.getDeadWorkerDirs(nowSecs, logDirs));\r\n    } finally {\r\n        SupervisorUtils.resetInstance();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\LogCleanerTest.java",
  "methodName" : "testCleanupFn",
  "sourceCode" : "/**\r\n * cleanup function forceDeletes files of dead workers.\r\n */\r\n@Test\r\npublic void testCleanupFn() throws IOException {\r\n    try (TmpPath dir1 = new TmpPath();\r\n        TmpPath dir2 = new TmpPath()) {\r\n        Files.createDirectory(dir1.getFile().toPath());\r\n        Files.createDirectory(dir2.getFile().toPath());\r\n        Map<String, Object> conf = Utils.readStormConfig();\r\n        StormMetricsRegistry metricRegistry = new StormMetricsRegistry();\r\n        WorkerLogs stubbedWorkerLogs = new WorkerLogs(conf, Paths.get(\"\"), metricRegistry);\r\n        LogCleaner logCleaner = new LogCleaner(conf, stubbedWorkerLogs, new DirectoryCleaner(metricRegistry), null, metricRegistry) {\r\n\r\n            @Override\r\n            Set<Path> selectDirsForCleanup(long nowMillis) {\r\n                return Collections.emptySet();\r\n            }\r\n\r\n            @Override\r\n            SortedSet<Path> getDeadWorkerDirs(int nowSecs, Set<Path> logDirs) {\r\n                SortedSet<Path> dirs = new TreeSet<>();\r\n                dirs.add(dir1.getFile().toPath());\r\n                dirs.add(dir2.getFile().toPath());\r\n                return dirs;\r\n            }\r\n\r\n            @Override\r\n            void cleanupEmptyTopoDirectory(Path dir) {\r\n            }\r\n        };\r\n        logCleaner.run();\r\n        assertThat(Files.exists(dir1.getFile().toPath()), is(false));\r\n        assertThat(Files.exists(dir2.getFile().toPath()), is(false));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserAllowClusterAdmin",
  "sourceCode" : "/**\r\n * allow cluster admin.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserAllowClusterAdmin() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    conf.put(NIMBUS_ADMINS, Collections.singletonList(\"alice\"));\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.emptySet(), Collections.emptySet())).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.emptySet()).when(authorizer).getUserGroups(anyString());\r\n    assertTrue(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserIgnoreAnyClusterSetTopologyUsersAndTopologyGroups",
  "sourceCode" : "/**\r\n * ignore any cluster-set topology.users topology.groups.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserIgnoreAnyClusterSetTopologyUsersAndTopologyGroups() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    conf.put(TOPOLOGY_USERS, Collections.singletonList(\"alice\"));\r\n    conf.put(TOPOLOGY_GROUPS, Collections.singletonList(\"alice-group\"));\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.emptySet(), Collections.emptySet())).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.singleton(\"alice-group\")).when(authorizer).getUserGroups(anyString());\r\n    assertFalse(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserAllowClusterLogsUser",
  "sourceCode" : "/**\r\n * allow cluster logs user.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserAllowClusterLogsUser() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    conf.put(LOGS_USERS, Collections.singletonList(\"alice\"));\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.emptySet(), Collections.emptySet())).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.emptySet()).when(authorizer).getUserGroups(anyString());\r\n    assertTrue(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserAllowWhitelistedTopologyUser",
  "sourceCode" : "/**\r\n * allow whitelisted topology user.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserAllowWhitelistedTopologyUser() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.singleton(\"alice\"), Collections.emptySet())).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.emptySet()).when(authorizer).getUserGroups(anyString());\r\n    assertTrue(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserAllowWhitelistedTopologyGroup",
  "sourceCode" : "/**\r\n * allow whitelisted topology group.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserAllowWhitelistedTopologyGroup() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.emptySet(), Collections.singleton(\"alice-group\"))).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.singleton(\"alice-group\")).when(authorizer).getUserGroups(anyString());\r\n    assertTrue(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testAuthorizedLogUserDisallowUserNotInNimbusAdminNorTopoUserNorLogsUserNotWhitelist",
  "sourceCode" : "/**\r\n * disallow user not in nimbus admin, topo user, logs user, or whitelist.\r\n */\r\n@Test\r\npublic void testAuthorizedLogUserDisallowUserNotInNimbusAdminNorTopoUserNorLogsUserNotWhitelist() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    doReturn(new ResourceAuthorizer.LogUserGroupWhitelist(Collections.emptySet(), Collections.emptySet())).when(authorizer).getLogUserGroupWhitelist(anyString());\r\n    doReturn(Collections.emptySet()).when(authorizer).getUserGroups(anyString());\r\n    assertFalse(authorizer.isAuthorizedLogUser(\"alice\", \"non-blank-fname\"));\r\n    verifyStubMethodsAreCalledProperly(authorizer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "testFailOnUpwardPathTraversal",
  "sourceCode" : "/**\r\n * disallow upward path traversal in filenames.\r\n */\r\n@Test\r\npublic void testFailOnUpwardPathTraversal() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    ResourceAuthorizer authorizer = new ResourceAuthorizer(conf);\r\n    Assertions.assertThrows(IllegalArgumentException.class, () -> authorizer.isAuthorizedLogUser(\"user\", Paths.get(\"some/../path\").toString()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\ResourceAuthorizerTest.java",
  "methodName" : "authorizationFailsWhenFilterConfigured",
  "sourceCode" : "@Test\r\npublic void authorizationFailsWhenFilterConfigured() {\r\n    Map<String, Object> stormConf = Utils.readStormConfig();\r\n    Map<String, Object> conf = new HashMap<>(stormConf);\r\n    ResourceAuthorizer authorizer = spy(new ResourceAuthorizer(conf));\r\n    Mockito.when(authorizer.isAuthorizedLogUser(anyString(), anyString())).thenReturn(false);\r\n    boolean authorized = authorizer.isUserAllowedToAccessFile(\"bob\", \"anyfile\");\r\n    // no filter configured, allow anyone\r\n    assertTrue(authorized);\r\n    conf.put(DaemonConfig.LOGVIEWER_FILTER, \"someFilter\");\r\n    authorized = authorizer.isUserAllowedToAccessFile(\"bob\", \"anyfile\");\r\n    // filter configured, should fail all users\r\n    assertFalse(authorized);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\logviewer\\utils\\WorkerLogsTest.java",
  "methodName" : "testIdentifyWorkerLogDirs",
  "sourceCode" : "/**\r\n * Build up workerid-workerlogdir map for the old workers' dirs.\r\n */\r\n@Test\r\npublic void testIdentifyWorkerLogDirs() throws Exception {\r\n    try (TmpPath testDir = new TmpPath()) {\r\n        Path port1Dir = Files.createDirectories(testDir.getFile().toPath().resolve(\"workers-artifacts/topo1/port1\"));\r\n        Path metaFile = Files.createFile(testDir.getFile().toPath().resolve(\"worker.yaml\"));\r\n        String expId = \"id12345\";\r\n        SortedSet<Path> expected = new TreeSet<>();\r\n        expected.add(port1Dir);\r\n        SupervisorUtils mockedSupervisorUtils = mock(SupervisorUtils.class);\r\n        SupervisorUtils.setInstance(mockedSupervisorUtils);\r\n        Map<String, Object> stormConf = Utils.readStormConfig();\r\n        WorkerLogs workerLogs = new WorkerLogs(stormConf, port1Dir, new StormMetricsRegistry()) {\r\n\r\n            @Override\r\n            public Optional<Path> getMetadataFileForWorkerLogDir(Path logDir) {\r\n                return Optional.of(metaFile);\r\n            }\r\n\r\n            @Override\r\n            public String getWorkerIdFromMetadataFile(Path metaFile) {\r\n                return expId;\r\n            }\r\n        };\r\n        when(mockedSupervisorUtils.readWorkerHeartbeatsImpl(anyMap())).thenReturn(null);\r\n        assertEquals(expected, workerLogs.getLogDirs(Collections.singleton(port1Dir), (wid) -> true));\r\n    } finally {\r\n        SupervisorUtils.resetInstance();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologyBoltAggStatsMap_includesLastError",
  "sourceCode" : "/**\r\n * Very narrow test case to validate that 'last error' fields are populated for a bolt\r\n * with an error is present.\r\n */\r\n@Test\r\nvoid test_getTopologyBoltAggStatsMap_includesLastError() {\r\n    // Define inputs\r\n    final String expectedBoltId = \"MyBoltId\";\r\n    final String expectedErrorMsg = \"This is my test error message\";\r\n    final int expectedErrorTime = Time.currentTimeSecs();\r\n    final int errorElapsedTimeSecs = 13;\r\n    final int expectedErrorElapsedTime = expectedErrorTime + errorElapsedTimeSecs;\r\n    final int expectedErrorPort = 4321;\r\n    final String expectedErrorHost = \"my.errored.host\";\r\n    // Define our Last Error\r\n    final ErrorInfo expectedLastError = new ErrorInfo(expectedErrorMsg, expectedErrorTime);\r\n    expectedLastError.set_port(expectedErrorPort);\r\n    expectedLastError.set_host(expectedErrorHost);\r\n    // Build stats instance for our bolt\r\n    final ComponentAggregateStats aggregateStats = buildBoltAggregateStatsBase();\r\n    aggregateStats.set_last_error(expectedLastError);\r\n    addBoltStats(expectedBoltId, aggregateStats);\r\n    // Advance time by 'errorElapsedTimeSecs'\r\n    Time.advanceTimeSecs(errorElapsedTimeSecs);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Bolt result\r\n    final Map<String, Object> boltResult = getBoltStatsFromTopologySummaryResult(result, expectedBoltId);\r\n    assertNotNull(boltResult, \"Should have an entry for bolt\");\r\n    // Verify each piece\r\n    assertEquals(expectedBoltId, boltResult.get(\"boltId\"));\r\n    assertEquals(expectedBoltId, boltResult.get(\"encodedBoltId\"));\r\n    // Verify error\r\n    assertEquals(expectedErrorMsg, boltResult.get(\"lastError\"));\r\n    assertEquals(expectedErrorPort, boltResult.get(\"errorPort\"));\r\n    assertEquals(expectedErrorHost, boltResult.get(\"errorHost\"));\r\n    assertEquals(expectedErrorTime, boltResult.get(\"errorTime\"));\r\n    assertEquals(expectedErrorElapsedTime, boltResult.get(\"errorLapsedSecs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologyBoltAggStatsMap_hasNoLastError",
  "sourceCode" : "/**\r\n * Very narrow test case to validate that 'last error' fields are NOT populated for a bolt\r\n * that does NOT have a last error associated.\r\n */\r\n@Test\r\nvoid test_getTopologyBoltAggStatsMap_hasNoLastError() {\r\n    // Define inputs\r\n    final String expectedBoltId = \"MyBoltId\";\r\n    // Build stats instance for our bolt\r\n    final ComponentAggregateStats aggregateStats = buildBoltAggregateStatsBase();\r\n    addBoltStats(expectedBoltId, aggregateStats);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Bolt result\r\n    final Map<String, Object> boltResult = getBoltStatsFromTopologySummaryResult(result, expectedBoltId);\r\n    assertNotNull(boltResult, \"Should have an entry for bolt\");\r\n    // Verify each piece\r\n    assertEquals(expectedBoltId, boltResult.get(\"boltId\"));\r\n    assertEquals(expectedBoltId, boltResult.get(\"encodedBoltId\"));\r\n    // Verify error fields exist, but are not populated.\r\n    // These fields default to empty string.\r\n    assertTrue(boltResult.containsKey(\"lastError\"));\r\n    assertEquals(\"\", boltResult.get(\"lastError\"), \"Backwards compat. with API docs say this should be empty string when empty\");\r\n    assertTrue(boltResult.containsKey(\"errorHost\"));\r\n    assertEquals(\"\", boltResult.get(\"errorHost\"));\r\n    assertTrue(boltResult.containsKey(\"errorWorkerLogLink\"));\r\n    assertEquals(\"\", boltResult.get(\"errorWorkerLogLink\"));\r\n    // These fields default to null.\r\n    assertTrue(boltResult.containsKey(\"errorPort\"));\r\n    assertNull(boltResult.get(\"errorPort\"));\r\n    assertTrue(boltResult.containsKey(\"errorTime\"));\r\n    assertNull(boltResult.get(\"errorTime\"));\r\n    assertTrue(boltResult.containsKey(\"errorLapsedSecs\"));\r\n    assertNull(boltResult.get(\"errorLapsedSecs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologyBoltAggStatsMap_generalFields",
  "sourceCode" : "/**\r\n * A more general test case that a bolt's aggregate stats are\r\n * correctly populated into the resulting map.\r\n */\r\n@Test\r\nvoid test_getTopologyBoltAggStatsMap_generalFields() {\r\n    // Define inputs\r\n    final String expectedBoltId = \"MyBoltId\";\r\n    final float expectedCapacity = 0.97f;\r\n    final double expectedProcessLatency = 432.0D;\r\n    final double expectedExecuteLatency = 122.0D;\r\n    final long expectedExecuted = 153343L;\r\n    final long expectedEmitted = 43234L;\r\n    final long expectedAcked = 5553L;\r\n    final long expectedFailed = 220L;\r\n    final int expectedExecutors = 2;\r\n    final int expectedTasks = 3;\r\n    final long expectedTransferred = 3423423L;\r\n    final double expectedOnMemoryHeap = 1024D;\r\n    final double expectedOffMemoryHeap = 2048D;\r\n    final double expectedCpuCorePercent = 75D;\r\n    // Build stats instance for our bolt\r\n    final ComponentAggregateStats aggregateStats = buildBoltAggregateStatsBase();\r\n    // Common stats\r\n    final CommonAggregateStats commonStats = aggregateStats.get_common_stats();\r\n    commonStats.set_acked(expectedAcked);\r\n    commonStats.set_emitted(expectedEmitted);\r\n    commonStats.set_failed(expectedFailed);\r\n    commonStats.set_num_executors(expectedExecutors);\r\n    commonStats.set_num_tasks(expectedTasks);\r\n    commonStats.set_transferred(expectedTransferred);\r\n    // Bolt stats\r\n    final BoltAggregateStats boltStats = aggregateStats.get_specific_stats().get_bolt();\r\n    boltStats.set_capacity(expectedCapacity);\r\n    boltStats.set_execute_latency_ms(expectedExecuteLatency);\r\n    boltStats.set_process_latency_ms(expectedProcessLatency);\r\n    boltStats.set_executed(expectedExecuted);\r\n    // Build Resources Map\r\n    final Map<String, Double> resourcesMap = new HashMap<>();\r\n    resourcesMap.put(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, expectedOnMemoryHeap);\r\n    resourcesMap.put(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, expectedOffMemoryHeap);\r\n    resourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, expectedCpuCorePercent);\r\n    commonStats.set_resources_map(resourcesMap);\r\n    // Add to TopologyPageInfo\r\n    addBoltStats(expectedBoltId, aggregateStats);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Bolt result\r\n    final Map<String, Object> boltResult = getBoltStatsFromTopologySummaryResult(result, expectedBoltId);\r\n    assertNotNull(boltResult, \"Should have an entry for bolt\");\r\n    // Validate fields\r\n    assertEquals(expectedBoltId, boltResult.get(\"boltId\"));\r\n    assertEquals(expectedBoltId, boltResult.get(\"encodedBoltId\"));\r\n    assertEquals(expectedTransferred, boltResult.get(\"transferred\"));\r\n    assertEquals(String.format(\"%.3f\", expectedExecuteLatency), boltResult.get(\"executeLatency\"));\r\n    assertEquals(String.format(\"%.3f\", expectedProcessLatency), boltResult.get(\"processLatency\"));\r\n    assertEquals(expectedExecuted, boltResult.get(\"executed\"));\r\n    assertEquals(expectedFailed, boltResult.get(\"failed\"));\r\n    assertEquals(expectedAcked, boltResult.get(\"acked\"));\r\n    assertEquals(String.format(\"%.3f\", expectedCapacity), boltResult.get(\"capacity\"));\r\n    assertEquals(expectedEmitted, boltResult.get(\"emitted\"));\r\n    assertEquals(expectedExecutors, boltResult.get(\"executors\"));\r\n    assertEquals(expectedTasks, boltResult.get(\"tasks\"));\r\n    // Validate resources\r\n    assertEquals(expectedOnMemoryHeap, (double) boltResult.get(\"requestedMemOnHeap\"), 0.01);\r\n    assertEquals(expectedOffMemoryHeap, (double) boltResult.get(\"requestedMemOffHeap\"), 0.01);\r\n    assertEquals(expectedCpuCorePercent, (double) boltResult.get(\"requestedCpu\"), 0.01);\r\n    assertEquals(\"\", boltResult.get(\"requestedGenericResourcesComp\"));\r\n    // We expect there to be no error populated.\r\n    assertEquals(\"\", boltResult.get(\"lastError\"), \"No error should be reported as empty string\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologySpoutAggStatsMap_includesLastError",
  "sourceCode" : "/**\r\n * Very narrow test case to validate that 'last error' fields are populated for a spout\r\n * with an error is present.\r\n */\r\n@Test\r\nvoid test_getTopologySpoutAggStatsMap_includesLastError() {\r\n    // Define inputs\r\n    final String expectedSpoutId = \"MySpoutId\";\r\n    final String expectedErrorMsg = \"This is my test error message\";\r\n    final int expectedErrorTime = Time.currentTimeSecs();\r\n    final int errorElapsedTimeSecs = 13;\r\n    final int expectedErrorElapsedTime = expectedErrorTime + errorElapsedTimeSecs;\r\n    final int expectedErrorPort = 4321;\r\n    final String expectedErrorHost = \"my.errored.host\";\r\n    // Define our Last Error\r\n    final ErrorInfo expectedLastError = new ErrorInfo(expectedErrorMsg, expectedErrorTime);\r\n    expectedLastError.set_port(expectedErrorPort);\r\n    expectedLastError.set_host(expectedErrorHost);\r\n    // Build stats instance for our spout\r\n    final ComponentAggregateStats aggregateStats = buildSpoutAggregateStatsBase();\r\n    aggregateStats.set_last_error(expectedLastError);\r\n    addSpoutStats(expectedSpoutId, aggregateStats);\r\n    // Advance time by 'errorElapsedTimeSecs'\r\n    Time.advanceTimeSecs(errorElapsedTimeSecs);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Spout result\r\n    final Map<String, Object> spoutResult = getSpoutStatsFromTopologySummaryResult(result, expectedSpoutId);\r\n    assertNotNull(spoutResult, \"Should have an entry for spout\");\r\n    // Verify each piece\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"spoutId\"));\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"encodedSpoutId\"));\r\n    // Verify error\r\n    assertEquals(expectedErrorMsg, spoutResult.get(\"lastError\"));\r\n    assertEquals(expectedErrorPort, spoutResult.get(\"errorPort\"));\r\n    assertEquals(expectedErrorHost, spoutResult.get(\"errorHost\"));\r\n    assertEquals(expectedErrorTime, spoutResult.get(\"errorTime\"));\r\n    assertEquals(expectedErrorElapsedTime, spoutResult.get(\"errorLapsedSecs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologySpoutAggStatsMap_hasNoLastError",
  "sourceCode" : "/**\r\n * Very narrow test case to validate that 'last error' fields are NOT populated for a spout\r\n * that does NOT have a last error associated.\r\n */\r\n@Test\r\nvoid test_getTopologySpoutAggStatsMap_hasNoLastError() {\r\n    // Define inputs\r\n    final String expectedSpoutId = \"MySpoutId\";\r\n    // Build stats instance for our spout\r\n    final ComponentAggregateStats aggregateStats = buildSpoutAggregateStatsBase();\r\n    addSpoutStats(expectedSpoutId, aggregateStats);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Spout result\r\n    final Map<String, Object> spoutResult = getSpoutStatsFromTopologySummaryResult(result, expectedSpoutId);\r\n    assertNotNull(spoutResult, \"Should have an entry for spout\");\r\n    // Verify each piece\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"spoutId\"));\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"encodedSpoutId\"));\r\n    // Verify error fields exist, but are not populated.\r\n    // These fields default to empty string.\r\n    assertTrue(spoutResult.containsKey(\"lastError\"));\r\n    assertEquals(\"\", spoutResult.get(\"lastError\"), \"Backwards compat. with API docs say this should be empty string when empty\");\r\n    assertTrue(spoutResult.containsKey(\"errorHost\"));\r\n    assertEquals(\"\", spoutResult.get(\"errorHost\"));\r\n    assertTrue(spoutResult.containsKey(\"errorWorkerLogLink\"));\r\n    assertEquals(\"\", spoutResult.get(\"errorWorkerLogLink\"));\r\n    // These fields default to null.\r\n    assertTrue(spoutResult.containsKey(\"errorPort\"));\r\n    assertNull(spoutResult.get(\"errorPort\"));\r\n    assertTrue(spoutResult.containsKey(\"errorTime\"));\r\n    assertNull(spoutResult.get(\"errorTime\"));\r\n    assertTrue(spoutResult.containsKey(\"errorLapsedSecs\"));\r\n    assertNull(spoutResult.get(\"errorLapsedSecs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "test_getTopologySpoutAggStatsMap_generalFields",
  "sourceCode" : "/**\r\n * A more general test case that a spout's aggregate stats are\r\n * correctly populated into the resulting map.\r\n */\r\n@Test\r\nvoid test_getTopologySpoutAggStatsMap_generalFields() {\r\n    // Define inputs\r\n    final String expectedSpoutId = \"MySpoutId\";\r\n    final double expectedCompleteLatency = 432.0D;\r\n    final long expectedEmitted = 43234L;\r\n    final long expectedAcked = 5553L;\r\n    final long expectedFailed = 220L;\r\n    final int expectedExecutors = 2;\r\n    final int expectedTasks = 3;\r\n    final long expectedTransferred = 3423423L;\r\n    final double expectedOnMemoryHeap = 1024D;\r\n    final double expectedOffMemoryHeap = 2048D;\r\n    final double expectedCpuCorePercent = 75D;\r\n    // Build stats instance for our spout\r\n    final ComponentAggregateStats aggregateStats = buildSpoutAggregateStatsBase();\r\n    // Common stats\r\n    final CommonAggregateStats commonStats = aggregateStats.get_common_stats();\r\n    commonStats.set_acked(expectedAcked);\r\n    commonStats.set_emitted(expectedEmitted);\r\n    commonStats.set_failed(expectedFailed);\r\n    commonStats.set_num_executors(expectedExecutors);\r\n    commonStats.set_num_tasks(expectedTasks);\r\n    commonStats.set_transferred(expectedTransferred);\r\n    // Spout stats\r\n    final SpoutAggregateStats spoutStats = aggregateStats.get_specific_stats().get_spout();\r\n    spoutStats.set_complete_latency_ms(expectedCompleteLatency);\r\n    // Build Resources Map\r\n    final Map<String, Double> resourcesMap = new HashMap<>();\r\n    resourcesMap.put(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, expectedOnMemoryHeap);\r\n    resourcesMap.put(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, expectedOffMemoryHeap);\r\n    resourcesMap.put(Constants.COMMON_CPU_RESOURCE_NAME, expectedCpuCorePercent);\r\n    commonStats.set_resources_map(resourcesMap);\r\n    // Add to TopologyPageInfo\r\n    addSpoutStats(expectedSpoutId, aggregateStats);\r\n    // Call method under test.\r\n    final Map<String, Object> result = UIHelpers.getTopologySummary(topoPageInfo, WINDOW, new HashMap<>(), \"spp\");\r\n    // Validate\r\n    assertNotNull(result, \"Should never return null\");\r\n    // Validate our Spout result\r\n    final Map<String, Object> spoutResult = getSpoutStatsFromTopologySummaryResult(result, expectedSpoutId);\r\n    assertNotNull(spoutResult, \"Should have an entry for spout\");\r\n    // Validate fields\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"spoutId\"));\r\n    assertEquals(expectedSpoutId, spoutResult.get(\"encodedSpoutId\"));\r\n    assertEquals(expectedTransferred, spoutResult.get(\"transferred\"));\r\n    assertEquals(String.format(\"%.3f\", expectedCompleteLatency), spoutResult.get(\"completeLatency\"));\r\n    assertEquals(expectedFailed, spoutResult.get(\"failed\"));\r\n    assertEquals(expectedAcked, spoutResult.get(\"acked\"));\r\n    assertEquals(expectedEmitted, spoutResult.get(\"emitted\"));\r\n    assertEquals(expectedExecutors, spoutResult.get(\"executors\"));\r\n    assertEquals(expectedTasks, spoutResult.get(\"tasks\"));\r\n    // Validate resources\r\n    assertEquals(expectedOnMemoryHeap, (double) spoutResult.get(\"requestedMemOnHeap\"), 0.01);\r\n    assertEquals(expectedOffMemoryHeap, (double) spoutResult.get(\"requestedMemOffHeap\"), 0.01);\r\n    assertEquals(expectedCpuCorePercent, (double) spoutResult.get(\"requestedCpu\"), 0.01);\r\n    assertEquals(\"\", spoutResult.get(\"requestedGenericResourcesComp\"));\r\n    // We expect there to be no error populated.\r\n    assertEquals(\"\", spoutResult.get(\"lastError\"), \"No error should be reported as empty string\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\storm\\storm-webapp\\src\\test\\java\\org\\apache\\storm\\daemon\\ui\\UIHelpersTest.java",
  "methodName" : "testSanitizeStreamName",
  "sourceCode" : "/**\r\n * Tests that santizeStreamName() does the expected manipulations\r\n */\r\n@Test\r\npublic void testSanitizeStreamName() {\r\n    // replaces the expected characters with underscores (everything except A-Z, a-z, dot, dash, and underscore)\r\n    assertEquals(\"my-stream_with.all_characterClasses____\", UIHelpers.sanitizeStreamName(\"my-stream:with.all_characterClasses1/\\\\2\"));\r\n    // has the expected effect when streamName begins with a non-alpha character\r\n    assertEquals(\"_s_foo\", UIHelpers.sanitizeStreamName(\"3foo\"));\r\n    // handles empty string, though that's not an expected stream name\r\n    assertEquals(\"_s\", UIHelpers.sanitizeStreamName(\"\"));\r\n}",
  "annotations" : [ "Test" ]
} ]