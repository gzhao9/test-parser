[ {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\checkpoint\\TestCheckpointId.java",
  "methodName" : "testSerializationDeserialization",
  "sourceCode" : "@Test\r\npublic void testSerializationDeserialization() {\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    CheckpointId deserializedCheckpointId = CheckpointId.deserialize(checkpointId.serialize());\r\n    assertEquals(checkpointId.getMillis(), deserializedCheckpointId.getMillis());\r\n    assertEquals(checkpointId.getNanoId(), deserializedCheckpointId.getNanoId());\r\n    assertEquals(checkpointId, deserializedCheckpointId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\checkpoint\\TestCheckpointId.java",
  "methodName" : "testSerializationFormatForBackwardsCompatibility",
  "sourceCode" : "@Test\r\npublic void testSerializationFormatForBackwardsCompatibility() {\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    String serializedCheckpointId = checkpointId.serialize();\r\n    // WARNING: This format is written to persisted remotes stores and local files, making a change in the format\r\n    // would be backwards incompatible\r\n    String legacySerializedFormat = serializeLegacy(checkpointId);\r\n    assertEquals(checkpointId, CheckpointId.deserialize(legacySerializedFormat));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\config\\TestConfig.java",
  "methodName" : "testgetShortAndLong",
  "sourceCode" : "@Test\r\npublic void testgetShortAndLong() {\r\n    Map<String, String> m = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"testkey\", \"11\");\r\n        }\r\n    };\r\n    MapConfig mc = new MapConfig(m);\r\n    short defaultShort = 0;\r\n    long defaultLong = 0;\r\n    Class c1 = getClass(mc.getShort(\"testkey\"));\r\n    assertEquals(Short.class, c1);\r\n    Class c2 = getClass(mc.getShort(\"testkey\", defaultShort));\r\n    assertEquals(Short.class, c2);\r\n    Class c3 = getClass(mc.getLong(\"testkey\"));\r\n    assertEquals(Long.class, c3);\r\n    Class c4 = getClass(mc.getLong(\"testkey\", defaultLong));\r\n    assertEquals(Long.class, c4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\config\\TestConfig.java",
  "methodName" : "testSanitize",
  "sourceCode" : "@Test\r\npublic void testSanitize() {\r\n    Map<String, String> m = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"key1\", \"value1\");\r\n            put(\"key2\", \"value2\");\r\n            put(\"sensitive.key3\", \"secret1\");\r\n            put(\"sensitive.key4\", \"secret2\");\r\n        }\r\n    };\r\n    Config config = new MapConfig(m);\r\n    assertFalse(config.toString().contains(\"secret\"));\r\n    Config sanitized = config.sanitize();\r\n    assertEquals(\"value1\", sanitized.get(\"key1\"));\r\n    assertEquals(\"value2\", sanitized.get(\"key2\"));\r\n    assertEquals(Config.SENSITIVE_MASK, sanitized.get(\"sensitive.key3\"));\r\n    assertEquals(Config.SENSITIVE_MASK, sanitized.get(\"sensitive.key4\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\config\\TestConfig.java",
  "methodName" : "testGetList",
  "sourceCode" : "@Test\r\npublic void testGetList() {\r\n    Map<String, String> m = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"key1\", \" \");\r\n            put(\"key2\", \"\");\r\n            put(\"key3\", \"  value1  \");\r\n            put(\"key4\", \"value1,value2\");\r\n            put(\"key5\", \"value1, value2\");\r\n            put(\"key6\", \"value1  ,   value2\");\r\n        }\r\n    };\r\n    Config config = new MapConfig(m);\r\n    List<String> list = config.getList(\"key1\", Collections.<String>emptyList());\r\n    assertEquals(0, list.size());\r\n    list = config.getList(\"key2\", Collections.<String>emptyList());\r\n    assertEquals(0, list.size());\r\n    list = config.getList(\"key3\");\r\n    assertEquals(\"  value1  \", list.get(0));\r\n    list = config.getList(\"key4\");\r\n    assertEquals(\"value1\", list.get(0));\r\n    assertEquals(\"value2\", list.get(1));\r\n    list = config.getList(\"key5\");\r\n    assertEquals(\"value1\", list.get(0));\r\n    assertEquals(\"value2\", list.get(1));\r\n    list = config.getList(\"key6\");\r\n    assertEquals(\"value1\", list.get(0));\r\n    assertEquals(\"value2\", list.get(1));\r\n    list = config.getList(\"UndefinedKey\", Collections.<String>emptyList());\r\n    assertEquals(0, list.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestSamzaHistogram.java",
  "methodName" : "testCreateHistogramGaugeNullCheck",
  "sourceCode" : "@Test\r\npublic void testCreateHistogramGaugeNullCheck() {\r\n    metricsRegistry = mock(MetricsRegistry.class);\r\n    doAnswer((Answer<Gauge<Double>>) invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        return new Gauge<>((String) args[0], (Double) ((Gauge) args[1]).getValue());\r\n    }).when(metricsRegistry).newGauge(anyString(), any(Gauge.class));\r\n    SamzaHistogram histogram = new SamzaHistogram(metricsRegistry, GROUP, METRIC_NAME);\r\n    assertNotNull(histogram);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestSlidingTimeWindowReservoir.java",
  "methodName" : "testUpdateSizeSnapshot",
  "sourceCode" : "@Test\r\npublic void testUpdateSizeSnapshot() {\r\n    SlidingTimeWindowReservoir slidingTimeWindowReservoir = new SlidingTimeWindowReservoir(300, 8, clock);\r\n    when(clock.currentTimeMillis()).thenReturn(0L);\r\n    slidingTimeWindowReservoir.update(1L);\r\n    when(clock.currentTimeMillis()).thenReturn(1L);\r\n    slidingTimeWindowReservoir.update(2L);\r\n    when(clock.currentTimeMillis()).thenReturn(2L);\r\n    slidingTimeWindowReservoir.update(3L);\r\n    assertEquals(3, slidingTimeWindowReservoir.size());\r\n    Snapshot snapshot = slidingTimeWindowReservoir.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(1L, 2L, 3L)));\r\n    assertEquals(3, snapshot.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestSlidingTimeWindowReservoir.java",
  "methodName" : "testDuplicateTime",
  "sourceCode" : "@Test\r\npublic void testDuplicateTime() {\r\n    SlidingTimeWindowReservoir slidingTimeWindowReservoir = new SlidingTimeWindowReservoir(300, 2, clock);\r\n    when(clock.currentTimeMillis()).thenReturn(1L);\r\n    slidingTimeWindowReservoir.update(1L);\r\n    slidingTimeWindowReservoir.update(2L);\r\n    Snapshot snapshot = slidingTimeWindowReservoir.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(1L, 2L)));\r\n    assertEquals(2, snapshot.getSize());\r\n    // update causes collision, will override the last update\r\n    slidingTimeWindowReservoir.update(3L);\r\n    snapshot = slidingTimeWindowReservoir.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(1L, 3L)));\r\n    assertEquals(2, snapshot.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestSlidingTimeWindowReservoir.java",
  "methodName" : "testRemoveExpiredValues",
  "sourceCode" : "@Test\r\npublic void testRemoveExpiredValues() {\r\n    SlidingTimeWindowReservoir slidingTimeWindowReservoir = new SlidingTimeWindowReservoir(300, 8, clock);\r\n    when(clock.currentTimeMillis()).thenReturn(0L);\r\n    slidingTimeWindowReservoir.update(1L);\r\n    when(clock.currentTimeMillis()).thenReturn(100L);\r\n    slidingTimeWindowReservoir.update(2L);\r\n    when(clock.currentTimeMillis()).thenReturn(301L);\r\n    slidingTimeWindowReservoir.update(3L);\r\n    when(clock.currentTimeMillis()).thenReturn(500L);\r\n    slidingTimeWindowReservoir.update(4L);\r\n    Snapshot snapshot = slidingTimeWindowReservoir.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(3L, 4L)));\r\n    assertEquals(2, snapshot.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestSnapshot.java",
  "methodName" : "testGetMaxMinAverageSumSize",
  "sourceCode" : "@Test\r\npublic void testGetMaxMinAverageSumSize() {\r\n    Snapshot snapshot = new Snapshot(Arrays.asList(1L, 2L, 3L, 4L, 5L));\r\n    assertEquals(5, snapshot.getMax());\r\n    assertEquals(1, snapshot.getMin());\r\n    assertEquals(3, snapshot.getAverage(), 0);\r\n    assertEquals(15, snapshot.getSum(), 0);\r\n    assertEquals(5, snapshot.getSize());\r\n    Snapshot emptySnapshot = new Snapshot(new ArrayList<>());\r\n    assertEquals(0, emptySnapshot.getMax());\r\n    assertEquals(0, emptySnapshot.getMin());\r\n    assertEquals(0, emptySnapshot.getAverage(), 0);\r\n    assertEquals(0, emptySnapshot.getSum(), 0);\r\n    assertEquals(0, emptySnapshot.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestTimer.java",
  "methodName" : "testDefaultTimerUpdateAndGetSnapshot",
  "sourceCode" : "@Test\r\npublic void testDefaultTimerUpdateAndGetSnapshot() {\r\n    Timer timer = new Timer(\"test\", 300, clock);\r\n    timer.update(1L);\r\n    timer.update(2L);\r\n    Snapshot snapshot = timer.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(1L, 2L)));\r\n    assertEquals(2, snapshot.getValues().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\metrics\\TestTimer.java",
  "methodName" : "testTimerWithDifferentWindowSize",
  "sourceCode" : "@Test\r\npublic void testTimerWithDifferentWindowSize() {\r\n    Timer timer = new Timer(\"test\", 300, clock);\r\n    timer.update(1L);\r\n    timer.update(2L);\r\n    timer.update(3L);\r\n    Snapshot snapshot = timer.getSnapshot();\r\n    assertTrue(snapshot.getValues().containsAll(Arrays.asList(1L, 2L, 3L)));\r\n    assertEquals(3, snapshot.getValues().size());\r\n    // The time is 500 for update(4L) because getSnapshot calls clock once + 3\r\n    // updates that call clock 3 times\r\n    timer.update(4L);\r\n    Snapshot snapshot2 = timer.getSnapshot();\r\n    assertTrue(snapshot2.getValues().containsAll(Arrays.asList(3L, 4L)));\r\n    assertEquals(2, snapshot2.getValues().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\operators\\windows\\TestWindowPane.java",
  "methodName" : "testConstructor",
  "sourceCode" : "@Test\r\npublic void testConstructor() {\r\n    WindowPane<String, Integer> wndOutput = new WindowPane<>(new WindowKey<>(\"testMsg\", null), 10, AccumulationMode.DISCARDING, FiringType.EARLY);\r\n    assertEquals(wndOutput.getKey().getKey(), \"testMsg\");\r\n    assertEquals(wndOutput.getMessage(), Integer.valueOf(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestByteBufferSerde.java",
  "methodName" : "testSerde",
  "sourceCode" : "@Test\r\npublic void testSerde() {\r\n    ByteBufferSerde serde = new ByteBufferSerde();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    byte[] bytes = \"A lazy way of creating a byte array\".getBytes();\r\n    ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\r\n    byteBuffer.mark();\r\n    assertArrayEquals(bytes, serde.toBytes(byteBuffer));\r\n    byteBuffer.reset();\r\n    assertEquals(byteBuffer, serde.fromBytes(bytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestByteBufferSerde.java",
  "methodName" : "testSerializationPreservesInput",
  "sourceCode" : "@Test\r\npublic void testSerializationPreservesInput() {\r\n    ByteBufferSerde serde = new ByteBufferSerde();\r\n    byte[] bytes = \"A lazy way of creating a byte array\".getBytes();\r\n    ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\r\n    // advance position by 1\r\n    byteBuffer.get();\r\n    serde.toBytes(byteBuffer);\r\n    assertEquals(byteBuffer.capacity(), byteBuffer.limit());\r\n    assertEquals(1, byteBuffer.position());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestByteSerde.java",
  "methodName" : "testByteSerde",
  "sourceCode" : "@Test\r\npublic void testByteSerde() {\r\n    ByteSerde serde = new ByteSerde();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    byte[] testBytes = \"A lazy way of creating a byte array\".getBytes();\r\n    assertArrayEquals(testBytes, serde.toBytes(testBytes));\r\n    assertArrayEquals(testBytes, serde.fromBytes(testBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestDoubleSerde.java",
  "methodName" : "testDoubleSerde",
  "sourceCode" : "@Test\r\npublic void testDoubleSerde() {\r\n    DoubleSerde serde = new DoubleSerde();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    Double fooBar = 9.156013e-002;\r\n    byte[] fooBarBytes = serde.toBytes(fooBar);\r\n    assertArrayEquals(new byte[] { 63, -73, 112, 124, 19, -9, -82, -93 }, fooBarBytes);\r\n    assertEquals(fooBar, serde.fromBytes(fooBarBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestIntegerSerde.java",
  "methodName" : "testIntegerSerde",
  "sourceCode" : "@Test\r\npublic void testIntegerSerde() {\r\n    IntegerSerde serde = new IntegerSerde();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    Integer fooBar = 37;\r\n    byte[] fooBarBytes = serde.toBytes(fooBar);\r\n    assertArrayEquals(new byte[] { 0, 0, 0, 37 }, fooBarBytes);\r\n    assertEquals(fooBar, serde.fromBytes(fooBarBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestJsonSerdeV2.java",
  "methodName" : "testJsonSerdeV2ShouldWork",
  "sourceCode" : "@Test\r\npublic void testJsonSerdeV2ShouldWork() {\r\n    JsonSerdeV2<HashMap<String, Object>> serde = new JsonSerdeV2<>();\r\n    HashMap<String, Object> obj = new HashMap<>();\r\n    obj.put(\"hi\", \"bye\");\r\n    obj.put(\"why\", 2);\r\n    byte[] bytes = serde.toBytes(obj);\r\n    assertEquals(obj, serde.fromBytes(bytes));\r\n    JsonSerdeV2<Map.Entry<String, Object>> serdeHashMapEntry = new JsonSerdeV2<>();\r\n    obj.entrySet().forEach(entry -> {\r\n        try {\r\n            serdeHashMapEntry.toBytes(entry);\r\n        } catch (Exception e) {\r\n            fail(\"HashMap Entry serialization failed!\");\r\n        }\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestLongSerde.java",
  "methodName" : "testLongSerde",
  "sourceCode" : "@Test\r\npublic void testLongSerde() {\r\n    LongSerde serde = new LongSerde();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    Long fooBar = 1234123412341234L;\r\n    byte[] fooBarBytes = serde.toBytes(fooBar);\r\n    assertArrayEquals(new byte[] { 0, 4, 98, 109, -65, -102, 1, -14 }, fooBarBytes);\r\n    assertEquals(fooBar, serde.fromBytes(fooBarBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestSerializableSerde.java",
  "methodName" : "testSerializableSerde",
  "sourceCode" : "@Test\r\npublic void testSerializableSerde() {\r\n    SerializableSerde<String> serde = new SerializableSerde<>();\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    String obj = \"String is serializable\";\r\n    // Serialized string is prefix + string itself\r\n    List<Byte> expectedBytesList = new ArrayList<>(Arrays.asList((byte) 0xAC, (byte) 0xED, (byte) 0x00, (byte) 0x05, (byte) 0x74, (byte) 0x00, (byte) 0x16));\r\n    for (byte b : obj.getBytes(StandardCharsets.UTF_8)) {\r\n        expectedBytesList.add(b);\r\n    }\r\n    // need to unbox to primitive byte\r\n    byte[] expected = new byte[expectedBytesList.size()];\r\n    for (int i = 0; i < expectedBytesList.size(); i++) {\r\n        expected[i] = expectedBytesList.get(i);\r\n    }\r\n    byte[] bytes = serde.toBytes(obj);\r\n    assertArrayEquals(expected, bytes);\r\n    String objRoundTrip = serde.fromBytes(bytes);\r\n    assertEquals(obj, objRoundTrip);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestStringSerde.java",
  "methodName" : "testStringSerde",
  "sourceCode" : "@Test\r\npublic void testStringSerde() {\r\n    StringSerde serde = new StringSerde(\"UTF-8\");\r\n    assertNull(serde.toBytes(null));\r\n    assertNull(serde.fromBytes(null));\r\n    String fooBar = \"foo bar\";\r\n    byte[] fooBarBytes = serde.toBytes(fooBar);\r\n    assertArrayEquals(fooBar.getBytes(StandardCharsets.UTF_8), fooBarBytes);\r\n    assertEquals(fooBar, serde.fromBytes(fooBarBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestUUIDSerde.java",
  "methodName" : "testUUIDSerde",
  "sourceCode" : "@Test\r\npublic void testUUIDSerde() {\r\n    UUID uuid = new UUID(13, 42);\r\n    byte[] bytes = serde.toBytes(uuid);\r\n    assertArrayEquals(new byte[] { 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 42 }, bytes);\r\n    assertEquals(uuid, serde.fromBytes(bytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestUUIDSerde.java",
  "methodName" : "testToBytesWhenNull",
  "sourceCode" : "@Test\r\npublic void testToBytesWhenNull() {\r\n    assertNull(serde.toBytes(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestUUIDSerde.java",
  "methodName" : "testFromBytesWhenNull",
  "sourceCode" : "@Test\r\npublic void testFromBytesWhenNull() {\r\n    assertNull(serde.fromBytes(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\serializers\\TestUUIDSerde.java",
  "methodName" : "testFromBytesWhenInvalid",
  "sourceCode" : "@Test(expected = BufferUnderflowException.class)\r\npublic void testFromBytesWhenInvalid() {\r\n    serde.fromBytes(new byte[] { 0 });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\sql\\TestSamzaSqlRelRecord.java",
  "methodName" : "testEquality",
  "sourceCode" : "@Test\r\npublic void testEquality() {\r\n    SamzaSqlRelRecord relRecord1 = new SamzaSqlRelRecord(Arrays.asList(\"id\", \"name\"), Arrays.asList(1L, \"object\"));\r\n    SamzaSqlRelRecord relRecord2 = new SamzaSqlRelRecord(Arrays.asList(\"id\", \"name\"), Arrays.asList(1L, \"object\"));\r\n    assertEquals(relRecord1, relRecord2);\r\n    assertEquals(relRecord1.hashCode(), relRecord2.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\sql\\TestSamzaSqlRelRecord.java",
  "methodName" : "testInEquality",
  "sourceCode" : "@Test\r\npublic void testInEquality() {\r\n    SamzaSqlRelRecord relRecord1 = new SamzaSqlRelRecord(Arrays.asList(\"id\", \"name\"), Arrays.asList(1L, \"object\"));\r\n    SamzaSqlRelRecord relRecord2 = new SamzaSqlRelRecord(Arrays.asList(\"id\", \"name\"), Arrays.asList(1L, null));\r\n    assertNotEquals(relRecord1, relRecord2);\r\n    assertNotEquals(relRecord1.hashCode(), relRecord2.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointSpecific",
  "sourceCode" : "@Test\r\npublic void testStartpointSpecific() {\r\n    StartpointSpecific startpoint = new StartpointSpecific(\"123\");\r\n    Assert.assertEquals(\"123\", startpoint.getSpecificOffset());\r\n    Assert.assertTrue(startpoint.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    MockStartpointVisitor mockStartpointVisitorConsumer = new MockStartpointVisitor();\r\n    startpoint.apply(new SystemStreamPartition(\"sys\", \"stream\", new Partition(1)), mockStartpointVisitorConsumer);\r\n    Assert.assertEquals(StartpointSpecific.class, mockStartpointVisitorConsumer.visitedClass);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointTimestamp",
  "sourceCode" : "@Test\r\npublic void testStartpointTimestamp() {\r\n    StartpointTimestamp startpoint = new StartpointTimestamp(2222222L);\r\n    Assert.assertEquals(2222222L, startpoint.getTimestampOffset().longValue());\r\n    Assert.assertTrue(startpoint.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    MockStartpointVisitor mockStartpointVisitorConsumer = new MockStartpointVisitor();\r\n    startpoint.apply(new SystemStreamPartition(\"sys\", \"stream\", new Partition(1)), mockStartpointVisitorConsumer);\r\n    Assert.assertEquals(StartpointTimestamp.class, mockStartpointVisitorConsumer.visitedClass);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointEarliest",
  "sourceCode" : "@Test\r\npublic void testStartpointEarliest() {\r\n    StartpointOldest startpoint = new StartpointOldest();\r\n    Assert.assertTrue(startpoint.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    MockStartpointVisitor mockStartpointVisitorConsumer = new MockStartpointVisitor();\r\n    startpoint.apply(new SystemStreamPartition(\"sys\", \"stream\", new Partition(1)), mockStartpointVisitorConsumer);\r\n    Assert.assertEquals(StartpointOldest.class, mockStartpointVisitorConsumer.visitedClass);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointLatest",
  "sourceCode" : "@Test\r\npublic void testStartpointLatest() {\r\n    StartpointUpcoming startpoint = new StartpointUpcoming();\r\n    Assert.assertTrue(startpoint.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    MockStartpointVisitor mockStartpointVisitorConsumer = new MockStartpointVisitor();\r\n    startpoint.apply(new SystemStreamPartition(\"sys\", \"stream\", new Partition(1)), mockStartpointVisitorConsumer);\r\n    Assert.assertEquals(StartpointUpcoming.class, mockStartpointVisitorConsumer.visitedClass);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestExpandingInputDescriptor.java",
  "methodName" : "testISDObjectsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDObjectsWithOverrides() {\r\n    ExampleExpandingSystemDescriptor expandingSystem = new ExampleExpandingSystemDescriptor(\"expandingSystem\");\r\n    IntegerSerde streamSerde = new IntegerSerde();\r\n    ExampleExpandingInputDescriptor<Long> expandingISD = expandingSystem.getInputDescriptor(\"input-stream\", streamSerde);\r\n    assertEquals(streamSerde, expandingISD.getSerde());\r\n    assertEquals(expandingSystem.getTransformer().get(), expandingISD.getTransformer().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericInputDescriptor.java",
  "methodName" : "testAPIUsage",
  "sourceCode" : "@Test\r\npublic void testAPIUsage() {\r\n    // does not assert anything, but acts as a compile-time check on expected descriptor type parameters\r\n    // and validates that the method calls can be chained.\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\").withSystemConfigs(Collections.emptyMap()).withDefaultStreamConfigs(Collections.emptyMap());\r\n    GenericInputDescriptor<Integer> input1 = mySystem.getInputDescriptor(\"input1\", new IntegerSerde());\r\n    GenericOutputDescriptor<Integer> output1 = mySystem.getOutputDescriptor(\"output1\", new IntegerSerde());\r\n    input1.withPhysicalName(\"input-1\").shouldBootstrap().withOffsetDefault(SystemStreamMetadata.OffsetType.NEWEST).withPriority(1).shouldResetOffset().isBounded().shouldDeleteCommittedMessages().withStreamConfigs(Collections.emptyMap());\r\n    output1.withPhysicalName(\"output-1\").withStreamConfigs(Collections.emptyMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericInputDescriptor.java",
  "methodName" : "testISDConfigsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDConfigsWithOverrides() {\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\").withSystemConfigs(Collections.emptyMap()).withDefaultStreamConfigs(Collections.emptyMap());\r\n    GenericInputDescriptor<Double> isd = mySystem.getInputDescriptor(\"input-stream\", new DoubleSerde()).withPhysicalName(\"physical-name\").shouldBootstrap().isBounded().shouldDeleteCommittedMessages().withOffsetDefault(SystemStreamMetadata.OffsetType.OLDEST).withPriority(12).shouldResetOffset().withStreamConfigs(ImmutableMap.of(\"custom-config-key\", \"custom-config-value\"));\r\n    Map<String, String> generatedConfigs = isd.toConfig();\r\n    Map<String, String> expectedConfigs = new HashMap<>();\r\n    expectedConfigs.put(\"streams.input-stream.samza.system\", \"input-system\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.physical.name\", \"physical-name\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.bootstrap\", \"true\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.bounded\", \"true\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.delete.committed.messages\", \"true\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.reset.offset\", \"true\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.offset.default\", \"oldest\");\r\n    expectedConfigs.put(\"streams.input-stream.samza.priority\", \"12\");\r\n    expectedConfigs.put(\"streams.input-stream.custom-config-key\", \"custom-config-value\");\r\n    assertEquals(expectedConfigs, generatedConfigs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericInputDescriptor.java",
  "methodName" : "testISDConfigsWithDefaults",
  "sourceCode" : "@Test\r\npublic void testISDConfigsWithDefaults() {\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\").withSystemConfigs(Collections.emptyMap()).withDefaultStreamConfigs(Collections.emptyMap());\r\n    DoubleSerde streamSerde = new DoubleSerde();\r\n    GenericInputDescriptor<Double> isd = mySystem.getInputDescriptor(\"input-stream\", streamSerde);\r\n    Map<String, String> generatedConfigs = isd.toConfig();\r\n    Map<String, String> expectedConfigs = ImmutableMap.of(\"streams.input-stream.samza.system\", \"input-system\");\r\n    assertEquals(expectedConfigs, generatedConfigs);\r\n    assertEquals(streamSerde, isd.getSerde());\r\n    assertFalse(isd.getTransformer().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericInputDescriptor.java",
  "methodName" : "testISDObjectsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDObjectsWithOverrides() {\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\").withSystemConfigs(Collections.emptyMap()).withDefaultStreamConfigs(Collections.emptyMap());\r\n    IntegerSerde streamSerde = new IntegerSerde();\r\n    GenericInputDescriptor<Integer> isd = mySystem.getInputDescriptor(\"input-stream\", streamSerde);\r\n    assertEquals(streamSerde, isd.getSerde());\r\n    assertFalse(isd.getTransformer().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericSystemDescriptor.java",
  "methodName" : "testSDConfigs",
  "sourceCode" : "@Test\r\npublic void testSDConfigs() {\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\").withSystemConfigs(ImmutableMap.of(\"custom-config-key\", \"custom-config-value\")).withDefaultStreamConfigs(ImmutableMap.of(\"custom-stream-config-key\", \"custom-stream-config-value\")).withDefaultStreamOffsetDefault(SystemStreamMetadata.OffsetType.UPCOMING);\r\n    Map<String, String> generatedConfigs = mySystem.toConfig();\r\n    Map<String, String> expectedConfigs = ImmutableMap.of(\"systems.input-system.samza.factory\", \"factory.class.name\", \"systems.input-system.custom-config-key\", \"custom-config-value\", \"systems.input-system.default.stream.custom-stream-config-key\", \"custom-stream-config-value\", \"systems.input-system.default.stream.samza.offset.default\", \"upcoming\");\r\n    assertEquals(expectedConfigs, generatedConfigs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericSystemDescriptor.java",
  "methodName" : "testGetInputDescriptorWithNullSerde",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetInputDescriptorWithNullSerde() {\r\n    GenericSystemDescriptor mySystem = new GenericSystemDescriptor(\"input-system\", \"factory.class.name\");\r\n    // should throw an exception\r\n    mySystem.getInputDescriptor(\"streamId\", null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericSystemDescriptor.java",
  "methodName" : "testGetSystemDescriptorWithNullSystemName",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetSystemDescriptorWithNullSystemName() {\r\n    new GenericSystemDescriptor(null, \"factory.class.name\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestGenericSystemDescriptor.java",
  "methodName" : "testGetSystemDescriptorWithEmptySystemName",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetSystemDescriptorWithEmptySystemName() {\r\n    new GenericSystemDescriptor(\" \", \"factory.class.name\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestSimpleInputDescriptor.java",
  "methodName" : "testAPIUsage",
  "sourceCode" : "@Test\r\npublic void testAPIUsage() {\r\n    // does not assert anything, but acts as a compile-time check on expected descriptor type parameters\r\n    // and validates that the method calls can be chained.\r\n    ExampleSimpleSystemDescriptor kafkaSystem = new ExampleSimpleSystemDescriptor(\"kafka-system\").withSystemConfigs(Collections.emptyMap());\r\n    ExampleSimpleInputDescriptor<Integer> input1 = kafkaSystem.getInputDescriptor(\"input1\", new IntegerSerde());\r\n    ExampleSimpleOutputDescriptor<Integer> output1 = kafkaSystem.getOutputDescriptor(\"output1\", new IntegerSerde());\r\n    input1.shouldBootstrap().withOffsetDefault(SystemStreamMetadata.OffsetType.NEWEST).withPriority(1).shouldResetOffset().withStreamConfigs(Collections.emptyMap());\r\n    output1.withStreamConfigs(Collections.emptyMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestSimpleInputDescriptor.java",
  "methodName" : "testISDObjectsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDObjectsWithOverrides() {\r\n    ExampleSimpleSystemDescriptor ssd = new ExampleSimpleSystemDescriptor(\"kafka-system\");\r\n    IntegerSerde streamSerde = new IntegerSerde();\r\n    ExampleSimpleInputDescriptor<Integer> isd = ssd.getInputDescriptor(\"input-stream\", streamSerde);\r\n    assertEquals(streamSerde, isd.getSerde());\r\n    assertFalse(isd.getTransformer().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestTransformingInputDescriptor.java",
  "methodName" : "testAPIUsage",
  "sourceCode" : "@Test\r\npublic void testAPIUsage() {\r\n    // does not assert anything, but acts as a compile-time check on expected descriptor type parameters\r\n    // and validates that the method calls can be chained.\r\n    ExampleTransformingSystemDescriptor imeTransformingSystem = new ExampleTransformingSystemDescriptor(\"imeTransformingSystem\").withSystemConfigs(Collections.emptyMap());\r\n    ExampleTransformingInputDescriptor<Long> input1 = imeTransformingSystem.getInputDescriptor(\"input1\", new IntegerSerde());\r\n    ExampleTransformingOutputDescriptor<Integer> output1 = imeTransformingSystem.getOutputDescriptor(\"output1\", new IntegerSerde());\r\n    input1.shouldBootstrap().withOffsetDefault(SystemStreamMetadata.OffsetType.NEWEST).withPriority(1).shouldResetOffset().withStreamConfigs(Collections.emptyMap());\r\n    output1.withStreamConfigs(Collections.emptyMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\descriptors\\TestTransformingInputDescriptor.java",
  "methodName" : "testISDObjectsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDObjectsWithOverrides() {\r\n    ExampleTransformingSystemDescriptor imeTransformingSystem = new ExampleTransformingSystemDescriptor(\"imeTransformingSystem\");\r\n    IntegerSerde streamSerde = new IntegerSerde();\r\n    ExampleTransformingInputDescriptor<Long> overridingISD = imeTransformingSystem.getInputDescriptor(\"input-stream\", streamSerde);\r\n    assertEquals(streamSerde, overridingISD.getSerde());\r\n    assertEquals(imeTransformingSystem.getTransformer().get(), overridingISD.getTransformer().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestBoundedSSPIterator.java",
  "methodName" : "testHasNextFalseWhenEnvelopeOutOfBounds",
  "sourceCode" : "@Test\r\npublic void testHasNextFalseWhenEnvelopeOutOfBounds() throws InterruptedException {\r\n    SystemConsumer mockConsumer = mock(SystemConsumer.class);\r\n    SystemAdmin mockAdmin = buildMockSystemAdmin();\r\n    int numMessages = 10;\r\n    long endOffset = 5;\r\n    OngoingStubbing<Map<SystemStreamPartition, List<IncomingMessageEnvelope>>> stubbing = when(mockConsumer.poll(any(), anyLong()));\r\n    for (int i = 0; i < numMessages; i++) {\r\n        IncomingMessageEnvelope ime = new IncomingMessageEnvelope(SSP, String.valueOf(i), null, i);\r\n        stubbing = stubbing.thenReturn(ImmutableMap.of(SSP, ImmutableList.of(ime)));\r\n    }\r\n    stubbing.thenReturn(ImmutableMap.of(SSP, ImmutableList.of()));\r\n    BoundedSSPIterator iter = new BoundedSSPIterator(mockConsumer, SSP, String.valueOf(endOffset), mockAdmin);\r\n    int consumed = 0;\r\n    while (iter.hasNext()) {\r\n        iter.next();\r\n        consumed++;\r\n    }\r\n    Assert.assertEquals(consumed, endOffset + 1);\r\n    try {\r\n        iter.next();\r\n        Assert.fail(\"Iterator next call should have failed due to bound check\");\r\n    } catch (NoSuchElementException e) {\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestBoundedSSPIterator.java",
  "methodName" : "testConsumeAllWithNullBound",
  "sourceCode" : "@Test\r\npublic void testConsumeAllWithNullBound() throws InterruptedException {\r\n    SystemConsumer mockConsumer = mock(SystemConsumer.class);\r\n    SystemAdmin mockAdmin = buildMockSystemAdmin();\r\n    int numMessages = 10;\r\n    String endOffset = null;\r\n    OngoingStubbing<Map<SystemStreamPartition, List<IncomingMessageEnvelope>>> stubbing = when(mockConsumer.poll(any(), anyLong()));\r\n    for (int i = 0; i < numMessages; i++) {\r\n        IncomingMessageEnvelope ime = new IncomingMessageEnvelope(SSP, String.valueOf(i), null, i);\r\n        stubbing = stubbing.thenReturn(ImmutableMap.of(SSP, ImmutableList.of(ime)));\r\n    }\r\n    stubbing.thenReturn(ImmutableMap.of(SSP, ImmutableList.of()));\r\n    BoundedSSPIterator iter = new BoundedSSPIterator(mockConsumer, SSP, endOffset, mockAdmin);\r\n    int consumed = 0;\r\n    while (iter.hasNext()) {\r\n        iter.next();\r\n        consumed++;\r\n    }\r\n    Assert.assertEquals(consumed, numMessages);\r\n    Assert.assertFalse(iter.hasNext());\r\n    try {\r\n        iter.next();\r\n        Assert.fail(\"Iterator next call should have failed due to bound check\");\r\n    } catch (NoSuchElementException e) {\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestStreamSpec.java",
  "methodName" : "testBasicConstructor",
  "sourceCode" : "@Test\r\npublic void testBasicConstructor() {\r\n    StreamSpec streamSpec = new StreamSpec(\"dummyId\", \"dummyPhysicalName\", \"dummySystemName\", 1);\r\n    assertEquals(\"dummyId\", streamSpec.getId());\r\n    assertEquals(\"dummyPhysicalName\", streamSpec.getPhysicalName());\r\n    assertEquals(\"dummySystemName\", streamSpec.getSystemName());\r\n    assertEquals(1, streamSpec.getPartitionCount());\r\n    // SystemStream should use the physical name, not the streamId.\r\n    SystemStream systemStream = new SystemStream(\"dummySystemName\", \"dummyPhysicalName\");\r\n    assertEquals(systemStream, streamSpec.toSystemStream());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestStreamSpec.java",
  "methodName" : "testInvalidPartitionCount",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testInvalidPartitionCount() {\r\n    new StreamSpec(\"dummyId\", \"dummyPhysicalName\", \"dummySystemName\", -1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestStreamSpec.java",
  "methodName" : "testInvalidStreamId",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testInvalidStreamId() {\r\n    new StreamSpec(\"dummy.Id\", \"dummyPhysicalName\", \"dummySystemName\", 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestStreamSpec.java",
  "methodName" : "testInvalidSystemName",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testInvalidSystemName() {\r\n    new StreamSpec(\"dummyId\", \"dummyPhysicalName\", \"dummy.System.Name\", 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemAdmin.java",
  "methodName" : "testGetSSPMetadata",
  "sourceCode" : "/**\r\n * Given some SSPs, getSSPMetadata should delegate to getSystemStreamMetadata and properly extract the results for the\r\n * requested SSPs.\r\n */\r\n@Test\r\npublic void testGetSSPMetadata() {\r\n    SystemStreamPartition streamPartition0 = new SystemStreamPartition(SYSTEM, STREAM, new Partition(0));\r\n    SystemStreamPartition streamPartition1 = new SystemStreamPartition(SYSTEM, STREAM, new Partition(1));\r\n    SystemStreamPartition otherStreamPartition0 = new SystemStreamPartition(SYSTEM, OTHER_STREAM, new Partition(0));\r\n    SystemAdmin systemAdmin = mock(MySystemAdmin.class);\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata streamPartition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"1\", \"2\", \"3\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata streamPartition1Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"11\", \"12\", \"13\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata otherStreamPartition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"21\", \"22\", \"23\");\r\n    when(systemAdmin.getSystemStreamMetadata(ImmutableSet.of(STREAM, OTHER_STREAM))).thenReturn(ImmutableMap.of(STREAM, new SystemStreamMetadata(STREAM, ImmutableMap.of(new Partition(0), streamPartition0Metadata, new Partition(1), streamPartition1Metadata)), OTHER_STREAM, new SystemStreamMetadata(OTHER_STREAM, ImmutableMap.of(new Partition(0), otherStreamPartition0Metadata))));\r\n    Set<SystemStreamPartition> ssps = ImmutableSet.of(streamPartition0, streamPartition1, otherStreamPartition0);\r\n    when(systemAdmin.getSSPMetadata(ssps)).thenCallRealMethod();\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(streamPartition0, streamPartition0Metadata, streamPartition1, streamPartition1Metadata, otherStreamPartition0, otherStreamPartition0Metadata);\r\n    assertEquals(expected, systemAdmin.getSSPMetadata(ssps));\r\n    verify(systemAdmin).getSystemStreamMetadata(ImmutableSet.of(STREAM, OTHER_STREAM));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemAdmin.java",
  "methodName" : "testGetSSPMetadataMissingStream",
  "sourceCode" : "/**\r\n * Given some SSPs, but missing metadata for one of the streams, getSSPMetadata should delegate to\r\n * getSystemStreamMetadata and only fill in results for the SSPs corresponding to streams with metadata.\r\n */\r\n@Test\r\npublic void testGetSSPMetadataMissingStream() {\r\n    SystemStreamPartition streamPartition0 = new SystemStreamPartition(SYSTEM, STREAM, new Partition(0));\r\n    SystemStreamPartition otherStreamPartition0 = new SystemStreamPartition(SYSTEM, OTHER_STREAM, new Partition(0));\r\n    SystemAdmin systemAdmin = mock(MySystemAdmin.class);\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata streamPartition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"1\", \"2\", \"3\");\r\n    when(systemAdmin.getSystemStreamMetadata(ImmutableSet.of(STREAM, OTHER_STREAM))).thenReturn(ImmutableMap.of(STREAM, new SystemStreamMetadata(STREAM, ImmutableMap.of(new Partition(0), streamPartition0Metadata))));\r\n    Set<SystemStreamPartition> ssps = ImmutableSet.of(streamPartition0, otherStreamPartition0);\r\n    when(systemAdmin.getSSPMetadata(ssps)).thenCallRealMethod();\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(streamPartition0, streamPartition0Metadata);\r\n    assertEquals(expected, systemAdmin.getSSPMetadata(ssps));\r\n    verify(systemAdmin).getSystemStreamMetadata(ImmutableSet.of(STREAM, OTHER_STREAM));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemAdmin.java",
  "methodName" : "testGetSSPMetadataMissingPartition",
  "sourceCode" : "/**\r\n * Given some SSPs, but missing metadata for one of the SSPs, getSSPMetadata should delegate to\r\n * getSystemStreamMetadata and only fill in results for the SSPs that have metadata.\r\n */\r\n@Test\r\npublic void testGetSSPMetadataMissingPartition() {\r\n    SystemStreamPartition streamPartition0 = new SystemStreamPartition(SYSTEM, STREAM, new Partition(0));\r\n    SystemStreamPartition streamPartition1 = new SystemStreamPartition(SYSTEM, STREAM, new Partition(1));\r\n    SystemAdmin systemAdmin = mock(MySystemAdmin.class);\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata streamPartition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"1\", \"2\", \"3\");\r\n    when(systemAdmin.getSystemStreamMetadata(ImmutableSet.of(STREAM))).thenReturn(ImmutableMap.of(STREAM, new SystemStreamMetadata(STREAM, ImmutableMap.of(new Partition(0), streamPartition0Metadata))));\r\n    Set<SystemStreamPartition> ssps = ImmutableSet.of(streamPartition0, streamPartition1);\r\n    when(systemAdmin.getSSPMetadata(ssps)).thenCallRealMethod();\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(streamPartition0, streamPartition0Metadata);\r\n    assertEquals(expected, systemAdmin.getSSPMetadata(ssps));\r\n    verify(systemAdmin).getSystemStreamMetadata(ImmutableSet.of(STREAM));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemStreamPartitionIterator.java",
  "methodName" : "testHasNextShouldWork",
  "sourceCode" : "@Test\r\npublic void testHasNextShouldWork() {\r\n    int numMessages = 10;\r\n    MockSystemConsumer consumer = new MockSystemConsumer(numMessages);\r\n    SystemStreamPartitionIterator iterator = new SystemStreamPartitionIterator(consumer, SSP);\r\n    while (iterator.hasNext()) {\r\n        assertEquals(--numMessages, iterator.next().getMessage());\r\n    }\r\n    assertFalse(iterator.hasNext());\r\n    assertEquals(0, numMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemStreamPartitionIterator.java",
  "methodName" : "testNextWithoutHasNextCallShouldWorkWhenAvailableAndFailWhenNot",
  "sourceCode" : "@Test\r\npublic void testNextWithoutHasNextCallShouldWorkWhenAvailableAndFailWhenNot() {\r\n    int numMessages = 10;\r\n    MockSystemConsumer consumer = new MockSystemConsumer(numMessages);\r\n    SystemStreamPartitionIterator iterator = new SystemStreamPartitionIterator(consumer, SSP);\r\n    for (int i = 0; i < numMessages; ++i) {\r\n        assertEquals(numMessages - i - 1, iterator.next().getMessage());\r\n    }\r\n    assertFalse(iterator.hasNext());\r\n    try {\r\n        iterator.next();\r\n        fail(\"Expected not to get any more messages from iterator.\");\r\n    } catch (NoSuchElementException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\system\\TestSystemStreamPartitionIterator.java",
  "methodName" : "testNoMessages",
  "sourceCode" : "@Test\r\npublic void testNoMessages() {\r\n    int numMessages = 0;\r\n    MockSystemConsumer consumer = new MockSystemConsumer(numMessages);\r\n    SystemStreamPartitionIterator iterator = new SystemStreamPartitionIterator(consumer, SSP);\r\n    assertFalse(iterator.hasNext());\r\n    try {\r\n        iterator.next();\r\n        fail(\"Expected not to get any more messages from iterator.\");\r\n    } catch (NoSuchElementException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testCreditKeyOnly",
  "sourceCode" : "@Test\r\npublic void testCreditKeyOnly() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Assert.assertEquals(3, rateLimitHelper.getCredits(\"abc\", null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testCreditKeyValue",
  "sourceCode" : "@Test\r\npublic void testCreditKeyValue() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Assert.assertEquals(6, rateLimitHelper.getCredits(\"abc\", \"efg\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testCreditKeys",
  "sourceCode" : "@Test\r\npublic void testCreditKeys() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Assert.assertEquals(9, rateLimitHelper.getCredits(Arrays.asList(\"abc\", \"efg\", \"hij\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testCreditEntries",
  "sourceCode" : "@Test\r\npublic void testCreditEntries() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Assert.assertEquals(12, rateLimitHelper.getEntryCredits(Arrays.asList(new Entry<>(\"abc\", \"efg\"), new Entry<>(\"hij\", \"lmn\"))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testCreditOpId",
  "sourceCode" : "@Test\r\npublic void testCreditOpId() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Assert.assertEquals(1, rateLimitHelper.getCredits(1, 2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testThrottle",
  "sourceCode" : "@Test\r\npublic void testThrottle() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler();\r\n    Timer timer = mock(Timer.class);\r\n    rateLimitHelper.setTimerMetric(timer);\r\n    int times = 0;\r\n    rateLimitHelper.throttle(\"foo\");\r\n    verify(rateLimitHelper.rateLimiter, times(++times)).acquire(anyMapOf(String.class, Integer.class));\r\n    verify(timer, times(times)).update(anyLong());\r\n    rateLimitHelper.throttle(\"foo\", \"bar\");\r\n    verify(rateLimitHelper.rateLimiter, times(++times)).acquire(anyMapOf(String.class, Integer.class));\r\n    verify(timer, times(times)).update(anyLong());\r\n    rateLimitHelper.throttle(Arrays.asList(\"foo\", \"bar\"));\r\n    verify(rateLimitHelper.rateLimiter, times(++times)).acquire(anyMapOf(String.class, Integer.class));\r\n    verify(timer, times(times)).update(anyLong());\r\n    rateLimitHelper.throttle(1, 2);\r\n    verify(rateLimitHelper.rateLimiter, times(++times)).acquire(anyMapOf(String.class, Integer.class));\r\n    verify(timer, times(times)).update(anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestTableRateLimiter.java",
  "methodName" : "testThrottleUnknownTag",
  "sourceCode" : "@Test\r\npublic void testThrottleUnknownTag() {\r\n    TableRateLimiter<String, String> rateLimitHelper = getThrottler(\"unknown_tag\");\r\n    rateLimitHelper.throttle(\"foo\");\r\n    verify(rateLimitHelper.rateLimiter, times(0)).acquire(anyInt());\r\n    verify(rateLimitHelper.rateLimiter, times(1)).acquire(anyMapOf(String.class, Integer.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\TestBaseTableDescriptor.java",
  "methodName" : "testMinimal",
  "sourceCode" : "@Test\r\npublic void testMinimal() {\r\n    Map tableConfig = createTableDescriptor(TABLE_ID).toConfig(new MapConfig());\r\n    Assert.assertEquals(1, tableConfig.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\TestBaseTableDescriptor.java",
  "methodName" : "testProviderFactoryConfig",
  "sourceCode" : "@Test\r\npublic void testProviderFactoryConfig() {\r\n    Map tableConfig = createTableDescriptor(TABLE_ID).toConfig(new MapConfig());\r\n    Assert.assertEquals(1, tableConfig.size());\r\n    assertEquals(\"my.factory\", \"provider.factory\", TABLE_ID, tableConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\table\\TestBaseTableDescriptor.java",
  "methodName" : "testCustomConfig",
  "sourceCode" : "@Test\r\npublic void testCustomConfig() {\r\n    Map tableConfig = createTableDescriptor(TABLE_ID).withConfig(\"abc\", \"xyz\").toConfig(new MapConfig());\r\n    Assert.assertEquals(2, tableConfig.size());\r\n    Assert.assertEquals(\"xyz\", tableConfig.get(\"abc\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testEmptyMapReturnsEmptyList",
  "sourceCode" : "@Test\r\npublic void testEmptyMapReturnsEmptyList() throws InterruptedException {\r\n    BlockingEnvelopeMap map = new MockBlockingEnvelopeMap();\r\n    map.register(SSP, \"0\");\r\n    map.poll(FETCH, 0);\r\n    map.poll(FETCH, 30);\r\n    map.setIsAtHead(SSP, true);\r\n    map.poll(FETCH, -1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testShouldBlockAtLeast100Ms",
  "sourceCode" : "@Test\r\npublic void testShouldBlockAtLeast100Ms() throws InterruptedException {\r\n    BlockingEnvelopeMap map = new MockBlockingEnvelopeMap();\r\n    map.register(SSP, \"0\");\r\n    long now = System.currentTimeMillis();\r\n    map.poll(FETCH, 100);\r\n    assertTrue(System.currentTimeMillis() - now >= 100);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testShouldGetSomeMessages",
  "sourceCode" : "@Test\r\npublic void testShouldGetSomeMessages() throws InterruptedException {\r\n    BlockingEnvelopeMap map = new MockBlockingEnvelopeMap();\r\n    map.register(SSP, \"0\");\r\n    map.put(SSP, ENVELOPE);\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> envelopes = map.poll(FETCH, 0);\r\n    assertEquals(1, envelopes.size());\r\n    assertEquals(1, envelopes.get(SSP).size());\r\n    map.put(SSP, ENVELOPE);\r\n    map.put(SSP, ENVELOPE);\r\n    envelopes = map.poll(FETCH, 0);\r\n    assertEquals(1, envelopes.size());\r\n    assertEquals(2, envelopes.get(SSP).size());\r\n    // Size info.\r\n    assertEquals(0, map.getMessagesSizeInQueue(SSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testNoSizeComputation",
  "sourceCode" : "@Test\r\npublic void testNoSizeComputation() throws InterruptedException {\r\n    BlockingEnvelopeMap map = new MockBlockingEnvelopeMap();\r\n    map.register(SSP, \"0\");\r\n    map.put(SSP, ENVELOPE);\r\n    map.put(SSP, ENVELOPE);\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> envelopes = map.poll(FETCH, 0);\r\n    // Size info.\r\n    assertEquals(0, map.getMessagesSizeInQueue(SSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testSizeComputation",
  "sourceCode" : "@Test\r\npublic void testSizeComputation() throws InterruptedException {\r\n    BlockingEnvelopeMap map = new MockBlockingEnvelopeMap(true);\r\n    map.register(SSP, \"0\");\r\n    map.put(SSP, ENVELOPE_WITH_SIZE);\r\n    map.put(SSP, ENVELOPE_WITH_SIZE);\r\n    // Size info.\r\n    assertEquals(200, map.getMessagesSizeInQueue(SSP));\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> envelopes = map.poll(FETCH, 0);\r\n    assertEquals(0, map.getMessagesSizeInQueue(SSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testShouldBlockWhenNotAtHead",
  "sourceCode" : "@Test\r\npublic void testShouldBlockWhenNotAtHead() throws InterruptedException {\r\n    MockQueue q = new MockQueue();\r\n    final BlockingEnvelopeMap map = new MockBlockingEnvelopeMap(q);\r\n    map.register(SSP, \"0\");\r\n    Thread t = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                // Should trigger a take() call.\r\n                map.poll(FETCH, -1);\r\n            } catch (InterruptedException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        }\r\n    });\r\n    t.setDaemon(true);\r\n    t.start();\r\n    q.awaitPollTimeout();\r\n    t.join(60000);\r\n    // 1000 = blocking timeout constant\r\n    assertEquals(1000, q.timeout);\r\n    assertFalse(t.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestBlockingEnvelopeMap.java",
  "methodName" : "testShouldPollWithATimeout",
  "sourceCode" : "@Test\r\npublic void testShouldPollWithATimeout() throws InterruptedException {\r\n    MockQueue q = new MockQueue();\r\n    // Always use the same time in this test so that we can be sure we get a\r\n    // 100ms poll, rather than a 99ms poll (for example). Have to do this\r\n    // because BlockingEnvelopeMap calls clock.currentTimeMillis twice, and\r\n    // uses the second call to determine the actual poll time.\r\n    final BlockingEnvelopeMap map = new MockBlockingEnvelopeMap(q, new Clock() {\r\n\r\n        private final long now = System.currentTimeMillis();\r\n\r\n        public long currentTimeMillis() {\r\n            return now;\r\n        }\r\n    });\r\n    map.register(SSP, \"0\");\r\n    Thread t = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                // Should trigger a poll(100, TimeUnit.MILLISECONDS) call.\r\n                map.poll(FETCH, 100);\r\n            } catch (InterruptedException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n        }\r\n    });\r\n    t.setDaemon(true);\r\n    t.start();\r\n    q.awaitPollTimeout();\r\n    t.join(60000);\r\n    assertEquals(100, q.timeout);\r\n    assertFalse(t.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestNoOpMetricsRegistry.java",
  "methodName" : "testNoOpMetricsHappyPath",
  "sourceCode" : "@Test\r\npublic void testNoOpMetricsHappyPath() {\r\n    NoOpMetricsRegistry registry = new NoOpMetricsRegistry();\r\n    Counter counter1 = registry.newCounter(\"testc\", \"a\");\r\n    Counter counter2 = registry.newCounter(\"testc\", \"b\");\r\n    Counter counter3 = registry.newCounter(\"testc2\", \"c\");\r\n    Gauge<String> gauge1 = registry.newGauge(\"testg\", \"a\", \"1\");\r\n    Gauge<String> gauge2 = registry.newGauge(\"testg\", \"b\", \"2\");\r\n    Gauge<String> gauge3 = registry.newGauge(\"testg\", \"c\", \"3\");\r\n    Gauge<String> gauge4 = registry.newGauge(\"testg2\", \"d\", \"4\");\r\n    Timer timer1 = registry.newTimer(\"testt\", \"a\");\r\n    Timer timer2 = registry.newTimer(\"testt\", \"b\");\r\n    Timer timer3 = registry.newTimer(\"testt2\", \"c\");\r\n    counter1.inc();\r\n    counter2.inc(2);\r\n    counter3.inc(4);\r\n    gauge1.set(\"5\");\r\n    gauge2.set(\"6\");\r\n    gauge3.set(\"7\");\r\n    gauge4.set(\"8\");\r\n    timer1.update(1L);\r\n    timer2.update(2L);\r\n    timer3.update(3L);\r\n    assertEquals(1, counter1.getCount());\r\n    assertEquals(2, counter2.getCount());\r\n    assertEquals(4, counter3.getCount());\r\n    assertEquals(\"5\", gauge1.getValue());\r\n    assertEquals(\"6\", gauge2.getValue());\r\n    assertEquals(\"7\", gauge3.getValue());\r\n    assertEquals(\"8\", gauge4.getValue());\r\n    assertEquals(1, timer1.getSnapshot().getAverage(), 0);\r\n    assertEquals(2, timer2.getSnapshot().getAverage(), 0);\r\n    assertEquals(3, timer3.getSnapshot().getAverage(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-api\\src\\test\\java\\org\\apache\\samza\\util\\TestSinglePartitionWithoutOffsetsSystemAdmin.java",
  "methodName" : "testShouldGetASinglePartition",
  "sourceCode" : "@Test\r\npublic void testShouldGetASinglePartition() {\r\n    SinglePartitionWithoutOffsetsSystemAdmin admin = new SinglePartitionWithoutOffsetsSystemAdmin();\r\n    Set<String> streamNames = new HashSet<String>();\r\n    streamNames.add(\"a\");\r\n    streamNames.add(\"b\");\r\n    Map<String, SystemStreamMetadata> metadata = admin.getSystemStreamMetadata(streamNames);\r\n    assertEquals(2, metadata.size());\r\n    SystemStreamMetadata metadata1 = metadata.get(\"a\");\r\n    SystemStreamMetadata metadata2 = metadata.get(\"b\");\r\n    assertEquals(1, metadata1.getSystemStreamPartitionMetadata().size());\r\n    assertEquals(1, metadata2.getSystemStreamPartitionMetadata().size());\r\n    assertNull(metadata.get(new SystemStreamPartition(\"test-system\", \"c\", new Partition(0))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisRecordProcessor.java",
  "methodName" : "testLifeCycleWithEvents",
  "sourceCode" : "@Test\r\npublic void testLifeCycleWithEvents() {\r\n    testLifeCycleHelper(5);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisRecordProcessor.java",
  "methodName" : "testLifeCycleWithNoEvents",
  "sourceCode" : "@Test\r\npublic void testLifeCycleWithNoEvents() {\r\n    testLifeCycleHelper(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisRecordProcessor.java",
  "methodName" : "testCheckpointAfterInit",
  "sourceCode" : "/**\r\n * Test the scenario where a processor instance is created for a shard and while it is processing records, it got\r\n * re-assigned to the same consumer. This results in a new processor instance owning the shard and this instance\r\n * could receive checkpoint calls for the records that are processed by the old processor instance. This test covers\r\n * the scenario where the new instance receives the checkpoint call while it is done with the initialization phase and\r\n * before it processed any records.\r\n */\r\n@Test\r\npublic void testCheckpointAfterInit() {\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    final CountDownLatch receivedShutdownLatch = new CountDownLatch(1);\r\n    KinesisRecordProcessorListener listener = new KinesisRecordProcessorListener() {\r\n\r\n        @Override\r\n        public void onReceiveRecords(SystemStreamPartition ssp, List<Record> records, long millisBehindLatest) {\r\n        }\r\n\r\n        @Override\r\n        public void onShutdown(SystemStreamPartition ssp) {\r\n            receivedShutdownLatch.countDown();\r\n        }\r\n    };\r\n    KinesisRecordProcessor processor = new KinesisRecordProcessor(new SystemStreamPartition(system, stream, new Partition(0)), listener);\r\n    // Initialize the processor\r\n    ExtendedSequenceNumber seqNum = new ExtendedSequenceNumber(\"0000\");\r\n    InitializationInput initializationInput = new InitializationInput().withShardId(\"shard-0000\").withExtendedSequenceNumber(seqNum);\r\n    processor.initialize(initializationInput);\r\n    // Call checkpoint. This checkpoint could have originally headed to the processor instance for the same shard but\r\n    // due to reassignment a new processor instance is created.\r\n    processor.checkpoint(\"1234567\");\r\n    // Call shutdown (with ZOMBIE reason) on processor and verify that the processor calls shutdown on the listener.\r\n    shutDownProcessor(processor, ShutdownReason.ZOMBIE);\r\n    // Verify that the processor is shutdown.\r\n    Assert.assertEquals(\"Unable to shutdown processor.\", 0, receivedShutdownLatch.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisRecordProcessor.java",
  "methodName" : "testShutdownDuringReshardWithEvents",
  "sourceCode" : "@Test\r\npublic void testShutdownDuringReshardWithEvents() throws InterruptedException {\r\n    testShutdownDuringReshardHelper(5);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisRecordProcessor.java",
  "methodName" : "testShutdownDuringReshardWithNoEvents",
  "sourceCode" : "@Test\r\npublic void testShutdownDuringReshardWithNoEvents() throws InterruptedException {\r\n    testShutdownDuringReshardHelper(0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisSystemConsumer.java",
  "methodName" : "testProcessRecords",
  "sourceCode" : "@Test\r\npublic void testProcessRecords() throws InterruptedException, NoSuchFieldException, IllegalAccessException {\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    int numShards = 2;\r\n    int numRecordsPerShard = 5;\r\n    testProcessRecordsHelper(system, stream, numShards, numRecordsPerShard);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisSystemConsumer.java",
  "methodName" : "testProcessRecordsWithEmptyRecordList",
  "sourceCode" : "@Test\r\npublic void testProcessRecordsWithEmptyRecordList() throws InterruptedException, NoSuchFieldException, IllegalAccessException {\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    int numShards = 1;\r\n    int numRecordsPerShard = 0;\r\n    testProcessRecordsHelper(system, stream, numShards, numRecordsPerShard);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisSystemConsumerOffset.java",
  "methodName" : "testEquality",
  "sourceCode" : "@Test\r\npublic void testEquality() {\r\n    KinesisSystemConsumerOffset inCkpt = new KinesisSystemConsumerOffset(\"shard-00000\", \"123456\");\r\n    KinesisSystemConsumerOffset outCkpt = KinesisSystemConsumerOffset.parse(inCkpt.toString());\r\n    Assert.assertEquals(inCkpt, outCkpt);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestKinesisSystemConsumerOffset.java",
  "methodName" : "testInEquality",
  "sourceCode" : "@Test\r\npublic void testInEquality() {\r\n    KinesisSystemConsumerOffset inCkpt = new KinesisSystemConsumerOffset(\"shard-00000\", \"123456\");\r\n    // With different shardId\r\n    KinesisSystemConsumerOffset inCkpt1 = new KinesisSystemConsumerOffset(\"shard-00001\", \"123456\");\r\n    KinesisSystemConsumerOffset outCkpt = KinesisSystemConsumerOffset.parse(inCkpt1.toString());\r\n    Assert.assertTrue(!inCkpt.equals(outCkpt));\r\n    // With different seqNumber\r\n    inCkpt1 = new KinesisSystemConsumerOffset(\"shard-00000\", \"123457\");\r\n    outCkpt = KinesisSystemConsumerOffset.parse(inCkpt1.toString());\r\n    Assert.assertTrue(!inCkpt.equals(outCkpt));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestSSPAllocator.java",
  "methodName" : "testAllocateAndFree",
  "sourceCode" : "@Test\r\npublic void testAllocateAndFree() throws NoAvailablePartitionException, NoSuchFieldException, IllegalAccessException {\r\n    int numPartitions = 2;\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    List<SystemStreamPartition> ssps = new ArrayList<>();\r\n    IntStream.range(0, numPartitions).forEach(i -> ssps.add(new SystemStreamPartition(system, stream, new Partition(i))));\r\n    SSPAllocator allocator = new SSPAllocator();\r\n    ssps.forEach(allocator::free);\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(0)));\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(1)));\r\n    SystemStreamPartition ssp = allocator.allocate(stream);\r\n    Assert.assertFalse(isSspAvailable(allocator, ssps.get(0)));\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(1)));\r\n    Assert.assertEquals(ssp, ssps.get(0));\r\n    ssp = allocator.allocate(stream);\r\n    Assert.assertFalse(isSspAvailable(allocator, ssps.get(0)));\r\n    Assert.assertFalse(isSspAvailable(allocator, ssps.get(1)));\r\n    Assert.assertEquals(ssp, ssps.get(1));\r\n    allocator.free(ssps.get(1));\r\n    Assert.assertFalse(isSspAvailable(allocator, ssps.get(0)));\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(1)));\r\n    allocator.free(ssps.get(0));\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(0)));\r\n    Assert.assertTrue(isSspAvailable(allocator, ssps.get(1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestSSPAllocator.java",
  "methodName" : "testAssignMoreThanMaxPartitions",
  "sourceCode" : "@Test(expected = NoAvailablePartitionException.class)\r\npublic void testAssignMoreThanMaxPartitions() throws NoAvailablePartitionException {\r\n    int numPartitions = 2;\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    List<SystemStreamPartition> ssps = new ArrayList<>();\r\n    IntStream.range(0, numPartitions).forEach(i -> ssps.add(new SystemStreamPartition(system, stream, new Partition(i))));\r\n    SSPAllocator allocator = new SSPAllocator();\r\n    ssps.forEach(allocator::free);\r\n    allocator.allocate(stream);\r\n    allocator.allocate(stream);\r\n    // An exception should be thrown at this point.\r\n    allocator.allocate(stream);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestSSPAllocator.java",
  "methodName" : "testFreeSameSspTwice",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFreeSameSspTwice() throws NoAvailablePartitionException {\r\n    int numPartitions = 2;\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    List<SystemStreamPartition> ssps = new ArrayList<>();\r\n    IntStream.range(0, numPartitions).forEach(i -> ssps.add(new SystemStreamPartition(system, stream, new Partition(i))));\r\n    SSPAllocator allocator = new SSPAllocator();\r\n    ssps.forEach(allocator::free);\r\n    SystemStreamPartition ssp = allocator.allocate(stream);\r\n    allocator.free(ssp);\r\n    // An exception should be thrown at this point.\r\n    allocator.free(ssp);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\consumer\\TestSSPAllocator.java",
  "methodName" : "testFreeUnallocatedSsp",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFreeUnallocatedSsp() throws NoAvailablePartitionException {\r\n    int numPartitions = 2;\r\n    String system = \"kinesis\";\r\n    String stream = \"stream\";\r\n    List<SystemStreamPartition> ssps = new ArrayList<>();\r\n    IntStream.range(0, numPartitions).forEach(i -> ssps.add(new SystemStreamPartition(system, stream, new Partition(i))));\r\n    SSPAllocator allocator = new SSPAllocator();\r\n    ssps.forEach(allocator::free);\r\n    allocator.allocate(stream);\r\n    // An exception should be thrown at this point.\r\n    allocator.free(ssps.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\descriptors\\TestKinesisInputDescriptor.java",
  "methodName" : "testConfigGeneration",
  "sourceCode" : "@Test\r\npublic void testConfigGeneration() {\r\n    String systemName = \"kinesis\";\r\n    String streamName = \"Seine\";\r\n    KinesisSystemDescriptor sd = new KinesisSystemDescriptor(systemName);\r\n    Map<String, String> cliConfig = new HashMap<>();\r\n    cliConfig.put(\"key1\", \"value1\");\r\n    KinesisInputDescriptor<KV<String, byte[]>> id = sd.getInputDescriptor(streamName, new NoOpSerde<byte[]>()).withRegion(\"Paris\").withAccessKey(\"accessKey\").withSecretKey(\"secretKey\").withKCLConfig(cliConfig);\r\n    Map<String, String> generatedConfig = id.toConfig();\r\n    Assert.assertEquals(5, generatedConfig.size());\r\n    Assert.assertEquals(systemName, generatedConfig.get(\"streams.Seine.samza.system\"));\r\n    Assert.assertEquals(\"Paris\", generatedConfig.get(String.format(KinesisConfig.CONFIG_STREAM_REGION, systemName, streamName)));\r\n    Assert.assertEquals(\"accessKey\", generatedConfig.get(String.format(KinesisConfig.CONFIG_STREAM_ACCESS_KEY, systemName, streamName)));\r\n    Assert.assertEquals(\"secretKey\", generatedConfig.get(String.format(KinesisConfig.CONFIG_STREAM_SECRET_KEY, systemName, streamName)));\r\n    Assert.assertEquals(\"value1\", generatedConfig.get(String.format(KinesisConfig.CONFIG_STREAM_KINESIS_CLIENT_LIB_CONFIG, systemName, streamName) + \"key1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\descriptors\\TestKinesisSystemDescriptor.java",
  "methodName" : "testConfigGeneration",
  "sourceCode" : "@Test\r\npublic void testConfigGeneration() {\r\n    String systemName = \"kinesis\";\r\n    Map<String, String> kclConfig = new HashMap<>();\r\n    kclConfig.put(\"key1\", \"value1\");\r\n    Map<String, String> awsConfig = new HashMap<>();\r\n    awsConfig.put(\"key2\", \"value2\");\r\n    KinesisSystemDescriptor sd = new KinesisSystemDescriptor(systemName).withRegion(\"London\").withProxyHost(\"US\").withProxyPort(1776).withAWSConfig(awsConfig).withKCLConfig(kclConfig);\r\n    Map<String, String> generatedConfig = sd.toConfig();\r\n    Assert.assertEquals(6, generatedConfig.size());\r\n    Assert.assertEquals(KinesisSystemFactory.class.getName(), generatedConfig.get(\"systems.kinesis.samza.factory\"));\r\n    Assert.assertEquals(\"London\", generatedConfig.get(String.format(KinesisConfig.CONFIG_SYSTEM_REGION, systemName)));\r\n    Assert.assertEquals(\"US\", generatedConfig.get(String.format(KinesisConfig.CONFIG_PROXY_HOST, systemName)));\r\n    Assert.assertEquals(\"1776\", generatedConfig.get(String.format(KinesisConfig.CONFIG_PROXY_PORT, systemName)));\r\n    Assert.assertEquals(\"value1\", generatedConfig.get(String.format(KinesisConfig.CONFIG_SYSTEM_KINESIS_CLIENT_LIB_CONFIG, systemName) + \"key1\"));\r\n    Assert.assertEquals(\"value2\", generatedConfig.get(String.format(KinesisConfig.CONFIG_AWS_CLIENT_CONFIG, systemName) + \"key2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisAWSCredentialsProvider.java",
  "methodName" : "testCredentialsProviderWithNonNullKeys",
  "sourceCode" : "@Test\r\npublic void testCredentialsProviderWithNonNullKeys() {\r\n    String accessKey = \"accessKey\";\r\n    String secretKey = \"secretKey\";\r\n    KinesisAWSCredentialsProvider credProvider = new KinesisAWSCredentialsProvider(accessKey, secretKey);\r\n    assertEquals(credProvider.getCredentials().getAWSAccessKeyId(), accessKey);\r\n    assertEquals(credProvider.getCredentials().getAWSSecretKey(), secretKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisAWSCredentialsProvider.java",
  "methodName" : "testCredentialsProviderWithNullAccessKey",
  "sourceCode" : "@Test\r\npublic void testCredentialsProviderWithNullAccessKey() {\r\n    String secretKey = \"secretKey\";\r\n    KinesisAWSCredentialsProvider credProvider = new KinesisAWSCredentialsProvider(null, secretKey);\r\n    assertNull(credProvider.getCredentials().getAWSAccessKeyId());\r\n    assertNull(credProvider.getCredentials().getAWSSecretKey());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisAWSCredentialsProvider.java",
  "methodName" : "testCredentialsProviderWithNullSecretKey",
  "sourceCode" : "@Test\r\npublic void testCredentialsProviderWithNullSecretKey() {\r\n    String accessKey = \"accessKey\";\r\n    KinesisAWSCredentialsProvider credProvider = new KinesisAWSCredentialsProvider(accessKey, null);\r\n    assertNull(credProvider.getCredentials().getAWSAccessKeyId());\r\n    assertNull(credProvider.getCredentials().getAWSSecretKey());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisAWSCredentialsProvider.java",
  "methodName" : "testCredentialsProviderWithNullKeys",
  "sourceCode" : "@Test\r\npublic void testCredentialsProviderWithNullKeys() {\r\n    KinesisAWSCredentialsProvider credProvider = new KinesisAWSCredentialsProvider(null, null);\r\n    assertNull(credProvider.getCredentials().getAWSAccessKeyId());\r\n    assertNull(credProvider.getCredentials().getAWSSecretKey());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisConfig.java",
  "methodName" : "testGetKinesisStreams",
  "sourceCode" : "@Test\r\npublic void testGetKinesisStreams() {\r\n    Map<String, String> kv = new HashMap<>();\r\n    kv.put(\"systems.kinesis.streams.kinesis-stream1.prop1\", \"value1\");\r\n    kv.put(\"systems.kinesis.streams.kinesis-stream1.prop2\", \"value2\");\r\n    kv.put(\"systems.kinesis.streams.kinesis-stream2.prop1\", \"value3\");\r\n    Config config = new MapConfig(kv);\r\n    KinesisConfig kConfig = new KinesisConfig(config);\r\n    Set<String> streams = kConfig.getKinesisStreams(\"kinesis\");\r\n    assertEquals(2, streams.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisConfig.java",
  "methodName" : "testKinesisConfigs",
  "sourceCode" : "@Test\r\npublic void testKinesisConfigs() {\r\n    Map<String, String> kv = new HashMap<>();\r\n    String system = \"kinesis\";\r\n    String stream = \"kinesis-stream\";\r\n    String systemConfigPrefix = String.format(\"systems.%s.\", system);\r\n    String ssConfigPrefix = String.format(\"systems.%s.streams.%s.\", system, stream);\r\n    kv.put(\"sensitive.\" + ssConfigPrefix + \"aws.secretKey\", \"secretKey\");\r\n    kv.put(systemConfigPrefix + \"aws.region\", \"us-east-1\");\r\n    kv.put(ssConfigPrefix + \"aws.accessKey\", \"accessKey\");\r\n    Config config = new MapConfig(kv);\r\n    KinesisConfig kConfig = new KinesisConfig(config);\r\n    assertEquals(\"us-east-1\", kConfig.getRegion(system, stream).getName());\r\n    assertEquals(\"accessKey\", kConfig.getStreamAccessKey(system, stream));\r\n    assertEquals(\"secretKey\", kConfig.getStreamSecretKey(system, stream));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisConfig.java",
  "methodName" : "testAwsClientConfigs",
  "sourceCode" : "@Test\r\npublic void testAwsClientConfigs() {\r\n    Map<String, String> kv = new HashMap<>();\r\n    String system = \"kinesis\";\r\n    String systemConfigPrefix = String.format(\"systems.%s.\", system);\r\n    // Aws Client Configs\r\n    kv.put(systemConfigPrefix + \"aws.clientConfig.ProxyHost\", \"hostName\");\r\n    kv.put(systemConfigPrefix + \"aws.clientConfig.ProxyPort\", \"8080\");\r\n    Config config = new MapConfig(kv);\r\n    KinesisConfig kConfig = new KinesisConfig(config);\r\n    assertEquals(\"hostName\", kConfig.getAWSClientConfig(system).getProxyHost());\r\n    assertEquals(8080, kConfig.getAWSClientConfig(system).getProxyPort());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisConfig.java",
  "methodName" : "testKclConfigs",
  "sourceCode" : "@Test\r\npublic void testKclConfigs() {\r\n    Map<String, String> kv = new HashMap<>();\r\n    String system = \"kinesis\";\r\n    String stream = \"kinesis-stream\";\r\n    String systemConfigPrefix = String.format(\"systems.%s.\", system);\r\n    // region config is required for setting kcl config.\r\n    kv.put(systemConfigPrefix + \"aws.region\", \"us-east-1\");\r\n    // Kcl Configs\r\n    kv.put(systemConfigPrefix + \"aws.kcl.TableName\", \"sample-table\");\r\n    kv.put(systemConfigPrefix + \"aws.kcl.MaxRecords\", \"100\");\r\n    kv.put(systemConfigPrefix + \"aws.kcl.CallProcessRecordsEvenForEmptyRecordList\", \"true\");\r\n    kv.put(systemConfigPrefix + \"aws.kcl.InitialPositionInStream\", \"TRIM_HORIZON\");\r\n    // override one of the Kcl configs for kinesis-stream1\r\n    kv.put(systemConfigPrefix + \"streams.kinesis-stream1.aws.kcl.InitialPositionInStream\", \"LATEST\");\r\n    Config config = new MapConfig(kv);\r\n    KinesisConfig kConfig = new KinesisConfig(config);\r\n    KinesisClientLibConfiguration kclConfig = kConfig.getKinesisClientLibConfig(system, stream, \"sample-app\");\r\n    assertEquals(\"sample-table\", kclConfig.getTableName());\r\n    assertEquals(100, kclConfig.getMaxRecords());\r\n    assertTrue(kclConfig.shouldCallProcessRecordsEvenForEmptyRecordList());\r\n    assertEquals(InitialPositionInStream.TRIM_HORIZON, kclConfig.getInitialPositionInStream());\r\n    // verify if the overriden config is applied for kinesis-stream1\r\n    kclConfig = kConfig.getKinesisClientLibConfig(system, \"kinesis-stream1\", \"sample-app\");\r\n    assertEquals(InitialPositionInStream.LATEST, kclConfig.getInitialPositionInStream());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisConfig.java",
  "methodName" : "testgetKCLConfigWithUnknownConfigs",
  "sourceCode" : "@Test\r\npublic void testgetKCLConfigWithUnknownConfigs() {\r\n    Map<String, String> kv = new HashMap<>();\r\n    kv.put(\"systems.kinesis.aws.region\", \"us-east-1\");\r\n    kv.put(\"systems.kinesis.streams.kinesis-stream.aws.kcl.random\", \"value\");\r\n    Config config = new MapConfig(kv);\r\n    KinesisConfig kConfig = new KinesisConfig(config);\r\n    // Should not throw any exception and just ignore the unknown configs.\r\n    kConfig.getKinesisClientLibConfig(\"kinesis\", \"kinesis-stream\", \"sample-app\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisSystemFactory.java",
  "methodName" : "testGetConsumer",
  "sourceCode" : "@Test\r\npublic void testGetConsumer() {\r\n    String systemName = \"test\";\r\n    Config config = buildKinesisConsumerConfig(systemName);\r\n    KinesisSystemFactory factory = new KinesisSystemFactory();\r\n    MetricsRegistry metricsRegistry = new NoOpMetricsRegistry();\r\n    Assert.assertNotSame(factory.getConsumer(\"test\", config, metricsRegistry), factory.getAdmin(systemName, config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisSystemFactory.java",
  "methodName" : "testGetAdminWithIncorrectSspGrouper",
  "sourceCode" : "@Ignore\r\n@Test(expected = ConfigException.class)\r\npublic void testGetAdminWithIncorrectSspGrouper() {\r\n    String systemName = \"test\";\r\n    KinesisSystemFactory factory = new KinesisSystemFactory();\r\n    Config config = buildKinesisConsumerConfig(systemName, \"org.apache.samza.container.grouper.stream.SystemStreamPartitionGrouperFactory\");\r\n    factory.getAdmin(systemName, config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisSystemFactory.java",
  "methodName" : "testGetAdminWithBroadcastStreams",
  "sourceCode" : "@Ignore\r\n@Test(expected = ConfigException.class)\r\npublic void testGetAdminWithBroadcastStreams() {\r\n    String systemName = \"test\";\r\n    KinesisSystemFactory factory = new KinesisSystemFactory();\r\n    Config config = buildKinesisConsumerConfig(systemName, \"org.apache.samza.container.grouper.stream.AllSspToSingleTaskGrouperFactory\", \"test.stream#0\");\r\n    factory.getAdmin(systemName, config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-aws\\src\\test\\java\\org\\apache\\samza\\system\\kinesis\\TestKinesisSystemFactory.java",
  "methodName" : "testGetAdminWithBootstrapStream",
  "sourceCode" : "@Ignore\r\n@Test(expected = ConfigException.class)\r\npublic void testGetAdminWithBootstrapStream() {\r\n    String systemName = \"test\";\r\n    KinesisSystemFactory factory = new KinesisSystemFactory();\r\n    Config config = buildKinesisConsumerConfig(systemName, \"org.apache.samza.container.grouper.stream.AllSspToSingleTaskGrouperFactory\", null, \"kinesis-stream\");\r\n    factory.getAdmin(systemName, config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobAvroWriter.java",
  "methodName" : "encodeRecord",
  "sourceCode" : "@VisibleForTesting\r\nbyte[] encodeRecord(IndexedRecord record) {\r\n    ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n    Schema schema = record.getSchema();\r\n    try {\r\n        EncoderFactory encoderfactory = new EncoderFactory();\r\n        BinaryEncoder encoder = encoderfactory.binaryEncoder(out, null);\r\n        DatumWriter<IndexedRecord> writer;\r\n        if (record instanceof SpecificRecord) {\r\n            writer = new SpecificDatumWriter<>(schema);\r\n        } else {\r\n            writer = new GenericDatumWriter<>(schema);\r\n        }\r\n        writer.write(record, encoder);\r\n        //encoder may buffer\r\n        encoder.flush();\r\n    } catch (Exception e) {\r\n        throw new SamzaException(\"Unable to serialize Avro record using schema within the record: \" + schema.toString(), e);\r\n    }\r\n    return out.toByteArray();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobOutputStream.java",
  "methodName" : "commitBlob",
  "sourceCode" : "// SAMZA-2476 stubbing BlockBlobAsyncClient.commitBlockListWithResponse was causing flaky tests.\r\n@VisibleForTesting\r\nvoid commitBlob(ArrayList<String> blockList, Map<String, String> blobMetadata) {\r\n    blobAsyncClient.commitBlockListWithResponse(blockList, null, blobMetadata, null, null).block();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobOutputStream.java",
  "methodName" : "stageBlock",
  "sourceCode" : "// SAMZA-2476 stubbing BlockBlobAsyncClient.stageBlock was causing flaky tests.\r\n@VisibleForTesting\r\nvoid stageBlock(String blockIdEncoded, ByteBuffer outputStream, int blockSize) throws InterruptedException {\r\n    invokeBlobClientStageBlock(blockIdEncoded, outputStream, blockSize).subscribeOn(Schedulers.boundedElastic()).block(Duration.ofMillis(flushTimeoutMs));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobOutputStream.java",
  "methodName" : "invokeBlobClientStageBlock",
  "sourceCode" : "@VisibleForTesting\r\nMono<Void> invokeBlobClientStageBlock(String blockIdEncoded, ByteBuffer outputStream, int blockSize) {\r\n    return blobAsyncClient.stageBlock(blockIdEncoded, Flux.just(outputStream), blockSize);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobOutputStream.java",
  "methodName" : "clearAndMarkClosed",
  "sourceCode" : "// blockList cleared makes it hard to test close\r\n@VisibleForTesting\r\nvoid clearAndMarkClosed() {\r\n    blockList.clear();\r\n    pendingUpload.stream().forEach(future -> future.cancel(true));\r\n    pendingUpload.clear();\r\n    isClosed = true;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\avro\\AzureBlobOutputStream.java",
  "methodName" : "getBlobMetadataGenerator",
  "sourceCode" : "@VisibleForTesting\r\nBlobMetadataGenerator getBlobMetadataGenerator() throws Exception {\r\n    return blobMetadataGeneratorFactory.getBlobMetadataGeneratorInstance(blobMetadataGeneratorConfig);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\producer\\AzureBlobSystemProducer.java",
  "methodName" : "setupAzureContainer",
  "sourceCode" : "@VisibleForTesting\r\nvoid setupAzureContainer() {\r\n    try {\r\n        BlobServiceAsyncClient storageClient = clientFactory.getBlobClientBuilder(systemName, AZURE_URL, config).getBlobServiceAsyncClient();\r\n        validateFlushThresholdSizeSupported(storageClient);\r\n        containerAsyncClient = storageClient.getBlobContainerAsyncClient(systemName);\r\n        // Only way to check if container exists or not is by creating it and look for failure/success.\r\n        createContainerIfNotExists(containerAsyncClient);\r\n    } catch (Exception e) {\r\n        metrics.updateAzureContainerMetrics();\r\n        throw new SystemProducerException(\"Failed to set up Azure container for SystemName: \" + systemName, e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\producer\\AzureBlobSystemProducer.java",
  "methodName" : "getOrCreateWriter",
  "sourceCode" : "/**\r\n * // find the writer in the writerMap else create one\r\n * @param source for which to find/create the writer\r\n * @param messageEnvelope to fetch the schema from if writer needs to be created\r\n * @return an AzureBlobWriter object\r\n */\r\n@VisibleForTesting\r\nAzureBlobWriter getOrCreateWriter(String source, OutgoingMessageEnvelope messageEnvelope) {\r\n    String writerMapKey;\r\n    String blobURLPrefix;\r\n    String partitionKey = getPartitionKey(messageEnvelope);\r\n    // using most significant bits in UUID (8 digits) to avoid collision in blob names\r\n    if (partitionKey == null) {\r\n        writerMapKey = messageEnvelope.getSystemStream().getStream();\r\n        blobURLPrefix = String.format(BLOB_NAME_PREFIX, messageEnvelope.getSystemStream().getStream());\r\n    } else {\r\n        writerMapKey = messageEnvelope.getSystemStream().getStream() + \"/\" + partitionKey;\r\n        blobURLPrefix = String.format(BLOB_NAME_PARTITION_PREFIX, messageEnvelope.getSystemStream().getStream(), partitionKey);\r\n    }\r\n    Map<String, AzureBlobWriter> sourceWriterMap = writerMap.get(source);\r\n    if (sourceWriterMap == null) {\r\n        throw new SystemProducerException(\"Attempting to send to source: \" + source + \" but it is not registered\");\r\n    }\r\n    AzureBlobWriter writer = sourceWriterMap.get(writerMapKey);\r\n    if (writer == null) {\r\n        synchronized (sourceWriterCreationLockMap.get(source)) {\r\n            writer = sourceWriterMap.get(writerMapKey);\r\n            if (writer == null) {\r\n                AzureBlobWriterMetrics writerMetrics = new AzureBlobWriterMetrics(metrics.getAggregateMetrics(), metrics.getSystemMetrics(), metrics.getSourceMetrics(source));\r\n                writer = createNewWriter(blobURLPrefix, writerMetrics, messageEnvelope.getSystemStream().getStream());\r\n                sourceWriterMap.put(writerMapKey, writer);\r\n            }\r\n        }\r\n    }\r\n    return writer;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\main\\java\\org\\apache\\samza\\system\\azureblob\\producer\\AzureBlobSystemProducer.java",
  "methodName" : "createNewWriter",
  "sourceCode" : "@VisibleForTesting\r\nAzureBlobWriter createNewWriter(String blobURL, AzureBlobWriterMetrics writerMetrics, String streamName) {\r\n    try {\r\n        return writerFactory.getWriterInstance(containerAsyncClient, blobURL, asyncBlobThreadPool, writerMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, streamName, blockFlushThresholdSize, flushTimeoutMs, CompressionFactory.getInstance().getCompression(config.getCompressionType(systemName)), config.getSuffixRandomStringToBlobName(systemName), config.getMaxBlobSize(systemName), config.getMaxMessagesPerBlob(systemName), config.getInitBufferSizeBytes(systemName));\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(\"Failed to create a writer for the producer.\", e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\checkpoint\\azure\\ITestAzureCheckpointManager.java",
  "methodName" : "testStoringAndReadingCheckpointsSamePartition",
  "sourceCode" : "@Test\r\npublic void testStoringAndReadingCheckpointsSamePartition() {\r\n    Partition partition = new Partition(0);\r\n    TaskName taskName = new TaskName(\"taskName0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"Azure\", \"Stream\", partition);\r\n    Map<SystemStreamPartition, String> sspMap = new HashMap<>();\r\n    sspMap.put(ssp, \"12345\");\r\n    Checkpoint cp0 = new CheckpointV1(sspMap);\r\n    sspMap.put(ssp, \"54321\");\r\n    Checkpoint cp1 = new CheckpointV1(sspMap);\r\n    checkpointManager.register(taskName);\r\n    checkpointManager.writeCheckpoint(taskName, cp0);\r\n    Checkpoint readCp = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp0, readCp);\r\n    checkpointManager.writeCheckpoint(taskName, cp1);\r\n    Checkpoint readCp1 = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp1, readCp1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\checkpoint\\azure\\ITestAzureCheckpointManager.java",
  "methodName" : "testStoringAndReadingCheckpointsMultiPartitions",
  "sourceCode" : "@Test\r\npublic void testStoringAndReadingCheckpointsMultiPartitions() {\r\n    Partition partition = new Partition(0);\r\n    Partition partition1 = new Partition(1);\r\n    TaskName taskName = new TaskName(\"taskName\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"Azure\", \"Stream\", partition);\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"Azure\", \"Stream\", partition1);\r\n    Map<SystemStreamPartition, String> sspMap = new HashMap<>();\r\n    sspMap.put(ssp, \"12345\");\r\n    sspMap.put(ssp1, \"54321\");\r\n    Checkpoint cp1 = new CheckpointV1(sspMap);\r\n    Map<SystemStreamPartition, String> sspMap2 = new HashMap<>();\r\n    sspMap2.put(ssp, \"12347\");\r\n    sspMap2.put(ssp1, \"54323\");\r\n    Checkpoint cp2 = new CheckpointV1(sspMap2);\r\n    checkpointManager.register(taskName);\r\n    checkpointManager.writeCheckpoint(taskName, cp1);\r\n    Checkpoint readCp1 = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp1, readCp1);\r\n    checkpointManager.writeCheckpoint(taskName, cp2);\r\n    Checkpoint readCp2 = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp2, readCp2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\checkpoint\\azure\\ITestAzureCheckpointManager.java",
  "methodName" : "testStoringAndReadingCheckpointsMultiTasks",
  "sourceCode" : "@Test\r\npublic void testStoringAndReadingCheckpointsMultiTasks() {\r\n    Partition partition = new Partition(0);\r\n    Partition partition1 = new Partition(1);\r\n    TaskName taskName = new TaskName(\"taskName1\");\r\n    TaskName taskName1 = new TaskName(\"taskName2\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"Azure\", \"Stream\", partition);\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"Azure\", \"Stream\", partition1);\r\n    Map<SystemStreamPartition, String> sspMap = new HashMap<>();\r\n    sspMap.put(ssp, \"12345\");\r\n    sspMap.put(ssp1, \"54321\");\r\n    Checkpoint cp1 = new CheckpointV1(sspMap);\r\n    Map<SystemStreamPartition, String> sspMap2 = new HashMap<>();\r\n    sspMap2.put(ssp, \"12347\");\r\n    sspMap2.put(ssp1, \"54323\");\r\n    Checkpoint cp2 = new CheckpointV1(sspMap2);\r\n    checkpointManager.register(taskName);\r\n    checkpointManager.register(taskName1);\r\n    checkpointManager.writeCheckpoint(taskName, cp1);\r\n    checkpointManager.writeCheckpoint(taskName1, cp2);\r\n    Checkpoint readCp1 = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertNotNull(readCp1);\r\n    Assert.assertEquals(cp1, readCp1);\r\n    Checkpoint readCp2 = checkpointManager.readLastCheckpoint(taskName1);\r\n    Assert.assertNotNull(readCp2);\r\n    Assert.assertEquals(cp2, readCp2);\r\n    checkpointManager.writeCheckpoint(taskName, cp2);\r\n    checkpointManager.writeCheckpoint(taskName1, cp1);\r\n    readCp1 = checkpointManager.readLastCheckpoint(taskName1);\r\n    Assert.assertEquals(cp1, readCp1);\r\n    readCp2 = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp2, readCp2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\checkpoint\\azure\\ITestAzureCheckpointManager.java",
  "methodName" : "testMultipleBatchWrites",
  "sourceCode" : "@Test\r\npublic void testMultipleBatchWrites() {\r\n    TaskName taskName = new TaskName(\"taskName3\");\r\n    Map<SystemStreamPartition, String> sspMap = new HashMap<>();\r\n    final int testBatchNum = 2;\r\n    final int testOffsetNum = testBatchNum * AzureCheckpointManager.MAX_WRITE_BATCH_SIZE;\r\n    for (int i = 0; i < testOffsetNum; i++) {\r\n        Partition partition = new Partition(i);\r\n        SystemStreamPartition ssp = new SystemStreamPartition(\"Azure\", \"Stream\", partition);\r\n        sspMap.put(ssp, String.valueOf(i));\r\n    }\r\n    Checkpoint cp0 = new CheckpointV1(sspMap);\r\n    checkpointManager.register(taskName);\r\n    checkpointManager.writeCheckpoint(taskName, cp0);\r\n    Checkpoint readCp = checkpointManager.readLastCheckpoint(taskName);\r\n    Assert.assertEquals(cp0, readCp);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWrite",
  "sourceCode" : "@Test\r\npublic void testWrite() throws Exception {\r\n    int numberOfMessages = 10;\r\n    for (int i = 0; i < numberOfMessages; ++i) {\r\n        azureBlobAvroWriter.write(ome);\r\n    }\r\n    verify(mockDataFileWriter, times(numberOfMessages)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockAzureBlobOutputStream, times(numberOfMessages)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWriteGenericFixed",
  "sourceCode" : "@Test\r\npublic void testWriteGenericFixed() throws Exception {\r\n    OutgoingMessageEnvelope omeGenericFixed = createOMEGenericFixed(\"Topic1\", encodedRecord);\r\n    int numberOfMessages = 10;\r\n    for (int i = 0; i < numberOfMessages; ++i) {\r\n        azureBlobAvroWriter.write(omeGenericFixed);\r\n    }\r\n    verify(mockDataFileWriter, times(numberOfMessages)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockAzureBlobOutputStream, times(numberOfMessages)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWriteGenericRecord",
  "sourceCode" : "@Test\r\npublic void testWriteGenericRecord() throws Exception {\r\n    OutgoingMessageEnvelope omeGenericRecord = createOMEGenericRecord(\"Topic1\");\r\n    doReturn(encodedRecord).when(azureBlobAvroWriter).encodeRecord((IndexedRecord) omeGenericRecord.getMessage());\r\n    int numberOfMessages = 10;\r\n    for (int i = 0; i < numberOfMessages; ++i) {\r\n        azureBlobAvroWriter.write(omeGenericRecord);\r\n    }\r\n    verify(mockDataFileWriter, times(numberOfMessages)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockAzureBlobOutputStream, times(numberOfMessages)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWriteByteArray",
  "sourceCode" : "@Test\r\npublic void testWriteByteArray() throws Exception {\r\n    OutgoingMessageEnvelope omeEncoded = new OutgoingMessageEnvelope(new SystemStream(SYSTEM_NAME, \"Topic1\"), \"randomString\".getBytes());\r\n    int numberOfMessages = 10;\r\n    azureBlobAvroWriter.write(ome);\r\n    for (int i = 0; i < numberOfMessages; ++i) {\r\n        azureBlobAvroWriter.write(omeEncoded);\r\n    }\r\n    verify(mockDataFileWriter).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockDataFileWriter, times(numberOfMessages)).appendEncoded(ByteBuffer.wrap((byte[]) omeEncoded.getMessage()));\r\n    // +1 to account for first ome which is not encoded\r\n    verify(mockAzureBlobOutputStream, times(numberOfMessages + 1)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWriteByteArrayWithoutSchema",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testWriteByteArrayWithoutSchema() throws Exception {\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(PowerMockito.mock(BlobContainerAsyncClient.class), mock(AzureBlobWriterMetrics.class), threadPool, THRESHOLD, 60000, \"test\", null, null, null, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, 1000, 100, mockCompression, false, INIT_SIZE));\r\n    OutgoingMessageEnvelope omeEncoded = new OutgoingMessageEnvelope(new SystemStream(SYSTEM_NAME, \"Topic1\"), new byte[100]);\r\n    azureBlobAvroWriter.write(omeEncoded);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testWriteWhenDataFileWriterFails",
  "sourceCode" : "@Test(expected = IOException.class)\r\npublic void testWriteWhenDataFileWriterFails() throws Exception {\r\n    doThrow(new IOException(\"Failed\")).when(mockDataFileWriter).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    azureBlobAvroWriter.write(ome);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testClose",
  "sourceCode" : "@Test\r\npublic void testClose() throws Exception {\r\n    azureBlobAvroWriter.close();\r\n    verify(mockDataFileWriter).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testCloseWhenDataFileWriterFails",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testCloseWhenDataFileWriterFails() throws Exception {\r\n    doThrow(new IOException(\"Failed\")).when(mockDataFileWriter).close();\r\n    azureBlobAvroWriter.flush();\r\n    azureBlobAvroWriter.close();\r\n    verify(mockAzureBlobOutputStream, never()).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testCloseWhenOutputStreamFails",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testCloseWhenOutputStreamFails() throws Exception {\r\n    doThrow(new IOException(\"DataFileWriter failed\")).when(mockDataFileWriter).close();\r\n    doThrow(new RuntimeException(\"failed\")).when(mockAzureBlobOutputStream).close();\r\n    azureBlobAvroWriter.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() throws Exception {\r\n    azureBlobAvroWriter.flush();\r\n    verify(mockDataFileWriter).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testFlushWhenDataFileWriterFails",
  "sourceCode" : "@Test(expected = IOException.class)\r\npublic void testFlushWhenDataFileWriterFails() throws Exception {\r\n    doThrow(new IOException(\"Failed\")).when(mockDataFileWriter).flush();\r\n    azureBlobAvroWriter.flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testNPEinFlush",
  "sourceCode" : "@Test\r\npublic void testNPEinFlush() throws Exception {\r\n    // do not provide the dataFileWrite, azureBloboutputstream and blockblob client -- to force creation during first write\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(PowerMockito.mock(BlobContainerAsyncClient.class), mock(AzureBlobWriterMetrics.class), threadPool, THRESHOLD, 60000, \"test\", null, null, null, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, Long.MAX_VALUE, Long.MAX_VALUE, mockCompression, false, // keeping blob size and number of records unlimited\r\n    INIT_SIZE));\r\n    when(azureBlobAvroWriter.encodeRecord((IndexedRecord) ome.getMessage())).thenThrow(IllegalStateException.class);\r\n    // No NPE because has null check for currentBlobWriterComponents\r\n    azureBlobAvroWriter.flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMaxBlobSizeExceeded",
  "sourceCode" : "@Test\r\npublic void testMaxBlobSizeExceeded() throws Exception {\r\n    String blobUrlPrefix = \"test\";\r\n    String blobNameRegex = \"test/[0-9]{4}/[0-9]{2}/[0-9]{2}/[0-9]{2}/[0-9]{2}-[0-9]{2}-.{8}.avro.gz\";\r\n    long maxBlobSize = 1000;\r\n    AzureBlobWriterMetrics mockMetrics = mock(AzureBlobWriterMetrics.class);\r\n    BlobContainerAsyncClient mockContainerClient = PowerMockito.mock(BlobContainerAsyncClient.class);\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(mockContainerClient, mockMetrics, threadPool, THRESHOLD, 60000, blobUrlPrefix, null, null, null, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, maxBlobSize, 10, mockCompression, true, INIT_SIZE));\r\n    DataFileWriter<Object> mockDataFileWriter1 = (DataFileWriter<Object>) mock(DataFileWriter.class);\r\n    PowerMockito.whenNew(DataFileWriter.class).withAnyArguments().thenReturn(mockDataFileWriter1);\r\n    BlobAsyncClient mockBlobAsyncClient1 = mock(BlobAsyncClient.class);\r\n    doReturn(mockBlobAsyncClient1).when(mockContainerClient).getBlobAsyncClient(Matchers.matches(blobNameRegex));\r\n    BlockBlobAsyncClient mockBlockBlobAsyncClient1 = mock(BlockBlobAsyncClient.class);\r\n    doReturn(mockBlockBlobAsyncClient1).when(mockBlobAsyncClient1).getBlockBlobAsyncClient();\r\n    AzureBlobOutputStream mockAzureBlobOutputStream1 = mock(AzureBlobOutputStream.class);\r\n    PowerMockito.whenNew(AzureBlobOutputStream.class).withArguments(mockBlockBlobAsyncClient1, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, (long) 60000, THRESHOLD, mockCompression, INIT_SIZE).thenReturn(mockAzureBlobOutputStream1);\r\n    when(mockAzureBlobOutputStream1.getSize()).thenReturn((long) maxBlobSize - 1);\r\n    // first OME creates the first blob\r\n    azureBlobAvroWriter.write(ome);\r\n    OutgoingMessageEnvelope ome2 = createOME(\"Topic2\");\r\n    DataFileWriter<Object> mockDataFileWriter2 = (DataFileWriter<Object>) mock(DataFileWriter.class);\r\n    PowerMockito.whenNew(DataFileWriter.class).withAnyArguments().thenReturn(mockDataFileWriter2);\r\n    BlobAsyncClient mockBlobAsyncClient2 = mock(BlobAsyncClient.class);\r\n    doReturn(mockBlobAsyncClient2).when(mockContainerClient).getBlobAsyncClient(Matchers.matches(blobNameRegex));\r\n    BlockBlobAsyncClient mockBlockBlobAsyncClient2 = mock(BlockBlobAsyncClient.class);\r\n    doReturn(mockBlockBlobAsyncClient2).when(mockBlobAsyncClient2).getBlockBlobAsyncClient();\r\n    AzureBlobOutputStream mockAzureBlobOutputStream2 = mock(AzureBlobOutputStream.class);\r\n    PowerMockito.whenNew(AzureBlobOutputStream.class).withArguments(mockBlockBlobAsyncClient2, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, (long) 60000, THRESHOLD, mockCompression, INIT_SIZE).thenReturn(mockAzureBlobOutputStream2);\r\n    when(mockAzureBlobOutputStream2.getSize()).thenReturn((long) maxBlobSize - 1);\r\n    // Second OME creates the second blob because maxBlobSize is 1000 and mockAzureBlobOutputStream.getSize is 999.\r\n    azureBlobAvroWriter.write(ome2);\r\n    ArgumentCaptor<String> argument = ArgumentCaptor.forClass(String.class);\r\n    verify(mockContainerClient, times(2)).getBlobAsyncClient(argument.capture());\r\n    argument.getAllValues().forEach(blobName -> {\r\n        Assert.assertTrue(blobName.contains(blobUrlPrefix));\r\n    });\r\n    List<String> allBlobNames = argument.getAllValues();\r\n    Assert.assertNotEquals(allBlobNames.get(0), allBlobNames.get(1));\r\n    verify(mockDataFileWriter1).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome.getMessage())));\r\n    verify(mockDataFileWriter2).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome2.getMessage())));\r\n    verify(mockDataFileWriter1).create(((IndexedRecord) ome.getMessage()).getSchema(), mockAzureBlobOutputStream1);\r\n    verify(mockDataFileWriter2).create(((IndexedRecord) ome2.getMessage()).getSchema(), mockAzureBlobOutputStream2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testRecordLimitExceeded",
  "sourceCode" : "@Test\r\npublic void testRecordLimitExceeded() throws Exception {\r\n    String blobUrlPrefix = \"test\";\r\n    String blobNameRegex = \"test/[0-9]{4}/[0-9]{2}/[0-9]{2}/[0-9]{2}/[0-9]{2}-[0-9]{2}-.{8}.avro.gz\";\r\n    AzureBlobWriterMetrics mockMetrics = mock(AzureBlobWriterMetrics.class);\r\n    long maxBlobSize = AzureBlobAvroWriter.DATAFILEWRITER_OVERHEAD + 1000;\r\n    long maxRecordsPerBlob = 10;\r\n    BlobContainerAsyncClient mockContainerClient = PowerMockito.mock(BlobContainerAsyncClient.class);\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(mockContainerClient, mockMetrics, threadPool, THRESHOLD, 60000, blobUrlPrefix, null, null, null, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, maxBlobSize, maxRecordsPerBlob, mockCompression, true, INIT_SIZE));\r\n    DataFileWriter<Object> mockDataFileWriter1 = (DataFileWriter<Object>) mock(DataFileWriter.class);\r\n    PowerMockito.whenNew(DataFileWriter.class).withAnyArguments().thenReturn(mockDataFileWriter1);\r\n    BlobAsyncClient mockBlobAsyncClient1 = mock(BlobAsyncClient.class);\r\n    doReturn(mockBlobAsyncClient1).when(mockContainerClient).getBlobAsyncClient(Matchers.matches(blobNameRegex));\r\n    BlockBlobAsyncClient mockBlockBlobAsyncClient1 = mock(BlockBlobAsyncClient.class);\r\n    doReturn(mockBlockBlobAsyncClient1).when(mockBlobAsyncClient1).getBlockBlobAsyncClient();\r\n    AzureBlobOutputStream mockAzureBlobOutputStream1 = mock(AzureBlobOutputStream.class);\r\n    PowerMockito.whenNew(AzureBlobOutputStream.class).withArguments(mockBlockBlobAsyncClient1, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, (long) 60000, THRESHOLD, mockCompression, INIT_SIZE).thenReturn(mockAzureBlobOutputStream1);\r\n    when(mockAzureBlobOutputStream1.getSize()).thenReturn((long) 1);\r\n    // first OME creates the first blob and 11th OME (ome2) creates the second blob.\r\n    for (int i = 0; i < maxRecordsPerBlob; i++) {\r\n        azureBlobAvroWriter.write(ome);\r\n    }\r\n    OutgoingMessageEnvelope ome2 = createOME(\"Topic2\");\r\n    DataFileWriter<Object> mockDataFileWriter2 = (DataFileWriter<Object>) mock(DataFileWriter.class);\r\n    PowerMockito.whenNew(DataFileWriter.class).withAnyArguments().thenReturn(mockDataFileWriter2);\r\n    BlobAsyncClient mockBlobAsyncClient2 = mock(BlobAsyncClient.class);\r\n    doReturn(mockBlobAsyncClient2).when(mockContainerClient).getBlobAsyncClient(Matchers.matches(blobNameRegex));\r\n    BlockBlobAsyncClient mockBlockBlobAsyncClient2 = mock(BlockBlobAsyncClient.class);\r\n    doReturn(mockBlockBlobAsyncClient2).when(mockBlobAsyncClient2).getBlockBlobAsyncClient();\r\n    AzureBlobOutputStream mockAzureBlobOutputStream2 = mock(AzureBlobOutputStream.class);\r\n    PowerMockito.whenNew(AzureBlobOutputStream.class).withArguments(mockBlockBlobAsyncClient2, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, (long) 60000, THRESHOLD, mockCompression, INIT_SIZE).thenReturn(mockAzureBlobOutputStream2);\r\n    when(mockAzureBlobOutputStream2.getSize()).thenReturn((long) 1);\r\n    azureBlobAvroWriter.write(ome2);\r\n    ArgumentCaptor<String> argument = ArgumentCaptor.forClass(String.class);\r\n    verify(mockContainerClient, times(2)).getBlobAsyncClient(argument.capture());\r\n    argument.getAllValues().forEach(blobName -> {\r\n        Assert.assertTrue(blobName.contains(blobUrlPrefix));\r\n    });\r\n    List<String> allBlobNames = argument.getAllValues();\r\n    Assert.assertNotEquals(allBlobNames.get(0), allBlobNames.get(1));\r\n    verify(mockDataFileWriter1, times((int) maxRecordsPerBlob)).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome.getMessage())));\r\n    verify(mockDataFileWriter2).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome2.getMessage())));\r\n    verify(mockDataFileWriter1).create(((IndexedRecord) ome.getMessage()).getSchema(), mockAzureBlobOutputStream1);\r\n    verify(mockDataFileWriter2).create(((IndexedRecord) ome2.getMessage()).getSchema(), mockAzureBlobOutputStream2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMultipleBlobClose",
  "sourceCode" : "@Test\r\npublic void testMultipleBlobClose() throws Exception {\r\n    String blobUrlPrefix = \"test\";\r\n    long maxBlobSize = AzureBlobAvroWriter.DATAFILEWRITER_OVERHEAD + 1000;\r\n    long maxRecordsPerBlob = 10;\r\n    BlobContainerAsyncClient mockContainerClient = PowerMockito.mock(BlobContainerAsyncClient.class);\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(mockContainerClient, mock(AzureBlobWriterMetrics.class), threadPool, THRESHOLD, 60000, blobUrlPrefix, mockDataFileWriter, mockAzureBlobOutputStream, mockBlockBlobAsyncClient, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, maxBlobSize, maxRecordsPerBlob, mockCompression, false, INIT_SIZE));\r\n    DataFileWriter<Object> mockDataFileWriter2 = mock(DataFileWriter.class);\r\n    AzureBlobOutputStream mockAzureBlobOutputStream2 = mock(AzureBlobOutputStream.class);\r\n    when(mockAzureBlobOutputStream.getSize()).thenReturn((long) 1);\r\n    BlobAsyncClient mockBlobAsyncClient = mock(BlobAsyncClient.class);\r\n    doReturn(mockBlobAsyncClient).when(mockContainerClient).getBlobAsyncClient(anyString());\r\n    doReturn(mockBlockBlobAsyncClient).when(mockBlobAsyncClient).getBlockBlobAsyncClient();\r\n    PowerMockito.whenNew(AzureBlobOutputStream.class).withAnyArguments().thenReturn(mockAzureBlobOutputStream2);\r\n    PowerMockito.whenNew(DataFileWriter.class).withAnyArguments().thenReturn(mockDataFileWriter2);\r\n    for (int i = 0; i <= maxRecordsPerBlob; i++) {\r\n        azureBlobAvroWriter.write(ome);\r\n    }\r\n    // first OME creates the first blob and 11th OME creates the second blob.\r\n    azureBlobAvroWriter.close();\r\n    verify(mockDataFileWriter).close();\r\n    verify(mockDataFileWriter2).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testEncodeRecord",
  "sourceCode" : "@Test\r\npublic void testEncodeRecord() throws Exception {\r\n    azureBlobAvroWriter = spy(new AzureBlobAvroWriter(PowerMockito.mock(BlobContainerAsyncClient.class), mock(AzureBlobWriterMetrics.class), threadPool, THRESHOLD, 60000, \"test\", mockDataFileWriter, mockAzureBlobOutputStream, mockBlockBlobAsyncClient, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, STREAM_NAME, Long.MAX_VALUE, Long.MAX_VALUE, mockCompression, false, INIT_SIZE));\r\n    IndexedRecord record = new GenericRecordEvent();\r\n    Assert.assertTrue(Arrays.equals(encodeRecord(record), azureBlobAvroWriter.encodeRecord(record)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMultipleThreadWrites",
  "sourceCode" : "@Test\r\npublic void testMultipleThreadWrites() throws Exception {\r\n    Thread t1 = writeInThread(ome, azureBlobAvroWriter, 10);\r\n    OutgoingMessageEnvelope ome2 = createOMEGenericRecord(\"TOPIC2\");\r\n    Thread t2 = writeInThread(ome2, azureBlobAvroWriter, 10);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome2.getMessage())));\r\n    verify(mockAzureBlobOutputStream, times(20)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMultipleThreadWriteFlush",
  "sourceCode" : "@Test\r\npublic void testMultipleThreadWriteFlush() throws Exception {\r\n    Thread t1 = writeInThread(ome, azureBlobAvroWriter, 10);\r\n    Thread t2 = flushInThread(azureBlobAvroWriter);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockAzureBlobOutputStream, times(10)).incrementNumberOfRecordsInBlob();\r\n    verify(mockDataFileWriter).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMultipleThreadWriteFlushInBoth",
  "sourceCode" : "@Test\r\npublic void testMultipleThreadWriteFlushInBoth() throws Exception {\r\n    Thread t1 = writeFlushInThread(ome, azureBlobAvroWriter, 10);\r\n    OutgoingMessageEnvelope ome2 = createOMEGenericRecord(\"TOPIC2\");\r\n    Thread t2 = writeFlushInThread(ome2, azureBlobAvroWriter, 10);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome2.getMessage())));\r\n    verify(mockDataFileWriter, times(2)).flush();\r\n    verify(mockAzureBlobOutputStream, times(20)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobAvroWriter.java",
  "methodName" : "testMultipleThreadWriteFlushFinallyClose",
  "sourceCode" : "@Test\r\npublic void testMultipleThreadWriteFlushFinallyClose() throws Exception {\r\n    Thread t1 = writeFlushInThread(ome, azureBlobAvroWriter, 10);\r\n    OutgoingMessageEnvelope ome2 = createOMEGenericRecord(\"TOPIC2\");\r\n    Thread t2 = writeFlushInThread(ome2, azureBlobAvroWriter, 10);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    azureBlobAvroWriter.close();\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodedRecord));\r\n    verify(mockDataFileWriter, times(10)).appendEncoded(ByteBuffer.wrap(encodeRecord((IndexedRecord) ome2.getMessage())));\r\n    verify(mockDataFileWriter, times(2)).flush();\r\n    verify(mockDataFileWriter).close();\r\n    verify(mockAzureBlobOutputStream, times(20)).incrementNumberOfRecordsInBlob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWrite",
  "sourceCode" : "@Test\r\npublic void testWrite() throws InterruptedException {\r\n    byte[] b = new byte[THRESHOLD - 10];\r\n    azureBlobOutputStream.write(b, 0, THRESHOLD - 10);\r\n    verify(azureBlobOutputStream, never()).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n    verify(mockMetrics).updateWriteByteMetrics(THRESHOLD - 10);\r\n    verify(mockMetrics, never()).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteLargerThanThreshold",
  "sourceCode" : "@Test\r\npublic void testWriteLargerThanThreshold() throws InterruptedException {\r\n    byte[] largeRecord = RANDOM_STRING.substring(0, 2 * THRESHOLD).getBytes();\r\n    byte[] largeRecordFirstHalf = RANDOM_STRING.substring(0, THRESHOLD).getBytes();\r\n    byte[] largeRecordSecondHalf = RANDOM_STRING.substring(THRESHOLD, 2 * THRESHOLD).getBytes();\r\n    byte[] compressB1 = RANDOM_STRING.substring(0, THRESHOLD / 2).getBytes();\r\n    byte[] compressB2 = RANDOM_STRING.substring(THRESHOLD / 2, THRESHOLD).getBytes();\r\n    doReturn(compressB1).when(mockCompression).compress(largeRecordFirstHalf);\r\n    doReturn(compressB2).when(mockCompression).compress(largeRecordSecondHalf);\r\n    azureBlobOutputStream.write(largeRecord, 0, 2 * THRESHOLD);\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n    // invoked 2 times for the data which is 2*threshold\r\n    verify(mockCompression).compress(largeRecordFirstHalf);\r\n    verify(mockCompression).compress(largeRecordSecondHalf);\r\n    ArgumentCaptor<ByteBuffer> argument0 = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    ArgumentCaptor<ByteBuffer> argument1 = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(0)), argument0.capture(), eq((int) compressB1.length));\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(1)), argument1.capture(), eq((int) compressB2.length));\r\n    Assert.assertEquals(ByteBuffer.wrap(compressB1), argument0.getAllValues().get(0));\r\n    Assert.assertEquals(ByteBuffer.wrap(compressB2), argument1.getAllValues().get(0));\r\n    verify(mockMetrics).updateWriteByteMetrics(2 * THRESHOLD);\r\n    verify(mockMetrics, times(2)).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteLargeRecordWithSmallRecordInBuffer",
  "sourceCode" : "@Test\r\npublic void testWriteLargeRecordWithSmallRecordInBuffer() throws InterruptedException {\r\n    byte[] halfBlock = new byte[THRESHOLD / 2];\r\n    byte[] fullBlock = new byte[THRESHOLD];\r\n    byte[] largeRecord = new byte[2 * THRESHOLD];\r\n    byte[] fullBlockCompressedByte = new byte[50];\r\n    byte[] halfBlockCompressedByte = new byte[25];\r\n    doReturn(fullBlockCompressedByte).when(mockCompression).compress(fullBlock);\r\n    doReturn(halfBlockCompressedByte).when(mockCompression).compress(halfBlock);\r\n    // FIRST write a small record = same as half block\r\n    azureBlobOutputStream.write(halfBlock, 0, THRESHOLD / 2);\r\n    verify(mockMetrics).updateWriteByteMetrics(THRESHOLD / 2);\r\n    // SECOND write the large record\r\n    azureBlobOutputStream.write(largeRecord, 0, 2 * THRESHOLD);\r\n    verify(mockMetrics).updateWriteByteMetrics(2 * THRESHOLD);\r\n    // to flush out buffered data\r\n    azureBlobOutputStream.flush();\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n    verify(mockCompression, times(2)).compress(fullBlock);\r\n    verify(mockCompression).compress(halfBlock);\r\n    ArgumentCaptor<ByteBuffer> argument = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    ArgumentCaptor<ByteBuffer> argument2 = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(0)), argument.capture(), eq((int) fullBlockCompressedByte.length));\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(1)), argument.capture(), eq((int) fullBlockCompressedByte.length));\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(2)), argument2.capture(), eq((int) halfBlockCompressedByte.length));\r\n    argument.getAllValues().forEach(byteBuffer -> {\r\n        Assert.assertEquals(ByteBuffer.wrap(fullBlockCompressedByte), byteBuffer);\r\n    });\r\n    Assert.assertEquals(ByteBuffer.wrap(halfBlockCompressedByte), argument2.getAllValues().get(0));\r\n    verify(mockMetrics, times(3)).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteThresholdCrossed",
  "sourceCode" : "@Test\r\npublic void testWriteThresholdCrossed() throws Exception {\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, THRESHOLD / 2, THRESHOLD / 2);\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n    verify(mockCompression).compress(BYTES);\r\n    ArgumentCaptor<ByteBuffer> argument = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    // since size of byte[] written is less than threshold\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(0)), argument.capture(), eq((int) COMPRESSED_BYTES.length));\r\n    Assert.assertEquals(ByteBuffer.wrap(COMPRESSED_BYTES), argument.getAllValues().get(0));\r\n    verify(mockMetrics, times(2)).updateWriteByteMetrics(THRESHOLD / 2);\r\n    verify(mockMetrics, times(1)).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteFailed",
  "sourceCode" : "@Test(expected = AzureException.class)\r\npublic void testWriteFailed() {\r\n    when(mockBlobAsyncClient.stageBlock(anyString(), any(), anyLong())).thenReturn(Mono.error(new Exception(\"Test Failed\")));\r\n    byte[] b = new byte[100];\r\n    // threshold crossed so stageBlock is scheduled.\r\n    azureBlobOutputStream.write(b, 0, THRESHOLD);\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteFailedInterruptedException",
  "sourceCode" : "@Test(expected = AzureException.class)\r\npublic void testWriteFailedInterruptedException() throws InterruptedException {\r\n    doThrow(new InterruptedException(\"Lets interrupt the thread\")).when(azureBlobOutputStream).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n    byte[] b = new byte[100];\r\n    doReturn(COMPRESSED_BYTES).when(mockCompression).compress(b);\r\n    try {\r\n        // threshold crossed so stageBlock is scheduled.\r\n        azureBlobOutputStream.write(b, 0, THRESHOLD);\r\n        // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n        azureBlobOutputStream.close();\r\n    } catch (AzureException exception) {\r\n        // get root cause of the exception - to confirm its an InterruptedException\r\n        Throwable dupException = exception;\r\n        while (dupException.getCause() != null && dupException.getCause() != dupException) {\r\n            dupException = dupException.getCause();\r\n        }\r\n        Assert.assertTrue(dupException.getClass().getName().equals(InterruptedException.class.getCanonicalName()));\r\n        Assert.assertEquals(\"Lets interrupt the thread\", dupException.getMessage());\r\n        // verify stageBlock was called exactly once - aka no retries happen when interrupted exception is thrown\r\n        verify(azureBlobOutputStream).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n        // rethrow the exception so that the test will fail if no exception was thrown in the try block\r\n        throw exception;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testClose",
  "sourceCode" : "@Test\r\npublic void testClose() {\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD);\r\n    azureBlobOutputStream.incrementNumberOfRecordsInBlob();\r\n    int blockNum = 0;\r\n    String blockId = String.format(\"%05d\", blockNum);\r\n    String blockIdEncoded = Base64.getEncoder().encodeToString(blockId.getBytes());\r\n    azureBlobOutputStream.close();\r\n    verify(mockMetrics).updateAzureCommitMetrics();\r\n    ArgumentCaptor<ArrayList> blockListArgument = ArgumentCaptor.forClass(ArrayList.class);\r\n    ArgumentCaptor<Map> blobMetadataArg = ArgumentCaptor.forClass(Map.class);\r\n    verify(azureBlobOutputStream).commitBlob(blockListArgument.capture(), blobMetadataArg.capture());\r\n    Assert.assertEquals(Arrays.asList(blockIdEncoded), blockListArgument.getAllValues().get(0));\r\n    Map<String, String> blobMetadata = (Map<String, String>) blobMetadataArg.getAllValues().get(0);\r\n    Assert.assertEquals(blobMetadata.get(BLOB_RAW_SIZE_BYTES_METADATA), Long.toString(THRESHOLD));\r\n    Assert.assertEquals(blobMetadata.get(BLOB_STREAM_NAME_METADATA), FAKE_STREAM);\r\n    Assert.assertEquals(blobMetadata.get(BLOB_RECORD_NUMBER_METADATA), Long.toString(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testCloseMultipleBlocks",
  "sourceCode" : "@Test\r\npublic void testCloseMultipleBlocks() {\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD);\r\n    azureBlobOutputStream.incrementNumberOfRecordsInBlob();\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD);\r\n    azureBlobOutputStream.incrementNumberOfRecordsInBlob();\r\n    int blockNum = 0;\r\n    String blockId = String.format(\"%05d\", blockNum);\r\n    String blockIdEncoded = Base64.getEncoder().encodeToString(blockId.getBytes());\r\n    int blockNum1 = 1;\r\n    String blockId1 = String.format(\"%05d\", blockNum1);\r\n    String blockIdEncoded1 = Base64.getEncoder().encodeToString(blockId1.getBytes());\r\n    azureBlobOutputStream.close();\r\n    verify(mockMetrics).updateAzureCommitMetrics();\r\n    ArgumentCaptor<ArrayList> blockListArgument = ArgumentCaptor.forClass(ArrayList.class);\r\n    ArgumentCaptor<Map> blobMetadataArg = ArgumentCaptor.forClass(Map.class);\r\n    verify(azureBlobOutputStream).commitBlob(blockListArgument.capture(), blobMetadataArg.capture());\r\n    Assert.assertEquals(blockIdEncoded, blockListArgument.getAllValues().get(0).toArray()[0]);\r\n    Assert.assertEquals(blockIdEncoded1, blockListArgument.getAllValues().get(0).toArray()[1]);\r\n    Map<String, String> blobMetadata = (Map<String, String>) blobMetadataArg.getAllValues().get(0);\r\n    Assert.assertEquals(blobMetadata.get(BLOB_RAW_SIZE_BYTES_METADATA), Long.toString(2 * THRESHOLD));\r\n    Assert.assertEquals(blobMetadata.get(BLOB_STREAM_NAME_METADATA), FAKE_STREAM);\r\n    Assert.assertEquals(blobMetadata.get(BLOB_RECORD_NUMBER_METADATA), Long.toString(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testCloseFailed",
  "sourceCode" : "@Test(expected = AzureException.class)\r\npublic void testCloseFailed() throws InterruptedException {\r\n    azureBlobOutputStream = spy(new AzureBlobOutputStream(mockBlobAsyncClient, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, FAKE_STREAM, 60000, THRESHOLD, mockByteArrayOutputStream, mockCompression));\r\n    //doNothing().when(azureBlobOutputStream).commitBlob(any(ArrayList.class), anyMap());\r\n    doNothing().when(azureBlobOutputStream).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n    doThrow(new IllegalArgumentException(\"Test Failed\")).when(azureBlobOutputStream).commitBlob(any(ArrayList.class), anyMap());\r\n    doNothing().when(azureBlobOutputStream).clearAndMarkClosed();\r\n    byte[] b = new byte[100];\r\n    azureBlobOutputStream.write(b, 0, THRESHOLD);\r\n    azureBlobOutputStream.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testMultipleClose",
  "sourceCode" : "@Test\r\npublic void testMultipleClose() {\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD);\r\n    azureBlobOutputStream.close();\r\n    azureBlobOutputStream.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() throws Exception {\r\n    azureBlobOutputStream.write(BYTES);\r\n    azureBlobOutputStream.flush();\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n    // as there is only one block and its id will be 0\r\n    int blockNum = 0;\r\n    String blockId = String.format(\"%05d\", blockNum);\r\n    String blockIdEncoded = Base64.getEncoder().encodeToString(blockId.getBytes());\r\n    verify(mockCompression).compress(BYTES);\r\n    ArgumentCaptor<ByteBuffer> argument = ArgumentCaptor.forClass(ByteBuffer.class);\r\n    // since size of byte[] written is less than threshold\r\n    verify(azureBlobOutputStream).stageBlock(eq(blockIdEncoded(0)), argument.capture(), eq((int) COMPRESSED_BYTES.length));\r\n    Assert.assertEquals(ByteBuffer.wrap(COMPRESSED_BYTES), argument.getAllValues().get(0));\r\n    verify(mockMetrics).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testFlushFailed",
  "sourceCode" : "@Test(expected = AzureException.class)\r\npublic void testFlushFailed() throws IOException, InterruptedException {\r\n    azureBlobOutputStream = spy(new AzureBlobOutputStream(mockBlobAsyncClient, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, FAKE_STREAM, 60000, THRESHOLD, mockByteArrayOutputStream, mockCompression));\r\n    doNothing().when(azureBlobOutputStream).commitBlob(any(ArrayList.class), anyMap());\r\n    //doNothing().when(azureBlobOutputStream).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n    doThrow(new IllegalArgumentException(\"Test Failed\")).when(azureBlobOutputStream).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n    doNothing().when(azureBlobOutputStream).clearAndMarkClosed();\r\n    azureBlobOutputStream.write(BYTES);\r\n    azureBlobOutputStream.flush();\r\n    // azureBlobOutputStream.close waits on the CompletableFuture which does the actual stageBlock in uploadBlockAsync\r\n    azureBlobOutputStream.close();\r\n    verify(mockMetrics).updateAzureUploadMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testReleaseBuffer",
  "sourceCode" : "@Test\r\npublic void testReleaseBuffer() throws Exception {\r\n    azureBlobOutputStream.releaseBuffer();\r\n    verify(mockByteArrayOutputStream).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testWriteAfterReleaseBuffer",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testWriteAfterReleaseBuffer() throws Exception {\r\n    azureBlobOutputStream.releaseBuffer();\r\n    azureBlobOutputStream.write(new byte[10], 0, 10);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testCloseAfterReleaseBuffer",
  "sourceCode" : "@Test\r\npublic void testCloseAfterReleaseBuffer() throws Exception {\r\n    azureBlobOutputStream.write(BYTES, 0, 100);\r\n    azureBlobOutputStream.releaseBuffer();\r\n    azureBlobOutputStream.close();\r\n    // mockByteArrayOutputStream.close called only once during releaseBuffer and not during azureBlobOutputStream.close\r\n    verify(mockByteArrayOutputStream).close();\r\n    // azureBlobOutputStream.close still commits the list of blocks.\r\n    verify(azureBlobOutputStream).commitBlob(any(ArrayList.class), anyMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testFlushAfterReleaseBuffer",
  "sourceCode" : "@Test\r\npublic void testFlushAfterReleaseBuffer() throws Exception {\r\n    azureBlobOutputStream.releaseBuffer();\r\n    // becomes no-op after release buffer\r\n    azureBlobOutputStream.flush();\r\n    verify(azureBlobOutputStream, never()).stageBlock(anyString(), any(ByteBuffer.class), anyInt());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testGetSize",
  "sourceCode" : "@Test\r\npublic void testGetSize() throws Exception {\r\n    Assert.assertEquals(0, azureBlobOutputStream.getSize());\r\n    azureBlobOutputStream.write(BYTES, 0, BYTES.length);\r\n    Assert.assertEquals(BYTES.length, azureBlobOutputStream.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testGetSizeAfterFlush",
  "sourceCode" : "@Test\r\npublic void testGetSizeAfterFlush() throws Exception {\r\n    azureBlobOutputStream.write(BYTES, 0, BYTES.length);\r\n    Assert.assertEquals(BYTES.length, azureBlobOutputStream.getSize());\r\n    azureBlobOutputStream.flush();\r\n    Assert.assertEquals(BYTES.length, azureBlobOutputStream.getSize());\r\n    azureBlobOutputStream.write(BYTES, 0, BYTES.length - 10);\r\n    Assert.assertEquals(BYTES.length + BYTES.length - 10, azureBlobOutputStream.getSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\avro\\TestAzureBlobOutputStream.java",
  "methodName" : "testRespectFlushTimeout",
  "sourceCode" : "/**\r\n * Test to ensure that flush timeout is respected even if the block upload to azure is stuck/ taking longer than flush timeout\r\n * a countdown latch is used to mimic the upload to azure stuck\r\n * if flush timeout is respected then an exception is thrown when the flushtimeout_ms duration expires\r\n * else if timeout is not respected (aka bug is not fixed) then no exception is thrown and test hangs\r\n * In this test, the flush timeout is chosen to be 10 milliseconds, at the end of which, an AzureException of upload failed is thrown.\r\n * @throws Exception\r\n * @throws InterruptedException\r\n */\r\n@Test(expected = AzureException.class)\r\npublic void testRespectFlushTimeout() throws Exception, InterruptedException {\r\n    // get the threadpool to be the exactly the same as that passed down to AzureBlobOutputStream from AzureBlobSystemProducer\r\n    threadPool = new ThreadPoolExecutor(1, 1, 60, TimeUnit.SECONDS, new LinkedBlockingDeque<Runnable>(1), new ThreadPoolExecutor.CallerRunsPolicy());\r\n    // set a very small flushtimeout of 10ms to avoid test taking too long to complete\r\n    azureBlobOutputStream = spy(new AzureBlobOutputStream(mockBlobAsyncClient, threadPool, mockMetrics, blobMetadataGeneratorFactory, blobMetadataGeneratorConfig, FAKE_STREAM, 10, THRESHOLD, mockByteArrayOutputStream, mockCompression));\r\n    doNothing().when(azureBlobOutputStream).clearAndMarkClosed();\r\n    doReturn(mockBlobMetadataGenerator).when(azureBlobOutputStream).getBlobMetadataGenerator();\r\n    when(mockCompression.compress(BYTES)).thenReturn(COMPRESSED_BYTES, COMPRESSED_BYTES, COMPRESSED_BYTES, COMPRESSED_BYTES);\r\n    // create a latch to mimic uploads getting stuck\r\n    // and hence unable to honor flush timeout without the fix in stageblock\r\n    // fix in stageBlock = subscribeOn(Schedulers.boundedElastic()).block(flushtimeout)\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        String blockid = invocation.getArgumentAt(0, String.class);\r\n        return Mono.just(1).map(integer -> {\r\n            try {\r\n                LOG.info(\"For block id = \" + blockid + \" start waiting on the countdown latch \");\r\n                // start indefinite stuck -> mimic upload stuck\r\n                latch.await();\r\n                // below log will never be reached\r\n                LOG.info(\"For block id = \" + blockid + \" done waiting on the countdown latch \");\r\n            } catch (Exception e) {\r\n                LOG.info(\"For block id = \" + blockid + \" an exception was caught \" + e);\r\n            }\r\n            return \"One\";\r\n        });\r\n    }).when(azureBlobOutputStream).invokeBlobClientStageBlock(anyString(), anyObject(), anyInt());\r\n    doAnswer(invocation -> {\r\n        LOG.info(\"commit block \");\r\n        return null;\r\n    }).when(azureBlobOutputStream).commitBlob(anyObject(), anyMap());\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, THRESHOLD / 2, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, THRESHOLD / 2, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, 0, THRESHOLD / 2);\r\n    azureBlobOutputStream.write(BYTES, THRESHOLD / 2, THRESHOLD / 2);\r\n    // close will wait for all pending uploads to finish\r\n    // since the uploads are \"stuck\" (waiting for latch countdown), flushtimeout will get triggered\r\n    // and throw an exception saying upload failed.\r\n    azureBlobOutputStream.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\compression\\TestGzipCompression.java",
  "methodName" : "testCompression",
  "sourceCode" : "@Test\r\npublic void testCompression() throws IOException {\r\n    byte[] input = \"This is fake input data\".getBytes();\r\n    byte[] result = compress(input);\r\n    Assert.assertArrayEquals(gzipCompression.compress(input), result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\compression\\TestGzipCompression.java",
  "methodName" : "testCompressionEmpty",
  "sourceCode" : "@Test\r\npublic void testCompressionEmpty() throws IOException {\r\n    byte[] input = \"\".getBytes();\r\n    byte[] result = compress(input);\r\n    Assert.assertArrayEquals(gzipCompression.compress(input), result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\compression\\TestGzipCompression.java",
  "methodName" : "testCompressionNull",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testCompressionNull() {\r\n    byte[] input = null;\r\n    gzipCompression.compress(input);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\compression\\TestGzipCompression.java",
  "methodName" : "testCompressionZero",
  "sourceCode" : "@Test\r\npublic void testCompressionZero() throws IOException {\r\n    byte[] input = new byte[100];\r\n    byte[] result = compress(input);\r\n    Assert.assertArrayEquals(gzipCompression.compress(input), result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testStart",
  "sourceCode" : "@Test\r\npublic void testStart() {\r\n    systemProducer.start();\r\n    verify(systemProducer).setupAzureContainer();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testStop",
  "sourceCode" : "@Test\r\npublic void testStop() throws Exception {\r\n    doNothing().when(mockAzureWriter).close();\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    systemProducer.flush(SOURCE);\r\n    systemProducer.stop();\r\n    // called during flush IN STOP\r\n    verify(mockAzureWriter).flush();\r\n    // called during flush in STOP\r\n    verify(mockAzureWriter).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testStopBeforeFlush",
  "sourceCode" : "@Test\r\npublic void testStopBeforeFlush() throws Exception {\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    systemProducer.stop();\r\n    // called during flush IN STOP\r\n    verify(mockAzureWriter).flush();\r\n    // called during flush in STOP\r\n    verify(mockAzureWriter).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testStopWhenThreadpoolShutdownFails",
  "sourceCode" : "@Test(expected = SystemProducerException.class)\r\npublic void testStopWhenThreadpoolShutdownFails() throws Exception {\r\n    doThrow(new SecurityException(\"failed\")).when(mockThreadPoolExecutor).shutdown();\r\n    systemProducer.start();\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testStopWhenWriterFails",
  "sourceCode" : "@Test(expected = SystemProducerException.class)\r\npublic void testStopWhenWriterFails() throws IOException {\r\n    doThrow(new SystemProducerException(\"Failed\")).when(mockAzureWriter).flush();\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testRegisterAfterStart",
  "sourceCode" : "@Test(expected = SystemProducerException.class)\r\npublic void testRegisterAfterStart() throws Exception {\r\n    systemProducer.start();\r\n    systemProducer.register(SOURCE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testRegisterMetrics",
  "sourceCode" : "@Test\r\npublic void testRegisterMetrics() throws Exception {\r\n    systemProducer.register(SOURCE);\r\n    // verify that new counter for system was created during constructor of producer\r\n    verify(mockMetricsRegistry).newCounter(String.format(AzureBlobSystemProducerMetrics.SYSTEM_METRIC_FORMAT, ACCOUNT_NAME, SYSTEM_NAME), AzureBlobBasicMetrics.EVENT_WRITE_RATE);\r\n    // verify that new counter for source was created during register\r\n    verify(mockMetricsRegistry).newCounter(SOURCE, AzureBlobBasicMetrics.EVENT_WRITE_RATE);\r\n    verify(mockMetricsRegistry).newCounter(SOURCE, AzureBlobBasicMetrics.EVENT_WRITE_BYTE_RATE);\r\n    verify(mockMetricsRegistry).newCounter(SOURCE, AzureBlobBasicMetrics.EVENT_PRODUCE_ERROR);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testRegisterWithSystemName",
  "sourceCode" : "@Test\r\npublic void testRegisterWithSystemName() throws Exception {\r\n    systemProducer.register(SYSTEM_NAME);\r\n    // verify that new counter for system was created during constructor of producer but not during register\r\n    verify(mockMetricsRegistry).newCounter(String.format(AzureBlobSystemProducerMetrics.SYSTEM_METRIC_FORMAT, ACCOUNT_NAME, SYSTEM_NAME), AzureBlobBasicMetrics.EVENT_WRITE_RATE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() throws IOException {\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    systemProducer.flush(SOURCE);\r\n    // called during flush\r\n    verify(mockAzureWriter).flush();\r\n    // called during flush\r\n    verify(mockAzureWriter).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testFlushBeforeStart",
  "sourceCode" : "@Test(expected = SystemProducerException.class)\r\npublic void testFlushBeforeStart() throws Exception {\r\n    systemProducer.flush(SOURCE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testFlushBeforeRegister",
  "sourceCode" : "@Test(expected = SystemProducerException.class)\r\npublic void testFlushBeforeRegister() throws Exception {\r\n    systemProducer.start();\r\n    systemProducer.flush(SOURCE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testFlushWhenWriterUploadFails",
  "sourceCode" : "@Test\r\npublic void testFlushWhenWriterUploadFails() throws Exception {\r\n    doThrow(new SystemProducerException(\"failed\")).when(mockAzureWriter).flush();\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    try {\r\n        systemProducer.flush(SOURCE);\r\n        Assert.fail(\"Expected exception not thrown.\");\r\n    } catch (SystemProducerException e) {\r\n    }\r\n    verify(mockErrorCounter).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testFlushWhenWriterCloseFails",
  "sourceCode" : "@Test\r\npublic void testFlushWhenWriterCloseFails() throws Exception {\r\n    doThrow(new SystemProducerException(\"failed\")).when(mockAzureWriter).close();\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    systemProducer.send(SOURCE, ome);\r\n    try {\r\n        systemProducer.flush(SOURCE);\r\n        Assert.fail(\"Expected exception not thrown.\");\r\n    } catch (SystemProducerException e) {\r\n    }\r\n    verify(mockErrorCounter).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testSend",
  "sourceCode" : "@Test\r\npublic void testSend() throws IOException {\r\n    int numberOfMessages = 10;\r\n    Counter mockWriteCounter = mock(Counter.class);\r\n    when(mockMetricsRegistry.newCounter(SOURCE, AzureBlobBasicMetrics.EVENT_WRITE_RATE)).thenReturn(mockWriteCounter);\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    for (int i = 0; i < numberOfMessages; i++) {\r\n        systemProducer.send(SOURCE, ome);\r\n    }\r\n    verify(mockAzureWriter, times(numberOfMessages)).write(ome);\r\n    // verify metrics\r\n    verify(mockWriteCounter, times(numberOfMessages)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testSendWhenWriterCreateFails",
  "sourceCode" : "@Test\r\npublic void testSendWhenWriterCreateFails() throws Exception {\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    PowerMockito.whenNew(AzureBlobAvroWriter.class).withAnyArguments().thenThrow(new SystemProducerException(\"Failed\"));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    try {\r\n        systemProducer.send(SOURCE, ome);\r\n        Assert.fail(\"Expected exception not thrown.\");\r\n    } catch (SystemProducerException e) {\r\n    }\r\n    verify(mockErrorCounter).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testSendWhenWriterFails",
  "sourceCode" : "@Test\r\npublic void testSendWhenWriterFails() throws Exception {\r\n    doThrow(new SystemProducerException(\"failed\")).when(mockAzureWriter).write(ome);\r\n    systemProducer.register(SOURCE);\r\n    systemProducer.start();\r\n    try {\r\n        systemProducer.send(SOURCE, ome);\r\n        Assert.fail(\"Expected exception not thrown.\");\r\n    } catch (SystemProducerException e) {\r\n    }\r\n    verify(mockErrorCounter).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testMutipleThread",
  "sourceCode" : "@Test\r\npublic void testMutipleThread() throws Exception {\r\n    String source1 = \"FAKE_SOURCE_1\";\r\n    String source2 = \"FAKE_SOURCE_2\";\r\n    String stream1 = \"FAKE_STREAM_1\";\r\n    String stream2 = \"FAKE_STREAM_2\";\r\n    int sendsInFirstThread = 10;\r\n    int sendsInSecondThread = 20;\r\n    OutgoingMessageEnvelope ome1 = createOME(stream1);\r\n    OutgoingMessageEnvelope ome2 = createAnotherOME(stream2);\r\n    AzureBlobWriter mockAzureWriter1 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter1).close();\r\n    AzureBlobWriter mockAzureWriter2 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter2).close();\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    doReturn(mockAzureWriter1).when(systemProducer).getOrCreateWriter(source1, ome1);\r\n    doReturn(mockAzureWriter2).when(systemProducer).getOrCreateWriter(source2, ome2);\r\n    systemProducer.register(source1);\r\n    systemProducer.register(source2);\r\n    systemProducer.start();\r\n    Thread t1 = sendFlushInThread(source1, ome1, systemProducer, sendsInFirstThread);\r\n    Thread t2 = sendFlushInThread(source2, ome2, systemProducer, sendsInSecondThread);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    systemProducer.stop();\r\n    verify(mockAzureWriter1, times(sendsInFirstThread)).write(ome1);\r\n    verify(mockAzureWriter2, times(sendsInSecondThread)).write(ome2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testMutipleThreadOneWriterFails",
  "sourceCode" : "@Test\r\npublic void testMutipleThreadOneWriterFails() throws Exception {\r\n    String source1 = \"FAKE_SOURCE_1\";\r\n    String source2 = \"FAKE_SOURCE_2\";\r\n    String stream1 = \"FAKE_STREAM_1\";\r\n    String stream2 = \"FAKE_STREAM_2\";\r\n    int sendsInFirstThread = 10;\r\n    int sendsInSecondThread = 20;\r\n    OutgoingMessageEnvelope ome1 = createOME(stream1);\r\n    OutgoingMessageEnvelope ome2 = createAnotherOME(stream2);\r\n    AzureBlobWriter mockAzureWriter1 = mock(AzureBlobWriter.class);\r\n    doThrow(new SystemProducerException(\"failed\")).when(mockAzureWriter1).write(ome1);\r\n    doNothing().when(mockAzureWriter1).close();\r\n    AzureBlobWriter mockAzureWriter2 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter2).close();\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    doReturn(mockAzureWriter1).when(systemProducer).getOrCreateWriter(source1, ome1);\r\n    doReturn(mockAzureWriter2).when(systemProducer).getOrCreateWriter(source2, ome2);\r\n    systemProducer.register(source1);\r\n    systemProducer.register(source2);\r\n    systemProducer.start();\r\n    Thread t1 = sendFlushInThread(source1, ome1, systemProducer, sendsInFirstThread);\r\n    Thread t2 = sendFlushInThread(source2, ome2, systemProducer, sendsInSecondThread);\r\n    Thread.UncaughtExceptionHandler handler = new Thread.UncaughtExceptionHandler() {\r\n\r\n        public void uncaughtException(Thread th, Throwable ex) {\r\n            if (ex instanceof SystemProducerException) {\r\n                exceptionOccured = true;\r\n            }\r\n        }\r\n    };\r\n    t1.setUncaughtExceptionHandler(handler);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    systemProducer.stop();\r\n    if (!exceptionOccured) {\r\n        Assert.fail(\"Expected SystemProducerException but did not occur.\");\r\n    }\r\n    verify(mockAzureWriter1).write(ome1);\r\n    verify(mockAzureWriter2, times(sendsInSecondThread)).write(ome2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testMutipleThreadSendFlushToSingleWriter",
  "sourceCode" : "@Test\r\npublic void testMutipleThreadSendFlushToSingleWriter() throws Exception {\r\n    String source1 = \"FAKE_SOURCE_1\";\r\n    String stream1 = \"FAKE_STREAM_1\";\r\n    int sendsInFirstThread = 10;\r\n    int sendsInSecondThread = 20;\r\n    OutgoingMessageEnvelope ome1 = createOME(stream1);\r\n    AzureBlobWriter mockAzureWriter1 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter1).close();\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    systemProducer.register(source1);\r\n    systemProducer.start();\r\n    setupWriterForProducer(systemProducer, mockAzureWriter1, stream1);\r\n    Thread t1 = sendFlushInThread(source1, ome1, systemProducer, sendsInFirstThread);\r\n    Thread t2 = sendFlushInThread(source1, ome1, systemProducer, sendsInSecondThread);\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    systemProducer.stop();\r\n    verify(mockAzureWriter1, times(sendsInFirstThread + sendsInSecondThread)).write(ome1);\r\n    verify(mockAzureWriter1, times(2)).flush();\r\n    verify(mockAzureWriter1, times(2)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testMutipleThreadSendToSingleWriter",
  "sourceCode" : "@Test\r\npublic void testMutipleThreadSendToSingleWriter() throws Exception {\r\n    String source1 = \"FAKE_SOURCE_1\";\r\n    String stream1 = \"FAKE_STREAM_1\";\r\n    int sendsInFirstThread = 10;\r\n    int sendsInSecondThread = 20;\r\n    OutgoingMessageEnvelope ome1 = createOME(stream1);\r\n    OutgoingMessageEnvelope ome2 = createAnotherOME(stream1);\r\n    AzureBlobWriter mockAzureWriter1 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter1).close();\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    setupWriterForProducer(systemProducer, mockAzureWriter1, stream1);\r\n    systemProducer.register(source1);\r\n    systemProducer.start();\r\n    Thread t1 = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            for (int i = 0; i < sendsInFirstThread; i++) {\r\n                systemProducer.send(source1, ome1);\r\n            }\r\n        }\r\n    };\r\n    Thread t2 = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            for (int i = 0; i < sendsInSecondThread; i++) {\r\n                systemProducer.send(source1, ome2);\r\n            }\r\n        }\r\n    };\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    systemProducer.stop();\r\n    verify(mockAzureWriter1, times(sendsInFirstThread)).write(ome1);\r\n    verify(mockAzureWriter1, times(sendsInSecondThread)).write(ome2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\producer\\TestAzureBlobSystemProducer.java",
  "methodName" : "testMutipleThreadFlushToSingleWriter",
  "sourceCode" : "@Test\r\npublic void testMutipleThreadFlushToSingleWriter() throws Exception {\r\n    String source1 = \"FAKE_SOURCE_1\";\r\n    AzureBlobWriter mockAzureWriter1 = mock(AzureBlobWriter.class);\r\n    doNothing().when(mockAzureWriter1).close();\r\n    AzureBlobConfig azureBlobConfig = new AzureBlobConfig(getBasicConfigs());\r\n    AzureBlobSystemProducer systemProducer = spy(new AzureBlobSystemProducer(SYSTEM_NAME, azureBlobConfig, mockMetricsRegistry));\r\n    // bypass Azure connection setup\r\n    doNothing().when(systemProducer).setupAzureContainer();\r\n    setupWriterForProducer(systemProducer, mockAzureWriter1, STREAM);\r\n    systemProducer.register(source1);\r\n    systemProducer.start();\r\n    //to create writer\r\n    systemProducer.send(source1, ome);\r\n    Thread t1 = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            systemProducer.flush(source1);\r\n        }\r\n    };\r\n    Thread t2 = new Thread() {\r\n\r\n        @Override\r\n        public void run() {\r\n            systemProducer.flush(source1);\r\n        }\r\n    };\r\n    t1.start();\r\n    t2.start();\r\n    t1.join(60000);\r\n    t2.join(60000);\r\n    systemProducer.stop();\r\n    // systemProducer.flush called twice but first flush clears the writer map of the source.\r\n    // hence, writer.flush and close called only once.\r\n    verify(mockAzureWriter1).flush();\r\n    verify(mockAzureWriter1).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\utils\\TestNullBlobMetadataGenerator.java",
  "methodName" : "testGetBlobMetadata",
  "sourceCode" : "@Test\r\npublic void testGetBlobMetadata() {\r\n    Assert.assertNull(nullBlobMetadataGenerator.getBlobMetadata(new BlobMetadataContext(\"fake_stream\", 100, 10)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\utils\\TestNullBlobMetadataGenerator.java",
  "methodName" : "testGetBlobMetadataEmptyInput",
  "sourceCode" : "@Test\r\npublic void testGetBlobMetadataEmptyInput() {\r\n    Assert.assertNull(nullBlobMetadataGenerator.getBlobMetadata(new BlobMetadataContext(\"\", 0, 0)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\azureblob\\utils\\TestNullBlobMetadataGenerator.java",
  "methodName" : "testGetBlobMetadataNullInput",
  "sourceCode" : "@Test\r\npublic void testGetBlobMetadataNullInput() {\r\n    Assert.assertNull(nullBlobMetadataGenerator.getBlobMetadata(new BlobMetadataContext(null, 0, 0)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testOffsetComparison",
  "sourceCode" : "@Test\r\npublic void testOffsetComparison() {\r\n    EventHubSystemFactory eventHubSystemFactory = new EventHubSystemFactory();\r\n    EventHubSystemAdmin eventHubSystemAdmin = (EventHubSystemAdmin) eventHubSystemFactory.getAdmin(SYSTEM_NAME, MockEventHubConfigFactory.getEventHubConfig(EventHubSystemProducer.PartitioningMethod.EVENT_HUB_HASHING));\r\n    Assert.assertEquals(-1, eventHubSystemAdmin.offsetComparator(\"100\", \"200\").intValue());\r\n    Assert.assertEquals(0, eventHubSystemAdmin.offsetComparator(\"150\", \"150\").intValue());\r\n    Assert.assertEquals(1, eventHubSystemAdmin.offsetComparator(\"200\", \"100\").intValue());\r\n    Assert.assertNull(eventHubSystemAdmin.offsetComparator(\"1\", \"a\"));\r\n    Assert.assertNull(eventHubSystemAdmin.offsetComparator(\"100\", EventHubSystemConsumer.END_OF_STREAM));\r\n    Assert.assertNull(eventHubSystemAdmin.offsetComparator(EventHubSystemConsumer.END_OF_STREAM, EventHubSystemConsumer.END_OF_STREAM));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testGetStreamMetadata",
  "sourceCode" : "@Ignore(\"Integration Test\")\r\n@Test\r\npublic void testGetStreamMetadata() {\r\n    EventHubSystemFactory eventHubSystemFactory = new EventHubSystemFactory();\r\n    SystemAdmin eventHubSystemAdmin = eventHubSystemFactory.getAdmin(SYSTEM_NAME, MockEventHubConfigFactory.getEventHubConfig(EventHubSystemProducer.PartitioningMethod.EVENT_HUB_HASHING));\r\n    Set<String> streams = new HashSet<>();\r\n    streams.add(STREAM_NAME1);\r\n    streams.add(STREAM_NAME2);\r\n    Map<String, SystemStreamMetadata> metadataMap = eventHubSystemAdmin.getSystemStreamMetadata(streams);\r\n    for (String stream : streams) {\r\n        Assert.assertTrue(metadataMap.containsKey(stream));\r\n        Assert.assertEquals(stream, metadataMap.get(stream).getStreamName());\r\n        Assert.assertNotNull(metadataMap.get(stream).getSystemStreamPartitionMetadata());\r\n        Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadataMap = metadataMap.get(stream).getSystemStreamPartitionMetadata();\r\n        Assert.assertTrue(partitionMetadataMap.size() >= MIN_EVENTHUB_ENTITY_PARTITION);\r\n        Assert.assertTrue(partitionMetadataMap.size() <= MAX_EVENTHUB_ENTITY_PARTITION);\r\n        partitionMetadataMap.forEach((partition, metadata) -> {\r\n            Assert.assertEquals(EventHubSystemConsumer.START_OF_STREAM, metadata.getOldestOffset());\r\n            Assert.assertNotSame(EventHubSystemConsumer.END_OF_STREAM, metadata.getNewestOffset());\r\n            String expectedUpcomingOffset = String.valueOf(Long.parseLong(metadata.getNewestOffset()) + 1);\r\n            Assert.assertEquals(expectedUpcomingOffset, metadata.getUpcomingOffset());\r\n        });\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testStartpointResolverShouldResolveTheStartpointOldestToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointResolverShouldResolveTheStartpointOldestToCorrectOffset() {\r\n    EventHubSystemAdmin mockEventHubSystemAdmin = Mockito.mock(EventHubSystemAdmin.class);\r\n    EventHubConfig eventHubConfig = Mockito.mock(EventHubConfig.class);\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    EventHubSamzaOffsetResolver resolver = new EventHubSamzaOffsetResolver(mockEventHubSystemAdmin, eventHubConfig);\r\n    Assert.assertEquals(EventHubSystemConsumer.START_OF_STREAM, resolver.visit(systemStreamPartition, new StartpointOldest()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testStartpointResolverShouldResolveTheStartpointUpcomingToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointResolverShouldResolveTheStartpointUpcomingToCorrectOffset() {\r\n    EventHubSystemAdmin mockEventHubSystemAdmin = Mockito.mock(EventHubSystemAdmin.class);\r\n    EventHubConfig eventHubConfig = Mockito.mock(EventHubConfig.class);\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    EventHubSamzaOffsetResolver resolver = new EventHubSamzaOffsetResolver(mockEventHubSystemAdmin, eventHubConfig);\r\n    Assert.assertEquals(EventHubSystemConsumer.END_OF_STREAM, resolver.visit(systemStreamPartition, new StartpointUpcoming()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testStartpointResolverShouldResolveTheStartpointSpecificToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointResolverShouldResolveTheStartpointSpecificToCorrectOffset() {\r\n    EventHubSystemAdmin mockEventHubSystemAdmin = Mockito.mock(EventHubSystemAdmin.class);\r\n    EventHubConfig eventHubConfig = Mockito.mock(EventHubConfig.class);\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    EventHubSamzaOffsetResolver resolver = new EventHubSamzaOffsetResolver(mockEventHubSystemAdmin, eventHubConfig);\r\n    Assert.assertEquals(\"100\", resolver.visit(systemStreamPartition, new StartpointSpecific(\"100\")));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\admin\\TestEventHubSystemAdmin.java",
  "methodName" : "testStartpointResolverShouldResolveTheStartpointTimestampToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointResolverShouldResolveTheStartpointTimestampToCorrectOffset() throws EventHubException {\r\n    // Initialize variables required for testing.\r\n    EventHubSystemAdmin mockEventHubSystemAdmin = Mockito.mock(EventHubSystemAdmin.class);\r\n    EventHubConfig eventHubConfig = Mockito.mock(EventHubConfig.class);\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    String mockedOffsetToReturn = \"100\";\r\n    // Setup the mock variables.\r\n    EventHubClientManager mockEventHubClientManager = Mockito.mock(EventHubClientManager.class);\r\n    EventHubClient mockEventHubClient = Mockito.mock(EventHubClient.class);\r\n    PartitionReceiver mockPartitionReceiver = Mockito.mock(PartitionReceiver.class);\r\n    EventData mockEventData = Mockito.mock(EventData.class);\r\n    EventData.SystemProperties mockSystemProperties = Mockito.mock(EventData.SystemProperties.class);\r\n    // Configure the mock variables to return the appropriate values.\r\n    Mockito.when(mockEventHubSystemAdmin.getOrCreateStreamEventHubClient(\"test-stream\")).thenReturn(mockEventHubClientManager);\r\n    Mockito.when(mockEventHubClientManager.getEventHubClient()).thenReturn(mockEventHubClient);\r\n    Mockito.when(mockEventHubClient.createReceiverSync(Mockito.anyString(), Mockito.anyString(), Mockito.any())).thenReturn(mockPartitionReceiver);\r\n    Mockito.when(mockPartitionReceiver.receiveSync(1)).thenReturn(Arrays.asList(mockEventData));\r\n    Mockito.when(mockEventData.getSystemProperties()).thenReturn(mockSystemProperties);\r\n    Mockito.when(mockSystemProperties.getOffset()).thenReturn(mockedOffsetToReturn);\r\n    // Test the Offset resolver.\r\n    EventHubSamzaOffsetResolver resolver = new EventHubSamzaOffsetResolver(mockEventHubSystemAdmin, eventHubConfig);\r\n    String resolvedOffset = resolver.visit(systemStreamPartition, new StartpointTimestamp(100L));\r\n    Assert.assertEquals(mockedOffsetToReturn, resolvedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\ITestEventHubSystemConsumer.java",
  "methodName" : "testSinglePartitionConsumptionHappyPath",
  "sourceCode" : "@Test\r\npublic void testSinglePartitionConsumptionHappyPath() throws Exception {\r\n    int partitionId = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(SYSTEM_NAME, STREAM_NAME1, new Partition(partitionId));\r\n    Config eventHubConfig = createEventHubConfig();\r\n    EventHubSystemFactory factory = new EventHubSystemFactory();\r\n    SystemConsumer consumer = factory.getConsumer(SYSTEM_NAME, eventHubConfig, testMetrics);\r\n    consumer.register(ssp, EventHubSystemConsumer.START_OF_STREAM);\r\n    consumer.start();\r\n    int numEvents = 0;\r\n    int numRetries = 20;\r\n    while (numRetries-- > 0) {\r\n        List<IncomingMessageEnvelope> result = consumer.poll(Collections.singleton(ssp), 2000).get(ssp);\r\n        numEvents = result == null ? 0 : result.size();\r\n        if (numEvents > 0) {\r\n            EventHubIncomingMessageEnvelope eventData = (EventHubIncomingMessageEnvelope) result.get(0);\r\n            System.out.println(\"System properties: \" + eventData.getEventData().getSystemProperties());\r\n            System.out.println(\"Key: \" + new String((byte[]) eventData.getKey()));\r\n            System.out.println(\"Message: \" + new String((byte[]) eventData.getMessage()));\r\n        }\r\n        System.out.println(\"Retries left: \" + numRetries);\r\n    }\r\n    Assert.assertTrue(numEvents > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testMultipleRegistersToSameSSP",
  "sourceCode" : "@Test\r\npublic void testMultipleRegistersToSameSSP() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testStream\";\r\n    // needs to be less than BLOCKING_QUEUE_SIZE\r\n    int numEvents = 10;\r\n    int partitionId = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<SystemStreamPartition, List<EventData>> eventData = new HashMap<>();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(systemName, streamName, new Partition(partitionId));\r\n    Map<String, Interceptor> interceptors = new HashMap<>();\r\n    interceptors.put(streamName, new PassThroughInterceptor());\r\n    // create EventData\r\n    List<EventData> singlePartitionEventData = MockEventData.generateEventData(numEvents);\r\n    eventData.put(ssp, singlePartitionEventData);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), MOCK_ENTITY_1);\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory eventHubClientWrapperFactory = new MockEventHubClientManagerFactory(eventData);\r\n    EventHubSystemConsumer consumer = new EventHubSystemConsumer(new EventHubConfig(config), systemName, eventHubClientWrapperFactory, interceptors, testMetrics);\r\n    consumer.register(ssp, \"1\");\r\n    consumer.register(ssp, EventHubSystemConsumer.END_OF_STREAM);\r\n    consumer.register(ssp, EventHubSystemConsumer.START_OF_STREAM);\r\n    consumer.start();\r\n    Assert.assertEquals(EventPosition.fromOffset(EventHubSystemConsumer.START_OF_STREAM, false).toString(), eventHubClientWrapperFactory.getPartitionOffset(String.valueOf(partitionId)).toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testSinglePartitionConsumptionHappyPath",
  "sourceCode" : "@Test\r\npublic void testSinglePartitionConsumptionHappyPath() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testStream\";\r\n    // needs to be less than BLOCKING_QUEUE_SIZE\r\n    int numEvents = 10;\r\n    int partitionId = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<SystemStreamPartition, List<EventData>> eventData = new HashMap<>();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(systemName, streamName, new Partition(partitionId));\r\n    Map<String, Interceptor> interceptors = new HashMap<>();\r\n    interceptors.put(streamName, new PassThroughInterceptor());\r\n    // create EventData\r\n    List<EventData> singlePartitionEventData = MockEventData.generateEventData(numEvents);\r\n    eventData.put(ssp, singlePartitionEventData);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), MOCK_ENTITY_1);\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory eventHubClientWrapperFactory = new MockEventHubClientManagerFactory(eventData);\r\n    EventHubSystemConsumer consumer = new EventHubSystemConsumer(new EventHubConfig(config), systemName, eventHubClientWrapperFactory, interceptors, testMetrics);\r\n    consumer.register(ssp, EventHubSystemConsumer.END_OF_STREAM);\r\n    consumer.start();\r\n    // Mock received data from EventHub\r\n    eventHubClientWrapperFactory.sendToHandlers(consumer.streamPartitionHandlers);\r\n    List<IncomingMessageEnvelope> result = consumer.poll(Collections.singleton(ssp), 1000).get(ssp);\r\n    verifyEvents(result, singlePartitionEventData);\r\n    Assert.assertEquals(testMetrics.getCounters(streamName).size(), 3);\r\n    Assert.assertEquals(testMetrics.getGauges(streamName).size(), 2);\r\n    Map<String, Counter> counters = testMetrics.getCounters(streamName).stream().collect(Collectors.toMap(Counter::getName, Function.identity()));\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.EVENT_READ_RATE).getCount(), numEvents);\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.READ_ERRORS).getCount(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testSinglePartitionConsumptionInterceptor",
  "sourceCode" : "@Test\r\npublic void testSinglePartitionConsumptionInterceptor() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testStream\";\r\n    // needs to be less than BLOCKING_QUEUE_SIZE\r\n    int numEvents = 10;\r\n    int partitionId = 0;\r\n    Interceptor interceptor = new SwapFirstLastByteInterceptor();\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<SystemStreamPartition, List<EventData>> eventData = new HashMap<>();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(systemName, streamName, new Partition(partitionId));\r\n    Map<String, Interceptor> interceptors = new HashMap<>();\r\n    interceptors.put(streamName, interceptor);\r\n    // create EventData\r\n    List<EventData> singlePartitionEventData = MockEventData.generateEventData(numEvents);\r\n    eventData.put(ssp, singlePartitionEventData);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), MOCK_ENTITY_1);\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory eventHubClientWrapperFactory = new MockEventHubClientManagerFactory(eventData);\r\n    EventHubSystemConsumer consumer = new EventHubSystemConsumer(new EventHubConfig(config), systemName, eventHubClientWrapperFactory, interceptors, testMetrics);\r\n    consumer.register(ssp, EventHubSystemConsumer.END_OF_STREAM);\r\n    consumer.start();\r\n    // Mock received data from EventHub\r\n    eventHubClientWrapperFactory.sendToHandlers(consumer.streamPartitionHandlers);\r\n    List<IncomingMessageEnvelope> result = consumer.poll(Collections.singleton(ssp), 1000).get(ssp);\r\n    verifyEvents(result, singlePartitionEventData, interceptor);\r\n    Assert.assertEquals(testMetrics.getCounters(streamName).size(), 3);\r\n    Assert.assertEquals(testMetrics.getGauges(streamName).size(), 2);\r\n    Map<String, Counter> counters = testMetrics.getCounters(streamName).stream().collect(Collectors.toMap(Counter::getName, Function.identity()));\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.EVENT_READ_RATE).getCount(), numEvents);\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.READ_ERRORS).getCount(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testMultiPartitionConsumptionPerPartitionConnection",
  "sourceCode" : "@Test\r\npublic void testMultiPartitionConsumptionPerPartitionConnection() throws Exception {\r\n    testMultiPartitionConsumptionHappyPath(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testMultiPartitionConsumptionShareConnection",
  "sourceCode" : "@Test\r\npublic void testMultiPartitionConsumptionShareConnection() throws Exception {\r\n    testMultiPartitionConsumptionHappyPath(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testMultiStreamsConsumptionHappyPath",
  "sourceCode" : "@Test\r\npublic void testMultiStreamsConsumptionHappyPath() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName1 = \"testStream1\";\r\n    String streamName2 = \"testStream2\";\r\n    // needs to be less than BLOCKING_QUEUE_SIZE\r\n    int numEvents = 10;\r\n    int partitionId = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<SystemStreamPartition, List<EventData>> eventData = new HashMap<>();\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(systemName, streamName1, new Partition(partitionId));\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(systemName, streamName2, new Partition(partitionId));\r\n    Map<String, Interceptor> interceptor = new HashMap<>();\r\n    interceptor.put(streamName1, new PassThroughInterceptor());\r\n    interceptor.put(streamName2, new PassThroughInterceptor());\r\n    List<EventData> singlePartitionEventData1 = MockEventData.generateEventData(numEvents);\r\n    List<EventData> singlePartitionEventData2 = MockEventData.generateEventData(numEvents);\r\n    eventData.put(ssp1, singlePartitionEventData1);\r\n    eventData.put(ssp2, singlePartitionEventData2);\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), String.format(\"%s,%s\", streamName1, streamName2));\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName1), MOCK_ENTITY_1);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName1), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName1), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName1), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName2), MOCK_ENTITY_2);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName2), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName2), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName2), EVENTHUB_KEY);\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory eventHubClientWrapperFactory = new MockEventHubClientManagerFactory(eventData);\r\n    EventHubSystemConsumer consumer = new EventHubSystemConsumer(new EventHubConfig(config), systemName, eventHubClientWrapperFactory, interceptor, testMetrics);\r\n    consumer.register(ssp1, EventHubSystemConsumer.START_OF_STREAM);\r\n    consumer.register(ssp2, EventHubSystemConsumer.START_OF_STREAM);\r\n    consumer.start();\r\n    // Mock received data from EventHub\r\n    eventHubClientWrapperFactory.sendToHandlers(consumer.streamPartitionHandlers);\r\n    Set<SystemStreamPartition> ssps = new HashSet<>();\r\n    ssps.add(ssp1);\r\n    ssps.add(ssp2);\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> results = consumer.poll(ssps, 1000);\r\n    verifyEvents(results.get(ssp1), singlePartitionEventData1);\r\n    verifyEvents(results.get(ssp2), singlePartitionEventData2);\r\n    Assert.assertEquals(testMetrics.getCounters(streamName1).size(), 3);\r\n    Assert.assertEquals(testMetrics.getGauges(streamName1).size(), 2);\r\n    Assert.assertEquals(testMetrics.getCounters(streamName2).size(), 3);\r\n    Assert.assertEquals(testMetrics.getGauges(streamName2).size(), 2);\r\n    Map<String, Counter> counters1 = testMetrics.getCounters(streamName1).stream().collect(Collectors.toMap(Counter::getName, Function.identity()));\r\n    Assert.assertEquals(counters1.get(EventHubSystemConsumer.EVENT_READ_RATE).getCount(), numEvents);\r\n    Assert.assertEquals(counters1.get(EventHubSystemConsumer.READ_ERRORS).getCount(), 0);\r\n    Map<String, Counter> counters2 = testMetrics.getCounters(streamName2).stream().collect(Collectors.toMap(Counter::getName, Function.identity()));\r\n    Assert.assertEquals(counters2.get(EventHubSystemConsumer.EVENT_READ_RATE).getCount(), numEvents);\r\n    Assert.assertEquals(counters2.get(EventHubSystemConsumer.READ_ERRORS).getCount(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\consumer\\TestEventHubSystemConsumer.java",
  "methodName" : "testNonTransientErrorRetry",
  "sourceCode" : "@Test\r\npublic void testNonTransientErrorRetry() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testNonTransientErrorRetry\";\r\n    // needs to be less than BLOCKING_QUEUE_SIZE\r\n    int numEvents = 10;\r\n    int partitionId = 0;\r\n    TestClock testClock = new TestClock();\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<SystemStreamPartition, List<EventData>> eventData = new HashMap<>();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(systemName, streamName, new Partition(partitionId));\r\n    Map<String, Interceptor> interceptors = new HashMap<>();\r\n    interceptors.put(streamName, new PassThroughInterceptor());\r\n    // create EventData\r\n    List<EventData> singlePartitionEventData = MockEventData.generateEventData(numEvents);\r\n    eventData.put(ssp, singlePartitionEventData);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), MOCK_ENTITY_1);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_MAX_RETRY_COUNT, systemName), \"1\");\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory eventHubClientWrapperFactory = new MockEventHubClientManagerFactory(eventData);\r\n    EventHubSystemConsumer consumer = new EventHubSystemConsumer(new EventHubConfig(config), systemName, eventHubClientWrapperFactory, interceptors, testMetrics, testClock);\r\n    consumer.register(ssp, EventHubSystemConsumer.END_OF_STREAM);\r\n    consumer.start();\r\n    // 1st error should retry instead of throw\r\n    testClock.advanceTime(System.currentTimeMillis());\r\n    eventHubClientWrapperFactory.triggerError(consumer.streamPartitionHandlers, new EventHubException(false, /* is transient */\r\n    \"test\"));\r\n    consumer.poll(Collections.singleton(ssp), 0).get(ssp);\r\n    // assert that the reconnect task was submitted and completed eventually\r\n    Assert.assertNotNull(\"reconnect task should have been submitted\", consumer.reconnectTaskStatus);\r\n    Future lastReconnectTask = consumer.reconnectTaskStatus;\r\n    // should return instantaneously\r\n    lastReconnectTask.get(10000, TimeUnit.MILLISECONDS);\r\n    Assert.assertEquals(consumer.recentRetryAttempts.size(), 1);\r\n    // after retry should receive events normally\r\n    testClock.advanceTime(1);\r\n    eventHubClientWrapperFactory.sendToHandlers(consumer.streamPartitionHandlers);\r\n    List<IncomingMessageEnvelope> result = consumer.poll(Collections.singleton(ssp), 0).get(ssp);\r\n    verifyEvents(result, singlePartitionEventData);\r\n    Assert.assertEquals(testMetrics.getCounters(streamName).size(), 3);\r\n    Assert.assertEquals(testMetrics.getGauges(streamName).size(), 2);\r\n    Map<String, Counter> counters = testMetrics.getCounters(streamName).stream().collect(Collectors.toMap(Counter::getName, Function.identity()));\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.EVENT_READ_RATE).getCount(), numEvents);\r\n    // 2nd error: advance into next window, the older retry should have been evicted so this error should cause retry\r\n    testClock.advanceTime(EventHubConfig.DEFAULT_CONFIG_RETRY_WINDOW_MS + 1);\r\n    Assert.assertEquals(consumer.recentRetryAttempts.size(), 0);\r\n    eventHubClientWrapperFactory.triggerError(consumer.streamPartitionHandlers, new EventHubException(false, /* is transient */\r\n    \"test\"));\r\n    consumer.poll(Collections.singleton(ssp), 0).get(ssp);\r\n    Assert.assertNotNull(\"reconnect task should have been submitted\", consumer.reconnectTaskStatus);\r\n    lastReconnectTask = consumer.reconnectTaskStatus;\r\n    // should return instantaneously\r\n    lastReconnectTask.get(10000, TimeUnit.MILLISECONDS);\r\n    Assert.assertEquals(consumer.recentRetryAttempts.size(), 1);\r\n    // 3rd error: 1 ms is within the min retry interval; so poll should do nothing\r\n    testClock.advanceTime(1);\r\n    eventHubClientWrapperFactory.triggerError(consumer.streamPartitionHandlers, new EventHubException(false, /* is transient */\r\n    \"test\"));\r\n    consumer.poll(Collections.singleton(ssp), 0).get(ssp);\r\n    Assert.assertEquals(\"there shouldn't be another retry task within min retry interval\", consumer.reconnectTaskStatus, lastReconnectTask);\r\n    // 4th error: now the poll should throw\r\n    testClock.advanceTime(EventHubConfig.DEFAULT_CONFIG_RETRY_INTERVAL_MS + 1);\r\n    eventHubClientWrapperFactory.triggerError(consumer.streamPartitionHandlers, new EventHubException(false, /* is transient */\r\n    \"test\"));\r\n    try {\r\n        consumer.poll(Collections.singleton(ssp), 0).get(ssp);\r\n        Assert.fail(\"poll should have thrown\");\r\n    } catch (Exception e) {\r\n        Assert.assertEquals(e.getCause().getMessage(), \"test\");\r\n    }\r\n    Assert.assertEquals(counters.get(EventHubSystemConsumer.READ_ERRORS).getCount(), 4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsInputDescriptor.java",
  "methodName" : "testEntityConnectionConfigs",
  "sourceCode" : "@Test\r\npublic void testEntityConnectionConfigs() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"input-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsInputDescriptor<KV<String, String>> inputDescriptor = systemDescriptor.getInputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde()).withSasKeyName(\"secretkey\").withSasKey(\"sasToken-123\").withConsumerGroup(\"$notdefault\");\r\n    Map<String, String> generatedConfigs = inputDescriptor.toConfig();\r\n    assertEquals(\"eventHub\", generatedConfigs.get(\"streams.input-stream.samza.system\"));\r\n    assertEquals(\"entity-namespace\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamId)));\r\n    assertEquals(\"entity3\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamId)));\r\n    assertEquals(\"secretkey\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamId)));\r\n    assertEquals(\"sasToken-123\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamId)));\r\n    assertEquals(\"$notdefault\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_CONSUMER_GROUP, streamId)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsInputDescriptor.java",
  "methodName" : "testWithoutEntityConnectionConfigs",
  "sourceCode" : "@Test\r\npublic void testWithoutEntityConnectionConfigs() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"input-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsInputDescriptor<KV<String, String>> inputDescriptor = systemDescriptor.getInputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde());\r\n    Map<String, String> generatedConfigs = inputDescriptor.toConfig();\r\n    assertEquals(\"eventHub\", generatedConfigs.get(\"streams.input-stream.samza.system\"));\r\n    assertEquals(\"entity-namespace\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamId)));\r\n    assertEquals(\"entity3\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_CONSUMER_GROUP, streamId)));\r\n    // verify that there are no other configs\r\n    assertEquals(3, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsInputDescriptor.java",
  "methodName" : "testMissingInputDescriptorFields",
  "sourceCode" : "@Test\r\npublic void testMissingInputDescriptorFields() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"input-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    try {\r\n        systemDescriptor.getInputDescriptor(streamId, null, null, new StringSerde());\r\n        fail(\"Should have thrown Config Exception\");\r\n    } catch (ConfigException exception) {\r\n        assertEquals(String.format(//\r\n        \"Missing namespace and entity path Event Hubs input descriptor in \" + \"system: {%s}, stream: {%s}\", systemName, streamId), exception.getMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsInputDescriptor.java",
  "methodName" : "testStreamDescriptorContainsKVserde",
  "sourceCode" : "@Test\r\npublic void testStreamDescriptorContainsKVserde() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"input-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsInputDescriptor<KV<String, String>> outputDescriptor = systemDescriptor.getInputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde());\r\n    assertTrue(outputDescriptor.getSerde() instanceof KVSerde);\r\n    assertTrue(((KVSerde) outputDescriptor.getSerde()).getKeySerde() instanceof NoOpSerde);\r\n    assertTrue(((KVSerde) outputDescriptor.getSerde()).getValueSerde() instanceof StringSerde);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsOutputDescriptor.java",
  "methodName" : "testEntityConnectionConfigs",
  "sourceCode" : "@Test\r\npublic void testEntityConnectionConfigs() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"output-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsOutputDescriptor<KV<String, String>> outputDescriptor = systemDescriptor.getOutputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde()).withSasKeyName(\"secretkey\").withSasKey(\"sasToken-123\");\r\n    Map<String, String> generatedConfigs = outputDescriptor.toConfig();\r\n    assertEquals(\"eventHub\", generatedConfigs.get(\"streams.output-stream.samza.system\"));\r\n    assertEquals(\"entity-namespace\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamId)));\r\n    assertEquals(\"entity3\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamId)));\r\n    assertEquals(\"secretkey\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamId)));\r\n    assertEquals(\"sasToken-123\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamId)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsOutputDescriptor.java",
  "methodName" : "testWithoutEntityConnectionConfigs",
  "sourceCode" : "@Test\r\npublic void testWithoutEntityConnectionConfigs() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"output-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsOutputDescriptor<KV<String, String>> outputDescriptor = systemDescriptor.getOutputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde());\r\n    Map<String, String> generatedConfigs = outputDescriptor.toConfig();\r\n    assertEquals(\"eventHub\", generatedConfigs.get(\"streams.output-stream.samza.system\"));\r\n    assertEquals(\"entity-namespace\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamId)));\r\n    assertEquals(\"entity3\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamId)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_CONSUMER_GROUP, streamId)));\r\n    // verify that there are no other configs\r\n    assertEquals(3, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsOutputDescriptor.java",
  "methodName" : "testMissingOutputDescriptorFields",
  "sourceCode" : "@Test\r\npublic void testMissingOutputDescriptorFields() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"input-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    try {\r\n        systemDescriptor.getOutputDescriptor(streamId, null, null, new StringSerde());\r\n        fail(\"Should have thrown Config Exception\");\r\n    } catch (ConfigException exception) {\r\n        assertEquals(String.format(//\r\n        \"Missing namespace and entity path Event Hubs output descriptor in \" + \"system: {%s}, stream: {%s}\", systemName, streamId), exception.getMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsOutputDescriptor.java",
  "methodName" : "testStreamDescriptorContainsKVserde",
  "sourceCode" : "@Test\r\npublic void testStreamDescriptorContainsKVserde() {\r\n    String systemName = \"eventHub\";\r\n    String streamId = \"output-stream\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    EventHubsOutputDescriptor<KV<String, String>> outputDescriptor = systemDescriptor.getOutputDescriptor(streamId, \"entity-namespace\", \"entity3\", new StringSerde());\r\n    assertTrue(outputDescriptor.getSerde() instanceof KVSerde);\r\n    assertTrue(((KVSerde) outputDescriptor.getSerde()).getKeySerde() instanceof NoOpSerde);\r\n    assertTrue(((KVSerde) outputDescriptor.getSerde()).getValueSerde() instanceof StringSerde);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsSystemDescriptor.java",
  "methodName" : "testWithDescriptorOverrides",
  "sourceCode" : "@Test\r\npublic void testWithDescriptorOverrides() {\r\n    String systemName = \"system-name\";\r\n    String streamId1 = \"input-stream1\";\r\n    String streamId2 = \"input-stream2\";\r\n    String streamId3 = \"output-stream1\";\r\n    String streamId4 = \"output-stream2\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName).withMaxEventCountPerPoll(1000).withNumClientThreads(5).withPartitioningMethod(PartitioningMethod.PARTITION_KEY_AS_PARTITION).withPrefetchCount(100).withReceiveQueueSize(500).withRuntimeInfoTimeout(60000).withSendKeys(false);\r\n    systemDescriptor.getInputDescriptor(streamId1, \"entity-namespace1\", \"entity1\", new StringSerde());\r\n    systemDescriptor.getInputDescriptor(streamId2, \"entity-namespace2\", \"entity2\", new StringSerde());\r\n    systemDescriptor.getOutputDescriptor(streamId3, \"entity-namespace3\", \"entity3\", new StringSerde());\r\n    systemDescriptor.getOutputDescriptor(streamId4, \"entity-namespace4\", \"entity4\", new StringSerde());\r\n    Map<String, String> generatedConfigs = systemDescriptor.toConfig();\r\n    assertEquals(\"org.apache.samza.system.eventhub.EventHubSystemFactory\", generatedConfigs.get(String.format(\"systems.%s.samza.factory\", systemName)));\r\n    assertEquals(\"1000\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_MAX_EVENT_COUNT_PER_POLL, systemName)));\r\n    assertEquals(\"5\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_SYSTEM_NUM_CLIENT_THREADS, systemName)));\r\n    assertEquals(\"PARTITION_KEY_AS_PARTITION\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName)));\r\n    assertEquals(\"100\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_PREFETCH_COUNT, systemName)));\r\n    assertEquals(\"500\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_CONSUMER_BUFFER_CAPACITY, systemName)));\r\n    assertEquals(\"60000\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_FETCH_RUNTIME_INFO_TIMEOUT_MILLIS, systemName)));\r\n    assertEquals(\"false\", generatedConfigs.get(String.format(EventHubConfig.CONFIG_SEND_KEY_IN_EVENT_PROPERTIES, systemName)));\r\n    assertEquals(streamId1 + \",\" + streamId2 + \",\" + streamId3 + \",\" + streamId4, generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsSystemDescriptor.java",
  "methodName" : "testWithoutDescriptorOverrides",
  "sourceCode" : "@Test\r\npublic void testWithoutDescriptorOverrides() {\r\n    String systemName = \"eventHub\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    Map<String, String> generatedConfigs = systemDescriptor.toConfig();\r\n    assertEquals(\"org.apache.samza.system.eventhub.EventHubSystemFactory\", generatedConfigs.get(String.format(\"systems.%s.samza.factory\", systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_MAX_EVENT_COUNT_PER_POLL, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_SYSTEM_NUM_CLIENT_THREADS, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_PREFETCH_COUNT, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_CONSUMER_BUFFER_CAPACITY, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_FETCH_RUNTIME_INFO_TIMEOUT_MILLIS, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_SEND_KEY_IN_EVENT_PROPERTIES, systemName)));\r\n    assertNull(generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName)));\r\n    assertEquals(1, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\descriptors\\TestEventHubsSystemDescriptor.java",
  "methodName" : "testWithInputOutputStreams",
  "sourceCode" : "@Test\r\npublic void testWithInputOutputStreams() {\r\n    String systemName = \"system-name\";\r\n    String streamId1 = \"input-stream1\";\r\n    String streamId2 = \"input-stream2\";\r\n    String streamId3 = \"output-stream1\";\r\n    String streamId4 = \"output-stream2\";\r\n    EventHubsSystemDescriptor systemDescriptor = new EventHubsSystemDescriptor(systemName);\r\n    systemDescriptor.getInputDescriptor(streamId1, \"entity-namespace1\", \"entity1\", new StringSerde());\r\n    systemDescriptor.getInputDescriptor(streamId2, \"entity-namespace2\", \"entity2\", new StringSerde());\r\n    systemDescriptor.getOutputDescriptor(streamId3, \"entity-namespace3\", \"entity3\", new StringSerde());\r\n    systemDescriptor.getOutputDescriptor(streamId4, \"entity-namespace4\", \"entity4\", new StringSerde());\r\n    Map<String, String> generatedConfigs = systemDescriptor.toConfig();\r\n    assertEquals(\"org.apache.samza.system.eventhub.EventHubSystemFactory\", generatedConfigs.get(String.format(\"systems.%s.samza.factory\", systemName)));\r\n    assertEquals(streamId1 + \",\" + streamId2 + \",\" + streamId3 + \",\" + streamId4, generatedConfigs.get(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName)));\r\n    assertEquals(2, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\ITestEventHubSystemProducer.java",
  "methodName" : "testSystemFactoryCreateAndStartProducer",
  "sourceCode" : "@Test\r\npublic void testSystemFactoryCreateAndStartProducer() {\r\n    Config eventHubConfig = createEventHubConfig();\r\n    EventHubSystemFactory systemFactory = new EventHubSystemFactory();\r\n    SystemProducer systemProducer = systemFactory.getProducer(SYSTEM_NAME, eventHubConfig, new NoOpMetricsRegistry());\r\n    Assert.assertNotNull(systemProducer);\r\n    systemProducer.register(STREAM_NAME1);\r\n    systemProducer.register(STREAM_NAME2);\r\n    systemProducer.start();\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\ITestEventHubSystemProducer.java",
  "methodName" : "testSend",
  "sourceCode" : "@Test\r\npublic void testSend() {\r\n    Config eventHubConfig = createEventHubConfig();\r\n    EventHubSystemFactory systemFactory = new EventHubSystemFactory();\r\n    SystemProducer systemProducer = systemFactory.getProducer(SYSTEM_NAME, eventHubConfig, new NoOpMetricsRegistry());\r\n    systemProducer.register(STREAM_NAME1);\r\n    try {\r\n        systemProducer.send(STREAM_NAME1, createMessageEnvelope(STREAM_NAME1));\r\n        Assert.fail(\"Sending event before starting producer should throw exception\");\r\n    } catch (SamzaException e) {\r\n    }\r\n    systemProducer.start();\r\n    systemProducer.send(STREAM_NAME1, createMessageEnvelope(STREAM_NAME1));\r\n    try {\r\n        systemProducer.send(\"unregistered_stream\", createMessageEnvelope(\"unregistered_stream\"));\r\n        Assert.fail(\"Sending event to destination that is not registered should throw exception\");\r\n    } catch (SamzaException e) {\r\n    }\r\n    try {\r\n        systemProducer.register(STREAM_NAME2);\r\n        Assert.fail(\"Trying to register after starting producer should throw exception\");\r\n    } catch (SamzaException e) {\r\n    }\r\n    systemProducer.flush(STREAM_NAME1);\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\ITestEventHubSystemProducer.java",
  "methodName" : "testReceive",
  "sourceCode" : "@Test\r\npublic void testReceive() throws EventHubException {\r\n    EventHubClientManagerFactory clientFactory = new EventHubClientManagerFactory();\r\n    EventHubClientManager wrapper = clientFactory.getEventHubClientManager(SYSTEM_NAME, STREAM_NAME1, new EventHubConfig(createEventHubConfig()));\r\n    wrapper.init();\r\n    EventHubClient client = wrapper.getEventHubClient();\r\n    PartitionReceiver receiver = client.createReceiverSync(EventHubClient.DEFAULT_CONSUMER_GROUP_NAME, \"0\", EventPosition.fromStartOfStream());\r\n    receiveMessages(receiver, 300);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\ITestEventHubSystemProducer.java",
  "methodName" : "testSendToSpecificPartition",
  "sourceCode" : "@Test\r\npublic void testSendToSpecificPartition() {\r\n    Config eventHubConfig = MockEventHubConfigFactory.getEventHubConfig(EventHubSystemProducer.PartitioningMethod.PARTITION_KEY_AS_PARTITION);\r\n    EventHubSystemFactory systemFactory = new EventHubSystemFactory();\r\n    SystemProducer systemProducer = systemFactory.getProducer(SYSTEM_NAME, eventHubConfig, new NoOpMetricsRegistry());\r\n    systemProducer.register(STREAM_NAME1);\r\n    systemProducer.start();\r\n    for (int i = 0; i < 100; i++) {\r\n        systemProducer.send(STREAM_NAME1, createMessageEnvelope(STREAM_NAME1, 0));\r\n    }\r\n    systemProducer.flush(STREAM_NAME1);\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\ITestEventHubSystemProducer.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    Config eventHubConfig = createEventHubConfig();\r\n    EventHubSystemFactory systemFactory = new EventHubSystemFactory();\r\n    EventHubSystemProducer systemProducer = (EventHubSystemProducer) systemFactory.getProducer(SYSTEM_NAME, eventHubConfig, new NoOpMetricsRegistry());\r\n    systemProducer.register(STREAM_NAME1);\r\n    systemProducer.register(STREAM_NAME2);\r\n    systemProducer.start();\r\n    int numEvents = 100;\r\n    for (int i = 0; i < numEvents; i++) {\r\n        systemProducer.send(STREAM_NAME1, createMessageEnvelope(STREAM_NAME1));\r\n        systemProducer.send(STREAM_NAME2, createMessageEnvelope(STREAM_NAME2));\r\n    }\r\n    systemProducer.flush(EVENTHUB_ENTITY1);\r\n    Assert.assertEquals(systemProducer.getPendingFutures().size(), 0);\r\n    systemProducer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSendingToSpecificPartitionsPerPartitionConnection",
  "sourceCode" : "@Test\r\npublic void testSendingToSpecificPartitionsPerPartitionConnection() throws Exception {\r\n    testSendingToSpecificPartitions(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSendingToSpecificPartitionsShareConnection",
  "sourceCode" : "@Test\r\npublic void testSendingToSpecificPartitionsShareConnection() throws Exception {\r\n    testSendingToSpecificPartitions(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSkipLargeMessageCheck",
  "sourceCode" : "@Test\r\npublic void testSkipLargeMessageCheck() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testLMStream\";\r\n    int numEvents = 10;\r\n    int partitionId0 = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<String, Interceptor> interceptor = new HashMap<>();\r\n    interceptor.put(streamName, new PassThroughInterceptor());\r\n    List<String> outgoingMessagesP0 = generateMessages(numEvents / 2);\r\n    outgoingMessagesP0.add(\"1234567890123456789012345678901234567890\");\r\n    outgoingMessagesP0.addAll(generateMessages(numEvents / 2));\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_SKIP_MESSAGES_LARGER_THAN, systemName), \"-1\");\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), EVENTHUB_ENTITY1);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName), PartitioningMethod.PARTITION_KEY_AS_PARTITION.toString());\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory factory = new MockEventHubClientManagerFactory();\r\n    EventHubSystemProducer producer = new EventHubSystemProducer(new EventHubConfig(config), systemName, factory, interceptor, testMetrics);\r\n    SystemStream systemStream = new SystemStream(systemName, streamName);\r\n    producer.register(SOURCE);\r\n    producer.start();\r\n    outgoingMessagesP0.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId0, null, message.getBytes())));\r\n    // Retrieve sent data\r\n    List<String> receivedData0 = factory.getSentData(systemName, streamName, partitionId0).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    Assert.assertEquals(outgoingMessagesP0.size(), receivedData0.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSendingLargeMessage",
  "sourceCode" : "@Test\r\npublic void testSendingLargeMessage() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testLMStream\";\r\n    int numEvents = 10;\r\n    int partitionId0 = 0;\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<String, Interceptor> interceptor = new HashMap<>();\r\n    interceptor.put(streamName, new PassThroughInterceptor());\r\n    List<String> outgoingMessagesP0 = generateMessages(numEvents / 2);\r\n    outgoingMessagesP0.add(\"1234567890123456789012345678901234567890\");\r\n    outgoingMessagesP0.addAll(generateMessages(numEvents / 2));\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_SKIP_MESSAGES_LARGER_THAN, systemName), \"30\");\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), EVENTHUB_ENTITY1);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName), PartitioningMethod.PARTITION_KEY_AS_PARTITION.toString());\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory factory = new MockEventHubClientManagerFactory();\r\n    EventHubSystemProducer producer = new EventHubSystemProducer(new EventHubConfig(config), systemName, factory, interceptor, testMetrics);\r\n    SystemStream systemStream = new SystemStream(systemName, streamName);\r\n    producer.register(SOURCE);\r\n    producer.start();\r\n    outgoingMessagesP0.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId0, null, message.getBytes())));\r\n    // Retrieve sent data\r\n    List<String> receivedData0 = factory.getSentData(systemName, streamName, partitionId0).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    Assert.assertEquals(outgoingMessagesP0.size(), receivedData0.size() + 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSendingToSpecificPartitionsWithInterceptor",
  "sourceCode" : "@Test\r\npublic void testSendingToSpecificPartitionsWithInterceptor() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testStream\";\r\n    int numEvents = 10;\r\n    int partitionId0 = 0;\r\n    int partitionId1 = 1;\r\n    Interceptor interceptor = new SwapFirstLastByteInterceptor();\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<String, Interceptor> interceptors = new HashMap<>();\r\n    interceptors.put(streamName, interceptor);\r\n    List<String> outgoingMessagesP0 = generateMessages(numEvents);\r\n    List<String> outgoingMessagesP1 = generateMessages(numEvents);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), EVENTHUB_ENTITY1);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName), PartitioningMethod.PARTITION_KEY_AS_PARTITION.toString());\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory factory = new MockEventHubClientManagerFactory();\r\n    EventHubSystemProducer producer = new EventHubSystemProducer(new EventHubConfig(config), systemName, factory, interceptors, testMetrics);\r\n    SystemStream systemStream = new SystemStream(systemName, streamName);\r\n    producer.register(SOURCE);\r\n    producer.start();\r\n    outgoingMessagesP0.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId0, null, message.getBytes())));\r\n    outgoingMessagesP1.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId1, null, message.getBytes())));\r\n    // Retrieve sent data\r\n    List<String> receivedData0 = factory.getSentData(systemName, streamName, partitionId0).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    List<String> receivedData1 = factory.getSentData(systemName, streamName, partitionId1).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    List<String> expectedP0 = outgoingMessagesP0.stream().map(message -> new String(interceptor.intercept(message.getBytes()))).collect(Collectors.toList());\r\n    List<String> expectedP1 = outgoingMessagesP1.stream().map(message -> new String(interceptor.intercept(message.getBytes()))).collect(Collectors.toList());\r\n    Assert.assertTrue(expectedP0.equals(receivedData0));\r\n    Assert.assertTrue(expectedP1.equals(receivedData1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-azure\\src\\test\\java\\org\\apache\\samza\\system\\eventhub\\producer\\TestEventHubSystemProducer.java",
  "methodName" : "testSendingToEventHubHashing",
  "sourceCode" : "@Test\r\npublic void testSendingToEventHubHashing() throws Exception {\r\n    String systemName = \"eventhubs\";\r\n    String streamName = \"testStream\";\r\n    int numEvents = 10;\r\n    String partitionId0 = \"124\";\r\n    String partitionId1 = \"235\";\r\n    TestMetricsRegistry testMetrics = new TestMetricsRegistry();\r\n    Map<String, Interceptor> interceptor = new HashMap<>();\r\n    interceptor.put(streamName, new PassThroughInterceptor());\r\n    List<String> outgoingMessagesP0 = generateMessages(numEvents);\r\n    List<String> outgoingMessagesP1 = generateMessages(numEvents);\r\n    // Set configs\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_LIST, systemName), streamName);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_NAMESPACE, streamName), EVENTHUB_NAMESPACE);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_KEY_NAME, streamName), EVENTHUB_KEY_NAME);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_SAS_TOKEN, streamName), EVENTHUB_KEY);\r\n    configMap.put(String.format(EventHubConfig.CONFIG_STREAM_ENTITYPATH, streamName), EVENTHUB_ENTITY1);\r\n    // mod 2 on the partitionid to simulate consistent hashing\r\n    configMap.put(String.format(EventHubConfig.CONFIG_PRODUCER_PARTITION_METHOD, systemName), PartitioningMethod.EVENT_HUB_HASHING.toString());\r\n    MapConfig config = new MapConfig(configMap);\r\n    MockEventHubClientManagerFactory factory = new MockEventHubClientManagerFactory();\r\n    EventHubSystemProducer producer = new EventHubSystemProducer(new EventHubConfig(config), systemName, factory, interceptor, testMetrics);\r\n    SystemStream systemStream = new SystemStream(systemName, streamName);\r\n    producer.register(SOURCE);\r\n    producer.start();\r\n    outgoingMessagesP0.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId0, null, message.getBytes())));\r\n    outgoingMessagesP1.forEach(message -> producer.send(SOURCE, new OutgoingMessageEnvelope(systemStream, partitionId1, null, message.getBytes())));\r\n    // Retrieve sent data\r\n    List<String> receivedData0 = factory.getSentData(systemName, streamName, 0).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    List<String> receivedData1 = factory.getSentData(systemName, streamName, 1).stream().map(eventData -> new String(eventData.getBytes())).collect(Collectors.toList());\r\n    Assert.assertTrue(outgoingMessagesP0.equals(receivedData0));\r\n    Assert.assertTrue(outgoingMessagesP1.equals(receivedData1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\application\\descriptors\\StreamApplicationDescriptorImpl.java",
  "methodName" : "getIntermediateStream",
  "sourceCode" : "/**\r\n * Internal helper for {@link MessageStreamImpl} to add an intermediate {@link MessageStream} to the graph.\r\n * An intermediate {@link MessageStream} is both an output and an input stream.\r\n *\r\n * @param streamId the id of the stream to be created.\r\n * @param serde the {@link Serde} to use for the message in the intermediate stream. If null, the default serde\r\n *              is used.\r\n * @param isBroadcast whether the stream is a broadcast stream.\r\n * @param <M> the type of messages in the intermediate {@link MessageStream}\r\n * @return  the intermediate {@link MessageStreamImpl}\r\n */\r\n@VisibleForTesting\r\npublic <M> IntermediateMessageStreamImpl<M> getIntermediateStream(String streamId, Serde<M> serde, boolean isBroadcast) {\r\n    Preconditions.checkNotNull(serde, \"serde must not be null for intermediate stream: \" + streamId);\r\n    Preconditions.checkState(!inputOperators.containsKey(streamId) && !outputStreams.containsKey(streamId), \"getIntermediateStream must not be called multiple times with the same streamId: \" + streamId);\r\n    if (isBroadcast) {\r\n        intermediateBroadcastStreamIds.add(streamId);\r\n    }\r\n    boolean isKeyed = serde instanceof KVSerde;\r\n    KV<Serde, Serde> kvSerdes = getOrCreateStreamSerdes(streamId, serde);\r\n    InputTransformer transformer = (InputTransformer) getDefaultSystemDescriptor().flatMap(SystemDescriptor::getTransformer).orElse(null);\r\n    InputOperatorSpec inputOperatorSpec = OperatorSpecs.createInputOperatorSpec(streamId, kvSerdes.getKey(), kvSerdes.getValue(), transformer, isKeyed, this.getNextOpId(OpCode.INPUT, null));\r\n    inputOperators.put(streamId, inputOperatorSpec);\r\n    outputStreams.put(streamId, new OutputStreamImpl(streamId, kvSerdes.getKey(), kvSerdes.getValue(), isKeyed));\r\n    return new IntermediateMessageStreamImpl<>(this, inputOperators.get(streamId), outputStreams.get(streamId));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "generateAndUpdateJobCoordinatorMetadata",
  "sourceCode" : "/**\r\n * Generate the job coordinator metadata for current application attempt and checks for changes in the\r\n * metadata from the previous attempt and writes the updates metadata to coordinator stream.\r\n *\r\n * @param jobModel job model used to generate the job coordinator metadata\r\n */\r\n@VisibleForTesting\r\nvoid generateAndUpdateJobCoordinatorMetadata(JobModel jobModel) {\r\n    JobCoordinatorMetadataManager jobCoordinatorMetadataManager = createJobCoordinatorMetadataManager();\r\n    JobCoordinatorMetadata previousMetadata = jobCoordinatorMetadataManager.readJobCoordinatorMetadata();\r\n    JobCoordinatorMetadata newMetadata = jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(jobModel, config);\r\n    if (!jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata).isEmpty()) {\r\n        jobCoordinatorMetadataManager.writeJobCoordinatorMetadata(newMetadata);\r\n        metadataChangedAcrossAttempts = true;\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "getAppStatus",
  "sourceCode" : "// The following two methods are package-private and for testing only\r\n@VisibleForTesting\r\nSamzaApplicationState.SamzaAppStatus getAppStatus() {\r\n    // make sure to only return a unmodifiable copy of the status variable\r\n    return state.status;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "getPartitionMonitor",
  "sourceCode" : "@VisibleForTesting\r\nStreamPartitionCountMonitor getPartitionMonitor() {\r\n    return partitionMonitor;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "createStartpointManager",
  "sourceCode" : "@VisibleForTesting\r\nStartpointManager createStartpointManager() {\r\n    return new StartpointManager(metadataStore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "createContainerProcessManager",
  "sourceCode" : "@VisibleForTesting\r\nContainerProcessManager createContainerProcessManager() {\r\n    return new ContainerProcessManager(config, state, metrics, containerPlacementMetadataStore, localityManager, metadataChangedAcrossAttempts);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "createJobCoordinatorMetadataManager",
  "sourceCode" : "@VisibleForTesting\r\nJobCoordinatorMetadataManager createJobCoordinatorMetadataManager() {\r\n    return new JobCoordinatorMetadataManager(new NamespaceAwareCoordinatorStreamStore(metadataStore, SetJobCoordinatorMetadataMessage.TYPE), JobCoordinatorMetadataManager.ClusterType.YARN, metrics);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "isApplicationMasterHighAvailabilityEnabled",
  "sourceCode" : "@VisibleForTesting\r\nboolean isApplicationMasterHighAvailabilityEnabled() {\r\n    return new JobConfig(config).getApplicationMasterHighAvailabilityEnabled();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "isMetadataChangedAcrossAttempts",
  "sourceCode" : "@VisibleForTesting\r\nboolean isMetadataChangedAcrossAttempts() {\r\n    return metadataChangedAcrossAttempts;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinator.java",
  "methodName" : "shouldFanoutStartpoint",
  "sourceCode" : "/**\r\n * We only fanout startpoint if and only if\r\n *  1. Startpoint is enabled\r\n *  2. If AM HA is enabled, fanout only if startpoint enabled and job coordinator metadata changed\r\n *\r\n * @return true if it satisfies above conditions, false otherwise\r\n */\r\n@VisibleForTesting\r\nboolean shouldFanoutStartpoint() {\r\n    JobConfig jobConfig = new JobConfig(config);\r\n    boolean startpointEnabled = jobConfig.getStartpointEnabled();\r\n    return isApplicationMasterHighAvailabilityEnabled() ? startpointEnabled && isMetadataChangedAcrossAttempts() : startpointEnabled;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinatorRunner.java",
  "methodName" : "runClusterBasedJobCoordinator",
  "sourceCode" : "/**\r\n * This is the actual execution for the {@link ClusterBasedJobCoordinator}.\r\n */\r\n@VisibleForTesting\r\nstatic void runClusterBasedJobCoordinator(String[] args) {\r\n    final String coordinatorSystemEnv = System.getenv(ShellCommandConfig.ENV_COORDINATOR_SYSTEM_CONFIG);\r\n    final String submissionEnv = System.getenv(ShellCommandConfig.ENV_SUBMISSION_CONFIG);\r\n    if (!StringUtils.isBlank(submissionEnv)) {\r\n        Config submissionConfig;\r\n        try {\r\n            //Read and parse the coordinator system config.\r\n            LOG.info(\"Parsing submission config {}\", submissionEnv);\r\n            submissionConfig = new MapConfig(SamzaObjectMapper.getObjectMapper().readValue(submissionEnv, Config.class));\r\n            LOG.info(\"Using the submission config: {}.\", submissionConfig);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Exception while reading submission config\", e);\r\n            throw new SamzaException(e);\r\n        }\r\n        ApplicationConfig appConfig = new ApplicationConfig(submissionConfig);\r\n        /*\r\n       * Invoke app.main.class with app.main.args when present.\r\n       * For Beam jobs, app.main.class will be Beam's main class\r\n       * and app.main.args will be Beam's pipeline options.\r\n       */\r\n        String className = appConfig.getAppMainClass();\r\n        String[] arguments = toArgs(appConfig);\r\n        LOG.info(\"Invoke main {} with args {}\", className, arguments);\r\n        try {\r\n            Class<?> cls = Class.forName(className);\r\n            Method mainMethod = cls.getMethod(\"main\", String[].class);\r\n            mainMethod.invoke(null, (Object) arguments);\r\n        } catch (Exception e) {\r\n            throw new SamzaException(e);\r\n        }\r\n    } else {\r\n        // TODO: Clean this up once SAMZA-2405 is completed when legacy flow is removed.\r\n        Config coordinatorSystemConfig;\r\n        try {\r\n            //Read and parse the coordinator system config.\r\n            LOG.info(\"Parsing coordinator system config {}\", coordinatorSystemEnv);\r\n            coordinatorSystemConfig = new MapConfig(SamzaObjectMapper.getObjectMapper().readValue(coordinatorSystemEnv, Config.class));\r\n            LOG.info(\"Using the coordinator system config: {}.\", coordinatorSystemConfig);\r\n        } catch (IOException e) {\r\n            LOG.error(\"Exception while reading coordinator stream config\", e);\r\n            throw new SamzaException(e);\r\n        }\r\n        ClusterBasedJobCoordinator jc = createFromMetadataStore(coordinatorSystemConfig);\r\n        jc.run();\r\n    }\r\n    LOG.info(\"Finished running ClusterBasedJobCoordinator\");\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinatorRunner.java",
  "methodName" : "createFromMetadataStore",
  "sourceCode" : "/**\r\n * Initialize {@link ClusterBasedJobCoordinator} with coordinator stream config, full job config will be fetched from\r\n * coordinator stream.\r\n *\r\n * @param metadataStoreConfig to initialize {@link org.apache.samza.metadatastore.MetadataStore}\r\n * @return {@link ClusterBasedJobCoordinator}\r\n */\r\n// TODO SAMZA-2432: Clean this up once SAMZA-2405 is completed when legacy flow is removed.\r\n@VisibleForTesting\r\nstatic ClusterBasedJobCoordinator createFromMetadataStore(Config metadataStoreConfig) {\r\n    MetricsRegistryMap metrics = new MetricsRegistryMap();\r\n    CoordinatorStreamStore coordinatorStreamStore = new CoordinatorStreamStore(metadataStoreConfig, metrics);\r\n    coordinatorStreamStore.init();\r\n    Config config = CoordinatorStreamUtil.readConfigFromCoordinatorStream(coordinatorStreamStore);\r\n    return new ClusterBasedJobCoordinator(metrics, coordinatorStreamStore, config);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ClusterBasedJobCoordinatorRunner.java",
  "methodName" : "toArgs",
  "sourceCode" : "/**\r\n * Convert Samza config to command line arguments to invoke app.main.class\r\n *\r\n * @param config Samza config to convert.\r\n * @return converted command line arguments.\r\n */\r\n@VisibleForTesting\r\nstatic String[] toArgs(ApplicationConfig config) {\r\n    List<String> args = new ArrayList<>(config.size() * 2);\r\n    config.forEach((key, value) -> {\r\n        if (key.equals(ApplicationConfig.APP_MAIN_ARGS)) {\r\n            /*\r\n         * Converts native beam pipeline options such as\r\n         * --runner=SamzaRunner --maxSourceParallelism=1024\r\n         */\r\n            args.addAll(Arrays.asList(value.split(\"\\\\s\")));\r\n        } else {\r\n            /*\r\n         * Converts native Samza configs to config override format such as\r\n         * --config job.name=test\r\n         */\r\n            args.add(\"--config\");\r\n            args.add(String.format(\"%s=%s\", key, value));\r\n        }\r\n    });\r\n    return args.toArray(new String[0]);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\ContainerPlacementMetadataStore.java",
  "methodName" : "readAllContainerPlacementRequestMessages",
  "sourceCode" : "@VisibleForTesting\r\nList<ContainerPlacementRequestMessage> readAllContainerPlacementRequestMessages() {\r\n    Preconditions.checkState(!stopped, \"Underlying metadata store not available\");\r\n    List<ContainerPlacementRequestMessage> newActions = new ArrayList<>();\r\n    Map<String, byte[]> messageBytes = containerPlacementMessageStore.all();\r\n    for (byte[] action : messageBytes.values()) {\r\n        try {\r\n            ContainerPlacementMessage message = objectMapper.readValue(action, ContainerPlacementMessage.class);\r\n            if (message instanceof ContainerPlacementRequestMessage) {\r\n                newActions.add((ContainerPlacementRequestMessage) message);\r\n            }\r\n        } catch (IOException e) {\r\n            throw new SamzaException(e);\r\n        }\r\n    }\r\n    // Sort the actions in order of timestamp\r\n    newActions.sort(Comparator.comparingLong(ContainerPlacementRequestMessage::getTimestamp));\r\n    return newActions;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\ContainerPlacementMetadataStore.java",
  "methodName" : "getContainerPlacementStore",
  "sourceCode" : "@VisibleForTesting\r\nMetadataStore getContainerPlacementStore() {\r\n    return containerPlacementMessageStore;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ContainerManager.java",
  "methodName" : "handleExpiredRequest",
  "sourceCode" : "/**\r\n * Handles an expired resource request for both active and standby containers.\r\n *\r\n * Case 1. If this expired request is due to a container placement action mark the request as failed and return\r\n * Case 2: Otherwise for a normal resource request following cases are possible\r\n *    Case 2.1  If StandbyContainer is present refer to {@code StandbyContainerManager#handleExpiredResourceRequest}\r\n *    Case 2.2: host-affinity is enabled, allocator thread looks for allocated resources on ANY_HOST and issues a\r\n *              container start if available, otherwise issue an ANY_HOST request\r\n *    Case 2.2: host-affinity is disabled, allocator thread does not handle expired requests, it waits for cluster\r\n *              manager to return resources on ANY_HOST\r\n *\r\n * @param processorId logical id of the container\r\n * @param preferredHost host on which container is requested to be deployed\r\n * @param request pending request for the preferred host\r\n * @param allocator allocator for requesting resources\r\n * @param resourceRequestState state of request in {@link ContainerAllocator}\r\n */\r\n@VisibleForTesting\r\nvoid handleExpiredRequest(String processorId, String preferredHost, SamzaResourceRequest request, ContainerAllocator allocator, ResourceRequestState resourceRequestState) {\r\n    boolean resourceAvailableOnAnyHost = allocator.hasAllocatedResource(ResourceRequestState.ANY_HOST);\r\n    // Case 1. Container placement actions can be taken in either cases of host affinity being set, in both cases\r\n    // mark the container placement action failed\r\n    if (hasActiveContainerPlacementAction(processorId)) {\r\n        resourceRequestState.cancelResourceRequest(request);\r\n        markContainerPlacementActionFailed(getPlacementActionMetadata(processorId).get(), \"failed the ContainerPlacement action because request for resources to ClusterManager expired\");\r\n        return;\r\n    }\r\n    // Case 2. When host affinity is disabled wait for cluster resource manager return resources\r\n    // TODO: SAMZA-2330: Handle expired request for host affinity disabled case by retying request for getting ANY_HOST\r\n    if (!hostAffinityEnabled) {\r\n        return;\r\n    }\r\n    // Case 2. When host affinity is enabled handle the expired requests\r\n    if (standbyContainerManager.isPresent()) {\r\n        standbyContainerManager.get().handleExpiredResourceRequest(processorId, request, Optional.ofNullable(allocator.peekAllocatedResource(ResourceRequestState.ANY_HOST)), allocator, resourceRequestState);\r\n    } else if (resourceAvailableOnAnyHost) {\r\n        LOG.info(\"Request for Processor ID: {} on host: {} has expired. Running on ANY_HOST\", processorId, preferredHost);\r\n        allocator.runStreamProcessor(request, ResourceRequestState.ANY_HOST);\r\n    } else {\r\n        LOG.info(\"Request for Processor ID: {} on host: {} has expired. Requesting additional resources on ANY_HOST.\", processorId, preferredHost);\r\n        resourceRequestState.cancelResourceRequest(request);\r\n        allocator.requestResource(processorId, ResourceRequestState.ANY_HOST);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ContainerManager.java",
  "methodName" : "registerContainerPlacementActionForTest",
  "sourceCode" : "/**\r\n * This method is only used for Testing the Container Placement actions to get a hold of {@link ContainerPlacementMetadata}\r\n * for assertions. Not intended to be used in src\r\n */\r\n@VisibleForTesting\r\nContainerPlacementMetadata registerContainerPlacementActionForTest(ContainerPlacementRequestMessage requestMessage, ContainerAllocator containerAllocator) {\r\n    registerContainerPlacementAction(requestMessage, containerAllocator);\r\n    if (hasActiveContainerPlacementAction(requestMessage.getProcessorId())) {\r\n        return getPlacementActionMetadata(requestMessage.getProcessorId()).get();\r\n    }\r\n    return null;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ContainerProcessManager.java",
  "methodName" : "getJobFailureCriteriaMet",
  "sourceCode" : "@VisibleForTesting\r\nboolean getJobFailureCriteriaMet() {\r\n    return jobFailureCriteriaMet;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ContainerProcessManager.java",
  "methodName" : "getProcessorFailures",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, ProcessorFailure> getProcessorFailures() {\r\n    return processorFailures;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ContainerProcessManager.java",
  "methodName" : "onResourceCompletedWithUnknownStatus",
  "sourceCode" : "/**\r\n * Called within {@link #onResourceCompleted(SamzaResourceStatus)} for unknown exit statuses. These exit statuses\r\n * correspond to container completion other than container run-to-completion, abort or preemption, or disk failure\r\n * (e.g., detected by YARN's NM healthchecks).\r\n * @param resourceStatus reported resource status.\r\n * @param containerId container ID\r\n * @param processorId processor ID (aka. logical container ID)\r\n * @param exitStatus exit status from the {@link #onResourceCompleted(SamzaResourceStatus)} callback.\r\n */\r\n@VisibleForTesting\r\nvoid onResourceCompletedWithUnknownStatus(SamzaResourceStatus resourceStatus, String containerId, String processorId, int exitStatus) {\r\n    LOG.info(\"Container ID: {} for Processor ID: {} failed with exit code: {}.\", containerId, processorId, exitStatus);\r\n    Instant now = Instant.now();\r\n    state.failedContainers.incrementAndGet();\r\n    state.jobHealthy.set(false);\r\n    state.neededProcessors.incrementAndGet();\r\n    // Find out previously running container location\r\n    String lastSeenOn = Optional.ofNullable(localityManager.readLocality().getProcessorLocality(processorId)).map(ProcessorLocality::host).orElse(null);\r\n    if (!hostAffinityEnabled || StringUtils.isBlank(lastSeenOn)) {\r\n        lastSeenOn = ResourceRequestState.ANY_HOST;\r\n    }\r\n    LOG.info(\"Container ID: {} for Processor ID: {} was last seen on host {}.\", containerId, processorId, lastSeenOn);\r\n    // A container failed for an unknown reason. Let's check to see if\r\n    // we need to shutdown the whole app master if too many container\r\n    // failures have happened. The rules for failing are that the\r\n    // failure count for a task group id must be > the configured retry\r\n    // count, and the last failure (the one prior to this one) must have\r\n    // happened less than retry window ms ago. If retry count is set to\r\n    // 0, the app master will fail on any container failure. If the\r\n    // retry count is set to a number < 0, a container failure will\r\n    // never trigger an app master failure.\r\n    int retryCount = clusterManagerConfig.getContainerRetryCount();\r\n    int retryWindowMs = clusterManagerConfig.getContainerRetryWindowMs();\r\n    int currentFailCount;\r\n    boolean retryContainerRequest = true;\r\n    if (retryCount == 0) {\r\n        // Failure criteria met only if failed containers can fail the job.\r\n        jobFailureCriteriaMet = clusterManagerConfig.shouldFailJobAfterContainerRetries();\r\n        if (jobFailureCriteriaMet) {\r\n            LOG.error(\"Processor ID: {} (current Container ID: {}) failed, and retry count is set to 0, \" + \"so shutting down the application master and marking the job as failed.\", processorId, containerId);\r\n        } else {\r\n            LOG.error(\"Processor ID: {} (current Container ID: {}) failed, and retry count is set to 0, \" + \"but the job will continue to run with the failed container.\", processorId, containerId);\r\n            state.failedProcessors.put(processorId, resourceStatus);\r\n        }\r\n        retryContainerRequest = false;\r\n    } else if (retryCount > 0) {\r\n        long durationSinceLastRetryMs;\r\n        if (processorFailures.containsKey(processorId)) {\r\n            ProcessorFailure failure = processorFailures.get(processorId);\r\n            currentFailCount = failure.getCount() + 1;\r\n            Duration lastRetryDelay = getRetryDelay(processorId);\r\n            Instant retryAttemptedAt = failure.getLastFailure().plus(lastRetryDelay);\r\n            durationSinceLastRetryMs = now.toEpochMilli() - retryAttemptedAt.toEpochMilli();\r\n            if (durationSinceLastRetryMs < 0) {\r\n                // This should never happen without changes to the system clock or time travel. Log a warning just in case.\r\n                LOG.warn(\"Last failure at: {} with a retry attempted at: {} which is supposed to be before current time of: {}\", failure.getLastFailure(), retryAttemptedAt, now);\r\n            }\r\n        } else {\r\n            currentFailCount = 1;\r\n            durationSinceLastRetryMs = 0;\r\n        }\r\n        if (durationSinceLastRetryMs >= retryWindowMs) {\r\n            LOG.info(\"Resetting failure count for Processor ID: {} back to 1, since last failure \" + \"(for Container ID: {}) was outside the bounds of the retry window.\", processorId, containerId);\r\n            // Reset counter back to 1, since the last failure for this\r\n            // container happened outside the window boundary.\r\n            currentFailCount = 1;\r\n        }\r\n        // if fail count is (1 initial failure + max retries) then fail job.\r\n        if (currentFailCount > retryCount) {\r\n            LOG.error(\"Processor ID: {} (current Container ID: {}) has failed {} times. \" + \"This is greater that the retry count of {}.\" + \"The failure occurred {} ms after the previous one, which is less than the retry window of {} ms.\", processorId, containerId, currentFailCount, retryCount, durationSinceLastRetryMs, retryWindowMs);\r\n            // We have too many failures, and we're within the window\r\n            // boundary, so reset shut down the app master.\r\n            retryContainerRequest = false;\r\n            if (clusterManagerConfig.shouldFailJobAfterContainerRetries()) {\r\n                jobFailureCriteriaMet = true;\r\n                LOG.error(\"Shutting down the application master and marking the job as failed after max retry attempts.\");\r\n                state.status = SamzaApplicationState.SamzaAppStatus.FAILED;\r\n            } else {\r\n                LOG.warn(\"Processor ID: {} with Container ID: {} failed after all retry attempts. Job will continue to run without this container.\", processorId, containerId);\r\n                state.failedProcessors.put(processorId, resourceStatus);\r\n            }\r\n        } else {\r\n            LOG.info(\"Current failure count for Processor ID: {} is {}.\", processorId, currentFailCount);\r\n            Duration retryDelay = Duration.ZERO;\r\n            if (!ResourceRequestState.ANY_HOST.equals(lastSeenOn) && currentFailCount == retryCount) {\r\n                // Add the preferred host last retry delay on the last retry\r\n                retryDelay = Duration.ofMillis(clusterManagerConfig.getContainerPreferredHostLastRetryDelayMs());\r\n            }\r\n            processorFailures.put(processorId, new ProcessorFailure(currentFailCount, now, retryDelay));\r\n            retryContainerRequest = true;\r\n        }\r\n    }\r\n    if (retryContainerRequest) {\r\n        Duration retryDelay = getRetryDelay(processorId);\r\n        if (!retryDelay.isZero()) {\r\n            LOG.info(\"Adding a delay of: {} seconds on the last container retry request for preferred host: {}\", retryDelay.getSeconds(), lastSeenOn);\r\n        }\r\n        handleContainerStop(processorId, resourceStatus.getContainerId(), lastSeenOn, exitStatus, retryDelay);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\DefaultApplicationMain.java",
  "methodName" : "run",
  "sourceCode" : "@VisibleForTesting\r\nstatic void run(String[] args) {\r\n    // This branch is ONLY for Yarn deployments, standalone apps uses offspring\r\n    final ApplicationRunnerMain.ApplicationRunnerCommandLine cmdLine = new ApplicationRunnerMain.ApplicationRunnerCommandLine();\r\n    cmdLine.parser().allowsUnrecognizedOptions();\r\n    final OptionSet options = cmdLine.parser().parse(args);\r\n    // load full job config with ConfigLoader\r\n    final Config originalConfig = ConfigUtil.loadConfig(cmdLine.loadConfig(options));\r\n    JobCoordinatorLaunchUtil.run(ApplicationUtil.fromConfig(originalConfig), originalConfig);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\JobCoordinatorLaunchUtil.java",
  "methodName" : "addShutdownHook",
  "sourceCode" : "/**\r\n * This is a separate method so it can be stubbed in tests, since adding a real shutdown hook will cause the hook to\r\n * added to the test suite JVM.\r\n */\r\n@VisibleForTesting\r\nstatic void addShutdownHook(JobCoordinator jobCoordinator) {\r\n    Runtime.getRuntime().addShutdownHook(new Thread(jobCoordinator::stop, \"Samza Job Coordinator Shutdown Hook Thread\"));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ResourceRequestState.java",
  "methodName" : "sendResourceRequest",
  "sourceCode" : "/**\r\n * Sends the request to the {@link ClusterResourceManager} while queuing the request to be matched with the returned\r\n * {@link SamzaResource}. Caller must call this in a synchronized block.\r\n * @param request to be sent.\r\n */\r\n@VisibleForTesting\r\nvoid sendResourceRequest(SamzaResourceRequest request) {\r\n    requestsQueue.add(request);\r\n    String preferredHost = request.getPreferredHost();\r\n    // if host affinity is enabled, update state.\r\n    if (hostAffinityEnabled) {\r\n        //increment # of requests on the host.\r\n        if (hostRequestCounts.containsKey(preferredHost)) {\r\n            hostRequestCounts.get(preferredHost).incrementAndGet();\r\n        } else {\r\n            hostRequestCounts.put(preferredHost, new AtomicInteger(1));\r\n        }\r\n        /**\r\n         * The following is important to correlate allocated resource data with the requestsQueue made before. If\r\n         * the preferredHost is requested for the first time, the state should reflect that the allocatedResources\r\n         * list is empty and NOT null.\r\n         */\r\n        if (!allocatedResources.containsKey(preferredHost)) {\r\n            allocatedResources.put(preferredHost, new ArrayList<>());\r\n        }\r\n    }\r\n    manager.requestResources(request);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ResourceRequestState.java",
  "methodName" : "getHostRequestCounts",
  "sourceCode" : "// Package private, used only in tests.\r\n@VisibleForTesting\r\nMap<String, AtomicInteger> getHostRequestCounts() {\r\n    return Collections.unmodifiableMap(hostRequestCounts);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\clustermanager\\ResourceRequestState.java",
  "methodName" : "getDelayedRequestsQueue",
  "sourceCode" : "@VisibleForTesting\r\nDelayedRequestQueue getDelayedRequestsQueue() {\r\n    return delayedRequestsQueue;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\config\\ShellCommandConfig.java",
  "methodName" : "getFinalJvmOptions",
  "sourceCode" : "/**\r\n * Returns the final JVM options by applying the heap override if available to the jvm opts\r\n */\r\n@VisibleForTesting\r\nString getFinalJvmOptions(String jvmOpts, String maxHeapOverride) {\r\n    String finalJvmOpts = jvmOpts;\r\n    if (new JobConfig(this).getAutosizingEnabled() && StringUtils.isNotEmpty(maxHeapOverride)) {\r\n        String xmxSetting = \"-Xmx\" + maxHeapOverride + \"m\";\r\n        if (StringUtils.isNotBlank(jvmOpts)) {\r\n            if (jvmOpts.contains(\"-Xmx\")) {\r\n                finalJvmOpts = jvmOpts.replaceAll(\"-Xmx\\\\S+\", xmxSetting);\r\n            } else {\r\n                finalJvmOpts = jvmOpts.concat(\" \" + xmxSetting);\r\n            }\r\n        } else {\r\n            finalJvmOpts = xmxSetting;\r\n        }\r\n    }\r\n    return finalJvmOpts;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\container\\ContainerHeartbeatMonitor.java",
  "methodName" : "createContainerHeartbeatClient",
  "sourceCode" : "@VisibleForTesting\r\nContainerHeartbeatClient createContainerHeartbeatClient(String coordinatorUrl, String containerExecutionId) {\r\n    return new ContainerHeartbeatClient(coordinatorUrl, containerExecutionId);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\container\\ContainerHeartbeatMonitor.java",
  "methodName" : "getMetrics",
  "sourceCode" : "@VisibleForTesting\r\nContainerHeartbeatMetrics getMetrics() {\r\n    return metrics;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\container\\ContainerHeartbeatMonitor.java",
  "methodName" : "getHeartbeatEstablishedFailureCount",
  "sourceCode" : "@VisibleForTesting\r\nCounter getHeartbeatEstablishedFailureCount() {\r\n    return heartbeatEstablishedFailureCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\container\\ContainerHeartbeatMonitor.java",
  "methodName" : "getHeartbeatEstablishedWithNewAmCount",
  "sourceCode" : "@VisibleForTesting\r\nCounter getHeartbeatEstablishedWithNewAmCount() {\r\n    return heartbeatEstablishedWithNewAmCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\container\\ContainerHeartbeatMonitor.java",
  "methodName" : "getHeartbeatExpiredCount",
  "sourceCode" : "@VisibleForTesting\r\nCounter getHeartbeatExpiredCount() {\r\n    return heartbeatExpiredCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\MetadataResourceUtil.java",
  "methodName" : "createChangelogStreams",
  "sourceCode" : "@VisibleForTesting\r\nvoid createChangelogStreams() {\r\n    ChangelogStreamManager.createChangelogStreams(config, jobModel.getMaxChangeLogStreamPartitions());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\MetadataResourceUtil.java",
  "methodName" : "getCheckpointManager",
  "sourceCode" : "@VisibleForTesting\r\nCheckpointManager getCheckpointManager() {\r\n    return checkpointManager;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\metadatastore\\NamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "getCoordinatorMessageKey",
  "sourceCode" : "@VisibleForTesting\r\nString getCoordinatorMessageKey(String key) {\r\n    return CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, key);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\staticresource\\StaticResourceJobCoordinator.java",
  "methodName" : "doSetLoggingContextConfig",
  "sourceCode" : "/**\r\n * This is a helper method so that we can verify it is called in testing.\r\n */\r\n@VisibleForTesting\r\nvoid doSetLoggingContextConfig(Config config) {\r\n    LoggingContextHolder.INSTANCE.setConfig(config);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\staticresource\\StaticResourceJobCoordinator.java",
  "methodName" : "metadataResourceUtil",
  "sourceCode" : "/**\r\n * Wrapper around {@link MetadataResourceUtil} constructor so it can be stubbed during testing.\r\n */\r\n@VisibleForTesting\r\nMetadataResourceUtil metadataResourceUtil(JobModel jobModel) {\r\n    return new MetadataResourceUtil(jobModel, this.metrics, this.config);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\staticresource\\StaticResourceJobCoordinator.java",
  "methodName" : "buildDiagnosticsManager",
  "sourceCode" : "/**\r\n * Wrapper around {@link DiagnosticsUtil#buildDiagnosticsManager} so it can be stubbed during testing.\r\n */\r\n@VisibleForTesting\r\nOptional<DiagnosticsManager> buildDiagnosticsManager(String jobName, String jobId, JobModel jobModel, String containerId, Optional<String> executionEnvContainerId, Optional<String> samzaEpochId, Config config) {\r\n    return DiagnosticsUtil.buildDiagnosticsManager(jobName, jobId, jobModel, containerId, executionEnvContainerId, samzaEpochId, config);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\stream\\CoordinatorStreamSystemConsumer.java",
  "methodName" : "isStarted",
  "sourceCode" : "@VisibleForTesting\r\nboolean isStarted() {\r\n    return isStarted;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\stream\\CoordinatorStreamSystemProducer.java",
  "methodName" : "isStarted",
  "sourceCode" : "@VisibleForTesting\r\nboolean isStarted() {\r\n    return isStarted;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamPartitionCountMonitor.java",
  "methodName" : "updatePartitionCountMetric",
  "sourceCode" : "/**\r\n * Fetches the current partition count for each system stream from the cache, compares the current count to the\r\n * original count and updates the metric for that system stream with the delta.\r\n */\r\n@VisibleForTesting\r\npublic void updatePartitionCountMetric() {\r\n    try {\r\n        Map<SystemStream, SystemStreamMetadata> currentMetadata = getMetadata(streamsToMonitor, metadataCache);\r\n        Set<SystemStream> streamsChanged = new HashSet<>();\r\n        for (Map.Entry<SystemStream, SystemStreamMetadata> metadataEntry : initialMetadata.entrySet()) {\r\n            try {\r\n                SystemStream systemStream = metadataEntry.getKey();\r\n                SystemStreamMetadata metadata = metadataEntry.getValue();\r\n                int currentPartitionCount = currentMetadata.get(systemStream).getSystemStreamPartitionMetadata().size();\r\n                int prevPartitionCount = metadata.getSystemStreamPartitionMetadata().size();\r\n                Gauge gauge = gauges.get(systemStream);\r\n                gauge.set(currentPartitionCount);\r\n                if (currentPartitionCount != prevPartitionCount) {\r\n                    log.warn(String.format(\"Change of partition count detected in stream %s. old partition count: %d, current partition count: %d\", systemStream.toString(), prevPartitionCount, currentPartitionCount));\r\n                    if (currentPartitionCount > prevPartitionCount) {\r\n                        log.error(String.format(\"Shutting down (stateful) or restarting (stateless) the job since current \" + \"partition count %d is greater than the old partition count %d for stream %s.\", currentPartitionCount, prevPartitionCount, systemStream.toString()));\r\n                        streamsChanged.add(systemStream);\r\n                    }\r\n                }\r\n            } catch (Exception e) {\r\n                log.error(String.format(\"Error comparing partition count differences for stream: %s\", metadataEntry.getKey().toString()));\r\n            }\r\n        }\r\n        if (!streamsChanged.isEmpty() && this.callbackMethod != null) {\r\n            this.callbackMethod.onSystemStreamPartitionChange(streamsChanged);\r\n        }\r\n    } catch (Exception e) {\r\n        log.error(\"Exception while updating partition count metric.\", e);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamPartitionCountMonitor.java",
  "methodName" : "getGauges",
  "sourceCode" : "/**\r\n * For testing. Returns the metrics.\r\n */\r\n@VisibleForTesting\r\nMap<SystemStream, Gauge<Integer>> getGauges() {\r\n    return gauges;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamPartitionCountMonitor.java",
  "methodName" : "isRunning",
  "sourceCode" : "@VisibleForTesting\r\nboolean isRunning() {\r\n    return state == State.RUNNING;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamPartitionCountMonitor.java",
  "methodName" : "awaitTermination",
  "sourceCode" : "/**\r\n * Wait until this service has shutdown. Returns true if shutdown occurred within the timeout\r\n * and false otherwise.\r\n * <p>\r\n * This is currently exposed at the package private level for tests only.\r\n */\r\n@VisibleForTesting\r\nboolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException {\r\n    return schedulerService.awaitTermination(timeout, unit);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamRegexMonitor.java",
  "methodName" : "isRunning",
  "sourceCode" : "@VisibleForTesting\r\nboolean isRunning() {\r\n    return state == State.RUNNING;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\coordinator\\StreamRegexMonitor.java",
  "methodName" : "awaitTermination",
  "sourceCode" : "/**\r\n * Wait until this service has shutdown. Returns true if shutdown occurred within the timeout\r\n * and false otherwise.\r\n * <p>\r\n * This is currently exposed at the package private level for tests only.\r\n */\r\n@VisibleForTesting\r\nboolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException {\r\n    return schedulerService.awaitTermination(timeout, unit);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\drain\\DrainMonitor.java",
  "methodName" : "getState",
  "sourceCode" : "/**\r\n * Get the current state of the DrainMonitor.\r\n */\r\n@VisibleForTesting\r\nState getState() {\r\n    return state;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\drain\\DrainUtils.java",
  "methodName" : "writeDrainNotification",
  "sourceCode" : "/**\r\n * Writes a {@link DrainNotification} to an underlying metadata store.\r\n */\r\n@VisibleForTesting\r\nstatic UUID writeDrainNotification(MetadataStore metadataStore, DrainNotification drainNotification) {\r\n    Preconditions.checkArgument(metadataStore != null, \"MetadataStore cannot be null.\");\r\n    Preconditions.checkArgument(drainNotification != null, \"DrainNotification cannot be null.\");\r\n    final NamespaceAwareCoordinatorStreamStore drainMetadataStore = new NamespaceAwareCoordinatorStreamStore(metadataStore, DRAIN_METADATA_STORE_NAMESPACE);\r\n    final ObjectMapper objectMapper = DrainNotificationObjectMapper.getObjectMapper();\r\n    try {\r\n        drainMetadataStore.put(drainNotification.getUuid().toString(), objectMapper.writeValueAsBytes(drainNotification));\r\n        drainMetadataStore.flush();\r\n        LOG.info(\"DrainNotification with id {} written to metadata-store for the deployment ID {}\", drainNotification.getUuid(), drainNotification.getUuid());\r\n    } catch (Exception ex) {\r\n        throw new SamzaException(String.format(\"DrainNotification might have been not written to metastore %s\", drainNotification), ex);\r\n    }\r\n    return drainNotification.getUuid();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\execution\\IntermediateStreamManager.java",
  "methodName" : "setIntermediateStreamPartitions",
  "sourceCode" : "/**\r\n * Sets partition counts of intermediate streams which have not been assigned partition counts.\r\n */\r\n@VisibleForTesting\r\nvoid setIntermediateStreamPartitions(JobGraph jobGraph) {\r\n    final String defaultPartitionsConfigProperty = JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS;\r\n    int partitions = config.getInt(defaultPartitionsConfigProperty, StreamEdge.PARTITIONS_UNKNOWN);\r\n    if (partitions == StreamEdge.PARTITIONS_UNKNOWN) {\r\n        // use the following simple algo to figure out the partitions\r\n        // partition = MAX(MAX(Input topic partitions), MAX(Output topic partitions))\r\n        // partition will be further bounded by MAX_INFERRED_PARTITIONS.\r\n        // This is important when running in hadoop where an HDFS input can have lots of files (partitions).\r\n        int maxInPartitions = maxPartitions(jobGraph.getInputStreams());\r\n        int maxOutPartitions = maxPartitions(jobGraph.getOutputStreams());\r\n        partitions = Math.max(maxInPartitions, maxOutPartitions);\r\n        ApplicationMode applicationMode = getAppMode();\r\n        if (partitions > MAX_INFERRED_PARTITIONS && ApplicationMode.BATCH.equals(applicationMode)) {\r\n            partitions = MAX_INFERRED_PARTITIONS;\r\n            log.warn(String.format(\"Inferred intermediate stream partition count %d is greater than the max %d. Using the max.\", partitions, MAX_INFERRED_PARTITIONS));\r\n        }\r\n        log.info(\"Using {} as the default partition count for intermediate streams\", partitions);\r\n    } else {\r\n        // Reject any zero or other negative values explicitly specified in config.\r\n        if (partitions <= 0) {\r\n            throw new SamzaException(String.format(\"Invalid value %d specified for config property %s\", partitions, defaultPartitionsConfigProperty));\r\n        }\r\n        log.info(\"Using partition count value {} specified for config property {}\", partitions, defaultPartitionsConfigProperty);\r\n    }\r\n    for (StreamEdge edge : jobGraph.getIntermediateStreamEdges()) {\r\n        if (edge.getPartitionCount() <= 0) {\r\n            log.info(\"Set the partition count for intermediate stream {} to {}.\", edge.getName(), partitions);\r\n            edge.setPartitionCount(partitions);\r\n        }\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\execution\\IntermediateStreamManager.java",
  "methodName" : "getAppMode",
  "sourceCode" : "@VisibleForTesting\r\nApplicationMode getAppMode() {\r\n    return new ApplicationConfig(config).getAppMode();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\execution\\JobGraphJsonGenerator.java",
  "methodName" : "operatorToMap",
  "sourceCode" : "/**\r\n * Format the operator properties into a map\r\n * @param spec a {@link OperatorSpec} instance\r\n * @return map of the operator properties\r\n */\r\n@VisibleForTesting\r\nMap<String, Object> operatorToMap(OperatorSpec spec) {\r\n    Map<String, Object> map = new HashMap<>();\r\n    map.put(\"opCode\", spec.getOpCode().name());\r\n    map.put(\"opId\", spec.getOpId());\r\n    map.put(\"sourceLocation\", spec.getSourceLocation());\r\n    Collection<OperatorSpec> nextOperators = spec.getRegisteredOperatorSpecs();\r\n    map.put(\"nextOperatorIds\", nextOperators.stream().map(OperatorSpec::getOpId).collect(Collectors.toSet()));\r\n    if (spec instanceof OutputOperatorSpec) {\r\n        OutputStreamImpl outputStream = ((OutputOperatorSpec) spec).getOutputStream();\r\n        map.put(\"outputStreamId\", outputStream.getStreamId());\r\n    } else if (spec instanceof PartitionByOperatorSpec) {\r\n        OutputStreamImpl outputStream = ((PartitionByOperatorSpec) spec).getOutputStream();\r\n        map.put(\"outputStreamId\", outputStream.getStreamId());\r\n    }\r\n    if (spec instanceof StreamTableJoinOperatorSpec) {\r\n        String tableId = ((StreamTableJoinOperatorSpec) spec).getTableId();\r\n        map.put(\"tableId\", tableId);\r\n    }\r\n    if (spec instanceof SendToTableOperatorSpec) {\r\n        String tableId = ((SendToTableOperatorSpec) spec).getTableId();\r\n        map.put(\"tableId\", tableId);\r\n    }\r\n    if (spec instanceof JoinOperatorSpec) {\r\n        map.put(\"ttlMs\", ((JoinOperatorSpec) spec).getTtlMs());\r\n    }\r\n    return map;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "fetchEpochIdForJobCoordinator",
  "sourceCode" : "/**\r\n * The properties of the epoch identifier are as follows\r\n *  1. Unique across applications in the cluster\r\n *  2. Remains unchanged within a single deployment lifecycle\r\n *  3. Remains unchanged across application attempt within a single deployment lifecycle\r\n *  4. Changes across deployment lifecycle\r\n *\r\n * For YARN environment:\r\n * Generate the epoch id using the execution container id that is passed through system environment. This isn't ideal\r\n * way of generating this ID, since it is YARN-specific. This is left as a legacy flow for backwards compatibility, as\r\n * the original implementation did not define a cluster-agnostic contract.\r\n * The format and property used to generate ID is specific to YARN and the specific format of the container name\r\n * is a public contract by YARN which is likely to remain backward compatible.\r\n *\r\n * For non-YARN environments:\r\n * Extract this from the environment variable SAMZA_EPOCH_ID. This is a more generic way of obtaining an epoch id, but\r\n * it does require the resource management layer to inject this value.\r\n *\r\n * @return an identifier associated with the job coordinator satisfying the above properties\r\n */\r\n@VisibleForTesting\r\nString fetchEpochIdForJobCoordinator() {\r\n    if (ClusterType.YARN.equals(this.clusterType)) {\r\n        String[] containerIdParts = getEnvProperty(CONTAINER_ID_PROPERTY).split(CONTAINER_ID_DELIMITER);\r\n        return containerIdParts[1] + CONTAINER_ID_DELIMITER + containerIdParts[2];\r\n    } else {\r\n        return getEnvProperty(EnvironmentVariables.SAMZA_EPOCH_ID);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getEnvProperty",
  "sourceCode" : "@VisibleForTesting\r\nString getEnvProperty(String propertyName) {\r\n    return System.getenv(propertyName);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getMetrics",
  "sourceCode" : "@VisibleForTesting\r\nJobCoordinatorMetadataManagerMetrics getMetrics() {\r\n    return metrics;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getApplicationAttemptCount",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getApplicationAttemptCount() {\r\n    return applicationAttemptCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getMetadataGenerationFailedCount",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getMetadataGenerationFailedCount() {\r\n    return metadataGenerationFailedCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getMetadataReadFailedCount",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getMetadataReadFailedCount() {\r\n    return metadataReadFailedCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getMetadataWriteFailedCount",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getMetadataWriteFailedCount() {\r\n    return metadataWriteFailedCount;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getJobModelChangedAcrossApplicationAttempt",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getJobModelChangedAcrossApplicationAttempt() {\r\n    return jobModelChangedAcrossApplicationAttempt;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getConfigChangedAcrossApplicationAttempt",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getConfigChangedAcrossApplicationAttempt() {\r\n    return configChangedAcrossApplicationAttempt;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\job\\metadata\\JobCoordinatorMetadataManager.java",
  "methodName" : "getNewDeployment",
  "sourceCode" : "@VisibleForTesting\r\nGauge<Integer> getNewDeployment() {\r\n    return newDeployment;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\metrics\\reporter\\LoggingMetricsReporter.java",
  "methodName" : "doLog",
  "sourceCode" : "/**\r\n * VisibleForTesting so that the logging call can be verified in unit tests.\r\n */\r\n@VisibleForTesting\r\nvoid doLog(String logString) {\r\n    LOG.info(logString);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\operators\\impl\\OperatorImpl.java",
  "methodName" : "onMessage",
  "sourceCode" : "/* Package Private helper method for tests to perform onMessage synchronously\r\n   * Note: It is only intended for test use\r\n   */\r\n@VisibleForTesting\r\nfinal void onMessage(M message, MessageCollector collector, TaskCoordinator coordinator) {\r\n    onMessageAsync(message, collector, coordinator).toCompletableFuture().join();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\operators\\impl\\OperatorImpl.java",
  "methodName" : "handleMessage",
  "sourceCode" : "/* Package Private helper method for tests to perform handleMessage synchronously\r\n   * Note: It is only intended for test use\r\n   */\r\n@VisibleForTesting\r\nfinal Collection<RM> handleMessage(M message, MessageCollector collector, TaskCoordinator coordinator) {\r\n    return handleMessageAsync(message, collector, coordinator).toCompletableFuture().join();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\operators\\impl\\OperatorImpl.java",
  "methodName" : "composeFutureWithExecutor",
  "sourceCode" : "@VisibleForTesting\r\nfinal <T, U> CompletionStage<U> composeFutureWithExecutor(CompletionStage<T> futureToChain, Function<? super T, ? extends CompletionStage<U>> fn) {\r\n    return operatorExecutorEnabled ? futureToChain.thenComposeAsync(fn, operatorExecutor) : futureToChain.thenCompose(fn);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\operators\\impl\\OperatorImpl.java",
  "methodName" : "acceptFutureWithExecutor",
  "sourceCode" : "@VisibleForTesting\r\nfinal <T> CompletionStage<Void> acceptFutureWithExecutor(CompletionStage<T> futureToChain, Consumer<? super T> consumer) {\r\n    return operatorExecutorEnabled ? futureToChain.thenAcceptAsync(consumer, operatorExecutor) : futureToChain.thenAccept(consumer);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\operators\\MessageStreamImpl.java",
  "methodName" : "getOperatorSpec",
  "sourceCode" : "/**\r\n * Get the {@link OperatorSpec} associated with this {@link MessageStreamImpl}.\r\n * @return the {@link OperatorSpec} associated with this {@link MessageStreamImpl}.\r\n */\r\n@VisibleForTesting\r\npublic OperatorSpec<?, M> getOperatorSpec() {\r\n    return this.operatorSpec;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\processor\\StreamProcessor.java",
  "methodName" : "getCurrentJobCoordinator",
  "sourceCode" : "@VisibleForTesting\r\nJobCoordinator getCurrentJobCoordinator() {\r\n    return jobCoordinator;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\processor\\StreamProcessor.java",
  "methodName" : "getContainer",
  "sourceCode" : "@VisibleForTesting\r\nSamzaContainer getContainer() {\r\n    return container;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\processor\\StreamProcessor.java",
  "methodName" : "createSamzaContainer",
  "sourceCode" : "@VisibleForTesting\r\nSamzaContainer createSamzaContainer(String processorId, JobModel jobModel) {\r\n    // Creating diagnostics manager and wiring it respectively\r\n    String jobName = new JobConfig(config).getName().get();\r\n    String jobId = new JobConfig(config).getJobId();\r\n    Optional<DiagnosticsManager> diagnosticsManager = DiagnosticsUtil.buildDiagnosticsManager(jobName, jobId, jobModel, processorId, Optional.empty(), Optional.empty(), config);\r\n    // Metadata store lifecycle managed outside of the SamzaContainer.\r\n    // All manager lifecycles are managed in the SamzaContainer including startpointManager\r\n    StartpointManager startpointManager = null;\r\n    if (metadataStore != null && new JobConfig(config).getStartpointEnabled()) {\r\n        startpointManager = new StartpointManager(metadataStore);\r\n    } else if (!new JobConfig(config).getStartpointEnabled()) {\r\n        LOGGER.warn(\"StartpointManager not instantiated because startpoints is not enabled\");\r\n    } else {\r\n        LOGGER.warn(\"StartpointManager cannot be instantiated because no metadata store defined for this stream processor\");\r\n    }\r\n    /*\r\n     * StreamProcessor has a metricsRegistry instance variable, but StreamProcessor registers its metrics on its own\r\n     * with the reporters. Therefore, don't reuse the StreamProcessor.metricsRegistry, because SamzaContainer also\r\n     * registers the registry, and that will result in unnecessary duplicate metrics.\r\n     */\r\n    MetricsRegistryMap metricsRegistryMap = new MetricsRegistryMap();\r\n    DrainMonitor drainMonitor = null;\r\n    JobConfig jobConfig = new JobConfig(config);\r\n    if (metadataStore != null && jobConfig.getDrainMonitorEnabled()) {\r\n        drainMonitor = new DrainMonitor(metadataStore, config);\r\n    }\r\n    return SamzaContainer.apply(processorId, jobModel, ScalaJavaUtil.toScalaMap(this.customMetricsReporter), metricsRegistryMap, this.taskFactory, JobContextImpl.fromConfigWithDefaults(this.config, jobModel), Option.apply(this.applicationDefinedContainerContextFactoryOptional.orElse(null)), Option.apply(this.applicationDefinedTaskContextFactoryOptional.orElse(null)), Option.apply(this.externalContextOptional.orElse(null)), null, startpointManager, Option.apply(diagnosticsManager.orElse(null)), drainMonitor);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ClusterBasedProcessorLifecycleListener.java",
  "methodName" : "getShutdownHookThread",
  "sourceCode" : "@VisibleForTesting\r\nThread getShutdownHookThread() {\r\n    return this.shutdownHookThread;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ClusterBasedProcessorLifecycleListener.java",
  "methodName" : "addJVMShutdownHook",
  "sourceCode" : "@VisibleForTesting\r\nvoid addJVMShutdownHook(Thread shutdownHookThread) {\r\n    if (shutdownHookThread != null) {\r\n        Runtime.getRuntime().addShutdownHook(shutdownHookThread);\r\n        log.info(\"Added Samza container shutdown hook\");\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ClusterBasedProcessorLifecycleListener.java",
  "methodName" : "removeJVMShutdownHook",
  "sourceCode" : "@VisibleForTesting\r\nvoid removeJVMShutdownHook(Thread shutdownHookThread) {\r\n    try {\r\n        if (shutdownHookThread != null) {\r\n            Runtime.getRuntime().removeShutdownHook(shutdownHookThread);\r\n            log.info(\"Removed Samza container shutdown hook\");\r\n        }\r\n    } catch (IllegalStateException e) {\r\n        // Thrown when then JVM is already shutting down, so safe to ignore.\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ContainerLaunchUtil.java",
  "methodName" : "run",
  "sourceCode" : "@VisibleForTesting\r\nstatic int run(ApplicationDescriptorImpl<? extends ApplicationDescriptor> appDesc, String jobName, String jobId, String containerId, Optional<String> executionEnvContainerId, Optional<String> samzaEpochId, JobModel jobModel, Config config, Optional<ExternalContext> externalContextOptional) {\r\n    CoordinatorStreamStore coordinatorStreamStore = buildCoordinatorStreamStore(config, new MetricsRegistryMap());\r\n    coordinatorStreamStore.init();\r\n    /*\r\n     * We track the exit code and only trigger exit in the finally block to make sure we are able to execute all the\r\n     * clean up steps. Prior implementation had short circuited exit causing some of the clean up steps to be missed.\r\n     */\r\n    int exitCode = 0;\r\n    try {\r\n        TaskFactory taskFactory = TaskFactoryUtil.getTaskFactory(appDesc);\r\n        LocalityManager localityManager = new LocalityManager(new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetContainerHostMapping.TYPE));\r\n        // StartpointManager wraps the coordinatorStreamStore in the namespaces internally\r\n        StartpointManager startpointManager = null;\r\n        if (new JobConfig(config).getStartpointEnabled()) {\r\n            startpointManager = new StartpointManager(coordinatorStreamStore);\r\n        }\r\n        Map<String, MetricsReporter> metricsReporters = loadMetricsReporters(appDesc, containerId, config);\r\n        // Creating diagnostics manager and reporter, and wiring it respectively\r\n        Optional<DiagnosticsManager> diagnosticsManager = DiagnosticsUtil.buildDiagnosticsManager(jobName, jobId, jobModel, containerId, executionEnvContainerId, samzaEpochId, config);\r\n        MetricsRegistryMap metricsRegistryMap = new MetricsRegistryMap();\r\n        DrainMonitor drainMonitor = null;\r\n        JobConfig jobConfig = new JobConfig(config);\r\n        if (jobConfig.getDrainMonitorEnabled()) {\r\n            drainMonitor = new DrainMonitor(coordinatorStreamStore, config);\r\n        }\r\n        SamzaContainer container = SamzaContainer$.MODULE$.apply(containerId, jobModel, ScalaJavaUtil.toScalaMap(metricsReporters), metricsRegistryMap, taskFactory, JobContextImpl.fromConfigWithDefaults(config, jobModel), Option.apply(appDesc.getApplicationContainerContextFactory().orElse(null)), Option.apply(appDesc.getApplicationTaskContextFactory().orElse(null)), Option.apply(externalContextOptional.orElse(null)), localityManager, startpointManager, Option.apply(diagnosticsManager.orElse(null)), drainMonitor);\r\n        ProcessorLifecycleListener processorLifecycleListener = appDesc.getProcessorLifecycleListenerFactory().createInstance(new ProcessorContext() {\r\n        }, config);\r\n        ClusterBasedProcessorLifecycleListener listener = new ClusterBasedProcessorLifecycleListener(config, processorLifecycleListener, container::shutdown);\r\n        container.setContainerListener(listener);\r\n        ContainerHeartbeatMonitor heartbeatMonitor = createContainerHeartbeatMonitor(container, new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetConfig.TYPE), config);\r\n        if (heartbeatMonitor != null) {\r\n            heartbeatMonitor.start();\r\n        }\r\n        if (new JobConfig(config).getApplicationMasterHighAvailabilityEnabled()) {\r\n            executionEnvContainerId.ifPresent(execEnvContainerId -> {\r\n                ExecutionContainerIdManager executionContainerIdManager = new ExecutionContainerIdManager(new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetExecutionEnvContainerIdMapping.TYPE));\r\n                executionContainerIdManager.writeExecutionEnvironmentContainerIdMapping(containerId, execEnvContainerId);\r\n            });\r\n        }\r\n        container.run();\r\n        if (heartbeatMonitor != null) {\r\n            heartbeatMonitor.stop();\r\n        }\r\n        // Check to see if the HeartbeatMonitor has set an exception before\r\n        // overriding the value with what the listener returns\r\n        if (containerRunnerException == null) {\r\n            containerRunnerException = listener.getContainerException();\r\n        }\r\n        if (containerRunnerException != null) {\r\n            log.error(\"Container stopped with Exception. Exiting process now.\", containerRunnerException);\r\n            exitCode = 1;\r\n        }\r\n    } catch (Throwable e) {\r\n        /*\r\n       * Two separate log statements are intended to print the entire stack trace as part of the logs. Using\r\n       * single log statement with custom format requires explicitly fetching stack trace and null checks which makes\r\n       * the code slightly hard to read in comparison with the current choice.\r\n       */\r\n        log.error(\"Exiting the process due to\", e);\r\n        log.error(\"Container runner exception: \", containerRunnerException);\r\n        exitCode = 1;\r\n    } finally {\r\n        coordinatorStreamStore.close();\r\n        return exitCode;\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ContainerLaunchUtil.java",
  "methodName" : "buildCoordinatorStreamStore",
  "sourceCode" : "@VisibleForTesting\r\nstatic CoordinatorStreamStore buildCoordinatorStreamStore(Config config, MetricsRegistryMap metricsRegistryMap) {\r\n    return new CoordinatorStreamStore(config, metricsRegistryMap);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\ContainerLaunchUtil.java",
  "methodName" : "exitProcess",
  "sourceCode" : "@VisibleForTesting\r\nstatic void exitProcess(int status) {\r\n    System.exit(status);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "getDefaultCoordinatorStreamStoreFactory",
  "sourceCode" : "@VisibleForTesting\r\nstatic Optional<MetadataStoreFactory> getDefaultCoordinatorStreamStoreFactory(Config config) {\r\n    JobConfig jobConfig = new JobConfig(config);\r\n    String coordinatorSystemName = jobConfig.getCoordinatorSystemNameOrNull();\r\n    JobCoordinatorConfig jobCoordinatorConfig = new JobCoordinatorConfig(jobConfig);\r\n    String jobCoordinatorFactoryClassName = jobCoordinatorConfig.getJobCoordinatorFactoryClassName();\r\n    // TODO: Remove restriction to only ZkJobCoordinator after next phase of metadata store abstraction.\r\n    if (StringUtils.isNotBlank(coordinatorSystemName) && ZkJobCoordinatorFactory.class.getName().equals(jobCoordinatorFactoryClassName)) {\r\n        return Optional.of(new CoordinatorStreamMetadataStoreFactory());\r\n    }\r\n    LOG.warn(\"{} or {} not configured, or {} is not {}. No default coordinator stream metadata store will be created.\", JobConfig.JOB_COORDINATOR_SYSTEM, JobConfig.JOB_DEFAULT_SYSTEM, JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, ZkJobCoordinatorFactory.class.getName());\r\n    return Optional.empty();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "getPlanner",
  "sourceCode" : "/**\r\n * @return LocalJobPlanner created\r\n */\r\n@VisibleForTesting\r\nLocalJobPlanner getPlanner() {\r\n    boolean isAppModeBatch = new ApplicationConfig(appDesc.getConfig()).getAppMode() == ApplicationConfig.ApplicationMode.BATCH;\r\n    if (!isAppModeBatch) {\r\n        return new LocalJobPlanner(appDesc, PROCESSOR_ID);\r\n    }\r\n    CoordinationUtils coordinationUtils = this.coordinationUtils.orElse(null);\r\n    String runId = this.runId.orElse(null);\r\n    return new LocalJobPlanner(appDesc, coordinationUtils, PROCESSOR_ID, runId);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "getProcessors",
  "sourceCode" : "@VisibleForTesting\r\nprotected Set<StreamProcessor> getProcessors() {\r\n    return processors.stream().map(Pair::getLeft).collect(Collectors.toSet());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "getShutdownLatch",
  "sourceCode" : "@VisibleForTesting\r\nCountDownLatch getShutdownLatch() {\r\n    return shutdownLatch;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "createCoordinatorStreamStore",
  "sourceCode" : "@VisibleForTesting\r\nMetadataStore createCoordinatorStreamStore(Config config) {\r\n    if (metadataStoreFactory.isPresent()) {\r\n        // TODO: Add missing metadata store abstraction for creating the underlying store to address SAMZA-2182\r\n        if (metadataStoreFactory.get() instanceof CoordinatorStreamMetadataStoreFactory) {\r\n            if (createUnderlyingCoordinatorStream(config)) {\r\n                MetadataStore coordinatorStreamStore = metadataStoreFactory.get().getMetadataStore(\"NoOp\", config, new MetricsRegistryMap());\r\n                LOG.info(\"Created coordinator stream store of type: {}\", coordinatorStreamStore.getClass().getSimpleName());\r\n                return coordinatorStreamStore;\r\n            }\r\n        } else {\r\n            MetadataStore otherMetadataStore = metadataStoreFactory.get().getMetadataStore(\"NoOp\", config, new MetricsRegistryMap());\r\n            LOG.info(\"Created alternative coordinator stream store of type: {}\", otherMetadataStore.getClass().getSimpleName());\r\n            return otherMetadataStore;\r\n        }\r\n    }\r\n    LOG.warn(\"No coordinator stream store created.\");\r\n    return null;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "createUnderlyingCoordinatorStream",
  "sourceCode" : "@VisibleForTesting\r\nboolean createUnderlyingCoordinatorStream(Config config) {\r\n    // TODO: This work around method is necessary due to SAMZA-2182 - Metadata store: disconnect between creation and usage of the underlying storage\r\n    //  and will be addressed in the next phase of metadata store abstraction\r\n    if (new JobConfig(config).getCoordinatorSystemNameOrNull() == null) {\r\n        LOG.warn(\"{} or {} not configured. Coordinator stream not created.\", JobConfig.JOB_COORDINATOR_SYSTEM, JobConfig.JOB_DEFAULT_SYSTEM);\r\n        return false;\r\n    }\r\n    SystemStream coordinatorSystemStream = CoordinatorStreamUtil.getCoordinatorSystemStream(config);\r\n    SystemAdmins systemAdmins = new SystemAdmins(config, this.getClass().getSimpleName());\r\n    systemAdmins.start();\r\n    try {\r\n        SystemAdmin coordinatorSystemAdmin = systemAdmins.getSystemAdmin(coordinatorSystemStream.getSystem());\r\n        CoordinatorStreamUtil.createCoordinatorStream(coordinatorSystemStream, coordinatorSystemAdmin);\r\n    } finally {\r\n        systemAdmins.stop();\r\n    }\r\n    return true;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "createStreamProcessor",
  "sourceCode" : "@VisibleForTesting\r\nStreamProcessor createStreamProcessor(Config config, ApplicationDescriptorImpl<? extends ApplicationDescriptor> appDesc, StreamProcessor.StreamProcessorLifecycleListenerFactory listenerFactory, Optional<ExternalContext> externalContextOptional, MetadataStore coordinatorStreamStore) {\r\n    TaskFactory taskFactory = TaskFactoryUtil.getTaskFactory(appDesc);\r\n    Map<String, MetricsReporter> reporters = new HashMap<>();\r\n    String processorId = createProcessorId(new ApplicationConfig(config));\r\n    appDesc.getMetricsReporterFactories().forEach((name, factory) -> reporters.put(name, factory.getMetricsReporter(name, processorId, config)));\r\n    return new StreamProcessor(processorId, config, reporters, taskFactory, appDesc.getApplicationContainerContextFactory(), appDesc.getApplicationTaskContextFactory(), externalContextOptional, listenerFactory, null, coordinatorStreamStore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\runtime\\LocalApplicationRunner.java",
  "methodName" : "createProcessorId",
  "sourceCode" : "/**\r\n * Generates a unique logical identifier for the stream processor using the provided {@param appConfig}.\r\n * 1. If the processorId is defined in the configuration, then returns the value defined in the configuration.\r\n * 2. Else if the {@linkplain ProcessorIdGenerator} class is defined the configuration, then uses the {@linkplain ProcessorIdGenerator}\r\n * to generate the unique processorId.\r\n * 3. Else throws the {@see ConfigException} back to the caller.\r\n * @param appConfig the configuration of the samza application.\r\n * @throws ConfigException if neither processor.id nor app.processor-id-generator.class is defined in the configuration.\r\n * @return the generated processor identifier.\r\n */\r\n@VisibleForTesting\r\nstatic String createProcessorId(ApplicationConfig appConfig) {\r\n    if (StringUtils.isNotBlank(appConfig.getProcessorId())) {\r\n        return appConfig.getProcessorId();\r\n    } else if (StringUtils.isNotBlank(appConfig.getAppProcessorIdGeneratorClass())) {\r\n        ProcessorIdGenerator idGenerator = ReflectionUtil.getObj(appConfig.getAppProcessorIdGeneratorClass(), ProcessorIdGenerator.class);\r\n        return idGenerator.generateProcessorId(appConfig);\r\n    } else {\r\n        throw new ConfigException(String.format(\"Expected either %s or %s to be configured\", ApplicationConfig.PROCESSOR_ID, ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS));\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\scheduler\\EpochTimeScheduler.java",
  "methodName" : "getScheduledFutures",
  "sourceCode" : "@VisibleForTesting\r\nMap<Object, ScheduledFuture> getScheduledFutures() {\r\n    return scheduledFutures;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\startpoint\\StartpointManager.java",
  "methodName" : "readStartpoint",
  "sourceCode" : "/**\r\n * Returns the last {@link Startpoint} that defines the start position for a {@link SystemStreamPartition}.\r\n * @param ssp The {@link SystemStreamPartition} to fetch the {@link Startpoint} for.\r\n * @return {@link Optional} of {@link Startpoint} for the {@link SystemStreamPartition}.\r\n *         It is empty if it does not exist or if it is too stale.\r\n */\r\n@VisibleForTesting\r\npublic Optional<Startpoint> readStartpoint(SystemStreamPartition ssp) {\r\n    Map<String, byte[]> startpointBytes = readWriteStore.all();\r\n    // there is no task-name to use as key for the startpoint in this case (only the ssp), so we use a null task-name\r\n    return readStartpoint(startpointBytes, ssp, null);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\startpoint\\StartpointManager.java",
  "methodName" : "readStartpoint",
  "sourceCode" : "/**\r\n * Returns the last {@link Startpoint} that defines the start position for a {@link SystemStreamPartition} and {@link TaskName}.\r\n * @param ssp The {@link SystemStreamPartition} to fetch the {@link Startpoint} for.\r\n * @param taskName the {@link TaskName} to fetch the {@link Startpoint} for.\r\n * @return {@link Optional} of {@link Startpoint} for the {@link SystemStreamPartition}.\r\n *         It is empty if it does not exist or if it is too stale.\r\n */\r\n@VisibleForTesting\r\npublic Optional<Startpoint> readStartpoint(SystemStreamPartition ssp, TaskName taskName) {\r\n    Map<String, byte[]> startpointBytes = readWriteStore.all();\r\n    return readStartpoint(startpointBytes, ssp, taskName);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\startpoint\\StartpointManager.java",
  "methodName" : "getReadWriteStore",
  "sourceCode" : "@VisibleForTesting\r\nMetadataStore getReadWriteStore() {\r\n    return readWriteStore;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\startpoint\\StartpointManager.java",
  "methodName" : "getFanOutStore",
  "sourceCode" : "@VisibleForTesting\r\nMetadataStore getFanOutStore() {\r\n    return fanOutStore;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\startpoint\\StartpointManager.java",
  "methodName" : "getObjectMapper",
  "sourceCode" : "@VisibleForTesting\r\nObjectMapper getObjectMapper() {\r\n    return objectMapper;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreBackupManager.java",
  "methodName" : "createBlobStoreUtil",
  "sourceCode" : "@VisibleForTesting\r\nprotected BlobStoreUtil createBlobStoreUtil(BlobStoreManager blobStoreManager, ExecutorService executor, BlobStoreConfig blobStoreConfig, BlobStoreBackupManagerMetrics metrics) {\r\n    return new BlobStoreUtil(blobStoreManager, executor, blobStoreConfig, metrics, null);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreRestoreManager.java",
  "methodName" : "createBlobStoreUtil",
  "sourceCode" : "@VisibleForTesting\r\nprotected BlobStoreUtil createBlobStoreUtil(BlobStoreManager blobStoreManager, ExecutorService executor, BlobStoreConfig blobStoreConfig, BlobStoreRestoreManagerMetrics metrics) {\r\n    return new BlobStoreUtil(blobStoreManager, executor, blobStoreConfig, null, metrics);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreRestoreManager.java",
  "methodName" : "deleteUnusedStoresFromBlobStore",
  "sourceCode" : "/**\r\n * Deletes blob store contents for stores that were present in the last checkpoint but are either no longer\r\n * present in job configs (removed by user since last deployment) or are no longer configured to be backed\r\n * up using blob stores.\r\n *\r\n * This method blocks until all the necessary store contents and snapshot index blobs have been marked for deletion.\r\n */\r\n@VisibleForTesting\r\nstatic void deleteUnusedStoresFromBlobStore(String jobName, String jobId, String taskName, StorageConfig storageConfig, BlobStoreConfig blobStoreConfig, Map<String, Pair<String, SnapshotIndex>> initialStoreSnapshotIndexes, BlobStoreUtil blobStoreUtil, ExecutorService executor) {\r\n    List<String> storesToBackup = storageConfig.getStoresWithBackupFactory(BlobStoreStateBackendFactory.class.getName());\r\n    List<String> storesToRestore = storageConfig.getStoresWithRestoreFactory(BlobStoreStateBackendFactory.class.getName());\r\n    List<CompletionStage<Void>> storeDeletionFutures = new ArrayList<>();\r\n    initialStoreSnapshotIndexes.forEach((storeName, scmAndSnapshotIndex) -> {\r\n        if (!storesToBackup.contains(storeName) && !storesToRestore.contains(storeName)) {\r\n            LOG.info(\"Removing task: {} store: {} from blob store. It is either no longer used, \" + \"or is no longer configured to be backed up or restored with blob store.\", taskName, storeName);\r\n            Metadata requestMetadata = new Metadata(Metadata.SNAPSHOT_INDEX_PAYLOAD_PATH, Optional.empty(), jobName, jobId, taskName, storeName);\r\n            storeDeletionFutures.add(blobStoreUtil.cleanSnapshotIndex(scmAndSnapshotIndex.getLeft(), scmAndSnapshotIndex.getRight(), requestMetadata));\r\n        }\r\n    });\r\n    FutureUtil.allOf(storeDeletionFutures).join();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreRestoreManager.java",
  "methodName" : "restoreStores",
  "sourceCode" : "/**\r\n * Restores all eligible stores in the task.\r\n */\r\n@VisibleForTesting\r\nstatic CompletableFuture<Void> restoreStores(String jobName, String jobId, TaskName taskName, Set<String> storesToRestore, Map<String, Pair<String, SnapshotIndex>> prevStoreSnapshotIndexes, File loggedBaseDir, StorageConfig storageConfig, BlobStoreRestoreManagerMetrics metrics, StorageManagerUtil storageManagerUtil, BlobStoreUtil blobStoreUtil, DirDiffUtil dirDiffUtil, ExecutorService executor, boolean getDeleted, boolean compareFileOwners) {\r\n    long restoreStartTime = System.nanoTime();\r\n    List<CompletionStage<Void>> restoreFutures = new ArrayList<>();\r\n    LOG.debug(\"Starting restore for task: {} stores: {}\", taskName, storesToRestore);\r\n    storesToRestore.forEach(storeName -> {\r\n        if (!prevStoreSnapshotIndexes.containsKey(storeName)) {\r\n            LOG.info(\"No checkpointed snapshot index found for task: {} store: {}. Skipping restore.\", taskName, storeName);\r\n            // TODO HIGH shesharm what should we do with the local state already present on disk, if any?\r\n            // E.g. this will be the case if user changes a store from changelog based backup and restore to\r\n            // blob store based backup and restore, both at the same time.\r\n            return;\r\n        }\r\n        Pair<String, SnapshotIndex> scmAndSnapshotIndex = prevStoreSnapshotIndexes.get(storeName);\r\n        long storeRestoreStartTime = System.nanoTime();\r\n        SnapshotIndex snapshotIndex = scmAndSnapshotIndex.getRight();\r\n        DirIndex dirIndex = snapshotIndex.getDirIndex();\r\n        DirIndex.Stats stats = DirIndex.getStats(dirIndex);\r\n        metrics.filesToRestore.getValue().addAndGet(stats.filesPresent);\r\n        metrics.bytesToRestore.getValue().addAndGet(stats.bytesPresent);\r\n        metrics.filesRemaining.getValue().addAndGet(stats.filesPresent);\r\n        metrics.bytesRemaining.getValue().addAndGet(stats.bytesPresent);\r\n        CheckpointId checkpointId = snapshotIndex.getSnapshotMetadata().getCheckpointId();\r\n        File storeDir = storageManagerUtil.getTaskStoreDir(loggedBaseDir, storeName, taskName, TaskMode.Active);\r\n        Path storeCheckpointDir = Paths.get(storageManagerUtil.getStoreCheckpointDir(storeDir, checkpointId));\r\n        LOG.trace(\"Got task: {} store: {} local store directory: {} and local store checkpoint directory: {}\", taskName, storeName, storeDir, storeCheckpointDir);\r\n        // we always delete the store dir to preserve transactional state guarantees.\r\n        try {\r\n            LOG.debug(\"Deleting local store directory: {}. Will be restored from local store checkpoint directory \" + \"or remote snapshot.\", storeDir);\r\n            if (storeDir.exists() && storeDir.isDirectory()) {\r\n                PathUtils.deleteDirectory(storeDir.toPath());\r\n            }\r\n        } catch (IOException e) {\r\n            throw new SamzaException(String.format(\"Error deleting store directory: %s\", storeDir), e);\r\n        }\r\n        // Restore from blob store if:\r\n        // 1. shouldRestore() returns true - there is a diff between local and remote snapshot.\r\n        // 2. getDeleted is set - Some blobs in the blob store were deleted incorrectly (SAMZA-2787). Download/restore\r\n        //                             everything locally ignoring the diff. This will be backed up afresh by\r\n        //                             ContainerStorageManager recovery path.\r\n        boolean shouldRestore = getDeleted || shouldRestore(taskName.getTaskName(), storeName, dirIndex, storeCheckpointDir, storageConfig, dirDiffUtil);\r\n        if (shouldRestore) {\r\n            // restore the store from the remote blob store\r\n            // delete all store checkpoint directories. if we only delete the store directory and don't\r\n            // delete the checkpoint directories, the store size on disk will grow to 2x after restore\r\n            // until the first commit is completed and older checkpoint dirs are deleted. This is\r\n            // because the hard-linked checkpoint dir files will no longer be de-duped with the\r\n            // now-deleted main store directory contents and will take up additional space of their\r\n            // own during the restore.\r\n            deleteCheckpointDirs(taskName, storeName, loggedBaseDir, storageManagerUtil);\r\n            metrics.storePreRestoreNs.get(storeName).set(System.nanoTime() - storeRestoreStartTime);\r\n            enqueueRestore(jobName, jobId, taskName.toString(), storeName, storeDir, dirIndex, storeRestoreStartTime, restoreFutures, blobStoreUtil, dirDiffUtil, metrics, executor, getDeleted, compareFileOwners);\r\n        } else {\r\n            LOG.debug(\"Renaming store checkpoint directory: {} to store directory: {} since its contents are identical \" + \"to the remote snapshot.\", storeCheckpointDir, storeDir);\r\n            // atomically rename the checkpoint dir to the store dir\r\n            new FileUtil().move(storeCheckpointDir.toFile(), storeDir);\r\n            // delete any other checkpoint dirs.\r\n            deleteCheckpointDirs(taskName, storeName, loggedBaseDir, storageManagerUtil);\r\n        }\r\n    });\r\n    // wait for all restores to finish\r\n    return FutureUtil.allOf(restoreFutures).whenComplete((res, ex) -> {\r\n        LOG.info(\"Restore completed for task: {} stores\", taskName);\r\n        metrics.restoreNs.set(System.nanoTime() - restoreStartTime);\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreRestoreManager.java",
  "methodName" : "shouldRestore",
  "sourceCode" : "/**\r\n * Determines if the store needs to be restored from remote snapshot based on local and remote state.\r\n */\r\n@VisibleForTesting\r\nstatic boolean shouldRestore(String taskName, String storeName, DirIndex dirIndex, Path storeCheckpointDir, StorageConfig storageConfig, DirDiffUtil dirDiffUtil) {\r\n    // if a store checkpoint directory exists for the last successful task checkpoint, try to use it.\r\n    boolean restoreStore;\r\n    if (Files.exists(storeCheckpointDir)) {\r\n        if (storageConfig.cleanLoggedStoreDirsOnStart(storeName)) {\r\n            LOG.debug(\"Restoring task: {} store: {} from remote snapshot since the store is configured to be \" + \"restored on each restart.\", taskName, storeName);\r\n            restoreStore = true;\r\n        } else if (dirDiffUtil.areSameDir(FILES_TO_IGNORE, false, true).test(storeCheckpointDir.toFile(), dirIndex)) {\r\n            // no restore required for this store.\r\n            restoreStore = false;\r\n        } else {\r\n            // we don't optimize for the case when the local host doesn't contain the most recent store checkpoint\r\n            // directory but contains an older checkpoint directory which could have partial overlap with the remote\r\n            // snapshot. we also don't try to optimize for any edge cases where the most recent checkpoint directory\r\n            // contents could be partially different than the remote store (afaik, there is no known valid scenario\r\n            // where this could happen right now, except for the offset file handling above).\r\n            // it's simpler and fast enough for now to restore the entire store instead.\r\n            LOG.error(\"Local store checkpoint directory: {} contents are not the same as the remote snapshot. \" + \"Queuing for restore from remote snapshot.\", storeCheckpointDir);\r\n            restoreStore = true;\r\n        }\r\n    } else {\r\n        // did not find last checkpoint dir, restore the store from the remote blob store\r\n        LOG.debug(\"No local store checkpoint directory found at: {}. \" + \"Queuing for restore from remote snapshot.\", storeCheckpointDir);\r\n        restoreStore = true;\r\n    }\r\n    return restoreStore;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\BlobStoreRestoreManager.java",
  "methodName" : "enqueueRestore",
  "sourceCode" : "/**\r\n * Starts the restore for the store, enqueuing all restore-completion futures into {@param restoreFutures}.\r\n */\r\n@VisibleForTesting\r\nstatic void enqueueRestore(String jobName, String jobId, String taskName, String storeName, File storeDir, DirIndex dirIndex, long storeRestoreStartTime, List<CompletionStage<Void>> restoreFutures, BlobStoreUtil blobStoreUtil, DirDiffUtil dirDiffUtil, BlobStoreRestoreManagerMetrics metrics, ExecutorService executor, boolean getDeleted, boolean compareFileOwners) {\r\n    Metadata requestMetadata = new Metadata(storeDir.getAbsolutePath(), Optional.empty(), jobName, jobId, taskName, storeName);\r\n    CompletableFuture<Void> restoreFuture = blobStoreUtil.restoreDir(storeDir, dirIndex, requestMetadata, getDeleted).thenRunAsync(() -> {\r\n        metrics.storeRestoreNs.get(storeName).set(System.nanoTime() - storeRestoreStartTime);\r\n        long postRestoreStartTime = System.nanoTime();\r\n        LOG.trace(\"Comparing restored store directory: {} and remote directory to verify restore with compareFileOwners set to: {}\", storeDir, compareFileOwners);\r\n        if (!dirDiffUtil.areSameDir(FILES_TO_IGNORE, true, compareFileOwners).test(storeDir, dirIndex)) {\r\n            metrics.storePostRestoreNs.get(storeName).set(System.nanoTime() - postRestoreStartTime);\r\n            throw new SamzaException(String.format(\"Restored store directory: %s contents \" + \"are not the same as the remote snapshot.\", storeDir.getAbsolutePath()));\r\n        } else {\r\n            metrics.storePostRestoreNs.get(storeName).set(System.nanoTime() - postRestoreStartTime);\r\n            LOG.info(\"Restore from remote snapshot completed for store: {} with getDeleted set to {}\", storeDir, getDeleted);\r\n        }\r\n    }, executor);\r\n    restoreFutures.add(restoreFuture);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\util\\BlobStoreUtil.java",
  "methodName" : "getFile",
  "sourceCode" : "/**\r\n * Gets a file from the blob store.\r\n * @param fileBlobs List of {@link FileBlob}s that constitute this file.\r\n * @param fileToRestore File pointing to the local path where the file will be restored.\r\n * @param requestMetadata {@link Metadata} associated with this request\r\n * @param getDeleted Flag that indicates whether to try to get Deleted (but not yet compacted) files.\r\n * @return a future that completes when the file is downloaded and written or if an exception occurs.\r\n */\r\n@VisibleForTesting\r\nCompletableFuture<Void> getFile(List<FileBlob> fileBlobs, File fileToRestore, Metadata requestMetadata, boolean getDeleted) {\r\n    FileOutputStream outputStream = null;\r\n    try {\r\n        long restoreFileStartTime = System.nanoTime();\r\n        if (fileToRestore.exists()) {\r\n            // delete the file if it already exists, e.g. from a previous retry.\r\n            Files.delete(fileToRestore.toPath());\r\n        }\r\n        outputStream = new FileOutputStream(fileToRestore);\r\n        final FileOutputStream finalOutputStream = outputStream;\r\n        // TODO HIGH shesharm add integration tests to ensure empty files and directories are handled correctly E2E.\r\n        // create file for 0 byte files (fileIndex entry but no fileBlobs).\r\n        fileToRestore.createNewFile();\r\n        // create a copy to ensure list being sorted is mutable.\r\n        List<FileBlob> fileBlobsCopy = new ArrayList<>(fileBlobs);\r\n        // sort by offset.\r\n        fileBlobsCopy.sort(Comparator.comparingInt(FileBlob::getOffset));\r\n        // chain the futures such that write to file for blobs is sequential.\r\n        // can be optimized to write concurrently to the file later.\r\n        CompletableFuture<Void> resultFuture = CompletableFuture.completedFuture(null);\r\n        for (FileBlob fileBlob : fileBlobsCopy) {\r\n            resultFuture = resultFuture.thenComposeAsync(v -> {\r\n                LOG.debug(\"Starting restore for file: {} with blob id: {} at offset: {} with getDeleted set to: {}\", fileToRestore, fileBlob.getBlobId(), fileBlob.getOffset(), getDeleted);\r\n                return blobStoreManager.get(fileBlob.getBlobId(), finalOutputStream, requestMetadata, getDeleted);\r\n            }, executor);\r\n        }\r\n        resultFuture = resultFuture.thenRunAsync(() -> {\r\n            LOG.debug(\"Finished restore for file: {}. Closing output stream.\", fileToRestore);\r\n            try {\r\n                // flush the file contents to disk\r\n                finalOutputStream.getFD().sync();\r\n                finalOutputStream.close();\r\n            } catch (Exception e) {\r\n                throw new SamzaException(String.format(\"Error closing output stream for file: %s\", fileToRestore.getAbsolutePath()), e);\r\n            }\r\n        }, executor);\r\n        resultFuture.whenComplete((res, ex) -> {\r\n            if (restoreMetrics != null) {\r\n                restoreMetrics.avgFileRestoreNs.update(System.nanoTime() - restoreFileStartTime);\r\n                long fileSize = requestMetadata.getPayloadSize();\r\n                restoreMetrics.restoreRate.inc(fileSize);\r\n                restoreMetrics.filesRestored.getValue().addAndGet(1);\r\n                restoreMetrics.bytesRestored.getValue().addAndGet(fileSize);\r\n                restoreMetrics.filesRemaining.getValue().addAndGet(-1);\r\n                restoreMetrics.bytesRemaining.getValue().addAndGet(-1 * fileSize);\r\n            }\r\n        });\r\n        return resultFuture;\r\n    } catch (Exception exception) {\r\n        try {\r\n            if (outputStream != null) {\r\n                outputStream.close();\r\n            }\r\n        } catch (Exception err) {\r\n            LOG.error(\"Error closing output stream for file: {}\", fileToRestore.getAbsolutePath(), err);\r\n        }\r\n        throw new SamzaException(String.format(\"Error restoring file: %s in path: %s\", fileToRestore.getName(), requestMetadata.getPayloadPath()), exception);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\blobstore\\util\\BlobStoreUtil.java",
  "methodName" : "putFile",
  "sourceCode" : "/**\r\n * Upload a File to blob store.\r\n * @param file File to upload to blob store.\r\n * @return A future containing the {@link FileIndex} for the uploaded file.\r\n */\r\n@VisibleForTesting\r\npublic CompletableFuture<FileIndex> putFile(File file, SnapshotMetadata snapshotMetadata) {\r\n    if (file == null || !file.isFile()) {\r\n        String message = file != null ? \"Dir or Symbolic link\" : \"null\";\r\n        throw new SamzaException(String.format(\"Required a non-null parameter of type file, provided: %s\", message));\r\n    }\r\n    long putFileStartTime = System.nanoTime();\r\n    String opName = \"putFile: \" + file.getAbsolutePath();\r\n    Supplier<CompletionStage<FileIndex>> fileUploadAction = () -> {\r\n        LOG.debug(\"Putting file: {} to blob store.\", file.getPath());\r\n        CompletableFuture<FileIndex> fileBlobFuture;\r\n        CheckedInputStream inputStream = null;\r\n        try {\r\n            // TODO HIGH shesharm maybe use the more efficient CRC32C / PureJavaCRC32 impl\r\n            inputStream = new CheckedInputStream(new FileInputStream(file), new CRC32());\r\n            CheckedInputStream finalInputStream = inputStream;\r\n            FileMetadata fileMetadata = FileMetadata.fromFile(file);\r\n            if (backupMetrics != null) {\r\n                backupMetrics.avgFileSizeBytes.update(fileMetadata.getSize());\r\n            }\r\n            Metadata metadata = new Metadata(file.getAbsolutePath(), Optional.of(fileMetadata.getSize()), snapshotMetadata.getJobName(), snapshotMetadata.getJobId(), snapshotMetadata.getTaskName(), snapshotMetadata.getStoreName());\r\n            fileBlobFuture = blobStoreManager.put(inputStream, metadata).thenApplyAsync(id -> {\r\n                LOG.trace(\"Put complete. Received Blob ID {}. Closing input stream for file: {}.\", id, file.getPath());\r\n                try {\r\n                    finalInputStream.close();\r\n                } catch (Exception e) {\r\n                    throw new SamzaException(String.format(\"Error closing input stream for file: %s\", file.getAbsolutePath()), e);\r\n                }\r\n                LOG.trace(\"Returning new FileIndex for file: {}.\", file.getPath());\r\n                return new FileIndex(file.getName(), Collections.singletonList(new FileBlob(id, 0)), fileMetadata, finalInputStream.getChecksum().getValue());\r\n            }, executor).toCompletableFuture();\r\n        } catch (Exception e) {\r\n            try {\r\n                if (inputStream != null) {\r\n                    inputStream.close();\r\n                }\r\n            } catch (Exception err) {\r\n                LOG.error(\"Error closing input stream for file: {}\", file.getName(), err);\r\n            }\r\n            LOG.error(\"Error putting file: {}\", file.getName(), e);\r\n            throw new SamzaException(String.format(\"Error putting file %s\", file.getAbsolutePath()), e);\r\n        }\r\n        return fileBlobFuture;\r\n    };\r\n    return FutureUtil.executeAsyncWithRetries(opName, fileUploadAction, isCauseNonRetriable(), executor, retryPolicyConfig).whenComplete((res, ex) -> {\r\n        if (backupMetrics != null) {\r\n            backupMetrics.avgFileUploadNs.update(System.nanoTime() - putFileStartTime);\r\n            long fileSize = file.length();\r\n            backupMetrics.uploadRate.inc(fileSize);\r\n            backupMetrics.filesUploaded.getValue().addAndGet(1);\r\n            backupMetrics.bytesUploaded.getValue().addAndGet(fileSize);\r\n            backupMetrics.filesRemaining.getValue().addAndGet(-1);\r\n            backupMetrics.bytesRemaining.getValue().addAndGet(-1 * fileSize);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\KafkaChangelogStateBackendFactory.java",
  "methodName" : "getStreamCache",
  "sourceCode" : "/**\r\n * Shared cache across all KafkaRestoreManagers for the Kafka topic\r\n *\r\n * @param admins system admins used the fetch the stream metadata\r\n * @param clock for cache invalidation\r\n * @return StreamMetadataCache containing the stream metadata\r\n */\r\n@VisibleForTesting\r\nStreamMetadataCache getStreamCache(SystemAdmins admins, Clock clock) {\r\n    if (streamCache == null) {\r\n        streamCache = new StreamMetadataCache(admins, 5000, clock);\r\n    }\r\n    return streamCache;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\KafkaChangelogStateBackendFactory.java",
  "methodName" : "filterStandbySystemStreams",
  "sourceCode" : "@VisibleForTesting\r\nMap<String, SystemStream> filterStandbySystemStreams(Map<String, SystemStream> changelogSystemStreams, ContainerModel containerModel) {\r\n    Map<SystemStreamPartition, String> changelogSSPToStore = new HashMap<>();\r\n    changelogSystemStreams.forEach((storeName, systemStream) -> containerModel.getTasks().forEach((taskName, taskModel) -> changelogSSPToStore.put(new SystemStreamPartition(systemStream, taskModel.getChangelogPartition()), storeName)));\r\n    Set<TaskModel> standbyTaskModels = containerModel.getTasks().values().stream().filter(taskModel -> taskModel.getTaskMode().equals(TaskMode.Standby)).collect(Collectors.toSet());\r\n    // remove all standby task changelog ssps\r\n    standbyTaskModels.forEach((taskModel) -> {\r\n        changelogSystemStreams.forEach((storeName, systemStream) -> {\r\n            SystemStreamPartition ssp = new SystemStreamPartition(systemStream, taskModel.getChangelogPartition());\r\n            changelogSSPToStore.remove(ssp);\r\n        });\r\n    });\r\n    // changelogSystemStreams correspond only to active tasks (since those of standby-tasks moved to sideInputs above)\r\n    return MapUtils.invertMap(changelogSSPToStore).entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, x -> x.getValue().getSystemStream()));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TaskSideInputHandler.java",
  "methodName" : "getStartingOffsets",
  "sourceCode" : "/**\r\n * Gets the starting offsets for the {@link SystemStreamPartition}s belonging to all the side input stores. See doc\r\n * of {@link StorageManagerUtil#getStartingOffset} for how file offsets and oldest offsets for each SSP are\r\n * reconciled.\r\n *\r\n * @param fileOffsets offsets from the local offset file\r\n * @param oldestOffsets oldest offsets from the source\r\n * @return a {@link Map} of {@link SystemStreamPartition} to offset\r\n */\r\n@VisibleForTesting\r\nMap<SystemStreamPartition, String> getStartingOffsets(Map<SystemStreamPartition, String> fileOffsets, Map<SystemStreamPartition, String> oldestOffsets) {\r\n    Map<SystemStreamPartition, String> startingOffsets = new HashMap<>();\r\n    this.sspToStores.keySet().forEach(ssp -> {\r\n        String fileOffset = fileOffsets.get(ssp);\r\n        String oldestOffset = oldestOffsets.get(ssp);\r\n        startingOffsets.put(ssp, this.storageManagerUtil.getStartingOffset(ssp, this.systemAdmins.getSystemAdmin(ssp.getSystem()), fileOffset, oldestOffset));\r\n    });\r\n    return startingOffsets;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TaskSideInputHandler.java",
  "methodName" : "getOldestOffsets",
  "sourceCode" : "/**\r\n * Gets the oldest offset for the {@link SystemStreamPartition}s associated with all the store side inputs.\r\n *   1. Groups the list of the SSPs based on system stream\r\n *   2. Fetches the {@link SystemStreamMetadata} from {@link StreamMetadataCache}\r\n *   3. Fetches the partition metadata for each system stream and fetch the corresponding partition metadata\r\n *      and populates the oldest offset for SSPs belonging to the system stream.\r\n *\r\n * @return a {@link Map} of {@link SystemStreamPartition} to their oldest offset. If partitionMetadata could not be\r\n * obtained for any {@link SystemStreamPartition} the offset for it is populated as null.\r\n */\r\n@VisibleForTesting\r\nMap<SystemStreamPartition, String> getOldestOffsets() {\r\n    Map<SystemStreamPartition, String> oldestOffsets = new HashMap<>();\r\n    // Step 1\r\n    Map<SystemStream, List<SystemStreamPartition>> systemStreamToSsp = this.sspToStores.keySet().stream().collect(Collectors.groupingBy(SystemStreamPartition::getSystemStream));\r\n    // Step 2\r\n    Map<SystemStream, SystemStreamMetadata> metadata = JavaConverters.mapAsJavaMapConverter(this.streamMetadataCache.getStreamMetadata(JavaConverters.asScalaSetConverter(systemStreamToSsp.keySet()).asScala().toSet(), false)).asJava();\r\n    // Step 3\r\n    metadata.forEach((systemStream, systemStreamMetadata) -> {\r\n        // get the partition metadata for each system stream\r\n        Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = systemStreamMetadata.getSystemStreamPartitionMetadata();\r\n        // For SSPs belonging to the system stream, use the partition metadata to get the oldest offset\r\n        // if partitionMetadata was not obtained for any SSP, populate oldest-offset as null\r\n        // Because of https://bugs.openjdk.java.net/browse/JDK-8148463 using lambda will NPE when getOldestOffset() is null\r\n        for (SystemStreamPartition ssp : systemStreamToSsp.get(systemStream)) {\r\n            oldestOffsets.put(ssp, partitionMetadata.get(ssp.getPartition()).getOldestOffset());\r\n        }\r\n    });\r\n    return oldestOffsets;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TaskSideInputStorageManager.java",
  "methodName" : "getStoreLocation",
  "sourceCode" : "@VisibleForTesting\r\nFile getStoreLocation(String storeName) {\r\n    return storageManagerUtil.getTaskStoreDir(storeBaseDir, storeName, taskName, taskMode);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TaskStorageCommitManager.java",
  "methodName" : "writeChangelogOffsetFiles",
  "sourceCode" : "/**\r\n * Writes the newest changelog ssp offset for each logged and persistent store to the OFFSET file in the current\r\n * store directory (for allowing rollbacks). If the Kafka transactional backup manager is enabled, also writes to\r\n * the store checkpoint directory.\r\n *\r\n * These files are used during container startup to ensure transactional state, and to determine whether the\r\n * there is any new information in the changelog that is not reflected in the on-disk copy of the store.\r\n * If there is any delta, it is replayed from the changelog. E.g. this can happen if the job was run on this host,\r\n * then another host, and then back to this host.\r\n */\r\n@VisibleForTesting\r\nvoid writeChangelogOffsetFiles(Map<SystemStreamPartition, String> checkpointOffsets) {\r\n    if (storageEngines == null) {\r\n        throw new SamzaException(String.format(\"Storage engines are not initialized and writeChangelogOffsetFiles not be written for task %s\", taskName));\r\n    }\r\n    storeChangelogs.forEach((storeName, systemStream) -> {\r\n        SystemStreamPartition changelogSSP = new SystemStreamPartition(systemStream.getSystem(), systemStream.getStream(), taskChangelogPartition);\r\n        // Only write if the store is durable and persisted to disk\r\n        if (checkpointOffsets.containsKey(changelogSSP) && storageEngines.containsKey(storeName) && storageEngines.get(storeName).getStoreProperties().isDurableStore() && storageEngines.get(storeName).getStoreProperties().isPersistedToDisk()) {\r\n            LOG.debug(\"Writing changelog offset for taskName {} store {} changelog {}.\", taskName, storeName, systemStream);\r\n            File currentStoreDir = storageManagerUtil.getTaskStoreDir(durableStoreBaseDir, storeName, taskName, TaskMode.Active);\r\n            try {\r\n                KafkaChangelogSSPOffset kafkaChangelogSSPOffset = KafkaChangelogSSPOffset.fromString(checkpointOffsets.get(changelogSSP));\r\n                // Write offsets to file system if it is non-null\r\n                String newestOffset = kafkaChangelogSSPOffset.getChangelogOffset();\r\n                if (newestOffset != null) {\r\n                    // Write changelog SSP offset to the OFFSET files in the task store directory\r\n                    writeChangelogOffsetFile(storeName, changelogSSP, newestOffset, currentStoreDir);\r\n                    // Write changelog SSP offset to the OFFSET files in the store checkpoint directory\r\n                    File checkpointDir = Paths.get(storageManagerUtil.getStoreCheckpointDir(currentStoreDir, kafkaChangelogSSPOffset.getCheckpointId())).toFile();\r\n                    writeChangelogOffsetFile(storeName, changelogSSP, newestOffset, checkpointDir);\r\n                } else {\r\n                    // If newestOffset is null, then it means the changelog ssp is (or has become) empty. This could be\r\n                    // either because the changelog topic was newly added, repartitioned, or manually deleted and recreated.\r\n                    // No need to persist the offset file.\r\n                    LOG.debug(\"Deleting OFFSET file for taskName {} store {} changelog ssp {} since the newestOffset is null.\", taskName, storeName, changelogSSP);\r\n                    storageManagerUtil.deleteOffsetFile(currentStoreDir);\r\n                }\r\n            } catch (IOException e) {\r\n                throw new SamzaException(String.format(\"Error storing offset for taskName %s store %s changelog %s.\", taskName, storeName, systemStream), e);\r\n            }\r\n        }\r\n    });\r\n    LOG.debug(\"Done writing OFFSET files for logged persistent key value stores for task {}\", taskName);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TaskStorageCommitManager.java",
  "methodName" : "writeChangelogOffsetFile",
  "sourceCode" : "@VisibleForTesting\r\nvoid writeChangelogOffsetFile(String storeName, SystemStreamPartition ssp, String newestOffset, File writeDirectory) throws IOException {\r\n    LOG.debug(\"Storing newest offset {} for taskName {} store {} changelog ssp {} in OFFSET file at path: {}.\", newestOffset, taskName, storeName, ssp, writeDirectory);\r\n    storageManagerUtil.writeOffsetFile(writeDirectory, Collections.singletonMap(ssp, newestOffset), false);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TransactionalStateTaskRestoreManager.java",
  "methodName" : "getCurrentChangelogOffsets",
  "sourceCode" : "/**\r\n * Get offset metadata for each changelog SSP for this task. A task may have multiple changelog streams\r\n * (e.g., for different stores), but will have the same partition for all of them.\r\n */\r\n@VisibleForTesting\r\nstatic Map<SystemStreamPartition, SystemStreamPartitionMetadata> getCurrentChangelogOffsets(TaskModel taskModel, Map<String, SystemStream> storeChangelogs, SSPMetadataCache sspMetadataCache) {\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> changelogOffsets = new HashMap<>();\r\n    Partition changelogPartition = taskModel.getChangelogPartition();\r\n    for (Map.Entry<String, SystemStream> storeChangelog : storeChangelogs.entrySet()) {\r\n        SystemStream changelog = storeChangelog.getValue();\r\n        SystemStreamPartition changelogSSP = new SystemStreamPartition(changelog.getSystem(), changelog.getStream(), changelogPartition);\r\n        SystemStreamPartitionMetadata metadata = sspMetadataCache.getMetadata(changelogSSP);\r\n        changelogOffsets.put(changelogSSP, metadata);\r\n    }\r\n    LOG.info(\"Got current changelog offsets for taskName: {} as: {}\", taskModel.getTaskName(), changelogOffsets);\r\n    return changelogOffsets;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TransactionalStateTaskRestoreManager.java",
  "methodName" : "getStoreActions",
  "sourceCode" : "/**\r\n * Marks each persistent but non-logged store for deletion.\r\n *\r\n * For each logged store, based on the current, checkpointed and local changelog offsets,\r\n * 1. decides which directories (current and checkpoints) to delete for persistent stores.\r\n * 2. decides which directories (checkpoints) to retain for persistent stores.\r\n * 3. decides which stores (persistent or not) need to be restored, and the beginning and end offsets for the restore.\r\n *\r\n * When this method returns, in StoreActions,\r\n * 1. all persistent store current directories will be present in storeDirsToDelete\r\n * 2. each persistent store checkpoint directory will be present in either storeDirToRetain or storeDirsToDelete.\r\n * 3. there will be at most one storeDirToRetain per persistent store, which will be a checkpoint directory.\r\n * 4. any stores (persistent or not) that need to be restored from changelogs will be present in\r\n *    storesToRestore with appropriate offsets.\r\n */\r\n@VisibleForTesting\r\nstatic StoreActions getStoreActions(TaskModel taskModel, Map<String, StorageEngine> storeEngines, Map<String, SystemStream> storeChangelogs, Map<String, KafkaStateCheckpointMarker> kafkaStateCheckpointMarkers, CheckpointId checkpointId, Map<SystemStreamPartition, SystemStreamPartitionMetadata> currentChangelogOffsets, SystemAdmins systemAdmins, StorageManagerUtil storageManagerUtil, File loggedStoreBaseDirectory, File nonLoggedStoreBaseDirectory, Config config, Clock clock) {\r\n    TaskName taskName = taskModel.getTaskName();\r\n    TaskMode taskMode = taskModel.getTaskMode();\r\n    Map<String, File> storeDirToRetain = new HashMap<>();\r\n    ListMultimap<String, File> storeDirsToDelete = ArrayListMultimap.create();\r\n    Map<String, RestoreOffsets> storesToRestore = new HashMap<>();\r\n    storeEngines.forEach((storeName, storageEngine) -> {\r\n        // do nothing if store is non persistent and not logged (e.g. in memory cache only)\r\n        if (!storageEngine.getStoreProperties().isPersistedToDisk() && !storageEngine.getStoreProperties().isLoggedStore()) {\r\n            return;\r\n        }\r\n        // persistent but non-logged stores are always deleted\r\n        if (storageEngine.getStoreProperties().isPersistedToDisk() && !storageEngine.getStoreProperties().isLoggedStore()) {\r\n            File currentDir = storageManagerUtil.getTaskStoreDir(nonLoggedStoreBaseDirectory, storeName, taskName, taskMode);\r\n            LOG.info(\"Marking current directory: {} for store: {} in task: {} for deletion since it is not a logged store.\", currentDir, storeName, taskName);\r\n            storeDirsToDelete.put(storeName, currentDir);\r\n            // persistent but non-logged stores should not have checkpoint dirs\r\n            return;\r\n        }\r\n        // get the oldest and newest current changelog SSP offsets as well as the checkpointed changelog SSP offset\r\n        SystemStream changelog = storeChangelogs.get(storeName);\r\n        SystemStreamPartition changelogSSP = new SystemStreamPartition(changelog, taskModel.getChangelogPartition());\r\n        SystemAdmin admin = systemAdmins.getSystemAdmin(changelogSSP.getSystem());\r\n        SystemStreamPartitionMetadata changelogSSPMetadata = currentChangelogOffsets.get(changelogSSP);\r\n        String oldestOffset = changelogSSPMetadata.getOldestOffset();\r\n        String newestOffset = changelogSSPMetadata.getNewestOffset();\r\n        // can be null if no message, or message has null offset\r\n        String checkpointedOffset = null;\r\n        if (kafkaStateCheckpointMarkers.containsKey(storeName) && StringUtils.isNotBlank(kafkaStateCheckpointMarkers.get(storeName).getChangelogOffset())) {\r\n            checkpointedOffset = kafkaStateCheckpointMarkers.get(storeName).getChangelogOffset();\r\n        }\r\n        long timeSinceLastCheckpointInMs = checkpointId == null ? Long.MAX_VALUE : System.currentTimeMillis() - checkpointId.getMillis();\r\n        // if the clean.store.start config is set, delete current and checkpoint dirs, restore from oldest offset to checkpointed\r\n        if (storageEngine.getStoreProperties().isPersistedToDisk() && new StorageConfig(config).cleanLoggedStoreDirsOnStart(storeName)) {\r\n            File currentDir = storageManagerUtil.getTaskStoreDir(loggedStoreBaseDirectory, storeName, taskName, taskMode);\r\n            LOG.info(\"Marking current directory: {} for store: {} in task: {} for deletion due to clean.on.container.start config.\", currentDir, storeName, taskName);\r\n            storeDirsToDelete.put(storeName, currentDir);\r\n            storageManagerUtil.getTaskStoreCheckpointDirs(loggedStoreBaseDirectory, storeName, taskName, taskMode).forEach(checkpointDir -> {\r\n                LOG.info(\"Marking checkpoint directory: {} for store: {} in task: {} for deletion due to clean.on.container.start config.\", checkpointDir, storeName, taskName);\r\n                storeDirsToDelete.put(storeName, checkpointDir);\r\n            });\r\n            LOG.info(\"Marking restore offsets for store: {} in task: {} to {}, {} \", storeName, taskName, oldestOffset, checkpointedOffset);\r\n            storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, checkpointedOffset));\r\n            return;\r\n        }\r\n        Optional<File> currentDirOptional;\r\n        Optional<List<File>> checkpointDirsOptional;\r\n        if (!storageEngine.getStoreProperties().isPersistedToDisk()) {\r\n            currentDirOptional = Optional.empty();\r\n            checkpointDirsOptional = Optional.empty();\r\n        } else {\r\n            currentDirOptional = Optional.of(storageManagerUtil.getTaskStoreDir(loggedStoreBaseDirectory, storeName, taskName, taskMode));\r\n            checkpointDirsOptional = Optional.of(storageManagerUtil.getTaskStoreCheckpointDirs(loggedStoreBaseDirectory, storeName, taskName, taskMode));\r\n        }\r\n        LOG.info(\"For store: {} in task: {} got current dir: {}, checkpoint dirs: {}, checkpointed changelog offset: {}\", storeName, taskName, currentDirOptional, checkpointDirsOptional, checkpointedOffset);\r\n        currentDirOptional.ifPresent(currentDir -> {\r\n            LOG.info(\"Marking current directory: {} for store: {} in task: {} for deletion.\", currentDir, storeName, taskName);\r\n            storeDirsToDelete.put(storeName, currentDir);\r\n        });\r\n        if (checkpointedOffset == null && oldestOffset != null) {\r\n            // this can mean that either this is the initial migration for this feature and there are no previously\r\n            // checkpointed changelog offsets, or that this is a new store or changelog topic after the initial migration.\r\n            // if this is the first time migration, it might be desirable to retain existing data.\r\n            // if this is new store or topic, it is possible that the container previously died after writing some data to\r\n            // the changelog but before a commit, so it is desirable to delete the store, not restore anything and\r\n            // trim the changelog\r\n            // since we can't tell the difference b/w the two scenarios by just looking at the store and changelogs,\r\n            // we'll request users to indicate whether to retain existing data using a config flag. this flag should only\r\n            // be set during migrations, and turned off after the first successful commit of the new container (i.e. next\r\n            // deploy). for simplicity, we'll always delete the local store, and restore from changelog if necessary.\r\n            // the former scenario should not be common. the recommended way to opt-in to the transactional state feature\r\n            // is to first upgrade to the latest samza version but keep the transactional state restore config off.\r\n            // this will create the store checkpoint directories and write the changelog offset to the checkpoint, but\r\n            // will not use them during restore. once this is done (i.e. at least one commit after upgrade), the\r\n            // transactional state restore feature can be turned on on subsequent deploys. this code path exists as a\r\n            // fail-safe against clearing changelogs in case users do not follow upgrade instructions and enable the\r\n            // feature directly.\r\n            checkpointDirsOptional.ifPresent(checkpointDirs -> checkpointDirs.forEach(checkpointDir -> {\r\n                LOG.info(\"Marking checkpoint directory: {} for store: {} in task: {} for deletion since checkpointed \" + \"offset is null and oldest offset: {} is not.\", checkpointDir, storeName, taskName, oldestOffset);\r\n                storeDirsToDelete.put(storeName, checkpointDir);\r\n            }));\r\n            if (new TaskConfig(config).getTransactionalStateRetainExistingState()) {\r\n                // mark for restore from (oldest, newest) to recreate local state.\r\n                LOG.warn(\"Checkpointed offset for store: {} in task: {} is null. Since retain existing state is true, \" + \"local state will be fully restored from current changelog contents. \" + \"There is no transactional local state guarantee.\", storeName, taskName);\r\n                storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, newestOffset));\r\n            } else {\r\n                LOG.warn(\"Checkpointed offset for store: {} in task: {} is null. Since retain existing state is false, \" + \"any local state and changelog topic contents will be deleted.\", storeName, taskName);\r\n                // mark for restore from (oldest, null) to trim entire changelog.\r\n                storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, null));\r\n            }\r\n        } else if (// check if the checkpointed offset is out of range of current oldest and newest offsets\r\n        admin.offsetComparator(oldestOffset, checkpointedOffset) > 0 || admin.offsetComparator(checkpointedOffset, newestOffset) > 0) {\r\n            // checkpointed offset is out of range. this could mean that this is a TTL topic and the checkpointed\r\n            // offset was TTLd, or that the changelog topic was manually deleted and then recreated.\r\n            // we cannot guarantee transactional state for TTL stores, so delete everything and do a full restore\r\n            // for local store. if the topic was deleted and recreated, this will have the side effect of\r\n            // clearing the store as well.\r\n            LOG.warn(\"Checkpointed offset: {} for store: {} in task: {} is out of range of oldest: {} or newest: {} offset.\" + \"Deleting existing store and fully restoring from changelog topic from oldest to newest offset. If the topic \" + \"has time-based retention, there is no transactional local state guarantees. If the topic was changed,\" + \"local state will be cleaned up and fully restored to match the new topic contents.\", checkpointedOffset, storeName, taskName, oldestOffset, newestOffset);\r\n            checkpointDirsOptional.ifPresent(checkpointDirs -> checkpointDirs.forEach(checkpointDir -> storeDirsToDelete.put(storeName, checkpointDir)));\r\n            storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, newestOffset));\r\n        } else {\r\n            // happy path. checkpointed offset is in range of current oldest and newest offsets\r\n            if (!checkpointDirsOptional.isPresent()) {\r\n                // non-persistent logged store\r\n                LOG.info(\"Did not find any checkpoint directories for logged (maybe non-persistent) store: {}. Local state \" + \"will be fully restored from current changelog contents.\", storeName);\r\n                storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, checkpointedOffset));\r\n            } else {\r\n                // persistent logged store\r\n                String targetOffset;\r\n                // check checkpoint time against min.compaction.lag.ms. if older, restore from checkpointed offset to newest\r\n                // with no trim. be conservative. allow 10% safety margin to avoid deletions when the downtime is close\r\n                // to min.compaction.lag.ms\r\n                long minCompactionLagMs = new StorageConfig(config).getChangelogMinCompactionLagMs(storeName);\r\n                if (timeSinceLastCheckpointInMs > .9 * minCompactionLagMs) {\r\n                    LOG.warn(\"Checkpointed offset for store: {} in task: {} is: {}. It is in range of oldest: {} and \" + \"newest: {} changelog offset. However, time since last checkpoint is: {}, which is greater than \" + \"0.9 * min.compaction.lag.ms: {} for the changelog topic. Since there is a chance that\" + \"the changelog topic has been compacted, restoring store to the end of the current changelog contents.\" + \"There is no transactional local state guarantee.\", storeName, taskName, checkpointedOffset, oldestOffset, newestOffset, timeSinceLastCheckpointInMs, minCompactionLagMs);\r\n                    targetOffset = newestOffset;\r\n                } else {\r\n                    targetOffset = checkpointedOffset;\r\n                }\r\n                // if there exists a valid store checkpoint directory with oldest offset <= local offset <= target offset,\r\n                // retain it and restore the delta. delete all other checkpoint directories for the store. if more than one such\r\n                // checkpoint directory exists, retain the one with the highest local offset and delete the rest.\r\n                boolean hasValidCheckpointDir = false;\r\n                for (File checkpointDir : checkpointDirsOptional.get()) {\r\n                    if (storageManagerUtil.isLoggedStoreValid(storeName, checkpointDir, config, storeChangelogs, taskModel, clock, storeEngines)) {\r\n                        String localOffset = storageManagerUtil.readOffsetFile(checkpointDir, Collections.singleton(changelogSSP), false).get(changelogSSP);\r\n                        LOG.info(\"Read local offset: {} for store: {} checkpoint dir: {} in task: {}\", localOffset, storeName, checkpointDir, taskName);\r\n                        if (admin.offsetComparator(localOffset, oldestOffset) >= 0 && admin.offsetComparator(localOffset, targetOffset) <= 0 && (storesToRestore.get(storeName) == null || admin.offsetComparator(localOffset, storesToRestore.get(storeName).startingOffset) > 0)) {\r\n                            hasValidCheckpointDir = true;\r\n                            LOG.info(\"Temporarily marking checkpoint dir: {} for store: {} in task: {} for retention. \" + \"May be overridden later.\", checkpointDir, storeName, taskName);\r\n                            storeDirToRetain.put(storeName, checkpointDir);\r\n                            // mark for restore even if local == checkpointed, so that the changelog gets trimmed.\r\n                            LOG.info(\"Temporarily marking store: {} in task: {} for restore from beginning offset: {} to \" + \"ending offset: {}. May be overridden later\", storeName, taskName, localOffset, targetOffset);\r\n                            storesToRestore.put(storeName, new RestoreOffsets(localOffset, targetOffset));\r\n                        }\r\n                    }\r\n                }\r\n                // delete all non-retained checkpoint directories\r\n                for (File checkpointDir : checkpointDirsOptional.get()) {\r\n                    if (storeDirToRetain.get(storeName) == null || !storeDirToRetain.get(storeName).equals(checkpointDir)) {\r\n                        LOG.info(\"Marking checkpoint directory: {} for store: {} in task: {} for deletion since it is not \" + \"marked for retention.\", checkpointDir, storeName, taskName);\r\n                        storeDirsToDelete.put(storeName, checkpointDir);\r\n                    }\r\n                }\r\n                // if the store had not valid checkpoint dirs to retain, restore from changelog\r\n                if (!hasValidCheckpointDir) {\r\n                    storesToRestore.put(storeName, new RestoreOffsets(oldestOffset, targetOffset));\r\n                }\r\n            }\r\n        }\r\n    });\r\n    LOG.info(\"Store directories to be retained in Task: {} are: {}\", taskName, storeDirToRetain);\r\n    LOG.info(\"Store directories to be deleted in Task: {} are: {}\", taskName, storeDirsToDelete);\r\n    LOG.info(\"Stores to be restored in Task: {} are: {}\", taskName, storesToRestore);\r\n    return new StoreActions(storeDirToRetain, storeDirsToDelete, storesToRestore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TransactionalStateTaskRestoreManager.java",
  "methodName" : "setupStoreDirs",
  "sourceCode" : "/**\r\n * For each store for this task,\r\n * a. Deletes current directory if persistent but non-logged store.\r\n * b. Deletes current and checkpoint directories if persistent logged store and directory is marked for deletion\r\n * c. Moves the valid persistent logged store checkpoint directory to current directory if marked for retention.\r\n * d. Creates all missing (i.e. not retained in step c) persistent logged store dirs.\r\n *\r\n * When this method returns,\r\n * a. There will be a empty current dir for each persistent but non-logged store.\r\n * b. There will be a current dir for each persistent logged store. This dir may or may not be empty.\r\n * c. There will be no remaining checkpoint dirs for persistent logged stores.\r\n */\r\n@VisibleForTesting\r\nstatic void setupStoreDirs(TaskModel taskModel, Map<String, StorageEngine> storeEngines, StoreActions storeActions, StorageManagerUtil storageManagerUtil, FileUtil fileUtil, File loggedStoreBaseDirectory, File nonLoggedStoreBaseDirectory) {\r\n    TaskName taskName = taskModel.getTaskName();\r\n    TaskMode taskMode = taskModel.getTaskMode();\r\n    ListMultimap<String, File> storeDirsToDelete = storeActions.storeDirsToDelete;\r\n    Map<String, File> storeDirsToRetain = storeActions.storeDirsToRetain;\r\n    // delete all persistent store directories marked for deletion\r\n    storeDirsToDelete.entries().forEach(entry -> {\r\n        String storeName = entry.getKey();\r\n        File storeDirToDelete = entry.getValue();\r\n        LOG.info(\"Deleting persistent store directory: {} for store: {} in task: {}\", storeDirToDelete, storeName, taskName);\r\n        fileUtil.rm(storeDirToDelete);\r\n    });\r\n    // rename all retained persistent logged store checkpoint directories to current directory\r\n    storeDirsToRetain.forEach((storeName, storeDirToRetain) -> {\r\n        File currentDir = storageManagerUtil.getTaskStoreDir(loggedStoreBaseDirectory, storeName, taskName, taskMode);\r\n        LOG.info(\"Moving logged store checkpoint directory: {} for store: {} in task: {} to current directory: {}\", storeDirsToRetain.toString(), storeName, taskName, currentDir);\r\n        storageManagerUtil.restoreCheckpointFiles(storeDirToRetain, currentDir);\r\n        // do not remove the checkpoint directory yet. in case commit fails and container restarts,\r\n        // we can retry the move. if we delete the checkpoint, the current dir will be deleted as well on\r\n        // restart, and we will have to do a full restore.\r\n    });\r\n    // create any missing (not retained) current directories for persistent stores\r\n    storeEngines.forEach((storeName, storageEngine) -> {\r\n        if (storageEngine.getStoreProperties().isPersistedToDisk()) {\r\n            File currentDir;\r\n            if (storageEngine.getStoreProperties().isLoggedStore()) {\r\n                currentDir = storageManagerUtil.getTaskStoreDir(loggedStoreBaseDirectory, storeName, taskName, taskMode);\r\n            } else {\r\n                currentDir = storageManagerUtil.getTaskStoreDir(nonLoggedStoreBaseDirectory, storeName, taskName, taskMode);\r\n            }\r\n            try {\r\n                if (!fileUtil.exists(currentDir.toPath())) {\r\n                    LOG.info(\"Creating missing persistent store current directory: {} for store: {} in task: {}\", currentDir, storeName, taskName);\r\n                    fileUtil.createDirectories(currentDir.toPath());\r\n                }\r\n            } catch (Exception e) {\r\n                throw new SamzaException(String.format(\"Error setting up current directory for store: %s\", storeName), e);\r\n            }\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\storage\\TransactionalStateTaskRestoreManager.java",
  "methodName" : "registerStartingOffsets",
  "sourceCode" : "/**\r\n * Determines the starting offset for each store changelog SSP that needs to be restored from,\r\n * and registers it with the respective SystemConsumer.\r\n */\r\n@VisibleForTesting\r\nstatic void registerStartingOffsets(TaskModel taskModel, StoreActions storeActions, Map<String, SystemStream> storeChangelogs, SystemAdmins systemAdmins, Map<String, SystemConsumer> storeConsumers, Map<SystemStreamPartition, SystemStreamPartitionMetadata> currentChangelogOffsets) {\r\n    Map<String, RestoreOffsets> storesToRestore = storeActions.storesToRestore;\r\n    // must register at least one SSP with each changelog system consumer otherwise start will throw.\r\n    // hence we register upcoming offset as the dummy offset by default and override it later if necessary.\r\n    // using upcoming offset ensures that no messages are replayed by default.\r\n    storeChangelogs.forEach((storeName, changelog) -> {\r\n        SystemStreamPartition changelogSSP = new SystemStreamPartition(changelog, taskModel.getChangelogPartition());\r\n        SystemConsumer systemConsumer = storeConsumers.get(storeName);\r\n        SystemStreamPartitionMetadata currentOffsets = currentChangelogOffsets.get(changelogSSP);\r\n        String upcomingOffset = currentOffsets.getUpcomingOffset();\r\n        LOG.info(\"Temporarily registering upcoming offset: {} as the starting offest for changelog ssp: {}. \" + \"This might be overridden later for stores that need restoring.\", upcomingOffset, changelogSSP);\r\n        systemConsumer.register(changelogSSP, upcomingOffset);\r\n    });\r\n    // now register the actual starting offset if necessary. system consumer will ensure that the lower of the\r\n    // two registered offsets is used as the starting offset.\r\n    storesToRestore.forEach((storeName, restoreOffsets) -> {\r\n        SystemStream changelog = storeChangelogs.get(storeName);\r\n        SystemStreamPartition changelogSSP = new SystemStreamPartition(changelog, taskModel.getChangelogPartition());\r\n        SystemAdmin systemAdmin = systemAdmins.getSystemAdmin(changelog.getSystem());\r\n        validateRestoreOffsets(restoreOffsets, systemAdmin);\r\n        SystemConsumer systemConsumer = storeConsumers.get(storeName);\r\n        SystemStreamPartitionMetadata currentOffsets = currentChangelogOffsets.get(changelogSSP);\r\n        String oldestOffset = currentOffsets.getOldestOffset();\r\n        // if the starting offset equals oldest offset (e.g. for full restore), start from the oldest offset (inclusive).\r\n        // else, start from the next (upcoming) offset.\r\n        String startingOffset;\r\n        if (systemAdmin.offsetComparator(restoreOffsets.startingOffset, oldestOffset) == 0) {\r\n            startingOffset = oldestOffset;\r\n        } else {\r\n            Map<SystemStreamPartition, String> offsetMap = ImmutableMap.of(changelogSSP, restoreOffsets.startingOffset);\r\n            startingOffset = systemAdmin.getOffsetsAfter(offsetMap).get(changelogSSP);\r\n        }\r\n        LOG.info(\"Registering starting offset: {} for changelog ssp: {}\", startingOffset, changelogSSP);\r\n        systemConsumer.register(changelogSSP, startingOffset);\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\table\\batching\\AsyncBatchingTable.java",
  "methodName" : "createBatchProcessor",
  "sourceCode" : "@VisibleForTesting\r\nvoid createBatchProcessor(HighResolutionClock clock, BatchMetrics batchMetrics) {\r\n    batchProcessor = new BatchProcessor<>(batchMetrics, new TableBatchHandler<>(table), batchProvider, clock, batchTimerExecutorService);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\table\\batching\\AsyncBatchingTable.java",
  "methodName" : "getBatchProcessor",
  "sourceCode" : "@VisibleForTesting\r\nBatchProcessor<K, V, U> getBatchProcessor() {\r\n    return batchProcessor;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\table\\batching\\BatchProcessor.java",
  "methodName" : "size",
  "sourceCode" : "/**\r\n * Get the current number of operations received.\r\n */\r\n@VisibleForTesting\r\nint size() {\r\n    return batch == null ? 0 : batch.size();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\table\\batching\\BatchProcessor.java",
  "methodName" : "getLatestPutUpdateOrDelete",
  "sourceCode" : "/**\r\n * Get the latest Put/Update/Delete operation for the specified key.\r\n */\r\n@VisibleForTesting\r\nOperation<K, V, U> getLatestPutUpdateOrDelete(K key) {\r\n    final Collection<Operation<K, V, U>> operations = batch.getOperations();\r\n    final Iterator<Operation<K, V, U>> iterator = operations.iterator();\r\n    Operation<K, V, U> lastUpdate = null;\r\n    while (iterator.hasNext()) {\r\n        final Operation<K, V, U> operation = iterator.next();\r\n        if ((operation instanceof PutOperation || operation instanceof DeleteOperation || operation instanceof UpdateOperation) && operation.getKey().equals(key)) {\r\n            lastUpdate = operation;\r\n        }\r\n    }\r\n    return lastUpdate;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\task\\StreamOperatorTask.java",
  "methodName" : "setOperatorImplGraph",
  "sourceCode" : "/**\r\n * Package private setter for private var operatorImplGraph to be used in TestStreamOperatorTask tests.\r\n */\r\n@VisibleForTesting\r\nvoid setOperatorImplGraph(OperatorImplGraph operatorImplGraph) {\r\n    this.operatorImplGraph = operatorImplGraph;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "publishJobModelToMetadataStore",
  "sourceCode" : "@VisibleForTesting\r\nvoid publishJobModelToMetadataStore(JobModel jobModel, String nextJMVersion) {\r\n    JobModelUtil.writeJobModel(jobModel, nextJMVersion, jobModelMetadataStore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "readJobModelFromMetadataStore",
  "sourceCode" : "@VisibleForTesting\r\nJobModel readJobModelFromMetadataStore(String zkJobModelVersion) {\r\n    return JobModelUtil.readJobModel(zkJobModelVersion, jobModelMetadataStore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "loadMetadataResources",
  "sourceCode" : "/**\r\n * Stores the configuration of the job in the coordinator stream.\r\n */\r\n@VisibleForTesting\r\nvoid loadMetadataResources(JobModel jobModel) {\r\n    try {\r\n        MetadataResourceUtil metadataResourceUtil = createMetadataResourceUtil(jobModel, config);\r\n        metadataResourceUtil.createResources();\r\n        if (coordinatorStreamStore != null) {\r\n            // TODO: SAMZA-2273 - publish configs async\r\n            CoordinatorStreamValueSerde jsonSerde = new CoordinatorStreamValueSerde(SetConfig.TYPE);\r\n            NamespaceAwareCoordinatorStreamStore configStore = new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetConfig.TYPE);\r\n            for (Map.Entry<String, String> entry : config.entrySet()) {\r\n                byte[] serializedValue = jsonSerde.toBytes(entry.getValue());\r\n                configStore.put(entry.getKey(), serializedValue);\r\n            }\r\n            configStore.flush();\r\n            if (new JobConfig(config).getStartpointEnabled()) {\r\n                // fan out the startpoints\r\n                StartpointManager startpointManager = createStartpointManager();\r\n                startpointManager.start();\r\n                try {\r\n                    startpointManager.fanOut(JobModelUtil.getTaskToSystemStreamPartitions(jobModel));\r\n                } finally {\r\n                    startpointManager.stop();\r\n                }\r\n            }\r\n        } else {\r\n            LOG.warn(\"No metadata store registered to this job coordinator. Config not written to the metadata store and no Startpoints fan out.\");\r\n        }\r\n    } catch (IOException ex) {\r\n        throw new SamzaException(String.format(\"IO exception while loading metadata resources.\"), ex);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "createMetadataResourceUtil",
  "sourceCode" : "@VisibleForTesting\r\nMetadataResourceUtil createMetadataResourceUtil(JobModel jobModel, Config config) {\r\n    return new MetadataResourceUtil(jobModel, metrics.getMetricsRegistry(), config);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "generateNewJobModel",
  "sourceCode" : "/**\r\n * Generate new JobModel when becoming a leader or the list of processor changed.\r\n */\r\n@VisibleForTesting\r\nJobModel generateNewJobModel(List<ProcessorNode> processorNodes) {\r\n    String zkJobModelVersion = zkUtils.getJobModelVersion();\r\n    // If JobModel exists in zookeeper && cached JobModel version is unequal to JobModel version stored in zookeeper.\r\n    if (zkJobModelVersion != null && !Objects.equals(cachedJobModelVersion, zkJobModelVersion)) {\r\n        JobModel jobModel = readJobModelFromMetadataStore(zkJobModelVersion);\r\n        for (ContainerModel containerModel : jobModel.getContainers().values()) {\r\n            containerModel.getTasks().forEach((taskName, taskModel) -> changeLogPartitionMap.put(taskName, taskModel.getChangelogPartition().getPartitionId()));\r\n        }\r\n        cachedJobModelVersion = zkJobModelVersion;\r\n    }\r\n    GrouperMetadata grouperMetadata = getGrouperMetadata(zkJobModelVersion, processorNodes);\r\n    JobModel model = JobModelCalculator.INSTANCE.calculateJobModel(config, changeLogPartitionMap, streamMetadataCache, grouperMetadata);\r\n    return new JobModel(new MapConfig(), model.getContainers());\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "getPartitionCountMonitor",
  "sourceCode" : "@VisibleForTesting\r\nStreamPartitionCountMonitor getPartitionCountMonitor() {\r\n    StreamMetadataCache streamMetadata = new StreamMetadataCache(systemAdmins, 0, SystemClock.instance());\r\n    return new StreamPartitionCountMonitorFactory(streamMetadata, metrics.getMetricsRegistry()).build(config, streamsChanged -> {\r\n        if (leaderElector.amILeader()) {\r\n            debounceTimer.scheduleAfterDebounceTime(ON_PROCESSOR_CHANGE, 0, this::doOnProcessorChange);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "createStartpointManager",
  "sourceCode" : "@VisibleForTesting\r\nStartpointManager createStartpointManager() {\r\n    // This method is for easy mocking.\r\n    return new StartpointManager(coordinatorStreamStore);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "checkAndExpireJobModel",
  "sourceCode" : "/**\r\n * Check if the new job model contains a different work assignment for the processor compared the last active job\r\n * model. In case of different work assignment, expire the current job model by invoking the <i>onJobModelExpired</i>\r\n * on the registered {@link JobCoordinatorListener}.\r\n * At this phase, the job model is yet to be agreed by the quorum and hence, this optimization helps availability of\r\n * the processors in the event no changes in the work assignment.\r\n *\r\n * @param newJobModel new job model published by the leader\r\n */\r\n@VisibleForTesting\r\nvoid checkAndExpireJobModel(JobModel newJobModel) {\r\n    Preconditions.checkNotNull(newJobModel, \"JobModel cannot be null\");\r\n    if (coordinatorListener == null) {\r\n        LOG.info(\"Skipping job model expiration since there are no active listeners\");\r\n        return;\r\n    }\r\n    LOG.info(\"Checking for work assignment changes for processor {} between active job model {} and new job model {}\", processorId, activeJobModel, newJobModel);\r\n    if (JobModelUtil.compareContainerModelForProcessor(processorId, activeJobModel, newJobModel)) {\r\n        LOG.info(\"Skipping job model expiration for processor {} due to no change in work assignment.\", processorId);\r\n    } else {\r\n        LOG.info(\"Work assignment changed for the processor {}. Notifying job model expiration to coordinator listener\", processorId);\r\n        coordinatorListener.onJobModelExpired();\r\n        jobModelExpired.set(true);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "onNewJobModel",
  "sourceCode" : "/**\r\n * Checks if the new job model contains a different work assignment for the processor compared to the last active\r\n * job model. In case of different work assignment, update the task locality of the tasks associated with the\r\n * processor and notify new job model to the registered {@link JobCoordinatorListener}.\r\n *\r\n * @param newJobModel new job model agreed by the quorum\r\n */\r\n@VisibleForTesting\r\nvoid onNewJobModel(JobModel newJobModel) {\r\n    Preconditions.checkNotNull(newJobModel, \"JobModel cannot be null. Failing onNewJobModel\");\r\n    // start the container with the new model\r\n    if (jobModelExpired.compareAndSet(true, false)) {\r\n        LOG.info(\"Work assignment changed for the processor {}. Updating task locality and notifying coordinator listener\", processorId);\r\n        if (newJobModel.getContainers().containsKey(processorId)) {\r\n            for (TaskName taskName : JobModelUtil.getTaskNamesForProcessor(processorId, newJobModel)) {\r\n                zkUtils.writeTaskLocality(taskName, locationId);\r\n            }\r\n            if (coordinatorListener != null) {\r\n                coordinatorListener.onNewJobModel(processorId, newJobModel);\r\n            }\r\n        }\r\n    } else {\r\n        /*\r\n       * We don't expire the job model if the proposed work assignment is same as the current work assignment.\r\n       * The implication of work assignment remaining the same can be categorized into\r\n       *   1. Processor part of the job model\r\n       *   2. Processor not part of the job model.\r\n       * For both the state of the processor remains what it was when the rebalance started. e.g.,\r\n       *   [1] should continue to process its work assignment without any interruption as part of the rebalance. i.e.,\r\n       *       there will be no expiration of the existing work (a.k.a samza container won't be stopped) and also no\r\n       *       notification to StreamProcessor about the rebalance since work assignment didn't change.\r\n       *   [2] should have no work and be idle processor and will continue to be idle.\r\n       */\r\n        LOG.info(\"Skipping onNewJobModel since there are no changes in work assignment.\");\r\n    }\r\n    /*\r\n     * Update the last active job model to new job model regardless of whether the work assignment for the processor\r\n     * has changed or not. It is important to do it so that all the processors has a consistent view what the latest\r\n     * active job model is.\r\n     */\r\n    activeJobModel = newJobModel;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "getActiveJobModel",
  "sourceCode" : "@VisibleForTesting\r\nJobModel getActiveJobModel() {\r\n    return activeJobModel;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "setActiveJobModel",
  "sourceCode" : "@VisibleForTesting\r\nvoid setActiveJobModel(JobModel jobModel) {\r\n    activeJobModel = jobModel;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "getJobModelExpired",
  "sourceCode" : "@VisibleForTesting\r\nboolean getJobModelExpired() {\r\n    return jobModelExpired.get();\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "setJobModelExpired",
  "sourceCode" : "@VisibleForTesting\r\nvoid setJobModelExpired(boolean value) {\r\n    jobModelExpired.set(value);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "setDebounceTimer",
  "sourceCode" : "@VisibleForTesting\r\nvoid setDebounceTimer(ScheduleAfterDebounceTime scheduleAfterDebounceTime) {\r\n    debounceTimer = scheduleAfterDebounceTime;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "setLeaderElector",
  "sourceCode" : "@VisibleForTesting\r\nvoid setLeaderElector(ZkLeaderElector zkLeaderElector) {\r\n    leaderElector = zkLeaderElector;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "setZkBarrierUpgradeForVersion",
  "sourceCode" : "@VisibleForTesting\r\nvoid setZkBarrierUpgradeForVersion(ZkBarrierForVersionUpgrade barrierUpgradeForVersion) {\r\n    barrier = barrierUpgradeForVersion;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "startWorkWithLastActiveJobModel",
  "sourceCode" : "/**\r\n * Start the processor with the last known active job model. It is safe to start with last active job model\r\n * version in all the scenarios unless in the event of concurrent rebalance. We define safe as a way to ensure that no\r\n * two processors in the quorum have overlapping work assignments.\r\n * In case of a concurrent rebalance there two scenarios\r\n *   1. Job model version update happens before processor registration\r\n *   2. Job model version update happens after processor registration\r\n * ZK guarantees FIFO order for client operations, the processor is guaranteed to see all the state up until its\r\n * own registration.\r\n * For scenario 1, due to above guarantee, the processor will not start with old assignment due to mismatch in\r\n * latest vs last active. (If there is no mismatch, the scenario reduces to one of the safe scenarios)\r\n *\r\n * For scenario 2, it is possible for the processor to not see the writes by the leader about job model version change\r\n * but will eventually receive a notification on the job model version change and act on it (potentially stop\r\n * the work assignment if its not part of the job model).\r\n *\r\n * In the scenario where the processor doesn't start with last active job model version, it will continue to follow\r\n * the old protocol where leader should get notified about the processor registration and potentially trigger\r\n * rebalance and notify about changes in work assignment after consensus.\r\n * TODO: SAMZA-2635: Rebalances in standalone doesn't handle DAG changes for restarted processor\r\n */\r\n@VisibleForTesting\r\nvoid startWorkWithLastActiveJobModel() {\r\n    LOG.info(\"Starting the processor with the recent active job model\");\r\n    String lastActiveJobModelVersion = zkUtils.getLastActiveJobModelVersion();\r\n    String latestJobModelVersion = zkUtils.getJobModelVersion();\r\n    if (lastActiveJobModelVersion != null && lastActiveJobModelVersion.equals(latestJobModelVersion)) {\r\n        final JobModel lastActiveJobModel = readJobModelFromMetadataStore(lastActiveJobModelVersion);\r\n        /*\r\n       * TODO: SAMZA-2645: Allow onNewJobModel as a valid state transition. Due to this limitation, we are forced\r\n       *  to invoke onJobModelExpired even if there is nothing to expire.\r\n       */\r\n        checkAndExpireJobModel(lastActiveJobModel);\r\n        onNewJobModel(lastActiveJobModel);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\java\\org\\apache\\samza\\zk\\ZkJobCoordinator.java",
  "methodName" : "getZkUtils",
  "sourceCode" : "@VisibleForTesting\r\npublic ZkUtils getZkUtils() {\r\n    return zkUtils;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\scala\\org\\apache\\samza\\storage\\ContainerStorageManager.java",
  "methodName" : "stopStores",
  "sourceCode" : "@VisibleForTesting\r\npublic void stopStores() {\r\n    this.taskStores.forEach((taskName, storeMap) -> storeMap.forEach((storeName, store) -> store.stop()));\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\main\\scala\\org\\apache\\samza\\storage\\SideInputsManager.java",
  "methodName" : "getTaskSideInputSSPs",
  "sourceCode" : "/**\r\n * Add all sideInputs to a map of maps, indexed first by taskName, then by sideInput store name.\r\n *\r\n * @param sideInputSystemStreams the map of store to sideInput system stream\r\n * @param changelogSystemStreams the map of store to changelog system stream\r\n * @param containerModel the containerModel to use\r\n * @return taskSideInputSSPs map\r\n */\r\n@VisibleForTesting\r\nstatic Map<TaskName, Map<String, Set<SystemStreamPartition>>> getTaskSideInputSSPs(Map<String, Set<SystemStream>> sideInputSystemStreams, Map<String, SystemStream> changelogSystemStreams, ContainerModel containerModel) {\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> taskSideInputSSPs = new HashMap<>();\r\n    containerModel.getTasks().forEach((taskName, taskModel) -> {\r\n        taskSideInputSSPs.putIfAbsent(taskName, new HashMap<>());\r\n        sideInputSystemStreams.keySet().forEach(storeName -> {\r\n            Set<SystemStreamPartition> taskSideInputs = taskModel.getSystemStreamPartitions().stream().filter(ssp -> sideInputSystemStreams.get(storeName).contains(ssp.getSystemStream())).collect(Collectors.toSet());\r\n            taskSideInputSSPs.get(taskName).put(storeName, taskSideInputs);\r\n        });\r\n    });\r\n    ContainerStorageManagerUtil.getTasks(containerModel, TaskMode.Standby).forEach((taskName, taskModel) -> {\r\n        taskSideInputSSPs.putIfAbsent(taskName, new HashMap<>());\r\n        changelogSystemStreams.forEach((storeName, systemStream) -> {\r\n            SystemStreamPartition ssp = new SystemStreamPartition(systemStream, taskModel.getChangelogPartition());\r\n            taskSideInputSSPs.get(taskName).put(storeName, Collections.singleton(ssp));\r\n        });\r\n    });\r\n    return taskSideInputSSPs;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testConstructor",
  "sourceCode" : "@Test\r\npublic void testConstructor() {\r\n    StreamApplication mockApp = mock(StreamApplication.class);\r\n    Config mockConfig = getConfig();\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(mockApp, mockConfig);\r\n    verify(mockApp).describe(appDesc);\r\n    assertEquals(mockConfig, appDesc.getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamWithValueSerde() {\r\n    String streamId = \"test-stream-1\";\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd = sd.getInputDescriptor(streamId, mockValueSerde);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(streamId);\r\n    assertEquals(OpCode.INPUT, inputOpSpec.getOpCode());\r\n    assertEquals(streamId, inputOpSpec.getStreamId());\r\n    assertEquals(isd, streamAppDesc.getInputDescriptors().get(streamId));\r\n    assertTrue(inputOpSpec.getKeySerde() instanceof NoOpSerde);\r\n    assertEquals(mockValueSerde, inputOpSpec.getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithKeyValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamWithKeyValueSerde() {\r\n    String streamId = \"test-stream-1\";\r\n    KVSerde mockKVSerde = mock(KVSerde.class);\r\n    Serde mockKeySerde = mock(Serde.class);\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    doReturn(mockKeySerde).when(mockKVSerde).getKeySerde();\r\n    doReturn(mockValueSerde).when(mockKVSerde).getValueSerde();\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd = sd.getInputDescriptor(streamId, mockKVSerde);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(streamId);\r\n    assertEquals(OpCode.INPUT, inputOpSpec.getOpCode());\r\n    assertEquals(streamId, inputOpSpec.getStreamId());\r\n    assertEquals(isd, streamAppDesc.getInputDescriptors().get(streamId));\r\n    assertEquals(mockKeySerde, inputOpSpec.getKeySerde());\r\n    assertEquals(mockValueSerde, inputOpSpec.getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithNullSerde",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetInputStreamWithNullSerde() {\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd = sd.getInputDescriptor(\"mockStreamId\", null);\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithTransformFunction",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamWithTransformFunction() {\r\n    String streamId = \"test-stream-1\";\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    InputTransformer transformer = ime -> ime;\r\n    MockTransformingSystemDescriptor sd = new MockTransformingSystemDescriptor(\"mockSystem\", transformer);\r\n    MockInputDescriptor isd = sd.getInputDescriptor(streamId, mockValueSerde);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(streamId);\r\n    assertEquals(OpCode.INPUT, inputOpSpec.getOpCode());\r\n    assertEquals(streamId, inputOpSpec.getStreamId());\r\n    assertEquals(isd, streamAppDesc.getInputDescriptors().get(streamId));\r\n    assertEquals(transformer, inputOpSpec.getTransformer());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithExpandingSystem",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamWithExpandingSystem() {\r\n    String streamId = \"test-stream-1\";\r\n    String expandedStreamId = \"expanded-stream\";\r\n    AtomicInteger expandCallCount = new AtomicInteger();\r\n    StreamExpander expander = (sg, isd) -> {\r\n        expandCallCount.incrementAndGet();\r\n        InputDescriptor expandedISD = new GenericSystemDescriptor(\"expanded-system\", \"mockFactoryClass\").getInputDescriptor(expandedStreamId, new IntegerSerde());\r\n        return sg.getInputStream(expandedISD);\r\n    };\r\n    MockExpandingSystemDescriptor sd = new MockExpandingSystemDescriptor(\"mock-system\", expander);\r\n    MockInputDescriptor isd = sd.getInputDescriptor(streamId, new IntegerSerde());\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(expandedStreamId);\r\n    assertEquals(OpCode.INPUT, inputOpSpec.getOpCode());\r\n    assertEquals(1, expandCallCount.get());\r\n    assertFalse(streamAppDesc.getInputOperators().containsKey(streamId));\r\n    assertFalse(streamAppDesc.getInputDescriptors().containsKey(streamId));\r\n    assertTrue(streamAppDesc.getInputDescriptors().containsKey(expandedStreamId));\r\n    assertEquals(expandedStreamId, inputOpSpec.getStreamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamWithRelaxedTypes",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamWithRelaxedTypes() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd = sd.getInputDescriptor(streamId, mock(Serde.class));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(streamId);\r\n    assertEquals(OpCode.INPUT, inputOpSpec.getOpCode());\r\n    assertEquals(streamId, inputOpSpec.getStreamId());\r\n    assertEquals(isd, streamAppDesc.getInputDescriptors().get(streamId));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testMultipleGetInputStreams",
  "sourceCode" : "@Test\r\npublic void testMultipleGetInputStreams() {\r\n    String streamId1 = \"test-stream-1\";\r\n    String streamId2 = \"test-stream-2\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd1 = sd.getInputDescriptor(streamId1, mock(Serde.class));\r\n    GenericInputDescriptor isd2 = sd.getInputDescriptor(streamId2, mock(Serde.class));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd1);\r\n        appDesc.getInputStream(isd2);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec1 = streamAppDesc.getInputOperators().get(streamId1);\r\n    InputOperatorSpec inputOpSpec2 = streamAppDesc.getInputOperators().get(streamId2);\r\n    assertEquals(2, streamAppDesc.getInputOperators().size());\r\n    assertEquals(streamId1, inputOpSpec1.getStreamId());\r\n    assertEquals(streamId2, inputOpSpec2.getStreamId());\r\n    assertEquals(2, streamAppDesc.getInputDescriptors().size());\r\n    assertEquals(isd1, streamAppDesc.getInputDescriptors().get(streamId1));\r\n    assertEquals(isd2, streamAppDesc.getInputDescriptors().get(streamId2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetSameInputStreamTwice",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testGetSameInputStreamTwice() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd1 = sd.getInputDescriptor(streamId, mock(Serde.class));\r\n    GenericInputDescriptor isd2 = sd.getInputDescriptor(streamId, mock(Serde.class));\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd1);\r\n        // should throw exception\r\n        appDesc.getInputStream(isd2);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testMultipleSystemDescriptorForSameSystemName",
  "sourceCode" : "@Test\r\npublic void testMultipleSystemDescriptorForSameSystemName() {\r\n    GenericSystemDescriptor sd1 = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericSystemDescriptor sd2 = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd1 = sd1.getInputDescriptor(\"test-stream-1\", mock(Serde.class));\r\n    GenericInputDescriptor isd2 = sd2.getInputDescriptor(\"test-stream-2\", mock(Serde.class));\r\n    GenericOutputDescriptor osd1 = sd2.getOutputDescriptor(\"test-stream-3\", mock(Serde.class));\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd1);\r\n        try {\r\n            appDesc.getInputStream(isd2);\r\n            fail(\"Adding input stream with the same system name but different SystemDescriptor should have failed\");\r\n        } catch (IllegalStateException e) {\r\n        }\r\n        try {\r\n            appDesc.getOutputStream(osd1);\r\n            fail(\"adding output stream with the same system name but different SystemDescriptor should have failed\");\r\n        } catch (IllegalStateException e) {\r\n        }\r\n    }, getConfig());\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.withDefaultSystem(sd2);\r\n        try {\r\n            appDesc.getInputStream(isd1);\r\n            fail(\"Adding input stream with the same system name as the default system but different SystemDescriptor should have failed\");\r\n        } catch (IllegalStateException e) {\r\n        }\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetOutputStreamWithKeyValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetOutputStreamWithKeyValueSerde() {\r\n    String streamId = \"test-stream-1\";\r\n    KVSerde mockKVSerde = mock(KVSerde.class);\r\n    Serde mockKeySerde = mock(Serde.class);\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    doReturn(mockKeySerde).when(mockKVSerde).getKeySerde();\r\n    doReturn(mockValueSerde).when(mockKVSerde).getValueSerde();\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericOutputDescriptor osd = sd.getOutputDescriptor(streamId, mockKVSerde);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getOutputStream(osd);\r\n    }, getConfig());\r\n    OutputStreamImpl<TestMessageEnvelope> outputStreamImpl = streamAppDesc.getOutputStreams().get(streamId);\r\n    assertEquals(streamId, outputStreamImpl.getStreamId());\r\n    assertEquals(osd, streamAppDesc.getOutputDescriptors().get(streamId));\r\n    assertEquals(mockKeySerde, outputStreamImpl.getKeySerde());\r\n    assertEquals(mockValueSerde, outputStreamImpl.getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetOutputStreamWithNullSerde",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetOutputStreamWithNullSerde() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericOutputDescriptor osd = sd.getOutputDescriptor(streamId, null);\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getOutputStream(osd);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetOutputStreamWithValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetOutputStreamWithValueSerde() {\r\n    String streamId = \"test-stream-1\";\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericOutputDescriptor osd = sd.getOutputDescriptor(streamId, mockValueSerde);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getOutputStream(osd);\r\n    }, getConfig());\r\n    OutputStreamImpl<TestMessageEnvelope> outputStreamImpl = streamAppDesc.getOutputStreams().get(streamId);\r\n    assertEquals(streamId, outputStreamImpl.getStreamId());\r\n    assertEquals(osd, streamAppDesc.getOutputDescriptors().get(streamId));\r\n    assertTrue(outputStreamImpl.getKeySerde() instanceof NoOpSerde);\r\n    assertEquals(mockValueSerde, outputStreamImpl.getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testSetDefaultSystemDescriptorAfterGettingInputStream",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testSetDefaultSystemDescriptorAfterGettingInputStream() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericInputDescriptor isd = sd.getInputDescriptor(streamId, mock(Serde.class));\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(isd);\r\n        // should throw exception\r\n        appDesc.withDefaultSystem(sd);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testSetDefaultSystemDescriptorAfterGettingOutputStream",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testSetDefaultSystemDescriptorAfterGettingOutputStream() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericOutputDescriptor osd = sd.getOutputDescriptor(streamId, mock(Serde.class));\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getOutputStream(osd);\r\n        // should throw exception\r\n        appDesc.withDefaultSystem(sd);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testSetDefaultSystemDescriptorAfterGettingIntermediateStream",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testSetDefaultSystemDescriptorAfterGettingIntermediateStream() {\r\n    String streamId = \"test-stream-1\";\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, getConfig());\r\n    streamAppDesc.getIntermediateStream(streamId, mock(Serde.class), false);\r\n    // should throw exception\r\n    streamAppDesc.withDefaultSystem(mock(SystemDescriptor.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetSameOutputStreamTwice",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testGetSameOutputStreamTwice() {\r\n    String streamId = \"test-stream-1\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    GenericOutputDescriptor osd1 = sd.getOutputDescriptor(streamId, mock(Serde.class));\r\n    GenericOutputDescriptor osd2 = sd.getOutputDescriptor(streamId, mock(Serde.class));\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getOutputStream(osd1);\r\n        // should throw exception\r\n        appDesc.getOutputStream(osd2);\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetIntermediateStreamWithValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetIntermediateStreamWithValueSerde() {\r\n    String streamId = \"stream-1\";\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, getConfig());\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    IntermediateMessageStreamImpl<TestMessageEnvelope> intermediateStreamImpl = streamAppDesc.getIntermediateStream(streamId, mockValueSerde, false);\r\n    assertEquals(streamAppDesc.getInputOperators().get(streamId), intermediateStreamImpl.getOperatorSpec());\r\n    assertEquals(streamAppDesc.getOutputStreams().get(streamId), intermediateStreamImpl.getOutputStream());\r\n    assertEquals(streamId, intermediateStreamImpl.getStreamId());\r\n    assertTrue(intermediateStreamImpl.getOutputStream().getKeySerde() instanceof NoOpSerde);\r\n    assertEquals(mockValueSerde, intermediateStreamImpl.getOutputStream().getValueSerde());\r\n    assertTrue(((InputOperatorSpec) (OperatorSpec) intermediateStreamImpl.getOperatorSpec()).getKeySerde() instanceof NoOpSerde);\r\n    assertEquals(mockValueSerde, ((InputOperatorSpec) (OperatorSpec) intermediateStreamImpl.getOperatorSpec()).getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetIntermediateStreamWithKeyValueSerde",
  "sourceCode" : "@Test\r\npublic void testGetIntermediateStreamWithKeyValueSerde() {\r\n    String streamId = \"streamId\";\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, getConfig());\r\n    KVSerde mockKVSerde = mock(KVSerde.class);\r\n    Serde mockKeySerde = mock(Serde.class);\r\n    Serde mockValueSerde = mock(Serde.class);\r\n    doReturn(mockKeySerde).when(mockKVSerde).getKeySerde();\r\n    doReturn(mockValueSerde).when(mockKVSerde).getValueSerde();\r\n    IntermediateMessageStreamImpl<TestMessageEnvelope> intermediateStreamImpl = streamAppDesc.getIntermediateStream(streamId, mockKVSerde, false);\r\n    assertEquals(streamAppDesc.getInputOperators().get(streamId), intermediateStreamImpl.getOperatorSpec());\r\n    assertEquals(streamAppDesc.getOutputStreams().get(streamId), intermediateStreamImpl.getOutputStream());\r\n    assertEquals(streamId, intermediateStreamImpl.getStreamId());\r\n    assertEquals(mockKeySerde, intermediateStreamImpl.getOutputStream().getKeySerde());\r\n    assertEquals(mockValueSerde, intermediateStreamImpl.getOutputStream().getValueSerde());\r\n    assertEquals(mockKeySerde, ((InputOperatorSpec) (OperatorSpec) intermediateStreamImpl.getOperatorSpec()).getKeySerde());\r\n    assertEquals(mockValueSerde, ((InputOperatorSpec) (OperatorSpec) intermediateStreamImpl.getOperatorSpec()).getValueSerde());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetIntermediateStreamWithDefaultSystemDescriptor",
  "sourceCode" : "@Test\r\npublic void testGetIntermediateStreamWithDefaultSystemDescriptor() {\r\n    Config mockConfig = getConfig();\r\n    String streamId = \"streamId\";\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, mockConfig);\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mock-system\", \"mock-system-factory\");\r\n    streamAppDesc.withDefaultSystem(sd);\r\n    IntermediateMessageStreamImpl<TestMessageEnvelope> intermediateStreamImpl = streamAppDesc.getIntermediateStream(streamId, mock(Serde.class), false);\r\n    assertEquals(streamAppDesc.getInputOperators().get(streamId), intermediateStreamImpl.getOperatorSpec());\r\n    assertEquals(streamAppDesc.getOutputStreams().get(streamId), intermediateStreamImpl.getOutputStream());\r\n    assertEquals(streamId, intermediateStreamImpl.getStreamId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetIntermediateStreamWithNoSerde",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testGetIntermediateStreamWithNoSerde() {\r\n    Config mockConfig = getConfig();\r\n    String streamId = \"streamId\";\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, mockConfig);\r\n    IntermediateMessageStreamImpl<TestMessageEnvelope> intermediateStreamImpl = // should throw\r\n    streamAppDesc.getIntermediateStream(streamId, null, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetSameIntermediateStreamTwice",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testGetSameIntermediateStreamTwice() {\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, getConfig());\r\n    streamAppDesc.getIntermediateStream(\"test-stream-1\", mock(Serde.class), false);\r\n    // should throw exception\r\n    streamAppDesc.getIntermediateStream(\"test-stream-1\", mock(Serde.class), false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetNextOpIdIncrementsId",
  "sourceCode" : "@Test\r\npublic void testGetNextOpIdIncrementsId() {\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_NAME, \"appName\");\r\n    configMap.put(ApplicationConfig.APP_ID, \"1234\");\r\n    Config config = new MapConfig(configMap);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, config);\r\n    assertEquals(\"appName-1234-merge-0\", streamAppDesc.getNextOpId(OpCode.MERGE, null));\r\n    assertEquals(\"appName-1234-join-customName\", streamAppDesc.getNextOpId(OpCode.JOIN, \"customName\"));\r\n    assertEquals(\"appName-1234-map-2\", streamAppDesc.getNextOpId(OpCode.MAP, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetNextOpIdRejectsDuplicates",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetNextOpIdRejectsDuplicates() {\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_NAME, \"appName\");\r\n    configMap.put(ApplicationConfig.APP_ID, \"1234\");\r\n    Config config = new MapConfig(configMap);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, config);\r\n    assertEquals(\"appName-1234-join-customName\", streamAppDesc.getNextOpId(OpCode.JOIN, \"customName\"));\r\n    // should throw\r\n    streamAppDesc.getNextOpId(OpCode.JOIN, \"customName\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testOpIdValidation",
  "sourceCode" : "@Test\r\npublic void testOpIdValidation() {\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_NAME, \"appName\");\r\n    configMap.put(ApplicationConfig.APP_ID, \"1234\");\r\n    Config config = new MapConfig(configMap);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, config);\r\n    // null and empty userDefinedIDs should fall back to autogenerated IDs.\r\n    try {\r\n        streamAppDesc.getNextOpId(OpCode.FILTER, null);\r\n        streamAppDesc.getNextOpId(OpCode.FILTER, \"\");\r\n        streamAppDesc.getNextOpId(OpCode.FILTER, \" \");\r\n        streamAppDesc.getNextOpId(OpCode.FILTER, \"\\t\");\r\n    } catch (SamzaException e) {\r\n        fail(\"Received an error with a null or empty operator ID instead of defaulting to auto-generated ID.\");\r\n    }\r\n    List<String> validOpIds = ImmutableList.of(\"op_id\", \"op-id\", \"1000\", \"op_1\", \"OP_ID\");\r\n    for (String validOpId : validOpIds) {\r\n        try {\r\n            streamAppDesc.getNextOpId(OpCode.FILTER, validOpId);\r\n        } catch (Exception e) {\r\n            fail(\"Received an exception with a valid operator ID: \" + validOpId);\r\n        }\r\n    }\r\n    List<String> invalidOpIds = ImmutableList.of(\"op id\", \"op#id\");\r\n    for (String invalidOpId : invalidOpIds) {\r\n        try {\r\n            streamAppDesc.getNextOpId(OpCode.FILTER, invalidOpId);\r\n            fail(\"Did not receive an exception with an invalid operator ID: \" + invalidOpId);\r\n        } catch (SamzaException e) {\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetInputStreamPreservesInsertionOrder",
  "sourceCode" : "@Test\r\npublic void testGetInputStreamPreservesInsertionOrder() {\r\n    Config mockConfig = getConfig();\r\n    String testStreamId1 = \"test-stream-1\";\r\n    String testStreamId2 = \"test-stream-2\";\r\n    String testStreamId3 = \"test-stream-3\";\r\n    GenericSystemDescriptor sd = new GenericSystemDescriptor(\"mockSystem\", \"mockSystemFactoryClass\");\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        appDesc.getInputStream(sd.getInputDescriptor(testStreamId1, mock(Serde.class)));\r\n        appDesc.getInputStream(sd.getInputDescriptor(testStreamId2, mock(Serde.class)));\r\n        appDesc.getInputStream(sd.getInputDescriptor(testStreamId3, mock(Serde.class)));\r\n    }, mockConfig);\r\n    List<InputOperatorSpec> inputSpecs = new ArrayList<>(streamAppDesc.getInputOperators().values());\r\n    assertEquals(inputSpecs.size(), 3);\r\n    assertEquals(inputSpecs.get(0).getStreamId(), testStreamId1);\r\n    assertEquals(inputSpecs.get(1).getStreamId(), testStreamId2);\r\n    assertEquals(inputSpecs.get(2).getStreamId(), testStreamId3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetTable",
  "sourceCode" : "@Test\r\npublic void testGetTable() throws Exception {\r\n    Config mockConfig = getConfig();\r\n    String tableId = \"t1\";\r\n    BaseTableDescriptor mockTableDescriptor = mock(BaseTableDescriptor.class);\r\n    when(mockTableDescriptor.getTableId()).thenReturn(tableId);\r\n    AtomicReference<TableImpl> table = new AtomicReference<>();\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        table.set((TableImpl) appDesc.getTable(mockTableDescriptor));\r\n    }, mockConfig);\r\n    assertEquals(tableId, table.get().getTableId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testApplicationContainerContextFactory",
  "sourceCode" : "@Test\r\npublic void testApplicationContainerContextFactory() {\r\n    ApplicationContainerContextFactory factory = mock(ApplicationContainerContextFactory.class);\r\n    StreamApplication testApp = appDesc -> appDesc.withApplicationContainerContextFactory(factory);\r\n    StreamApplicationDescriptorImpl appSpec = new StreamApplicationDescriptorImpl(testApp, getConfig());\r\n    assertEquals(appSpec.getApplicationContainerContextFactory(), Optional.of(factory));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testNoApplicationContainerContextFactory",
  "sourceCode" : "@Test\r\npublic void testNoApplicationContainerContextFactory() {\r\n    StreamApplication testApp = appDesc -> {\r\n    };\r\n    StreamApplicationDescriptorImpl appSpec = new StreamApplicationDescriptorImpl(testApp, getConfig());\r\n    assertEquals(appSpec.getApplicationContainerContextFactory(), Optional.empty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testApplicationTaskContextFactory",
  "sourceCode" : "@Test\r\npublic void testApplicationTaskContextFactory() {\r\n    ApplicationTaskContextFactory factory = mock(ApplicationTaskContextFactory.class);\r\n    StreamApplication testApp = appDesc -> appDesc.withApplicationTaskContextFactory(factory);\r\n    StreamApplicationDescriptorImpl appSpec = new StreamApplicationDescriptorImpl(testApp, getConfig());\r\n    assertEquals(appSpec.getApplicationTaskContextFactory(), Optional.of(factory));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testNoApplicationTaskContextFactory",
  "sourceCode" : "@Test\r\npublic void testNoApplicationTaskContextFactory() {\r\n    StreamApplication testApp = appDesc -> {\r\n    };\r\n    StreamApplicationDescriptorImpl appSpec = new StreamApplicationDescriptorImpl(testApp, getConfig());\r\n    assertEquals(appSpec.getApplicationTaskContextFactory(), Optional.empty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testProcessorLifecycleListenerFactory",
  "sourceCode" : "@Test\r\npublic void testProcessorLifecycleListenerFactory() {\r\n    ProcessorLifecycleListenerFactory mockFactory = mock(ProcessorLifecycleListenerFactory.class);\r\n    StreamApplication testApp = appSpec -> appSpec.withProcessorLifecycleListenerFactory(mockFactory);\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(testApp, getConfig());\r\n    assertEquals(appDesc.getProcessorLifecycleListenerFactory(), mockFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestStreamApplicationDescriptorImpl.java",
  "methodName" : "testGetTableWithBadId",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testGetTableWithBadId() {\r\n    Config mockConfig = getConfig();\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        BaseTableDescriptor mockTableDescriptor = mock(BaseTableDescriptor.class);\r\n        when(mockTableDescriptor.getTableId()).thenReturn(\"my.table\");\r\n        appDesc.getTable(mockTableDescriptor);\r\n    }, mockConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testConstructor",
  "sourceCode" : "@Test\r\npublic void testConstructor() {\r\n    TaskApplication mockApp = mock(TaskApplication.class);\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(mockApp, config);\r\n    verify(mockApp).describe(appDesc);\r\n    assertEquals(config, appDesc.getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testAddInputStreams",
  "sourceCode" : "@Test\r\npublic void testAddInputStreams() {\r\n    TaskApplication testApp = appDesc -> {\r\n        mockInputs.forEach(appDesc::withInputStream);\r\n    };\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(testApp, config);\r\n    assertEquals(mockInputs.toArray(), appDesc.getInputDescriptors().values().toArray());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testAddOutputStreams",
  "sourceCode" : "@Test\r\npublic void testAddOutputStreams() {\r\n    TaskApplication testApp = appDesc -> {\r\n        mockOutputs.forEach(appDesc::withOutputStream);\r\n    };\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(testApp, config);\r\n    assertEquals(mockOutputs.toArray(), appDesc.getOutputDescriptors().values().toArray());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testAddTables",
  "sourceCode" : "@Test\r\npublic void testAddTables() {\r\n    TaskApplication testApp = appDesc -> {\r\n        mockTables.forEach(appDesc::withTable);\r\n    };\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(testApp, config);\r\n    assertEquals(mockTables, appDesc.getTableDescriptors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testWithTaskFactory",
  "sourceCode" : "@Test\r\npublic void testWithTaskFactory() {\r\n    TaskFactory mockTf = mock(TaskFactory.class);\r\n    TaskApplication testApp = appDesc -> appDesc.withTaskFactory(mockTf);\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(testApp, config);\r\n    assertEquals(appDesc.getTaskFactory(), mockTf);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testApplicationContainerContextFactory",
  "sourceCode" : "@Test\r\npublic void testApplicationContainerContextFactory() {\r\n    ApplicationContainerContextFactory factory = mock(ApplicationContainerContextFactory.class);\r\n    TaskApplication testApp = appDesc -> appDesc.withApplicationContainerContextFactory(factory);\r\n    TaskApplicationDescriptorImpl appSpec = new TaskApplicationDescriptorImpl(testApp, mock(Config.class));\r\n    assertEquals(appSpec.getApplicationContainerContextFactory(), Optional.of(factory));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testNoApplicationContainerContextFactory",
  "sourceCode" : "@Test\r\npublic void testNoApplicationContainerContextFactory() {\r\n    TaskApplication testApp = appDesc -> {\r\n    };\r\n    TaskApplicationDescriptorImpl appSpec = new TaskApplicationDescriptorImpl(testApp, mock(Config.class));\r\n    assertEquals(appSpec.getApplicationContainerContextFactory(), Optional.empty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testApplicationTaskContextFactory",
  "sourceCode" : "@Test\r\npublic void testApplicationTaskContextFactory() {\r\n    ApplicationTaskContextFactory factory = mock(ApplicationTaskContextFactory.class);\r\n    TaskApplication testApp = appDesc -> appDesc.withApplicationTaskContextFactory(factory);\r\n    TaskApplicationDescriptorImpl appSpec = new TaskApplicationDescriptorImpl(testApp, mock(Config.class));\r\n    assertEquals(appSpec.getApplicationTaskContextFactory(), Optional.of(factory));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testNoApplicationTaskContextFactory",
  "sourceCode" : "@Test\r\npublic void testNoApplicationTaskContextFactory() {\r\n    TaskApplication testApp = appDesc -> {\r\n    };\r\n    TaskApplicationDescriptorImpl appSpec = new TaskApplicationDescriptorImpl(testApp, mock(Config.class));\r\n    assertEquals(appSpec.getApplicationTaskContextFactory(), Optional.empty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\descriptors\\TestTaskApplicationDescriptorImpl.java",
  "methodName" : "testProcessorLifecycleListener",
  "sourceCode" : "@Test\r\npublic void testProcessorLifecycleListener() {\r\n    ProcessorLifecycleListenerFactory mockFactory = mock(ProcessorLifecycleListenerFactory.class);\r\n    TaskApplication testApp = appDesc -> {\r\n        appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    };\r\n    TaskApplicationDescriptorImpl appDesc = new TaskApplicationDescriptorImpl(testApp, config);\r\n    assertEquals(appDesc.getProcessorLifecycleListenerFactory(), mockFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\TestApplicationUtil.java",
  "methodName" : "testStreamAppClass",
  "sourceCode" : "@Test\r\npublic void testStreamAppClass() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName());\r\n    SamzaApplication app = ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n    assertTrue(app instanceof MockStreamApplication);\r\n    configMap.put(TaskConfig.TASK_CLASS, MockStreamTask.class.getName());\r\n    app = ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n    assertTrue(app instanceof MockStreamApplication);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\TestApplicationUtil.java",
  "methodName" : "testTaskAppClass",
  "sourceCode" : "@Test\r\npublic void testTaskAppClass() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_CLASS, MockTaskApplication.class.getName());\r\n    SamzaApplication app = ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n    assertTrue(app instanceof MockTaskApplication);\r\n    configMap.put(TaskConfig.TASK_CLASS, MockStreamTask.class.getName());\r\n    app = ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n    assertTrue(app instanceof MockTaskApplication);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\TestApplicationUtil.java",
  "methodName" : "testTaskClassOnly",
  "sourceCode" : "@Test\r\npublic void testTaskClassOnly() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TASK_CLASS, MockStreamTask.class.getName());\r\n    Config config = new MapConfig(configMap);\r\n    SamzaApplication app = ApplicationUtil.fromConfig(config);\r\n    assertTrue(app instanceof TaskApplication);\r\n    TaskApplicationDescriptorImpl appSpec = new TaskApplicationDescriptorImpl((TaskApplication) app, config);\r\n    assertTrue(appSpec.getTaskFactory().createInstance() instanceof MockStreamTask);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\TestApplicationUtil.java",
  "methodName" : "testEmptyTaskClassOnly",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testEmptyTaskClassOnly() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TASK_CLASS, \"\");\r\n    ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\application\\TestApplicationUtil.java",
  "methodName" : "testNoAppClassNoTaskClass",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testNoAppClassNoTaskClass() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    ApplicationUtil.fromConfig(new MapConfig(configMap));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaChangelogSSPOffset.java",
  "methodName" : "testSerializeDeserialize",
  "sourceCode" : "@Test\r\npublic void testSerializeDeserialize() {\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(CheckpointId.create(), \"offset\");\r\n    KafkaChangelogSSPOffset deserializedKafkaChangelogSSPOffset = KafkaChangelogSSPOffset.fromString(kafkaChangelogSSPOffset.toString());\r\n    assertEquals(kafkaChangelogSSPOffset.getCheckpointId(), deserializedKafkaChangelogSSPOffset.getCheckpointId());\r\n    assertEquals(\"offset\", deserializedKafkaChangelogSSPOffset.getChangelogOffset());\r\n    assertEquals(kafkaChangelogSSPOffset, deserializedKafkaChangelogSSPOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaChangelogSSPOffset.java",
  "methodName" : "testSerializeDeserializeNullOffsets",
  "sourceCode" : "@Test\r\npublic void testSerializeDeserializeNullOffsets() {\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(CheckpointId.create(), null);\r\n    KafkaChangelogSSPOffset deserializedKafkaChangelogSSPOffset = KafkaChangelogSSPOffset.fromString(kafkaChangelogSSPOffset.toString());\r\n    assertEquals(kafkaChangelogSSPOffset.getCheckpointId(), deserializedKafkaChangelogSSPOffset.getCheckpointId());\r\n    assertNull(deserializedKafkaChangelogSSPOffset.getChangelogOffset());\r\n    assertEquals(kafkaChangelogSSPOffset, deserializedKafkaChangelogSSPOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaChangelogSSPOffset.java",
  "methodName" : "testSerializationFormatForBackwardsCompatibility",
  "sourceCode" : "@Test\r\npublic void testSerializationFormatForBackwardsCompatibility() {\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(CheckpointId.create(), \"offset\");\r\n    // WARNING: This format is written to persisted remotes stores and local files, making a change in the format\r\n    // would be backwards incompatible\r\n    String expectedSerializationFormat = kafkaChangelogSSPOffset.getCheckpointId() + KafkaChangelogSSPOffset.SEPARATOR + kafkaChangelogSSPOffset.getChangelogOffset();\r\n    assertEquals(expectedSerializationFormat, kafkaChangelogSSPOffset.toString());\r\n    assertEquals(kafkaChangelogSSPOffset, KafkaChangelogSSPOffset.fromString(expectedSerializationFormat));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaChangelogSSPOffset.java",
  "methodName" : "testNullSerializationFormatForBackwardsCompatibility",
  "sourceCode" : "@Test\r\npublic void testNullSerializationFormatForBackwardsCompatibility() {\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(CheckpointId.create(), null);\r\n    // WARNING: This format is written to persisted remotes stores and local files, making a change in the format\r\n    // would be backwards incompatible\r\n    String expectedSerializationFormat = kafkaChangelogSSPOffset.getCheckpointId() + KafkaChangelogSSPOffset.SEPARATOR + \"null\";\r\n    assertEquals(expectedSerializationFormat, kafkaChangelogSSPOffset.toString());\r\n    assertEquals(kafkaChangelogSSPOffset, KafkaChangelogSSPOffset.fromString(expectedSerializationFormat));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaStateCheckpointMarker.java",
  "methodName" : "testSerializeDeserialize",
  "sourceCode" : "@Test\r\npublic void testSerializeDeserialize() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"system\", \"stream\", new Partition(1));\r\n    KafkaStateCheckpointMarker marker = new KafkaStateCheckpointMarker(ssp, \"offset\");\r\n    KafkaStateCheckpointMarker deserializedMarker = KafkaStateCheckpointMarker.deserialize(KafkaStateCheckpointMarker.serialize(marker));\r\n    assertEquals(MARKER_VERSION, deserializedMarker.getVersion());\r\n    assertEquals(marker.getChangelogOffset(), deserializedMarker.getChangelogOffset());\r\n    assertEquals(marker.getChangelogSSP(), deserializedMarker.getChangelogSSP());\r\n    assertEquals(marker, deserializedMarker);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaStateCheckpointMarker.java",
  "methodName" : "testSerializeDeserializeNullOffsets",
  "sourceCode" : "@Test\r\npublic void testSerializeDeserializeNullOffsets() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"system\", \"stream\", new Partition(1));\r\n    KafkaStateCheckpointMarker marker = new KafkaStateCheckpointMarker(ssp, null);\r\n    KafkaStateCheckpointMarker deserializedMarker = KafkaStateCheckpointMarker.deserialize(KafkaStateCheckpointMarker.serialize(marker));\r\n    assertEquals(MARKER_VERSION, deserializedMarker.getVersion());\r\n    assertNull(deserializedMarker.getChangelogOffset());\r\n    assertEquals(marker.getChangelogSSP(), deserializedMarker.getChangelogSSP());\r\n    assertEquals(marker, deserializedMarker);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaStateCheckpointMarker.java",
  "methodName" : "testStateCheckpointMarkerToSSPOffsetMap",
  "sourceCode" : "@Test\r\npublic void testStateCheckpointMarkerToSSPOffsetMap() {\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"system1\", \"stream1\", new Partition(1));\r\n    KafkaStateCheckpointMarker marker1 = new KafkaStateCheckpointMarker(ssp1, \"offset1\");\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(\"system2\", \"stream2\", new Partition(2));\r\n    KafkaStateCheckpointMarker marker2 = new KafkaStateCheckpointMarker(ssp2, null);\r\n    Map<String, String> storesToKSCM = ImmutableMap.of(\"store1\", KafkaStateCheckpointMarker.serialize(marker1), \"store2\", KafkaStateCheckpointMarker.serialize(marker2));\r\n    Map<String, Map<String, String>> factoryToSCMs = ImmutableMap.of(KAFKA_STATE_BACKEND_FACTORY_NAME, storesToKSCM, // factory2 should be ignored\r\n    \"factory2\", // factory2 should be ignored\r\n    Collections.EMPTY_MAP);\r\n    Map<SystemStreamPartition, Option<String>> sspToOffsetOption = KafkaStateCheckpointMarker.scmsToSSPOffsetMap(factoryToSCMs);\r\n    assertEquals(2, sspToOffsetOption.size());\r\n    assertTrue(sspToOffsetOption.containsKey(ssp1));\r\n    assertEquals(sspToOffsetOption.get(ssp1).get(), marker1.getChangelogOffset());\r\n    assertEquals(ssp1, marker1.getChangelogSSP());\r\n    assertTrue(sspToOffsetOption.containsKey(ssp2));\r\n    assertTrue(sspToOffsetOption.get(ssp2).isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaStateCheckpointMarker.java",
  "methodName" : "testStateCheckpointMarkerToSSPOffsetMapNoFactoryFound",
  "sourceCode" : "@Test\r\npublic void testStateCheckpointMarkerToSSPOffsetMapNoFactoryFound() {\r\n    Map<String, Map<String, String>> factoryToSCMs = ImmutableMap.of(// factory1 should be ignored\r\n    \"factory1\", // factory1 should be ignored\r\n    Collections.EMPTY_MAP, // factory2 should be ignored\r\n    \"factory2\", // factory2 should be ignored\r\n    Collections.EMPTY_MAP);\r\n    Map<SystemStreamPartition, Option<String>> sspToOffsetOption = KafkaStateCheckpointMarker.scmsToSSPOffsetMap(factoryToSCMs);\r\n    assertEquals(0, sspToOffsetOption.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaStateCheckpointMarker.java",
  "methodName" : "testStateCheckpointMarkerToSSPOffsetMapDeserializationError",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testStateCheckpointMarkerToSSPOffsetMapDeserializationError() {\r\n    Map<String, String> storesToSCM = ImmutableMap.of(\"store1\", \"blobId-1234\");\r\n    Map<String, Map<String, String>> factoryToSCMs = ImmutableMap.of(// factory2 should be ignored\r\n    \"factory2\", // factory2 should be ignored\r\n    Collections.EMPTY_MAP, KAFKA_STATE_BACKEND_FACTORY_NAME, storesToSCM);\r\n    KafkaStateCheckpointMarker.scmsToSSPOffsetMap(factoryToSCMs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementMetadataStore.java",
  "methodName" : "testDefaultMetadataStore",
  "sourceCode" : "@Test\r\npublic void testDefaultMetadataStore() {\r\n    Assert.assertNotNull(containerPlacementMetadataStore);\r\n    Assert.assertEquals(NamespaceAwareCoordinatorStreamStore.class, containerPlacementMetadataStore.getContainerPlacementStore().getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementMetadataStore.java",
  "methodName" : "testReadWriteContainerPlacementRequestMessages",
  "sourceCode" : "@Test\r\npublic void testReadWriteContainerPlacementRequestMessages() {\r\n    Long timestamp = System.currentTimeMillis();\r\n    UUID uuid = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"app-attempt-001\", \"4\", \"ANY_HOST\", null, timestamp);\r\n    Optional<ContainerPlacementRequestMessage> messageReadFromMetastore = containerPlacementMetadataStore.readContainerPlacementRequestMessage(uuid);\r\n    Assert.assertTrue(messageReadFromMetastore.isPresent());\r\n    assertContainerPlacementRequestMessage(uuid, \"app-attempt-001\", \"4\", \"ANY_HOST\", null, timestamp, messageReadFromMetastore.get());\r\n    // Check for non existent key\r\n    Optional<ContainerPlacementRequestMessage> readNull = containerPlacementMetadataStore.readContainerPlacementRequestMessage(UUID.randomUUID());\r\n    Assert.assertTrue(!readNull.isPresent());\r\n    // No response messages should exist\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementResponseMessage(uuid).isPresent());\r\n    Assert.assertEquals(1, containerPlacementMetadataStore.readAllContainerPlacementRequestMessages().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementMetadataStore.java",
  "methodName" : "testReadWriteContainerPlacementResponseMessages",
  "sourceCode" : "@Test\r\npublic void testReadWriteContainerPlacementResponseMessages() {\r\n    ContainerPlacementResponseMessage messageWrittenToMetastore = new ContainerPlacementResponseMessage(UUID.randomUUID(), \"app-attempt-001\", Integer.toString(new Random().nextInt(5)), \"ANY_HOST\", ContainerPlacementMessage.StatusCode.BAD_REQUEST, \"Request ignored redundant\", System.currentTimeMillis());\r\n    containerPlacementMetadataStore.writeContainerPlacementResponseMessage(messageWrittenToMetastore);\r\n    Optional<ContainerPlacementResponseMessage> messageReadFromMetastore = containerPlacementMetadataStore.readContainerPlacementResponseMessage(messageWrittenToMetastore.getUuid());\r\n    Assert.assertTrue(messageReadFromMetastore.isPresent());\r\n    Assert.assertEquals(messageWrittenToMetastore, messageReadFromMetastore.get());\r\n    // Request store must not contain anything\r\n    Optional<ContainerPlacementRequestMessage> readNull = containerPlacementMetadataStore.readContainerPlacementRequestMessage(messageWrittenToMetastore.getUuid());\r\n    Assert.assertTrue(!readNull.isPresent());\r\n    // No request messages should exist\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementRequestMessage(messageWrittenToMetastore.getUuid()).isPresent());\r\n    Assert.assertTrue(containerPlacementMetadataStore.getContainerPlacementStore().all().size() == 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementMetadataStore.java",
  "methodName" : "testContainerPlacementMessageDeletion",
  "sourceCode" : "@Test\r\npublic void testContainerPlacementMessageDeletion() {\r\n    Long timestamp = System.currentTimeMillis();\r\n    UUID requestMessage1UUID = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"app-attempt-001\", \"4\", \"ANY_HOST\", null, timestamp);\r\n    UUID requestMessage2UUID = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"app-attempt-001\", \"1\", \"host2\", Duration.ofMillis(100), timestamp);\r\n    ContainerPlacementResponseMessage responseMessage1 = new ContainerPlacementResponseMessage(requestMessage1UUID, \"app-attempt-001\", \"4\", \"ANY_HOST\", ContainerPlacementMessage.StatusCode.BAD_REQUEST, \"Request ignored redundant\", System.currentTimeMillis());\r\n    ContainerPlacementResponseMessage responseMessage2 = new ContainerPlacementResponseMessage(requestMessage2UUID, \"app-attempt-001\", \"1\", \"ANY_HOST\", ContainerPlacementMessage.StatusCode.IN_PROGRESS, \"Requested resources\", System.currentTimeMillis());\r\n    containerPlacementMetadataStore.writeContainerPlacementResponseMessage(responseMessage1);\r\n    containerPlacementMetadataStore.writeContainerPlacementResponseMessage(responseMessage2);\r\n    Assert.assertTrue(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage1UUID).isPresent());\r\n    Assert.assertTrue(containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage2UUID).isPresent());\r\n    Assert.assertEquals(4, containerPlacementMetadataStore.getContainerPlacementStore().all().size());\r\n    containerPlacementMetadataStore.deleteContainerPlacementResponseMessage(requestMessage1UUID);\r\n    Assert.assertEquals(3, containerPlacementMetadataStore.getContainerPlacementStore().all().size());\r\n    Assert.assertTrue(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage1UUID).isPresent());\r\n    containerPlacementMetadataStore.deleteContainerPlacementRequestMessage(requestMessage1UUID);\r\n    Assert.assertEquals(2, containerPlacementMetadataStore.getContainerPlacementStore().all().size());\r\n    assertContainerPlacementRequestMessage(requestMessage2UUID, \"app-attempt-001\", \"1\", \"host2\", Duration.ofMillis(100), timestamp, containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage2UUID).get());\r\n    Assert.assertEquals(containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage2UUID).get(), responseMessage2);\r\n    // requestMessage1 & associated responseMessage1 should not be present\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage1UUID).isPresent());\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage1UUID).isPresent());\r\n    requestMessage1UUID = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"app-attempt-001\", \"4\", \"ANY_HOST\", null, System.currentTimeMillis());\r\n    containerPlacementMetadataStore.writeContainerPlacementResponseMessage(responseMessage1);\r\n    containerPlacementMetadataStore.deleteAllContainerPlacementMessages(requestMessage1UUID);\r\n    // requestMessage1 & associated responseMessage1 should not be present\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage1UUID).isPresent());\r\n    Assert.assertTrue(!containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage1UUID).isPresent());\r\n    containerPlacementMetadataStore.deleteAllContainerPlacementMessages();\r\n    Assert.assertEquals(0, containerPlacementMetadataStore.readAllContainerPlacementRequestMessages().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementObjectMapper.java",
  "methodName" : "testIncomingContainerMessageSerDe",
  "sourceCode" : "@Test\r\npublic void testIncomingContainerMessageSerDe() throws IOException {\r\n    testContainerPlacementRequestMessage(new ContainerPlacementRequestMessage(UUID.randomUUID(), \"app-attempt-001\", \"4\", \"ANY_HOST\", System.currentTimeMillis()));\r\n    testContainerPlacementRequestMessage(new ContainerPlacementRequestMessage(UUID.randomUUID(), \"app-attempt-001\", \"4\", \"ANY_HOST\", Duration.ofSeconds(10), System.currentTimeMillis()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\container\\placement\\TestContainerPlacementObjectMapper.java",
  "methodName" : "testOutgoingContainerMessageSerDe",
  "sourceCode" : "@Test\r\npublic void testOutgoingContainerMessageSerDe() throws IOException {\r\n    testContainerPlacementResponseMessage(new ContainerPlacementResponseMessage(UUID.randomUUID(), \"app-attempt-001\", Integer.toString(new Random().nextInt(5)), \"ANY_HOST\", ContainerPlacementMessage.StatusCode.BAD_REQUEST, \"Request ignored redundant\", System.currentTimeMillis()));\r\n    testContainerPlacementResponseMessage(new ContainerPlacementResponseMessage(UUID.randomUUID(), \"app-attempt-001\", Integer.toString(new Random().nextInt(5)), \"ANY_HOST\", Duration.ofSeconds(10), ContainerPlacementMessage.StatusCode.IN_PROGRESS, \"Request is in progress\", System.currentTimeMillis()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\MockClusterResourceManager.java",
  "methodName" : "stopStreamProcessor",
  "sourceCode" : "@VisibleForTesting\r\nvoid stopStreamProcessor(SamzaResource resource, int exitCode) {\r\n    SamzaResourceStatus status = new SamzaResourceStatus(resource.getContainerId(), \"diagnostics\", exitCode);\r\n    List<SamzaResourceStatus> statList = new ArrayList<>();\r\n    statList.add(status);\r\n    clusterManagerCallback.onResourcesCompleted(statList);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testPartitionCountMonitorWithDurableStates",
  "sourceCode" : "@Test\r\npublic void testPartitionCountMonitorWithDurableStates() {\r\n    configMap.put(\"stores.mystore.changelog\", \"mychangelog\");\r\n    configMap.put(JobConfig.JOB_CONTAINER_COUNT, \"1\");\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(new MapConfig(configMap));\r\n    Config config = new MapConfig(configMap);\r\n    // mimic job runner code to write the config to coordinator stream\r\n    CoordinatorStreamSystemProducer producer = new CoordinatorStreamSystemProducer(config, mock(MetricsRegistry.class));\r\n    producer.writeConfig(\"test-job\", config);\r\n    ClusterBasedJobCoordinator clusterCoordinator = ClusterBasedJobCoordinatorRunner.createFromMetadataStore(config);\r\n    // change the input system stream metadata\r\n    MockSystemFactory.MSG_QUEUES.put(new SystemStreamPartition(\"kafka\", \"topic1\", new Partition(1)), new ArrayList<>());\r\n    StreamPartitionCountMonitor monitor = clusterCoordinator.getPartitionMonitor();\r\n    monitor.updatePartitionCountMetric();\r\n    assertEquals(clusterCoordinator.getAppStatus(), SamzaApplicationState.SamzaAppStatus.FAILED);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testPartitionCountMonitorWithoutDurableStates",
  "sourceCode" : "@Test\r\npublic void testPartitionCountMonitorWithoutDurableStates() {\r\n    configMap.put(JobConfig.JOB_CONTAINER_COUNT, \"1\");\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(new MapConfig(configMap));\r\n    Config config = new MapConfig(configMap);\r\n    // mimic job runner code to write the config to coordinator stream\r\n    CoordinatorStreamSystemProducer producer = new CoordinatorStreamSystemProducer(config, mock(MetricsRegistry.class));\r\n    producer.writeConfig(\"test-job\", config);\r\n    ClusterBasedJobCoordinator clusterCoordinator = ClusterBasedJobCoordinatorRunner.createFromMetadataStore(config);\r\n    // change the input system stream metadata\r\n    MockSystemFactory.MSG_QUEUES.put(new SystemStreamPartition(\"kafka\", \"topic1\", new Partition(1)), new ArrayList<>());\r\n    StreamPartitionCountMonitor monitor = clusterCoordinator.getPartitionMonitor();\r\n    monitor.updatePartitionCountMetric();\r\n    assertEquals(clusterCoordinator.getAppStatus(), SamzaApplicationState.SamzaAppStatus.UNDEFINED);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testVerifyStartpointManagerFanOut",
  "sourceCode" : "@Test\r\npublic void testVerifyStartpointManagerFanOut() throws IOException {\r\n    configMap.put(JobConfig.JOB_CONTAINER_COUNT, \"1\");\r\n    configMap.put(\"job.jmx.enabled\", \"false\");\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(new MapConfig(configMap));\r\n    Config config = new MapConfig(configMap);\r\n    MockitoException stopException = new MockitoException(\"Stop\");\r\n    ClusterBasedJobCoordinator clusterCoordinator = spy(ClusterBasedJobCoordinatorRunner.createFromMetadataStore(config));\r\n    ContainerProcessManager mockContainerProcessManager = mock(ContainerProcessManager.class);\r\n    doReturn(true).when(mockContainerProcessManager).shouldShutdown();\r\n    StartpointManager mockStartpointManager = mock(StartpointManager.class);\r\n    // Stop ClusterBasedJobCoordinator#run after stop() method by throwing an exception to stop the run loop.\r\n    // ClusterBasedJobCoordinator will need to be refactored for better mock support.\r\n    doThrow(stopException).when(mockStartpointManager).stop();\r\n    doReturn(mockContainerProcessManager).when(clusterCoordinator).createContainerProcessManager();\r\n    doReturn(mockStartpointManager).when(clusterCoordinator).createStartpointManager();\r\n    try {\r\n        clusterCoordinator.run();\r\n    } catch (SamzaException ex) {\r\n        assertEquals(stopException, ex.getCause());\r\n        verify(mockStartpointManager).start();\r\n        verify(mockStartpointManager).fanOut(any());\r\n        verify(mockStartpointManager).stop();\r\n        return;\r\n    }\r\n    fail(\"Expected run() method to stop after StartpointManager#stop()\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testVerifyShouldFanoutStartpointWithoutAMHA",
  "sourceCode" : "@Test\r\npublic void testVerifyShouldFanoutStartpointWithoutAMHA() {\r\n    Config jobConfig = new MapConfig(configMap);\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(jobConfig);\r\n    ClusterBasedJobCoordinator clusterBasedJobCoordinator = spy(ClusterBasedJobCoordinatorRunner.createFromMetadataStore(jobConfig));\r\n    when(clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts()).thenReturn(true);\r\n    assertTrue(\"Startpoint should fanout even if metadata changed\", clusterBasedJobCoordinator.shouldFanoutStartpoint());\r\n    when(clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts()).thenReturn(false);\r\n    assertTrue(\"Startpoint should fanout even if metadata remains unchanged\", clusterBasedJobCoordinator.shouldFanoutStartpoint());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testVerifyShouldFanoutStartpointWithAMHA",
  "sourceCode" : "@Test\r\npublic void testVerifyShouldFanoutStartpointWithAMHA() {\r\n    Config jobConfig = new MapConfig(configMap);\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(jobConfig);\r\n    ClusterBasedJobCoordinator clusterBasedJobCoordinator = spy(ClusterBasedJobCoordinatorRunner.createFromMetadataStore(jobConfig));\r\n    when(clusterBasedJobCoordinator.isApplicationMasterHighAvailabilityEnabled()).thenReturn(true);\r\n    when(clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts()).thenReturn(true);\r\n    assertTrue(\"Startpoint should fanout with change in metadata\", clusterBasedJobCoordinator.shouldFanoutStartpoint());\r\n    when(clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts()).thenReturn(false);\r\n    assertFalse(\"Startpoint fan out shouldn't happen when metadata is unchanged\", clusterBasedJobCoordinator.shouldFanoutStartpoint());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testToArgs",
  "sourceCode" : "@Test\r\npublic void testToArgs() {\r\n    ApplicationConfig appConfig = new ApplicationConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_NAME, \"test1\", ApplicationConfig.APP_CLASS, \"class1\", ApplicationConfig.APP_MAIN_ARGS, \"--runner=SamzaRunner --maxSourceParallelism=1024\")));\r\n    List<String> expected = Arrays.asList(\"--config\", \"job.name=test1\", \"--config\", \"app.class=class1\", \"--runner=SamzaRunner\", \"--maxSourceParallelism=1024\");\r\n    List<String> actual = Arrays.asList(ClusterBasedJobCoordinatorRunner.toArgs(appConfig));\r\n    // cannot assert expected equals to actual as the order can be different.\r\n    assertEquals(expected.size(), actual.size());\r\n    assertTrue(actual.containsAll(expected));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinator.java",
  "methodName" : "testGenerateAndUpdateJobCoordinatorMetadata",
  "sourceCode" : "@Test\r\npublic void testGenerateAndUpdateJobCoordinatorMetadata() {\r\n    Config jobConfig = new MapConfig(configMap);\r\n    when(CoordinatorStreamUtil.readConfigFromCoordinatorStream(anyObject())).thenReturn(jobConfig);\r\n    ClusterBasedJobCoordinator clusterBasedJobCoordinator = spy(ClusterBasedJobCoordinatorRunner.createFromMetadataStore(jobConfig));\r\n    JobCoordinatorMetadata previousMetadata = mock(JobCoordinatorMetadata.class);\r\n    JobCoordinatorMetadata newMetadata = mock(JobCoordinatorMetadata.class);\r\n    JobCoordinatorMetadataManager jobCoordinatorMetadataManager = mock(JobCoordinatorMetadataManager.class);\r\n    JobModel mockJobModel = mock(JobModel.class);\r\n    when(jobCoordinatorMetadataManager.readJobCoordinatorMetadata()).thenReturn(previousMetadata);\r\n    when(jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(any(), any())).thenReturn(newMetadata);\r\n    when(jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata)).thenReturn(ImmutableSet.of());\r\n    when(clusterBasedJobCoordinator.createJobCoordinatorMetadataManager()).thenReturn(jobCoordinatorMetadataManager);\r\n    /*\r\n     * Verify if there are no changes to metadata, the metadata changed flag remains false and no interactions\r\n     * with job coordinator metadata manager\r\n     */\r\n    clusterBasedJobCoordinator.generateAndUpdateJobCoordinatorMetadata(mockJobModel);\r\n    assertFalse(\"JC metadata changed should remain unchanged\", clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts());\r\n    verify(jobCoordinatorMetadataManager, times(0)).writeJobCoordinatorMetadata(any());\r\n    /*\r\n     * Verify if there are changes to metadata, we persist the new metadata & update the metadata changed flag\r\n     */\r\n    when(jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata)).thenReturn(ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT));\r\n    clusterBasedJobCoordinator.generateAndUpdateJobCoordinatorMetadata(mockJobModel);\r\n    assertTrue(\"JC metadata changed should be true\", clusterBasedJobCoordinator.isMetadataChangedAcrossAttempts());\r\n    verify(jobCoordinatorMetadataManager, times(1)).writeJobCoordinatorMetadata(newMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestClusterBasedJobCoordinatorRunner.java",
  "methodName" : "testRunClusterBasedJobCoordinator",
  "sourceCode" : "@Test\r\npublic void testRunClusterBasedJobCoordinator() throws Exception {\r\n    Config submissionConfig = new MapConfig(ImmutableMap.of(JobConfig.CONFIG_LOADER_FACTORY, PropertiesConfigLoaderFactory.class.getName(), PropertiesConfigLoaderFactory.CONFIG_LOADER_PROPERTIES_PREFIX + \"path\", getClass().getResource(\"/test.properties\").getPath()));\r\n    Config fullConfig = ConfigUtil.loadConfig(submissionConfig);\r\n    StreamApplication mockApplication = mock(StreamApplication.class);\r\n    PowerMockito.mockStatic(System.class, ApplicationUtil.class, JobCoordinatorLaunchUtil.class);\r\n    PowerMockito.when(System.getenv(eq(ShellCommandConfig.ENV_SUBMISSION_CONFIG))).thenReturn(SamzaObjectMapper.getObjectMapper().writeValueAsString(submissionConfig));\r\n    PowerMockito.when(ApplicationUtil.fromConfig(any())).thenReturn(mockApplication);\r\n    PowerMockito.doNothing().when(JobCoordinatorLaunchUtil.class, \"run\", mockApplication, fullConfig);\r\n    ClusterBasedJobCoordinatorRunner.runClusterBasedJobCoordinator(null);\r\n    PowerMockito.verifyStatic(times(1));\r\n    JobCoordinatorLaunchUtil.run(mockApplication, fullConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testRequestContainersWithNoMapping",
  "sourceCode" : "/**\r\n * Test request containers with no containerToHostMapping makes the right number of requests\r\n */\r\n@Test\r\npublic void testRequestContainersWithNoMapping() throws Exception {\r\n    int containerCount = 4;\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>();\r\n    for (int i = 0; i < containerCount; i++) {\r\n        containersToHostMapping.put(String.valueOf(i), null);\r\n    }\r\n    allocatorThread.start();\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertNotNull(requestState);\r\n    assertEquals(4, requestState.numPendingRequests());\r\n    assertNotNull(requestState.getHostRequestCounts());\r\n    assertEquals(1, requestState.getHostRequestCounts().keySet().size());\r\n    assertTrue(requestState.getHostRequestCounts().keySet().contains(ResourceRequestState.ANY_HOST));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testAddContainerWithHostAffinity",
  "sourceCode" : "/**\r\n * Add containers to the correct host in the request state\r\n */\r\n@Test\r\npublic void testAddContainerWithHostAffinity() throws Exception {\r\n    containerAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"abc\");\r\n            put(\"1\", \"xyz\");\r\n        }\r\n    });\r\n    assertNotNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(0, requestState.getResourcesOnAHost(\"abc\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(\"xyz\"));\r\n    assertEquals(0, requestState.getResourcesOnAHost(\"xyz\").size());\r\n    assertNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID1\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"def\", \"ID2\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"xyz\", \"ID3\"));\r\n    assertNotNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(1, requestState.getResourcesOnAHost(\"abc\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(\"xyz\"));\r\n    assertEquals(1, requestState.getResourcesOnAHost(\"xyz\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    assertTrue(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).size() == 1);\r\n    assertEquals(\"ID2\", requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).get(0).getContainerId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testSurplusResourcesAreBufferedUnderAnyHost",
  "sourceCode" : "/**\r\n * Test that extra resources are buffered under ANY_HOST\r\n */\r\n@Test\r\npublic void testSurplusResourcesAreBufferedUnderAnyHost() throws Exception {\r\n    containerAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"abc\");\r\n            put(\"1\", \"xyz\");\r\n        }\r\n    });\r\n    assertNotNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(0, requestState.getResourcesOnAHost(\"abc\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(\"xyz\"));\r\n    assertEquals(0, requestState.getResourcesOnAHost(\"xyz\").size());\r\n    assertNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID1\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"xyz\", \"ID2\"));\r\n    // surplus resources for host - \"abc\"\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID3\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID4\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID5\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 10, \"abc\", \"ID6\"));\r\n    assertNotNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(1, requestState.getResourcesOnAHost(\"abc\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(\"xyz\"));\r\n    assertEquals(1, requestState.getResourcesOnAHost(\"xyz\").size());\r\n    assertNotNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    // assert that the surplus resources goto the ANY_HOST buffer\r\n    assertTrue(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).size() == 4);\r\n    assertEquals(\"ID3\", requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).get(0).getContainerId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testAllocatorReleasesExtraContainers",
  "sourceCode" : "@Test\r\npublic void testAllocatorReleasesExtraContainers() throws Exception {\r\n    final SamzaResource resource0 = new SamzaResource(1, 1024, \"abc\", \"id1\");\r\n    final SamzaResource resource1 = new SamzaResource(1, 1024, \"abc\", \"id2\");\r\n    final SamzaResource resource2 = new SamzaResource(1, 1024, \"def\", \"id3\");\r\n    Runnable releasedAssertions = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            assertEquals(2, clusterResourceManager.releasedResources.size());\r\n            assertTrue(clusterResourceManager.releasedResources.contains(resource1));\r\n            assertTrue(clusterResourceManager.releasedResources.contains(resource2));\r\n            // Test that state is cleaned up\r\n            assertEquals(0, requestState.numPendingRequests());\r\n            assertEquals(0, requestState.getHostRequestCounts().size());\r\n            assertNull(requestState.getResourcesOnAHost(\"abc\"));\r\n            assertNull(requestState.getResourcesOnAHost(\"def\"));\r\n        }\r\n    };\r\n    // Set up our final asserts before starting the allocator thread\r\n    MockContainerListener listener = new MockContainerListener(3, 2, 0, 0, null, releasedAssertions, null, null);\r\n    requestState.registerContainerListener(listener);\r\n    allocatorThread.start();\r\n    containerAllocator.requestResource(\"0\", \"abc\");\r\n    containerAllocator.addResource(resource0);\r\n    containerAllocator.addResource(resource1);\r\n    containerAllocator.addResource(resource2);\r\n    listener.verify();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testRequestContainers",
  "sourceCode" : "@Test\r\npublic void testRequestContainers() throws Exception {\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"abc\");\r\n            put(\"1\", \"def\");\r\n            put(\"2\", null);\r\n            put(\"3\", \"abc\");\r\n        }\r\n    };\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertNotNull(clusterResourceManager.resourceRequests);\r\n    assertEquals(clusterResourceManager.resourceRequests.size(), 4);\r\n    assertEquals(requestState.numPendingRequests(), 4);\r\n    Map<String, AtomicInteger> requestsMap = requestState.getHostRequestCounts();\r\n    assertNotNull(requestsMap.get(\"abc\"));\r\n    assertEquals(2, requestsMap.get(\"abc\").get());\r\n    assertNotNull(requestsMap.get(\"def\"));\r\n    assertEquals(1, requestsMap.get(\"def\").get());\r\n    assertNotNull(requestsMap.get(ResourceRequestState.ANY_HOST));\r\n    assertEquals(1, requestsMap.get(ResourceRequestState.ANY_HOST).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testDelayedRequestedContainers",
  "sourceCode" : "@Test\r\npublic void testDelayedRequestedContainers() {\r\n    containerAllocator.requestResource(\"0\", \"abc\");\r\n    containerAllocator.requestResourceWithDelay(\"0\", \"efg\", Duration.ofHours(2));\r\n    containerAllocator.requestResourceWithDelay(\"0\", \"hij\", Duration.ofHours(3));\r\n    containerAllocator.requestResourceWithDelay(\"0\", \"klm\", Duration.ofHours(4));\r\n    assertNotNull(clusterResourceManager.resourceRequests);\r\n    assertEquals(clusterResourceManager.resourceRequests.size(), 1);\r\n    assertEquals(requestState.numPendingRequests(), 1);\r\n    assertEquals(requestState.numDelayedRequests(), 3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testExpiredRequestAreAssignedToAnyHost",
  "sourceCode" : "/**\r\n * Handles expired requests correctly and assigns ANY_HOST\r\n */\r\n@Test\r\npublic void testExpiredRequestAreAssignedToAnyHost() throws Exception {\r\n    final SamzaResource resource0 = new SamzaResource(1, 1000, \"xyz\", \"id1\");\r\n    final SamzaResource resource1 = new SamzaResource(1, 1000, \"zzz\", \"id2\");\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"abc\");\r\n            put(\"1\", \"def\");\r\n        }\r\n    };\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertEquals(requestState.numPendingRequests(), 2);\r\n    assertNotNull(requestState.getHostRequestCounts());\r\n    assertNotNull(requestState.getHostRequestCounts().get(\"abc\"));\r\n    assertTrue(requestState.getHostRequestCounts().get(\"abc\").get() == 1);\r\n    assertNotNull(requestState.getHostRequestCounts().get(\"def\"));\r\n    assertTrue(requestState.getHostRequestCounts().get(\"def\").get() == 1);\r\n    Runnable addContainerAssertions = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            assertNull(requestState.getResourcesOnAHost(\"xyz\"));\r\n            assertNull(requestState.getResourcesOnAHost(\"zzz\"));\r\n            assertNotNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n            assertTrue(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).size() == 2);\r\n        }\r\n    };\r\n    Runnable assignContainerAssertions = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            assertEquals(requestState.numPendingRequests(), 0);\r\n            assertNotNull(requestState.getHostRequestCounts());\r\n            assertNotNull(requestState.getHostRequestCounts().get(\"abc\"));\r\n            assertNotNull(requestState.getHostRequestCounts().get(\"def\"));\r\n        }\r\n    };\r\n    Runnable runningContainerAssertions = new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            assertTrue(clusterResourceManager.launchedResources.contains(resource0));\r\n            assertTrue(clusterResourceManager.launchedResources.contains(resource1));\r\n        }\r\n    };\r\n    MockContainerListener listener = new MockContainerListener(2, 0, 2, 2, addContainerAssertions, null, assignContainerAssertions, runningContainerAssertions);\r\n    requestState.registerContainerListener(listener);\r\n    ((MockClusterResourceManager) clusterResourceManager).registerContainerListener(listener);\r\n    containerAllocator.addResource(resource0);\r\n    containerAllocator.addResource(resource1);\r\n    allocatorThread.start();\r\n    listener.verify();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testExpiredRequestsAreCancelled",
  "sourceCode" : "@Test\r\npublic void testExpiredRequestsAreCancelled() throws Exception {\r\n    // request one container each on host-1 and host-2\r\n    containerAllocator.requestResources(ImmutableMap.of(\"0\", \"host-1\", \"1\", \"host-2\"));\r\n    // assert that the requests made it to YARN\r\n    Assert.assertEquals(clusterResourceManager.resourceRequests.size(), 2);\r\n    // allocate one resource from YARN on a different host (host-3)\r\n    SamzaResource resource0 = new SamzaResource(1, 1000, \"host-3\", \"id1\");\r\n    containerAllocator.addResource(resource0);\r\n    // let the matching begin\r\n    allocatorThread.start();\r\n    // verify that a container is launched on host-3 after the request expires\r\n    if (!clusterResourceManager.awaitContainerLaunch(1, 20, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"Timed out waiting container launch\");\r\n    }\r\n    Assert.assertEquals(1, clusterResourceManager.launchedResources.size());\r\n    Assert.assertEquals(clusterResourceManager.launchedResources.get(0).getHost(), \"host-3\");\r\n    Assert.assertEquals(clusterResourceManager.launchedResources.get(0).getContainerId(), \"id1\");\r\n    // Now, there are no more resources left to run the 2nd container. Verify that we eventually issue another request\r\n    if (!clusterResourceManager.awaitResourceRequests(4, 20, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"Timed out waiting for resource requests\");\r\n    }\r\n    // verify that we have cancelled previous requests and there's one outstanding request\r\n    Assert.assertEquals(clusterResourceManager.cancelledRequests.size(), 3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testRequestAllocationOnPreferredHostWithRunStreamProcessor",
  "sourceCode" : "@Test\r\npublic void testRequestAllocationOnPreferredHostWithRunStreamProcessor() throws Exception {\r\n    ClusterResourceManager.Callback mockCPM = mock(MockClusterResourceManagerCallback.class);\r\n    ClusterResourceManager mockClusterResourceManager = new MockClusterResourceManager(mockCPM, state);\r\n    ContainerManager containerManager = new ContainerManager(containerPlacementMetadataStore, state, mockClusterResourceManager, true, false, mock(LocalityManager.class), faultDomainManager, config);\r\n    // Mock the callback from ClusterManager to add resources to the allocator\r\n    doAnswer((InvocationOnMock invocation) -> {\r\n        SamzaResource resource = (SamzaResource) invocation.getArgumentAt(0, List.class).get(0);\r\n        spyAllocator.addResource(resource);\r\n        return null;\r\n    }).when(mockCPM).onResourcesAvailable(anyList());\r\n    spyAllocator = Mockito.spy(new ContainerAllocator(mockClusterResourceManager, config, state, true, containerManager));\r\n    // Request Resources\r\n    spyAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"abc\");\r\n            put(\"1\", \"xyz\");\r\n        }\r\n    });\r\n    spyAllocatorThread = new Thread(spyAllocator);\r\n    // Start the container allocator thread periodic assignment\r\n    spyAllocatorThread.start();\r\n    // Let Allocator thread periodically fulfill requests\r\n    Thread.sleep(100);\r\n    // Verify that all the request that were created were preferred host requests\r\n    ArgumentCaptor<SamzaResourceRequest> resourceRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);\r\n    verify(spyAllocator, times(2)).runStreamProcessor(resourceRequestCaptor.capture(), anyString());\r\n    resourceRequestCaptor.getAllValues().forEach(resourceRequest -> assertNotEquals(resourceRequest.getPreferredHost(), ResourceRequestState.ANY_HOST));\r\n    Set<String> hostNames = resourceRequestCaptor.getAllValues().stream().map(request -> request.getPreferredHost()).collect(Collectors.toSet());\r\n    assertTrue(hostNames.contains(\"abc\"));\r\n    assertTrue(hostNames.contains(\"xyz\"));\r\n    // No any host requests should be made if preferred host is satisfied\r\n    assertTrue(state.anyHostRequests.get() == 0);\r\n    // State check when host affinity is enabled\r\n    assertTrue(state.matchedResourceRequests.get() == 2);\r\n    assertTrue(state.preferredHostRequests.get() == 2);\r\n    spyAllocator.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testExpiredRequestAllocationOnAnyHost",
  "sourceCode" : "@Test\r\npublic void testExpiredRequestAllocationOnAnyHost() throws Exception {\r\n    MockClusterResourceManager spyManager = spy(new MockClusterResourceManager(callback, state));\r\n    ContainerManager spyContainerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, spyManager, true, false, mock(LocalityManager.class), faultDomainManager, config));\r\n    spyAllocator = Mockito.spy(new ContainerAllocator(spyManager, config, state, true, spyContainerManager));\r\n    // Request Preferred Resources\r\n    spyAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"hostname-0\");\r\n            put(\"1\", \"hostname-1\");\r\n        }\r\n    });\r\n    spyAllocatorThread = new Thread(spyAllocator);\r\n    // Start the container allocator thread periodic assignment\r\n    spyAllocatorThread.start();\r\n    // Let the preferred host requests and the follow-up ANY_HOST request expire, expiration timeout is 500 ms\r\n    Thread.sleep(1500);\r\n    // Verify that all the request that were created as preferred host requests expired\r\n    assertTrue(state.preferredHostRequests.get() == 2);\r\n    assertTrue(state.expiredPreferredHostRequests.get() == 2);\r\n    // expirations for initial preferred host requests\r\n    verify(spyContainerManager).handleExpiredRequest(eq(\"0\"), eq(\"hostname-0\"), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    verify(spyContainerManager).handleExpiredRequest(eq(\"1\"), eq(\"hostname-1\"), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    // expirations for follow-up ANY_HOST requests\r\n    // allocator keeps running in a loop so it might expire more than once during the wait time\r\n    verify(spyContainerManager, atLeast(1)).handleExpiredRequest(eq(\"0\"), eq(ResourceRequestState.ANY_HOST), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    verify(spyContainerManager, atLeast(1)).handleExpiredRequest(eq(\"1\"), eq(ResourceRequestState.ANY_HOST), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    // Verify that preferred host request were cancelled and since no surplus resources were available\r\n    // requestResource was invoked with ANY_HOST requests\r\n    ArgumentCaptor<SamzaResourceRequest> cancelledRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);\r\n    // preferred host requests and ANY_HOST requests got cancelled\r\n    verify(spyManager, atLeast(4)).cancelResourceRequest(cancelledRequestCaptor.capture());\r\n    assertEquals(ImmutableSet.of(\"hostname-0\", \"hostname-1\", ResourceRequestState.ANY_HOST), cancelledRequestCaptor.getAllValues().stream().map(SamzaResourceRequest::getPreferredHost).collect(Collectors.toSet()));\r\n    assertTrue(state.matchedResourceRequests.get() == 0);\r\n    // at least 2 for expired preferred host requests, 2 for expired follow-up ANY_HOST requests\r\n    assertTrue(state.anyHostRequests.get() >= 4);\r\n    spyAllocator.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testExpiredRequestAllocationOnSurplusAnyHostWithRunStreamProcessor",
  "sourceCode" : "@Test\r\npublic void testExpiredRequestAllocationOnSurplusAnyHostWithRunStreamProcessor() throws Exception {\r\n    // Add Extra Resources\r\n    MockClusterResourceManager spyClusterResourceManager = spy(new MockClusterResourceManager(callback, state));\r\n    ContainerManager spyContainerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, spyClusterResourceManager, true, false, mock(LocalityManager.class), faultDomainManager, config));\r\n    spyAllocator = Mockito.spy(new ContainerAllocator(spyClusterResourceManager, config, state, true, spyContainerManager));\r\n    spyAllocator.addResource(new SamzaResource(1, 1000, \"xyz\", \"id1\"));\r\n    spyAllocator.addResource(new SamzaResource(1, 1000, \"zzz\", \"id2\"));\r\n    // Request Preferred Resources\r\n    spyAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"hostname-0\");\r\n            put(\"1\", \"hostname-1\");\r\n        }\r\n    });\r\n    spyAllocatorThread = new Thread(spyAllocator);\r\n    // Start the container allocator thread periodic assignment\r\n    spyAllocatorThread.start();\r\n    // Let the request expire, expiration timeout is 500 ms\r\n    Thread.sleep(1000);\r\n    // Verify that all the request that were created as preferred host requests expired\r\n    assertEquals(state.expiredPreferredHostRequests.get(), 2);\r\n    verify(spyContainerManager, times(1)).handleExpiredRequest(eq(\"0\"), eq(\"hostname-0\"), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    verify(spyContainerManager, times(1)).handleExpiredRequest(eq(\"1\"), eq(\"hostname-1\"), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    // Verify that runStreamProcessor was invoked with already available ANY_HOST requests\r\n    ArgumentCaptor<SamzaResourceRequest> resourceRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);\r\n    ArgumentCaptor<String> hostCaptor = ArgumentCaptor.forClass(String.class);\r\n    verify(spyAllocator, times(2)).runStreamProcessor(resourceRequestCaptor.capture(), hostCaptor.capture());\r\n    // Resource request were preferred host requests\r\n    resourceRequestCaptor.getAllValues().forEach(resourceRequest -> assertNotEquals(resourceRequest.getPreferredHost(), ResourceRequestState.ANY_HOST));\r\n    // Since requests expired, allocator ran the requests on surplus available ANY_HOST\r\n    hostCaptor.getAllValues().forEach(host -> assertEquals(host, ResourceRequestState.ANY_HOST));\r\n    // State Update check\r\n    assertTrue(state.matchedResourceRequests.get() == 0);\r\n    assertTrue(state.preferredHostRequests.get() == 2);\r\n    assertTrue(state.anyHostRequests.get() == 0);\r\n    spyAllocator.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithHostAffinity.java",
  "methodName" : "testExpiredAllocatedResourcesAreReleased",
  "sourceCode" : "@Test(timeout = 5000)\r\npublic void testExpiredAllocatedResourcesAreReleased() throws Exception {\r\n    ClusterResourceManager.Callback mockCPM = mock(MockClusterResourceManagerCallback.class);\r\n    MockClusterResourceManager mockClusterResourceManager = new MockClusterResourceManager(mockCPM, state);\r\n    ContainerManager spyContainerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, mockClusterResourceManager, true, false, mock(LocalityManager.class), faultDomainManager, config));\r\n    SamzaResource expiredAllocatedResource = new SamzaResource(1, 1000, \"host-0\", \"id0\", System.currentTimeMillis() - Duration.ofMinutes(10).toMillis());\r\n    spyAllocator = Mockito.spy(new ContainerAllocator(mockClusterResourceManager, config, state, true, spyContainerManager));\r\n    spyAllocator.addResource(expiredAllocatedResource);\r\n    spyAllocator.addResource(new SamzaResource(1, 1000, \"host-1\", \"1d1\"));\r\n    // Request Preferred Resources\r\n    spyAllocator.requestResources(new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"host-0\");\r\n            put(\"1\", \"host-1\");\r\n        }\r\n    });\r\n    spyAllocatorThread = new Thread(spyAllocator);\r\n    // Start the container allocator thread periodic assignment\r\n    spyAllocatorThread.start();\r\n    // Wait until allocated resource is expired\r\n    while (state.preferredHostRequests.get() != 3) {\r\n        Thread.sleep(100);\r\n    }\r\n    // Verify that handleExpiredResource was invoked once for expired allocated resource\r\n    ArgumentCaptor<SamzaResourceRequest> resourceRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);\r\n    ArgumentCaptor<SamzaResource> resourceArgumentCaptor = ArgumentCaptor.forClass(SamzaResource.class);\r\n    verify(spyContainerManager, times(1)).handleExpiredResource(resourceRequestCaptor.capture(), resourceArgumentCaptor.capture(), eq(\"host-0\"), any(), any());\r\n    resourceRequestCaptor.getAllValues().forEach(resourceRequest -> assertEquals(resourceRequest.getProcessorId(), \"0\"));\r\n    resourceArgumentCaptor.getAllValues().forEach(resource -> assertEquals(resource.getHost(), \"host-0\"));\r\n    // Verify resources were released\r\n    assertTrue(mockClusterResourceManager.containsReleasedResource(expiredAllocatedResource));\r\n    spyAllocator.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testAddContainer",
  "sourceCode" : "/**\r\n * Adds all containers returned to ANY_HOST only\r\n */\r\n@Test\r\npublic void testAddContainer() throws Exception {\r\n    assertNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    containerAllocator.addResource(new SamzaResource(1, 1000, \"abc\", \"id1\"));\r\n    containerAllocator.addResource(new SamzaResource(1, 1000, \"xyz\", \"id1\"));\r\n    assertNull(requestState.getResourcesOnAHost(\"abc\"));\r\n    assertNotNull(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST));\r\n    assertTrue(requestState.getResourcesOnAHost(ResourceRequestState.ANY_HOST).size() == 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testRequestContainers",
  "sourceCode" : "/**\r\n * Test requestContainers\r\n */\r\n@Test\r\npublic void testRequestContainers() throws Exception {\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", null);\r\n            put(\"1\", null);\r\n            put(\"2\", null);\r\n            put(\"3\", null);\r\n        }\r\n    };\r\n    allocatorThread.start();\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertEquals(4, manager.resourceRequests.size());\r\n    assertNotNull(requestState);\r\n    assertEquals(requestState.numPendingRequests(), 4);\r\n    // If host-affinty is not enabled, it doesn't update the requestMap\r\n    assertNotNull(requestState.getHostRequestCounts());\r\n    assertEquals(requestState.getHostRequestCounts().keySet().size(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testExpiredRequestInfiniteLoop",
  "sourceCode" : "/**\r\n * See SAMZA-2601: we want to prevent an infinite loop in the case of expired request call with host affinity\r\n * disabled. This test make sure we don't have that infinite loop.\r\n */\r\n@Test\r\npublic void testExpiredRequestInfiniteLoop() throws Exception {\r\n    Config override = new MapConfig(new HashMap<String, String>() {\r\n\r\n        {\r\n            // override to have a proper sleep interval for this test\r\n            put(\"cluster-manager.allocator.sleep.ms\", \"100\");\r\n        }\r\n    });\r\n    LocalityManager mockLocalityManager = mock(LocalityManager.class);\r\n    when(mockLocalityManager.readLocality()).thenReturn(new LocalityModel(new HashMap<>()));\r\n    ContainerManager containerManager = new ContainerManager(containerPlacementMetadataStore, state, manager, false, false, mockLocalityManager, faultDomainManager, config);\r\n    containerAllocator = MockContainerAllocatorWithoutHostAffinity.createContainerAllocatorWithConfigOverride(manager, config, state, containerManager, override);\r\n    MockContainerAllocatorWithoutHostAffinity mockAllocator = (MockContainerAllocatorWithoutHostAffinity) containerAllocator;\r\n    mockAllocator.setOverrideIsRequestExpired();\r\n    allocatorThread = new Thread(containerAllocator);\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", null);\r\n            put(\"1\", null);\r\n            put(\"2\", null);\r\n            put(\"3\", null);\r\n        }\r\n    };\r\n    allocatorThread.start();\r\n    mockAllocator.requestResources(containersToHostMapping);\r\n    // Wait for at least one expired request call is made, which should happen.\r\n    // If the test passes, this should return immediately (within 100 ms). Only when the test fails will it exhaust the\r\n    // timeout, which is worth the wait to find out the failure\r\n    assertTrue(mockAllocator.awaitIsRequestExpiredCall(TimeUnit.SECONDS.toMillis(10)));\r\n    // TODO: we can eliminate the thread sleep if the whole container allocator and test codes are refactored to use\r\n    // a Clock which can be simulated and controlled.\r\n    Thread.sleep(500);\r\n    // Given that we wait for 500 ms above, and a sleep interval of 100 ms, we should roughly see 5 times the\r\n    // isRequestExpired is called. We give some extra buffer here (<100). Because if we do run into infinite loop,\r\n    // isRequestExpired would be called MILLIONS of times (4~5 million times after a dozen of runs on my machine).\r\n    assertTrue(String.format(\"Too many call count: %d. Seems to be in infinite loop\", mockAllocator.getExpiredRequestCallCount()), mockAllocator.getExpiredRequestCallCount() < 100);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testRequestContainersWithExistingHosts",
  "sourceCode" : "/**\r\n * Test requestContainers with containerToHostMapping with host.affinity disabled\r\n */\r\n@Test\r\npublic void testRequestContainersWithExistingHosts() throws Exception {\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"prev_host\");\r\n            put(\"1\", \"prev_host\");\r\n            put(\"2\", \"prev_host\");\r\n            put(\"3\", \"prev_host\");\r\n        }\r\n    };\r\n    allocatorThread.start();\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertEquals(4, manager.resourceRequests.size());\r\n    assertNotNull(requestState);\r\n    assertEquals(4, requestState.numPendingRequests());\r\n    assertEquals(0, requestState.numDelayedRequests());\r\n    // If host-affinty is not enabled, it doesn't update the requestMap\r\n    assertNotNull(requestState.getHostRequestCounts());\r\n    assertEquals(0, requestState.getHostRequestCounts().keySet().size());\r\n    assertNotNull(state);\r\n    assertEquals(4, state.anyHostRequests.get());\r\n    assertEquals(0, state.preferredHostRequests.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testRequestContainersWithNoMapping",
  "sourceCode" : "/**\r\n * Test request containers with no containerToHostMapping makes the right number of requests\r\n */\r\n@Test\r\npublic void testRequestContainersWithNoMapping() throws Exception {\r\n    int containerCount = 4;\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>();\r\n    for (int i = 0; i < containerCount; i++) {\r\n        containersToHostMapping.put(String.valueOf(i), null);\r\n    }\r\n    allocatorThread.start();\r\n    containerAllocator.requestResources(containersToHostMapping);\r\n    assertNotNull(requestState);\r\n    assertEquals(4, requestState.numPendingRequests());\r\n    assertEquals(0, requestState.numDelayedRequests());\r\n    // If host-affinty is not enabled, it doesn't update the requestMap\r\n    assertNotNull(requestState.getHostRequestCounts());\r\n    assertEquals(0, requestState.getHostRequestCounts().keySet().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testAllocatorReleasesExtraContainers",
  "sourceCode" : "/**\r\n * Extra allocated containers that are returned by the RM and unused by the AM should be released.\r\n * Containers are considered \"extra\" only when there are no more pending requests to fulfill\r\n * @throws Exception\r\n */\r\n@Test\r\npublic void testAllocatorReleasesExtraContainers() throws Exception {\r\n    final SamzaResource resource = new SamzaResource(1, 1000, \"abc\", \"id1\");\r\n    final SamzaResource resource1 = new SamzaResource(1, 1000, \"abc\", \"id2\");\r\n    final SamzaResource resource2 = new SamzaResource(1, 1000, \"def\", \"id3\");\r\n    // Set up our final asserts before starting the allocator thread\r\n    MockContainerListener listener = new MockContainerListener(3, 2, 0, 0, null, new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            assertTrue(manager.releasedResources.contains(resource1));\r\n            assertTrue(manager.releasedResources.contains(resource2));\r\n            // Test that state is cleaned up\r\n            assertEquals(0, requestState.numPendingRequests());\r\n            assertEquals(0, requestState.numDelayedRequests());\r\n            assertEquals(0, requestState.getHostRequestCounts().size());\r\n            assertNull(requestState.getResourcesOnAHost(\"abc\"));\r\n            assertNull(requestState.getResourcesOnAHost(\"def\"));\r\n        }\r\n    }, null, null);\r\n    requestState.registerContainerListener(listener);\r\n    allocatorThread.start();\r\n    containerAllocator.requestResource(\"0\", \"abc\");\r\n    containerAllocator.addResource(resource);\r\n    containerAllocator.addResource(resource1);\r\n    containerAllocator.addResource(resource2);\r\n    listener.verify();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerAllocatorWithoutHostAffinity.java",
  "methodName" : "testRequestAllocationWithRunStreamProcessor",
  "sourceCode" : "/**\r\n * Test the complete flow from container request creation to allocation when host affinity is disabled\r\n */\r\n@Test\r\npublic void testRequestAllocationWithRunStreamProcessor() throws Exception {\r\n    Map<String, String> containersToHostMapping = new HashMap<String, String>() {\r\n\r\n        {\r\n            put(\"0\", \"prev_host\");\r\n            put(\"1\", \"prev_host\");\r\n            put(\"2\", \"prev_host\");\r\n            put(\"3\", \"prev_host\");\r\n        }\r\n    };\r\n    ClusterResourceManager.Callback mockCPM = mock(ClusterResourceManager.Callback.class);\r\n    ClusterResourceManager mockManager = new MockClusterResourceManager(mockCPM, state);\r\n    ContainerManager spyContainerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, mockManager, false, false, mock(LocalityManager.class), faultDomainManager, config));\r\n    spyAllocator = Mockito.spy(new ContainerAllocator(mockManager, config, state, false, spyContainerManager));\r\n    // Mock the callback from ClusterManager to add resources to the allocator\r\n    doAnswer((InvocationOnMock invocation) -> {\r\n        SamzaResource resource = (SamzaResource) invocation.getArgumentAt(0, List.class).get(0);\r\n        spyAllocator.addResource(resource);\r\n        return null;\r\n    }).when(mockCPM).onResourcesAvailable(anyList());\r\n    // Request Resources\r\n    spyAllocator.requestResources(containersToHostMapping);\r\n    spyThread = new Thread(spyAllocator, \"Container Allocator Thread\");\r\n    // Start the container allocator thread periodic assignment\r\n    spyThread.start();\r\n    // TODO: we can eliminate the thread sleep if the whole container allocator and test codes are refactored to use\r\n    // a Clock which can be simulated and controlled.\r\n    Thread.sleep(1000);\r\n    // Verify that all the request that were created were \"ANY_HOST\" requests\r\n    ArgumentCaptor<SamzaResourceRequest> resourceRequestCaptor = ArgumentCaptor.forClass(SamzaResourceRequest.class);\r\n    verify(spyAllocator, times(4)).runStreamProcessor(resourceRequestCaptor.capture(), anyString());\r\n    resourceRequestCaptor.getAllValues().forEach(resourceRequest -> assertEquals(resourceRequest.getPreferredHost(), ResourceRequestState.ANY_HOST));\r\n    assertTrue(state.anyHostRequests.get() == containersToHostMapping.size());\r\n    // Expiry currently should not be invoked for host affinity enabled cases only\r\n    verify(spyContainerManager, never()).handleExpiredRequest(anyString(), anyString(), any(SamzaResourceRequest.class), any(ContainerAllocator.class), any(ResourceRequestState.class));\r\n    // Only updated when host affinity is enabled\r\n    assertTrue(state.matchedResourceRequests.get() == 0);\r\n    assertTrue(state.preferredHostRequests.get() == 0);\r\n    spyAllocator.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testContainerSuccessfulMoveActionWithoutStandby",
  "sourceCode" : "@Test(timeout = 10000)\r\npublic void testContainerSuccessfulMoveActionWithoutStandby() throws Exception {\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 5, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.preferredHostRequests.get(), 2);\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Initiate container placement action to move a container with container id 0\r\n    ContainerPlacementRequestMessage requestMessage = new ContainerPlacementRequestMessage(UUID.randomUUID(), \"appAttempt-001\", \"0\", \"host-3\", System.currentTimeMillis());\r\n    ContainerPlacementMetadata metadata = containerManager.registerContainerPlacementActionForTest(requestMessage, allocatorWithHostAffinity);\r\n    // Wait for the ControlAction to complete\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(1, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    Optional<ContainerPlacementResponseMessage> responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    // Wait for the placement action to be complete & get written to the underlying metastore\r\n    while (true) {\r\n        if (metadata.getActionStatus() == ContainerPlacementMessage.StatusCode.SUCCEEDED && responseMessage.isPresent() && responseMessage.get().getStatusCode() == ContainerPlacementMessage.StatusCode.SUCCEEDED) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n        responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    }\r\n    assertEquals(state.preferredHostRequests.get(), 3);\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-3\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    assertEquals(metadata.getActionStatus(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    assertTrue(responseMessage.isPresent());\r\n    assertEquals(responseMessage.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    assertResponseMessage(responseMessage.get(), requestMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testActionQueuingForConsecutivePlacementActions",
  "sourceCode" : "@Test(timeout = 30000)\r\npublic void testActionQueuingForConsecutivePlacementActions() throws Exception {\r\n    // Spawn a Request Allocator Thread\r\n    ContainerPlacementRequestAllocator requestAllocator = new ContainerPlacementRequestAllocator(containerPlacementMetadataStore, cpm, new ApplicationConfig(config), 100);\r\n    Thread requestAllocatorThread = new Thread(requestAllocator, \"ContainerPlacement Request Allocator Thread\");\r\n    requestAllocatorThread.start();\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 5, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.preferredHostRequests.get(), 2);\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Initiate container placement action to move a container with container id 0\r\n    UUID requestUUIDMove1 = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0\", \"host-3\", null, System.currentTimeMillis());\r\n    UUID requestUUIDMoveBad = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-002\", \"0\", \"host-4\", null, System.currentTimeMillis());\r\n    UUID requestUUIDMove2 = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0\", \"host-4\", null, System.currentTimeMillis());\r\n    // Wait for the ControlAction to complete\r\n    while (true) {\r\n        if (containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestUUIDMove2).isPresent() && containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestUUIDMove2).get().getStatusCode() == ContainerPlacementMessage.StatusCode.SUCCEEDED) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n    }\r\n    assertEquals(state.preferredHostRequests.get(), 4);\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-4\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    Optional<ContainerPlacementResponseMessage> responseMessageMove1 = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestUUIDMove1);\r\n    Optional<ContainerPlacementResponseMessage> responseMessageMove2 = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestUUIDMove2);\r\n    assertTrue(responseMessageMove1.isPresent());\r\n    assertEquals(responseMessageMove1.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    assertTrue(responseMessageMove2.isPresent());\r\n    assertEquals(responseMessageMove2.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    // Request should be deleted as soon as ita accepted / being acted upon\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestUUIDMove1).isPresent());\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestUUIDMove2).isPresent());\r\n    // Requests from Previous deploy must be cleaned\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestUUIDMoveBad).isPresent());\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestUUIDMoveBad).isPresent());\r\n    // Cleanup Request Allocator Thread\r\n    cleanUpRequestAllocatorThread(requestAllocator, requestAllocatorThread);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testContainerMoveActionExpiredRequestNotAffectRunningContainers",
  "sourceCode" : "@Test(timeout = 10000)\r\npublic void testContainerMoveActionExpiredRequestNotAffectRunningContainers() throws Exception {\r\n    // Mimic the behavior of Expired request\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                List<SamzaResource> resources = (List<SamzaResource>) args[0];\r\n                if (resources.get(0).getHost().equals(\"host-1\") || resources.get(0).getHost().equals(\"host-2\")) {\r\n                    cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n                }\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.preferredHostRequests.get(), 2);\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Initiate container placement action to move a container with container id 0\r\n    ContainerPlacementRequestMessage requestMessage = new ContainerPlacementRequestMessage(UUID.randomUUID(), \"appAttempt-001\", \"0\", \"host-3\", Duration.ofMillis(10), System.currentTimeMillis());\r\n    ContainerPlacementMetadata metadata = containerManager.registerContainerPlacementActionForTest(requestMessage, allocatorWithHostAffinity);\r\n    Optional<ContainerPlacementResponseMessage> responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    // Wait for the placement action to be complete & get written to the underlying metastore\r\n    while (true) {\r\n        if (metadata.getActionStatus() == ContainerPlacementMessage.StatusCode.FAILED && responseMessage.isPresent() && responseMessage.get().getStatusCode() == ContainerPlacementMessage.StatusCode.FAILED) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n        responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    }\r\n    assertEquals(state.preferredHostRequests.get(), 3);\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    // Container should not be stooped\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    assertTrue(responseMessage.isPresent());\r\n    assertEquals(responseMessage.get().getStatusCode(), ContainerPlacementMessage.StatusCode.FAILED);\r\n    assertResponseMessage(responseMessage.get(), requestMessage);\r\n    // Request shall be deleted as soon as it is acted upon\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage.getUuid()).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testActiveContainerLaunchFailureOnControlActionShouldFallbackToSourceHost",
  "sourceCode" : "@Test(timeout = 10000)\r\npublic void testActiveContainerLaunchFailureOnControlActionShouldFallbackToSourceHost() throws Exception {\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    // Mimic stream processor launch failure only on host-3\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                SamzaResource host3Resource = (SamzaResource) args[0];\r\n                if (host3Resource.getHost().equals(\"host-3\")) {\r\n                    cpm.onStreamProcessorLaunchFailure(host3Resource, new Throwable(\"Custom Exception for Host-3\"));\r\n                } else {\r\n                    cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n                }\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 5, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.preferredHostRequests.get(), 2);\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Take a container placement action to move a container with container id 0\r\n    ContainerPlacementRequestMessage requestMessage = new ContainerPlacementRequestMessage(UUID.randomUUID(), \"app-attempt-001\", \"0\", \"host-3\", System.currentTimeMillis());\r\n    ContainerPlacementMetadata metadata = containerManager.registerContainerPlacementActionForTest(requestMessage, allocatorWithHostAffinity);\r\n    // Wait for the ControlAction to complete\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(1, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    assertEquals(state.preferredHostRequests.get(), 4);\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    // Container 0 should fallback to source host\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Control Action should be failed in this case\r\n    assertEquals(metadata.getActionStatus(), ContainerPlacementMessage.StatusCode.FAILED);\r\n    Optional<ContainerPlacementResponseMessage> responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    assertTrue(responseMessage.isPresent());\r\n    assertEquals(responseMessage.get().getStatusCode(), ContainerPlacementMessage.StatusCode.FAILED);\r\n    assertResponseMessage(responseMessage.get(), requestMessage);\r\n    // Request shall be deleted as soon as it is acted upon\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage.getUuid()).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testContainerPlacementsForJobRunningInDegradedState",
  "sourceCode" : "@Test(timeout = 20000)\r\npublic void testContainerPlacementsForJobRunningInDegradedState() throws Exception {\r\n    // Set failure after retries to false to enable job running in degraded state\r\n    config = new MapConfig(configVals, getConfigWithHostAffinityAndRetries(true, 1, false));\r\n    state = new SamzaApplicationState(JobModelManagerTestUtil.getJobModelManager(getConfig(), 2, this.server));\r\n    callback = mock(ClusterResourceManager.Callback.class);\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    ClusterManagerConfig clusterManagerConfig = new ClusterManagerConfig(config);\r\n    containerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, true, false, localityManager, faultDomainManager, config));\r\n    allocatorWithHostAffinity = new MockContainerAllocatorWithHostAffinity(clusterResourceManager, config, state, containerManager);\r\n    cpm = new ContainerProcessManager(clusterManagerConfig, state, new MetricsRegistryMap(), clusterResourceManager, Optional.of(allocatorWithHostAffinity), containerManager, localityManager, false);\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    // Mimic stream processor launch failure only on host-2,\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 5, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-2\");\r\n    assertEquals(state.preferredHostRequests.get(), 2);\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Trigger a container failure\r\n    clusterResourceManager.stopStreamProcessor(state.runningProcessors.get(\"1\"), -103);\r\n    // Wait for container to start\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // Trigger a container failure again\r\n    clusterResourceManager.stopStreamProcessor(state.runningProcessors.get(\"1\"), -103);\r\n    // Ensure that this container has exhausted all retires\r\n    while (state.failedProcessors.size() != 1 && state.runningProcessors.size() != 1) {\r\n        Thread.sleep(100);\r\n    }\r\n    // At this point the application should only have one container running\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.size(), 1);\r\n    assertEquals(state.pendingProcessors.size(), 0);\r\n    assertTrue(state.failedProcessors.containsKey(\"1\"));\r\n    ContainerPlacementRequestMessage requestMessage = new ContainerPlacementRequestMessage(UUID.randomUUID(), \"app-attempt-001\", \"1\", \"host-3\", System.currentTimeMillis());\r\n    ContainerPlacementMetadata metadata = containerManager.registerContainerPlacementActionForTest(requestMessage, allocatorWithHostAffinity);\r\n    // Wait for the ControlAction to complete\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    // Wait for both the containers to be in running state & control action metadata to succeed\r\n    while (state.runningProcessors.size() != 2 && metadata.getActionStatus() != ContainerPlacementMessage.StatusCode.SUCCEEDED) {\r\n        Thread.sleep(100);\r\n    }\r\n    assertEquals(state.preferredHostRequests.get(), 4);\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    // Container 1 should not go to host-3\r\n    assertEquals(state.runningProcessors.get(\"0\").getHost(), \"host-1\");\r\n    assertEquals(state.runningProcessors.get(\"1\").getHost(), \"host-3\");\r\n    assertEquals(state.anyHostRequests.get(), 0);\r\n    // Failed processors must be empty\r\n    assertEquals(state.failedProcessors.size(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testAlwaysMoveToAnyHostForHostAffinityDisabled",
  "sourceCode" : "@Test(timeout = 10000)\r\npublic void testAlwaysMoveToAnyHostForHostAffinityDisabled() throws Exception {\r\n    Map<String, String> conf = new HashMap<>();\r\n    conf.putAll(getConfigWithHostAffinityAndRetries(false, 1, true));\r\n    SamzaApplicationState state = new SamzaApplicationState(JobModelManagerTestUtil.getJobModelManager(getConfig(), 2, this.server));\r\n    ClusterResourceManager.Callback callback = mock(ClusterResourceManager.Callback.class);\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    ContainerManager containerManager = new ContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, false, false, localityManager, faultDomainManager, config);\r\n    MockContainerAllocatorWithoutHostAffinity allocatorWithoutHostAffinity = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, new MapConfig(conf), state, containerManager);\r\n    ContainerProcessManager cpm = new ContainerProcessManager(new ClusterManagerConfig(new MapConfig(getConfig(), getConfigWithHostAffinityAndRetries(false, 1, true))), state, new MetricsRegistryMap(), clusterResourceManager, Optional.of(allocatorWithoutHostAffinity), containerManager, localityManager, false);\r\n    // Mimic Cluster Manager returning any request\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                List<SamzaResource> resources = (List<SamzaResource>) args[0];\r\n                SamzaResource preferredResource = resources.get(0);\r\n                SamzaResource anyResource = new SamzaResource(preferredResource.getNumCores(), preferredResource.getMemoryMb(), \"host-\" + RandomStringUtils.randomAlphanumeric(5), preferredResource.getContainerId());\r\n                cpm.onResourcesAvailable(ImmutableList.of(anyResource));\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    // This spawns async start request and waits for async requests to complete\r\n    if (!allocatorWithoutHostAffinity.awaitContainersStart(2, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 2) {\r\n        Thread.sleep(100);\r\n    }\r\n    // App is in running state with two containers running\r\n    assertEquals(state.runningProcessors.size(), 2);\r\n    assertEquals(state.preferredHostRequests.get(), 0);\r\n    assertEquals(state.anyHostRequests.get(), 2);\r\n    String previousHostOfContainer1 = state.runningProcessors.get(\"0\").getHost();\r\n    String previousHostOfContainer2 = state.runningProcessors.get(\"1\").getHost();\r\n    // Initiate container placement action to move a container with container id 0\r\n    ContainerPlacementRequestMessage requestMessage = new ContainerPlacementRequestMessage(UUID.randomUUID(), \"app-attempt-001\", \"0\", \"host-3\", System.currentTimeMillis());\r\n    ContainerPlacementMetadata metadata = containerManager.registerContainerPlacementActionForTest(requestMessage, allocatorWithoutHostAffinity);\r\n    // Wait for the ControlAction to complete and spawn an async request\r\n    if (!allocatorWithoutHostAffinity.awaitContainersStart(1, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    Optional<ContainerPlacementResponseMessage> responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    while (true) {\r\n        if (metadata.getActionStatus() == ContainerPlacementMessage.StatusCode.SUCCEEDED && responseMessage.isPresent() && responseMessage.get().getStatusCode() == ContainerPlacementMessage.StatusCode.SUCCEEDED) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n        responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    }\r\n    // We should have no preferred host request\r\n    assertEquals(0, state.preferredHostRequests.get());\r\n    // We should have one more ANY_HOST request\r\n    assertEquals(3, state.anyHostRequests.get());\r\n    assertEquals(2, state.runningProcessors.size());\r\n    assertNotEquals(previousHostOfContainer1, state.runningProcessors.get(\"0\").getHost());\r\n    // Container 2 should not be affected\r\n    assertEquals(previousHostOfContainer2, state.runningProcessors.get(\"1\").getHost());\r\n    assertEquals(3, state.anyHostRequests.get());\r\n    // Action should success\r\n    assertEquals(ContainerPlacementMessage.StatusCode.SUCCEEDED, metadata.getActionStatus());\r\n    assertTrue(responseMessage.isPresent());\r\n    assertEquals(responseMessage.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    assertResponseMessage(responseMessage.get(), requestMessage);\r\n    /**\r\n     * Inject a duplicate request and check it is not accepted\r\n     */\r\n    ContainerPlacementRequestMessage duplicateRequestToBeIgnored = new ContainerPlacementRequestMessage(requestMessage.getUuid(), \"app-attempt-001\", \"1\", \"host-3\", System.currentTimeMillis());\r\n    // Request with a dup uuid should not be accepted\r\n    metadata = containerManager.registerContainerPlacementActionForTest(duplicateRequestToBeIgnored, allocatorWithoutHostAffinity);\r\n    // metadata should be from the previous completed action\r\n    assertTrue(metadata == null || metadata.getUuid() != duplicateRequestToBeIgnored.getUuid());\r\n    responseMessage = containerPlacementMetadataStore.readContainerPlacementResponseMessage(requestMessage.getUuid());\r\n    assertTrue(responseMessage.isPresent());\r\n    assertEquals(responseMessage.get().getStatusCode(), ContainerPlacementMessage.StatusCode.BAD_REQUEST);\r\n    assertResponseMessage(responseMessage.get(), duplicateRequestToBeIgnored);\r\n    // Request shall be deleted as soon as it is acted upon\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(requestMessage.getUuid()).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testBadControlRequestRejected",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testBadControlRequestRejected() throws Exception {\r\n    SamzaApplicationState state = new SamzaApplicationState(JobModelManagerTestUtil.getJobModelManager(getConfig(), 2, this.server));\r\n    ClusterResourceManager.Callback callback = mock(ClusterResourceManager.Callback.class);\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    ContainerManager containerManager = spy(new ContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, true, false, localityManager, faultDomainManager, config));\r\n    MockContainerAllocatorWithHostAffinity allocatorWithHostAffinity = new MockContainerAllocatorWithHostAffinity(clusterResourceManager, config, state, containerManager);\r\n    ContainerProcessManager cpm = new ContainerProcessManager(new ClusterManagerConfig(new MapConfig(getConfig(), getConfigWithHostAffinityAndRetries(true, 1, true))), state, new MetricsRegistryMap(), clusterResourceManager, Optional.of(allocatorWithHostAffinity), containerManager, localityManager, false);\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            Object[] args = invocation.getArguments();\r\n            cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            Object[] args = invocation.getArguments();\r\n            cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(2, 3, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    assertBadRequests(null, \"host2\", containerManager, allocatorWithHostAffinity);\r\n    assertBadRequests(\"0\", null, containerManager, allocatorWithHostAffinity);\r\n    assertBadRequests(\"2\", \"host8\", containerManager, allocatorWithHostAffinity);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerPlacementActions.java",
  "methodName" : "testContainerSuccessfulMoveActionWithStandbyEnabled",
  "sourceCode" : "@Test(timeout = 30000)\r\npublic void testContainerSuccessfulMoveActionWithStandbyEnabled() throws Exception {\r\n    // Setup standby for job\r\n    setupStandby();\r\n    // Spawn a Request Allocator Thread\r\n    ContainerPlacementRequestAllocator requestAllocator = new ContainerPlacementRequestAllocator(containerPlacementMetadataStore, cpm, new ApplicationConfig(config), 100);\r\n    Thread requestAllocatorThread = new Thread(requestAllocator, \"ContainerPlacement Request Allocator Thread\");\r\n    requestAllocatorThread.start();\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesAvailable((List<SamzaResource>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesAvailable(anyList());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onStreamProcessorLaunchSuccess((SamzaResource) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onStreamProcessorLaunchSuccess(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        public Void answer(InvocationOnMock invocation) {\r\n            new Thread(() -> {\r\n                Object[] args = invocation.getArguments();\r\n                cpm.onResourcesCompleted((List<SamzaResourceStatus>) args[0]);\r\n            }, \"AMRMClientAsync\").start();\r\n            return null;\r\n        }\r\n    }).when(callback).onResourcesCompleted(anyList());\r\n    cpm.start();\r\n    if (!allocatorWithHostAffinity.awaitContainersStart(4, 4, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    while (state.runningProcessors.size() != 4) {\r\n        Thread.sleep(100);\r\n    }\r\n    // First running state of the app\r\n    Consumer<SamzaApplicationState> stateCheck = (SamzaApplicationState state) -> {\r\n        assertEquals(4, state.runningProcessors.size());\r\n        assertEquals(\"host-1\", state.runningProcessors.get(\"0\").getHost());\r\n        assertEquals(\"host-2\", state.runningProcessors.get(\"1\").getHost());\r\n        assertEquals(\"host-2\", state.runningProcessors.get(\"0-0\").getHost());\r\n        assertEquals(\"host-1\", state.runningProcessors.get(\"1-0\").getHost());\r\n        assertEquals(4, state.preferredHostRequests.get());\r\n        assertEquals(0, state.failedStandbyAllocations.get());\r\n        assertEquals(0, state.anyHostRequests.get());\r\n    };\r\n    // Invoke a state check\r\n    stateCheck.accept(state);\r\n    // Initiate a bad container placement action to move a standby to its active host and vice versa\r\n    // which should fail because this violates standby constraints\r\n    UUID badRequest1 = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0-0\", \"host-1\", null, System.currentTimeMillis());\r\n    UUID badRequest2 = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0\", \"host-2\", null, System.currentTimeMillis() + 100);\r\n    // Wait for the ControlActions to complete\r\n    while (true) {\r\n        if (containerPlacementMetadataStore.readContainerPlacementResponseMessage(badRequest2).isPresent() && containerPlacementMetadataStore.readContainerPlacementResponseMessage(badRequest2).get().getStatusCode() == ContainerPlacementMessage.StatusCode.BAD_REQUEST) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n    }\r\n    // App running state should remain the same\r\n    stateCheck.accept(state);\r\n    Optional<ContainerPlacementResponseMessage> responseMessageMove1 = containerPlacementMetadataStore.readContainerPlacementResponseMessage(badRequest1);\r\n    Optional<ContainerPlacementResponseMessage> responseMessageMove2 = containerPlacementMetadataStore.readContainerPlacementResponseMessage(badRequest2);\r\n    // Assert that both the requests were bad\r\n    assertTrue(responseMessageMove1.isPresent());\r\n    assertEquals(responseMessageMove1.get().getStatusCode(), ContainerPlacementMessage.StatusCode.BAD_REQUEST);\r\n    assertTrue(responseMessageMove2.isPresent());\r\n    assertEquals(responseMessageMove2.get().getStatusCode(), ContainerPlacementMessage.StatusCode.BAD_REQUEST);\r\n    // Initiate a standby failover which is supposed to be done in two steps\r\n    // Step 1. Move the standby container to any other host: move 0-0 to say host-3\r\n    // Step 2. Move the active container to the standby's host: move 0 to host-1\r\n    // Action will get executed first\r\n    UUID standbyMoveRequest = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0-0\", \"host-3\", null, System.currentTimeMillis());\r\n    // Action will get executed when standbyMoveRequest move request is complete\r\n    UUID activeMoveRequest = containerPlacementMetadataStore.writeContainerPlacementRequestMessage(\"appAttempt-001\", \"0\", \"host-2\", null, System.currentTimeMillis() + 100);\r\n    // Wait for the ControlActions to complete\r\n    while (true) {\r\n        if (containerPlacementMetadataStore.readContainerPlacementResponseMessage(activeMoveRequest).isPresent() && containerPlacementMetadataStore.readContainerPlacementResponseMessage(activeMoveRequest).get().getStatusCode() == ContainerPlacementMessage.StatusCode.SUCCEEDED) {\r\n            break;\r\n        }\r\n        Thread.sleep(100);\r\n    }\r\n    assertEquals(4, state.runningProcessors.size());\r\n    assertEquals(\"host-2\", state.runningProcessors.get(\"0\").getHost());\r\n    assertEquals(\"host-2\", state.runningProcessors.get(\"1\").getHost());\r\n    assertEquals(\"host-3\", state.runningProcessors.get(\"0-0\").getHost());\r\n    assertEquals(\"host-1\", state.runningProcessors.get(\"1-0\").getHost());\r\n    assertEquals(6, state.preferredHostRequests.get());\r\n    assertEquals(0, state.failedStandbyAllocations.get());\r\n    assertEquals(0, state.anyHostRequests.get());\r\n    Optional<ContainerPlacementResponseMessage> responseStandbyMove = containerPlacementMetadataStore.readContainerPlacementResponseMessage(standbyMoveRequest);\r\n    Optional<ContainerPlacementResponseMessage> responseActiveMove = containerPlacementMetadataStore.readContainerPlacementResponseMessage(activeMoveRequest);\r\n    assertTrue(responseStandbyMove.isPresent());\r\n    assertEquals(responseStandbyMove.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    assertTrue(responseActiveMove.isPresent());\r\n    assertEquals(responseActiveMove.get().getStatusCode(), ContainerPlacementMessage.StatusCode.SUCCEEDED);\r\n    // Request should be deleted as soon as ita accepted / being acted upon\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(standbyMoveRequest).isPresent());\r\n    assertFalse(containerPlacementMetadataStore.readContainerPlacementRequestMessage(activeMoveRequest).isPresent());\r\n    // Cleanup Request Allocator Thread\r\n    cleanUpRequestAllocatorThread(requestAllocator, requestAllocatorThread);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testContainerProcessManager",
  "sourceCode" : "@Test\r\npublic void testContainerProcessManager() throws Exception {\r\n    Map<String, String> conf = new HashMap<>();\r\n    conf.putAll(getConfig());\r\n    conf.put(\"cluster-manager.container.memory.mb\", \"500\");\r\n    conf.put(\"cluster-manager.container.cpu.cores\", \"5\");\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    LocalityManager mockLocalityManager = mock(LocalityManager.class);\r\n    when(mockLocalityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of(\"0\", new ProcessorLocality(\"0\", \"host1\"))));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, true, false, mockLocalityManager, faultDomainManager);\r\n    ContainerProcessManager cpm = buildContainerProcessManager(new ClusterManagerConfig(new MapConfig(conf)), state, clusterResourceManager, Optional.empty());\r\n    ContainerAllocator allocator = (ContainerAllocator) getPrivateFieldFromCpm(\"containerAllocator\", cpm).get(cpm);\r\n    assertEquals(ContainerAllocator.class, allocator.getClass());\r\n    // Asserts that samza exposed container configs is honored by allocator thread\r\n    assertEquals(500, allocator.containerMemoryMb);\r\n    assertEquals(5, allocator.containerNumCpuCores);\r\n    conf.clear();\r\n    conf.putAll(getConfigWithHostAffinity());\r\n    conf.put(\"cluster-manager.container.memory.mb\", \"500\");\r\n    conf.put(\"cluster-manager.container.cpu.cores\", \"5\");\r\n    state = new SamzaApplicationState(getJobModelManager(1));\r\n    callback = new MockClusterResourceManagerCallback();\r\n    clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    cpm = new ContainerProcessManager(new ClusterManagerConfig(new MapConfig(conf)), state, new MetricsRegistryMap(), clusterResourceManager, Optional.empty(), containerManager, mockLocalityManager, false);\r\n    allocator = (ContainerAllocator) getPrivateFieldFromCpm(\"containerAllocator\", cpm).get(cpm);\r\n    assertEquals(ContainerAllocator.class, allocator.getClass());\r\n    // Asserts that samza exposed container configs is honored by allocator thread\r\n    assertEquals(500, allocator.containerMemoryMb);\r\n    assertEquals(5, allocator.containerNumCpuCores);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testOnInit",
  "sourceCode" : "@Test\r\npublic void testOnInit() throws Exception {\r\n    Config conf = getConfig();\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    ClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    ContainerProcessManager cpm = buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.empty());\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    getPrivateFieldFromCpm(\"containerAllocator\", cpm).set(cpm, allocator);\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    getPrivateFieldFromCpm(\"allocatorThread\", cpm).set(cpm, new Thread() {\r\n\r\n        public void run() {\r\n            isRunning = true;\r\n            latch.countDown();\r\n        }\r\n    });\r\n    cpm.start();\r\n    if (!latch.await(2, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"timed out waiting for the latch to expire\");\r\n    }\r\n    // Verify Allocator thread has started running\r\n    assertTrue(isRunning);\r\n    // Verify the remaining state\r\n    assertEquals(1, state.neededProcessors.get());\r\n    assertEquals(1, allocator.requestedContainers);\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testOnInitAMHighAvailability",
  "sourceCode" : "@Test\r\npublic void testOnInitAMHighAvailability() throws Exception {\r\n    Map<String, String> configMap = new HashMap<>(configVals);\r\n    configMap.put(JobConfig.YARN_AM_HIGH_AVAILABILITY_ENABLED, \"true\");\r\n    Config conf = new MapConfig(configMap);\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(2));\r\n    state.runningProcessors.put(\"0\", new SamzaResource(1, 1024, \"host\", \"0\"));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    ClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    ContainerProcessManager cpm = buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.empty());\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    getPrivateFieldFromCpm(\"containerAllocator\", cpm).set(cpm, allocator);\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    getPrivateFieldFromCpm(\"allocatorThread\", cpm).set(cpm, new Thread() {\r\n\r\n        public void run() {\r\n            isRunning = true;\r\n            latch.countDown();\r\n        }\r\n    });\r\n    cpm.start();\r\n    if (!latch.await(2, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"timed out waiting for the latch to expire\");\r\n    }\r\n    // Verify Allocator thread has started running\r\n    assertTrue(isRunning);\r\n    // Verify only 1 was requested with allocator\r\n    assertEquals(1, allocator.requestedContainers);\r\n    assertTrue(\"Ensure no processors were forcefully restarted\", callback.resourceStatuses.isEmpty());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testOnInitToForceRestartAMHighAvailability",
  "sourceCode" : "@Test\r\npublic void testOnInitToForceRestartAMHighAvailability() throws Exception {\r\n    Map<String, String> configMap = new HashMap<>(configVals);\r\n    configMap.put(JobConfig.YARN_AM_HIGH_AVAILABILITY_ENABLED, \"true\");\r\n    Config conf = new MapConfig(configMap);\r\n    SamzaResource samzaResource = new SamzaResource(1, 1024, \"host\", \"0\");\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(2));\r\n    state.runningProcessors.put(\"0\", samzaResource);\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    ClusterResourceManager clusterResourceManager = spy(new MockClusterResourceManager(callback, state));\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    ContainerProcessManager cpm = buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.empty(), true);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    getPrivateFieldFromCpm(\"containerAllocator\", cpm).set(cpm, allocator);\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    getPrivateFieldFromCpm(\"allocatorThread\", cpm).set(cpm, new Thread() {\r\n\r\n        public void run() {\r\n            isRunning = true;\r\n            latch.countDown();\r\n        }\r\n    });\r\n    cpm.start();\r\n    if (!latch.await(2, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"timed out waiting for the latch to expire\");\r\n    }\r\n    verify(clusterResourceManager, times(1)).stopStreamProcessor(samzaResource);\r\n    assertEquals(\"CPM should stop the running container\", 1, callback.resourceStatuses.size());\r\n    SamzaResourceStatus actualResourceStatus = callback.resourceStatuses.get(0);\r\n    assertEquals(\"Container 0 should be stopped\", \"0\", actualResourceStatus.getContainerId());\r\n    assertEquals(\"Container 0 should have exited with preempted status\", SamzaResourceStatus.PREEMPTED, actualResourceStatus.getExitCode());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testOnShutdown",
  "sourceCode" : "@Test\r\npublic void testOnShutdown() throws Exception {\r\n    Config conf = getConfig();\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerProcessManager cpm = buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.empty());\r\n    cpm.start();\r\n    Thread allocatorThread = (Thread) getPrivateFieldFromCpm(\"allocatorThread\", cpm).get(cpm);\r\n    assertTrue(allocatorThread.isAlive());\r\n    cpm.stop();\r\n    assertFalse(allocatorThread.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testCpmShouldStopWhenContainersFinish",
  "sourceCode" : "/**\r\n * Test Container Process Manager should stop when all containers finish\r\n */\r\n@Test\r\npublic void testCpmShouldStopWhenContainersFinish() throws Exception {\r\n    Config conf = getConfig();\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.of(allocator)));\r\n    // start triggers a request\r\n    cpm.start();\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertEquals(0, allocator.getContainerRequestState().numDelayedRequests());\r\n    SamzaResource container = new SamzaResource(1, 1024, \"host1\", \"id0\");\r\n    cpm.onResourceAllocated(container);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container);\r\n    assertFalse(cpm.shouldShutdown());\r\n    cpm.onResourceCompleted(new SamzaResourceStatus(\"id0\", \"diagnostics\", SamzaResourceStatus.SUCCESS));\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(any(SamzaResourceStatus.class), anyString(), anyString(), anyInt());\r\n    assertTrue(cpm.shouldShutdown());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testNewContainerRequestedOnFailureWithUnknownCode",
  "sourceCode" : "/**\r\n * Test Container Process Manager should request a new container when a task fails with unknown exit code\r\n * When host-affinity is not enabled, it will always request for ANY_HOST\r\n */\r\n@Test\r\npublic void testNewContainerRequestedOnFailureWithUnknownCode() throws Exception {\r\n    Config conf = getConfig();\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.of(allocator)));\r\n    // start triggers a request\r\n    cpm.start();\r\n    verify(clusterManagerConfig, never()).getContainerPreferredHostLastRetryDelayMs();\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(any(), anyString(), anyString(), anyInt());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    SamzaResource container = new SamzaResource(1, 1024, \"host1\", \"id0\");\r\n    cpm.onResourceAllocated(container);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container);\r\n    // Create first container failure\r\n    SamzaResourceStatus samzaResourceStatus = new SamzaResourceStatus(container.getContainerId(), \"diagnostics\", 1);\r\n    cpm.onResourceCompleted(samzaResourceStatus);\r\n    // The above failure should trigger a container request\r\n    verify(cpm).onResourceCompletedWithUnknownStatus(eq(samzaResourceStatus), eq(container.getContainerId()), eq(\"0\"), eq(1));\r\n    verify(clusterManagerConfig, never()).getContainerPreferredHostLastRetryDelayMs();\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertEquals(ResourceRequestState.ANY_HOST, allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(2, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    cpm.onResourceAllocated(container);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container);\r\n    assertTrue(state.jobHealthy.get());\r\n    // Create a second failure\r\n    cpm.onResourceCompleted(samzaResourceStatus);\r\n    // The above failure should trigger a job shutdown because our retry count is set to 1\r\n    verify(cpm, times(2)).onResourceCompletedWithUnknownStatus(eq(samzaResourceStatus), eq(container.getContainerId()), eq(\"0\"), eq(1));\r\n    verify(clusterManagerConfig, never()).getContainerPreferredHostLastRetryDelayMs();\r\n    assertEquals(0, allocator.getContainerRequestState().numPendingRequests());\r\n    assertEquals(2, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertTrue(cpm.shouldShutdown());\r\n    assertEquals(SamzaApplicationState.SamzaAppStatus.FAILED, state.status);\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCodeWithNoHostAffinity",
  "sourceCode" : "/**\r\n * Test scenario where a container fails multiple times but failures are more than retryWindow apart without host affinity\r\n * @throws Exception\r\n */\r\n@Test\r\npublic void testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCodeWithNoHostAffinity() throws Exception {\r\n    testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCode(false, true);\r\n    testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCode(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCodeWithHostAffinity",
  "sourceCode" : "/**\r\n * Test scenario where a container fails multiple times but failures are more than retryWindow apart with host affinity\r\n * @throws Exception\r\n */\r\n@Test\r\npublic void testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCodeWithHostAffinity() throws Exception {\r\n    testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCode(true, true);\r\n    testContainerRequestedRetriesExceedingWindowOnFailureWithUnknownCode(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCodeWithNoHostAffinity",
  "sourceCode" : "@Test\r\npublic void testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCodeWithNoHostAffinity() throws Exception {\r\n    testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCode(false, true);\r\n    testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCode(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCodeWithHostAffinity",
  "sourceCode" : "@Test\r\npublic void testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCodeWithHostAffinity() throws Exception {\r\n    testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCode(true, true);\r\n    testContainerRequestedRetriesNotExceedingWindowOnFailureWithUnknownCode(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testInvalidNotificationsAreIgnored",
  "sourceCode" : "@Test\r\npublic void testInvalidNotificationsAreIgnored() throws Exception {\r\n    Config conf = getConfig();\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(conf));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.of(allocator)));\r\n    // Start the task clusterResourceManager\r\n    cpm.start();\r\n    SamzaResource container = new SamzaResource(1, 1000, \"host1\", \"id1\");\r\n    cpm.onResourceAllocated(container);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    // Create container failure - with ContainerExitStatus.DISKS_FAILED\r\n    cpm.onResourceCompleted(new SamzaResourceStatus(\"invalidContainerID\", \"Disk failure\", SamzaResourceStatus.DISK_FAIL));\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(any(SamzaResourceStatus.class), anyString(), anyString(), anyInt());\r\n    // The above failure should not trigger any container requests, since it is for an invalid container ID\r\n    assertEquals(0, allocator.getContainerRequestState().numPendingRequests());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertTrue(state.jobHealthy.get());\r\n    assertEquals(state.redundantNotifications.get(), 1);\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testRerequestOnAnyHostIfContainerStartFails",
  "sourceCode" : "@Test\r\npublic void testRerequestOnAnyHostIfContainerStartFails() throws Exception {\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.putAll(getConfig());\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    LocalityManager mockLocalityManager = mock(LocalityManager.class);\r\n    when(mockLocalityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of(\"0\", new ProcessorLocality(\"1\", \"host1\"))));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, Boolean.valueOf(config.get(ClusterManagerConfig.HOST_AFFINITY_ENABLED)), false, mockLocalityManager, faultDomainManager);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, new MapConfig(config), state, containerManager);\r\n    ContainerProcessManager manager = new ContainerProcessManager(new ClusterManagerConfig(config), state, new MetricsRegistryMap(), clusterResourceManager, Optional.of(allocator), containerManager, mockLocalityManager, false);\r\n    manager.start();\r\n    SamzaResource resource = new SamzaResource(1, 1024, \"host1\", \"resource-1\");\r\n    state.pendingProcessors.put(\"1\", resource);\r\n    Assert.assertEquals(clusterResourceManager.resourceRequests.size(), 1);\r\n    manager.onStreamProcessorLaunchFailure(resource, new Exception(\"cannot launch container!\"));\r\n    Assert.assertEquals(clusterResourceManager.resourceRequests.size(), 2);\r\n    Assert.assertEquals(clusterResourceManager.resourceRequests.get(1).getHost(), ResourceRequestState.ANY_HOST);\r\n    manager.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testAllBufferedResourcesAreUtilized",
  "sourceCode" : "@Test\r\npublic void testAllBufferedResourcesAreUtilized() throws Exception {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.putAll(getConfigWithHostAffinity());\r\n    config.put(\"job.container.count\", \"2\");\r\n    config.put(\"cluster-manager.container.retry.count\", \"2\");\r\n    config.put(\"cluster-manager.container.request.timeout.ms\", \"10000\");\r\n    Config cfg = new MapConfig(config);\r\n    // 1. Request two containers on hosts - host1 and host2\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(2));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    FaultDomainManager faultDomainManager = mock(FaultDomainManager.class);\r\n    LocalityManager mockLocalityManager = mock(LocalityManager.class);\r\n    when(mockLocalityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of(\"0\", new ProcessorLocality(\"0\", \"host1\"), \"1\", new ProcessorLocality(\"1\", \"host2\"))));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, Boolean.parseBoolean(config.get(ClusterManagerConfig.HOST_AFFINITY_ENABLED)), false, mockLocalityManager, faultDomainManager);\r\n    MockContainerAllocatorWithHostAffinity allocator = new MockContainerAllocatorWithHostAffinity(clusterResourceManager, cfg, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(new ClusterManagerConfig(cfg), state, clusterResourceManager, Optional.of(allocator), mockLocalityManager, false, faultDomainManager));\r\n    cpm.start();\r\n    assertFalse(cpm.shouldShutdown());\r\n    // 2. When the task manager starts, there should have been a pending request on host1 and host2\r\n    assertEquals(2, allocator.getContainerRequestState().numPendingRequests());\r\n    // 3. Allocate an extra resource on host1 and no resource on host2 yet.\r\n    SamzaResource resource1 = new SamzaResource(1, 1000, \"host1\", \"id1\");\r\n    SamzaResource resource2 = new SamzaResource(1, 1000, \"host1\", \"id2\");\r\n    cpm.onResourceAllocated(resource1);\r\n    cpm.onResourceAllocated(resource2);\r\n    // 4. Wait for the container to start on host1 and immediately fail\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(resource1);\r\n    assertEquals(\"host2\", allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    cpm.onResourceCompleted(new SamzaResourceStatus(resource1.getContainerId(), \"App Error\", 1));\r\n    verify(cpm).onResourceCompletedWithUnknownStatus(any(SamzaResourceStatus.class), anyString(), anyString(), anyInt());\r\n    assertEquals(2, allocator.getContainerRequestState().numPendingRequests());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(3, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    // 5. Do not allocate any further resource on host1, and verify that the re-run of the container on host1 uses the\r\n    // previously allocated extra resource\r\n    SamzaResource resource3 = new SamzaResource(1, 1000, \"host2\", \"id3\");\r\n    cpm.onResourceAllocated(resource3);\r\n    if (!allocator.awaitContainersStart(2, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(resource2);\r\n    cpm.onStreamProcessorLaunchSuccess(resource3);\r\n    assertTrue(state.jobHealthy.get());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testDuplicateNotificationsDoNotAffectJobHealth",
  "sourceCode" : "@Test\r\npublic void testDuplicateNotificationsDoNotAffectJobHealth() throws Exception {\r\n    Config conf = getConfig();\r\n    Map<String, String> config = new HashMap<>();\r\n    config.putAll(getConfig());\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(new MapConfig(conf)));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.of(allocator)));\r\n    // Start the task manager\r\n    cpm.start();\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    SamzaResource container1 = new SamzaResource(1, 1000, \"host1\", \"id1\");\r\n    cpm.onResourceAllocated(container1);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container1);\r\n    assertEquals(0, allocator.getContainerRequestState().numPendingRequests());\r\n    // Create container failure - with ContainerExitStatus.DISKS_FAILED\r\n    cpm.onResourceCompleted(new SamzaResourceStatus(container1.getContainerId(), \"Disk failure\", SamzaResourceStatus.DISK_FAIL));\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(any(SamzaResourceStatus.class), anyString(), anyString(), anyInt());\r\n    // The above failure should trigger a container request\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(2, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    assertEquals(ResourceRequestState.ANY_HOST, allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    SamzaResource container2 = new SamzaResource(1, 1000, \"host1\", \"id2\");\r\n    cpm.onResourceAllocated(container2);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container2);\r\n    assertTrue(state.jobHealthy.get());\r\n    // Simulate a duplicate notification for container 1 with a different exit code\r\n    cpm.onResourceCompleted(new SamzaResourceStatus(container1.getContainerId(), \"Disk failure\", SamzaResourceStatus.PREEMPTED));\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(any(SamzaResourceStatus.class), anyString(), anyString(), anyInt());\r\n    // assert that a duplicate notification does not change metrics (including job health)\r\n    assertEquals(state.redundantNotifications.get(), 1);\r\n    assertEquals(2, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    assertTrue(state.jobHealthy.get());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerProcessManager.java",
  "methodName" : "testNewContainerRequestedOnFailureWithKnownCode",
  "sourceCode" : "/**\r\n * Test AM requests a new container when a task fails\r\n * Error codes with same behavior - Disk failure, preemption and aborted\r\n */\r\n@Test\r\npublic void testNewContainerRequestedOnFailureWithKnownCode() throws Exception {\r\n    Config conf = getConfig();\r\n    Map<String, String> config = new HashMap<>();\r\n    config.putAll(getConfig());\r\n    SamzaApplicationState state = new SamzaApplicationState(getJobModelManager(1));\r\n    MockClusterResourceManagerCallback callback = new MockClusterResourceManagerCallback();\r\n    MockClusterResourceManager clusterResourceManager = new MockClusterResourceManager(callback, state);\r\n    ClusterManagerConfig clusterManagerConfig = spy(new ClusterManagerConfig(new MapConfig(config)));\r\n    ContainerManager containerManager = buildContainerManager(containerPlacementMetadataStore, state, clusterResourceManager, clusterManagerConfig.getHostAffinityEnabled(), false);\r\n    MockContainerAllocatorWithoutHostAffinity allocator = new MockContainerAllocatorWithoutHostAffinity(clusterResourceManager, conf, state, containerManager);\r\n    ContainerProcessManager cpm = spy(buildContainerProcessManager(clusterManagerConfig, state, clusterResourceManager, Optional.of(allocator)));\r\n    // Start the task clusterResourceManager\r\n    cpm.start();\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    SamzaResource container1 = new SamzaResource(1, 1000, \"host1\", \"id1\");\r\n    cpm.onResourceAllocated(container1);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    assertEquals(0, allocator.getContainerRequestState().numPendingRequests());\r\n    cpm.onStreamProcessorLaunchSuccess(container1);\r\n    // Create container failure - with ContainerExitStatus.DISKS_FAILED\r\n    SamzaResourceStatus resourceStatusOnAppError = new SamzaResourceStatus(container1.getContainerId(), \"App error\", 1);\r\n    cpm.onResourceCompleted(resourceStatusOnAppError);\r\n    verify(cpm).onResourceCompletedWithUnknownStatus(eq(resourceStatusOnAppError), anyString(), anyString(), anyInt());\r\n    // The above failure should trigger a container request\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(2, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    assertEquals(ResourceRequestState.ANY_HOST, allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    SamzaResource container2 = new SamzaResource(1, 1000, \"host1\", \"id2\");\r\n    cpm.onResourceAllocated(container2);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container2);\r\n    // Create container failure - with ContainerExitStatus.PREEMPTED\r\n    SamzaResourceStatus resourceStatusOnPreemption = new SamzaResourceStatus(container2.getContainerId(), \"Preemption\", SamzaResourceStatus.PREEMPTED);\r\n    cpm.onResourceCompleted(resourceStatusOnPreemption);\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(eq(resourceStatusOnPreemption), anyString(), anyString(), anyInt());\r\n    assertEquals(3, clusterResourceManager.resourceRequests.size());\r\n    // The above failure should trigger a container request\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(ResourceRequestState.ANY_HOST, allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    SamzaResource container3 = new SamzaResource(1, 1000, \"host1\", \"id3\");\r\n    cpm.onResourceAllocated(container3);\r\n    // Allow container to run and update state\r\n    if (!allocator.awaitContainersStart(1, 2, TimeUnit.SECONDS)) {\r\n        fail(\"timed out waiting for the containers to start\");\r\n    }\r\n    cpm.onStreamProcessorLaunchSuccess(container3);\r\n    // Create container failure - with ContainerExitStatus.ABORTED\r\n    SamzaResourceStatus resourceStatusOnAborted = new SamzaResourceStatus(container3.getContainerId(), \"Aborted\", SamzaResourceStatus.ABORTED);\r\n    cpm.onResourceCompleted(resourceStatusOnAborted);\r\n    verify(cpm, never()).onResourceCompletedWithUnknownStatus(eq(resourceStatusOnAborted), anyString(), anyString(), anyInt());\r\n    // The above failure should trigger a container request\r\n    assertEquals(1, allocator.getContainerRequestState().numPendingRequests());\r\n    assertEquals(4, clusterResourceManager.resourceRequests.size());\r\n    assertEquals(0, clusterResourceManager.releasedResources.size());\r\n    assertFalse(cpm.shouldShutdown());\r\n    assertFalse(state.jobHealthy.get());\r\n    assertEquals(ResourceRequestState.ANY_HOST, allocator.getContainerRequestState().peekPendingRequest().getPreferredHost());\r\n    cpm.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testUpdateRequestState",
  "sourceCode" : "/**\r\n * Test state after a request is submitted\r\n */\r\n@Test\r\npublic void testUpdateRequestState() {\r\n    // Host-affinity is enabled\r\n    ResourceRequestState state = new ResourceRequestState(true, manager);\r\n    SamzaResourceRequest request = new SamzaResourceRequest(1, 1024, \"abc\", \"0\");\r\n    state.addResourceRequest(request);\r\n    assertNotNull(manager.resourceRequests);\r\n    assertEquals(1, manager.resourceRequests.size());\r\n    assertNotNull(state.numPendingRequests() == 1);\r\n    assertNotNull(state.getHostRequestCounts());\r\n    assertNotNull(state.getHostRequestCounts().get(\"abc\"));\r\n    assertEquals(1, state.getHostRequestCounts().get(\"abc\").get());\r\n    assertNotNull(state.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(0, state.getResourcesOnAHost(\"abc\").size());\r\n    // Host-affinity is not enabled\r\n    ResourceRequestState state1 = new ResourceRequestState(false, manager);\r\n    SamzaResourceRequest request1 = new SamzaResourceRequest(1, 1024, null, \"1\");\r\n    state1.addResourceRequest(request1);\r\n    assertNotNull(manager.resourceRequests);\r\n    assertEquals(2, manager.resourceRequests.size());\r\n    assertTrue(state1.numPendingRequests() == 1);\r\n    assertNotNull(state1.getHostRequestCounts());\r\n    assertNull(state1.getHostRequestCounts().get(ANY_HOST));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testAddContainer",
  "sourceCode" : "/**\r\n * Test addContainer() updates the state correctly\r\n */\r\n@Test\r\npublic void testAddContainer() {\r\n    // Add container to ANY_LIST when host-affinity is not enabled\r\n    ResourceRequestState state = new ResourceRequestState(false, manager);\r\n    SamzaResource resource = new SamzaResource(1, 1024, \"abc\", \"id1\");\r\n    state.addResource(resource);\r\n    assertNotNull(state.getHostRequestCounts());\r\n    assertNotNull(state.getResourcesOnAHost(ANY_HOST));\r\n    assertEquals(1, state.getResourcesOnAHost(ANY_HOST).size());\r\n    assertEquals(resource, state.getResourcesOnAHost(ANY_HOST).get(0));\r\n    // Container Allocated when there is no request in queue\r\n    ResourceRequestState state1 = spy(new ResourceRequestState(true, manager));\r\n    SamzaResource container1 = new SamzaResource(1, 1024, \"zzz\", \"id2\");\r\n    state1.addResource(container1);\r\n    assertEquals(0, state1.numPendingRequests());\r\n    assertNull(state1.getResourcesOnAHost(\"zzz\"));\r\n    assertNotNull(state1.getResourcesOnAHost(ANY_HOST));\r\n    assertEquals(1, state1.getResourcesOnAHost(ANY_HOST).size());\r\n    assertEquals(container1, state1.getResourcesOnAHost(ANY_HOST).get(0));\r\n    // Container Allocated on a Requested Host\r\n    state1.addResourceRequest(new SamzaResourceRequest(1, 1024, \"abc\", \"0\"));\r\n    // Delayed request\r\n    state1.addResourceRequest(new SamzaResourceRequest(1, 1024, \"def\", \"1\", Instant.now().plus(Duration.ofHours(1))));\r\n    state1.addResourceRequest(new SamzaResourceRequest(1, 1024, \"ghi\", \"2\", Instant.now().plus(Duration.ofHours(2))));\r\n    assertEquals(1, state1.numPendingRequests());\r\n    assertEquals(2, state1.numDelayedRequests());\r\n    // Verify request sent only once for the non-delayed request\r\n    verify(state1).sendResourceRequest(any(SamzaResourceRequest.class));\r\n    assertNotNull(state1.getHostRequestCounts());\r\n    assertNotNull(state1.getHostRequestCounts().get(\"abc\"));\r\n    assertEquals(1, state1.getHostRequestCounts().get(\"abc\").get());\r\n    state1.addResource(resource);\r\n    assertNotNull(state1.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(1, state1.getResourcesOnAHost(\"abc\").size());\r\n    assertEquals(resource, state1.getResourcesOnAHost(\"abc\").get(0));\r\n    // Container Allocated on host that was not requested\r\n    SamzaResource container2 = new SamzaResource(1, 1024, \"xyz\", \"id2\");\r\n    state1.addResource(container2);\r\n    assertNull(state1.getResourcesOnAHost(\"xyz\"));\r\n    assertNotNull(state1.getResourcesOnAHost(ANY_HOST));\r\n    assertEquals(2, state1.getResourcesOnAHost(ANY_HOST).size());\r\n    assertEquals(container2, state1.getResourcesOnAHost(ANY_HOST).get(1));\r\n    // Extra containers were allocated on a host that was requested\r\n    SamzaResource container3 = new SamzaResource(1, 1024, \"abc\", \"id3\");\r\n    state1.addResource(container3);\r\n    assertEquals(3, state1.getResourcesOnAHost(ANY_HOST).size());\r\n    assertEquals(container3, state1.getResourcesOnAHost(ANY_HOST).get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testContainerAssignment",
  "sourceCode" : "/**\r\n * Test request state after container is assigned to a host\r\n * * Assigned on requested host\r\n * * Assigned on any host\r\n */\r\n@Test\r\npublic void testContainerAssignment() throws Exception {\r\n    // Host-affinity enabled\r\n    ResourceRequestState state = new ResourceRequestState(true, manager);\r\n    SamzaResourceRequest request = new SamzaResourceRequest(1, 1024, \"abc\", \"0\");\r\n    SamzaResourceRequest request1 = new SamzaResourceRequest(1, 1024, \"def\", \"0\");\r\n    state.addResourceRequest(request);\r\n    state.addResourceRequest(request1);\r\n    SamzaResource container = new SamzaResource(1, 1024, \"abc\", \"id0\");\r\n    SamzaResource container1 = new SamzaResource(1, 1024, \"zzz\", \"id1\");\r\n    state.addResource(container);\r\n    state.addResource(container1);\r\n    assertEquals(2, state.numPendingRequests());\r\n    assertEquals(2, state.getHostRequestCounts().size());\r\n    assertNotNull(state.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(1, state.getResourcesOnAHost(\"abc\").size());\r\n    assertEquals(container, state.getResourcesOnAHost(\"abc\").get(0));\r\n    assertNotNull(state.getResourcesOnAHost(\"def\"));\r\n    assertEquals(0, state.getResourcesOnAHost(\"def\").size());\r\n    assertNotNull(state.getResourcesOnAHost(ANY_HOST));\r\n    assertEquals(1, state.getResourcesOnAHost(ANY_HOST).size());\r\n    assertEquals(container1, state.getResourcesOnAHost(ANY_HOST).get(0));\r\n    // Container assigned on the requested host\r\n    state.updateStateAfterAssignment(request, \"abc\", container);\r\n    assertEquals(request1, state.peekPendingRequest());\r\n    assertNotNull(state.getHostRequestCounts().get(\"abc\"));\r\n    assertEquals(0, state.getHostRequestCounts().get(\"abc\").get());\r\n    assertNotNull(state.getResourcesOnAHost(\"abc\"));\r\n    assertEquals(0, state.getResourcesOnAHost(\"abc\").size());\r\n    // Container assigned on any host\r\n    state.updateStateAfterAssignment(request1, ANY_HOST, container1);\r\n    assertEquals(0, state.numPendingRequests());\r\n    assertNotNull(state.getHostRequestCounts().get(\"def\"));\r\n    assertEquals(0, state.getHostRequestCounts().get(\"def\").get());\r\n    assertNotNull(state.getResourcesOnAHost(ANY_HOST));\r\n    assertEquals(0, state.getResourcesOnAHost(ANY_HOST).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testReleaseResource",
  "sourceCode" : "@Test\r\npublic void testReleaseResource() {\r\n    // Host-affinity is enabled\r\n    ResourceRequestState state = new ResourceRequestState(true, manager);\r\n    SamzaResourceRequest request = new SamzaResourceRequest(1, 1024, \"abc\", \"0\");\r\n    SamzaResourceRequest request1 = new SamzaResourceRequest(1, 1024, \"def\", \"0\");\r\n    state.addResourceRequest(request);\r\n    state.addResourceRequest(request1);\r\n    SamzaResource container = new SamzaResource(1, 1024, \"abc\", \"id0\");\r\n    SamzaResource container1 = new SamzaResource(1, 1024, ANY_HOST, \"id1\");\r\n    state.addResource(container);\r\n    state.addResource(container1);\r\n    state.releaseResource(\"id0\");\r\n    assertEquals(0, state.getResourcesOnAHost(\"abc\").size());\r\n    assertEquals(1, state.getResourcesOnAHost(ANY_HOST).size());\r\n    state.releaseResource(\"id1\");\r\n    assertEquals(0, state.getResourcesOnAHost(\"abc\").size());\r\n    assertEquals(0, state.getResourcesOnAHost(ANY_HOST).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testPriorityQueueOrdering",
  "sourceCode" : "@Test\r\npublic void testPriorityQueueOrdering() {\r\n    PriorityQueue<SamzaResourceRequest> pq = new PriorityQueue<>();\r\n    Instant now = Instant.now();\r\n    ImmutableList<SamzaResourceRequest> expectedOrder = ImmutableList.of(createRequestForActive(now.minusSeconds(120)), createRequestForActive(now), createRequestForActive(now.plusSeconds(120)), createRequestForActive(now.plusSeconds(240)), createRequestForStandby(now.minusSeconds(120)), createRequestForStandby(now), createRequestForStandby(now.plusSeconds(120)), createRequestForStandby(now.plusSeconds(240)));\r\n    SamzaResourceRequest[] copyExpectedOrder = new SamzaResourceRequest[expectedOrder.size()];\r\n    copyExpectedOrder = expectedOrder.toArray(copyExpectedOrder);\r\n    List<SamzaResourceRequest> shuffled = Arrays.asList(copyExpectedOrder);\r\n    Collections.shuffle(shuffled, new Random(Instant.now().toEpochMilli()));\r\n    pq.addAll(shuffled);\r\n    ArrayList priorityQueueOrder = new ArrayList();\r\n    for (int i = 0; i < expectedOrder.size(); ++i) {\r\n        priorityQueueOrder.add(pq.poll());\r\n    }\r\n    assertEquals(expectedOrder, priorityQueueOrder);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestContainerRequestState.java",
  "methodName" : "testDelayedQueueOrdering",
  "sourceCode" : "@Test\r\npublic void testDelayedQueueOrdering() {\r\n    ResourceRequestState.DelayedRequestQueue delayedRequestQueue = new ResourceRequestState.DelayedRequestQueue();\r\n    Instant now = Instant.now();\r\n    // Expected priority by request timestamp only, regardless of active or standby\r\n    ImmutableList<SamzaResourceRequest> expectedOrder = ImmutableList.of(createRequestForActive(now), createRequestForStandby(now.plusSeconds(60)), createRequestForActive(now.plusSeconds(120)), createRequestForStandby(now.plusSeconds(121)), createRequestForActive(now.plusSeconds(240)), createRequestForStandby(now.plusSeconds(241)));\r\n    SamzaResourceRequest[] copyExpectedOrder = new SamzaResourceRequest[expectedOrder.size()];\r\n    copyExpectedOrder = expectedOrder.toArray(copyExpectedOrder);\r\n    List<SamzaResourceRequest> shuffled = Arrays.asList(copyExpectedOrder);\r\n    Collections.shuffle(shuffled, new Random(Instant.now().toEpochMilli()));\r\n    delayedRequestQueue.addAll(shuffled);\r\n    ArrayList priorityQueueOrder = new ArrayList();\r\n    for (int i = 0; i < expectedOrder.size(); ++i) {\r\n        priorityQueueOrder.add(delayedRequestQueue.poll());\r\n    }\r\n    assertEquals(expectedOrder, priorityQueueOrder);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestDefaultApplicationMain.java",
  "methodName" : "testRun",
  "sourceCode" : "@Test\r\npublic void testRun() throws Exception {\r\n    String[] args = new String[] { \"--config\", JobConfig.CONFIG_LOADER_FACTORY + \"=\" + PropertiesConfigLoaderFactory.class.getName(), \"--config\", PropertiesConfigLoaderFactory.CONFIG_LOADER_PROPERTIES_PREFIX + \"path=\" + getClass().getResource(\"/test.properties\").getPath() };\r\n    StreamApplication mockApplication = mock(StreamApplication.class);\r\n    Config mockConfig = mock(Config.class);\r\n    mockStatic(JobCoordinatorLaunchUtil.class, ApplicationUtil.class, ConfigUtil.class);\r\n    when(ApplicationUtil.fromConfig(any())).thenReturn(mockApplication);\r\n    when(ConfigUtil.loadConfig(any())).thenReturn(mockConfig);\r\n    doNothing().when(JobCoordinatorLaunchUtil.class, \"run\", mockApplication, mockConfig);\r\n    DefaultApplicationMain.run(args);\r\n    verifyStatic(times(1));\r\n    JobCoordinatorLaunchUtil.run(mockApplication, mockConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestJobCoordinatorLaunchUtil.java",
  "methodName" : "testRunClusterBasedJobCoordinator",
  "sourceCode" : "@Test\r\npublic void testRunClusterBasedJobCoordinator() throws Exception {\r\n    Config originalConfig = buildOriginalConfig(ImmutableMap.of());\r\n    JobConfig fullConfig = new JobConfig(new MapConfig(originalConfig, Collections.singletonMap(\"isAfterPlanning\", \"true\")));\r\n    Config autoSizingConfig = new MapConfig(Collections.singletonMap(JobConfig.JOB_AUTOSIZING_CONTAINER_COUNT, \"10\"));\r\n    Config finalConfig = new MapConfig(autoSizingConfig, fullConfig);\r\n    RemoteJobPlanner mockJobPlanner = mock(RemoteJobPlanner.class);\r\n    CoordinatorStreamStore mockCoordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    ClusterBasedJobCoordinator mockJC = mock(ClusterBasedJobCoordinator.class);\r\n    PowerMockito.mockStatic(CoordinatorStreamUtil.class);\r\n    PowerMockito.doNothing().when(CoordinatorStreamUtil.class, \"createCoordinatorStream\", any());\r\n    PowerMockito.doReturn(new MapConfig()).when(CoordinatorStreamUtil.class, \"buildCoordinatorStreamConfig\", any());\r\n    PowerMockito.doReturn(autoSizingConfig).when(CoordinatorStreamUtil.class, \"readLaunchConfigFromCoordinatorStream\", any(), any());\r\n    PowerMockito.whenNew(CoordinatorStreamStore.class).withAnyArguments().thenReturn(mockCoordinatorStreamStore);\r\n    PowerMockito.whenNew(RemoteJobPlanner.class).withAnyArguments().thenReturn(mockJobPlanner);\r\n    PowerMockito.whenNew(ClusterBasedJobCoordinator.class).withAnyArguments().thenReturn(mockJC);\r\n    when(mockJobPlanner.prepareJobs()).thenReturn(Collections.singletonList(fullConfig));\r\n    JobCoordinatorLaunchUtil.run(new MockStreamApplication(), originalConfig);\r\n    verifyNew(ClusterBasedJobCoordinator.class).withArguments(any(MetricsRegistryMap.class), eq(mockCoordinatorStreamStore), eq(finalConfig));\r\n    verify(mockJC, times(1)).run();\r\n    verifyStatic(times(1));\r\n    CoordinatorStreamUtil.createCoordinatorStream(fullConfig);\r\n    verifyStatic(times(1));\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(finalConfig, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestJobCoordinatorLaunchUtil.java",
  "methodName" : "testRunJobCoordinator",
  "sourceCode" : "@Test\r\npublic void testRunJobCoordinator() throws Exception {\r\n    String jobCoordinatorFactoryClass = \"org.apache.samza.custom.MyJobCoordinatorFactory\";\r\n    Config originalConfig = buildOriginalConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, jobCoordinatorFactoryClass));\r\n    JobConfig fullConfig = new JobConfig(new MapConfig(originalConfig, Collections.singletonMap(\"isAfterPlanning\", \"true\")));\r\n    Config autoSizingConfig = new MapConfig(Collections.singletonMap(JobConfig.JOB_AUTOSIZING_CONTAINER_COUNT, \"10\"));\r\n    Config finalConfig = new MapConfig(autoSizingConfig, fullConfig);\r\n    RemoteJobPlanner remoteJobPlanner = mock(RemoteJobPlanner.class);\r\n    CoordinatorStreamStore coordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    JobCoordinatorFactory jobCoordinatorFactory = mock(JobCoordinatorFactory.class);\r\n    JobCoordinator jobCoordinator = mock(JobCoordinator.class);\r\n    PowerMockito.mockStatic(CoordinatorStreamUtil.class);\r\n    PowerMockito.doNothing().when(CoordinatorStreamUtil.class, \"createCoordinatorStream\", any());\r\n    PowerMockito.doReturn(new MapConfig()).when(CoordinatorStreamUtil.class, \"buildCoordinatorStreamConfig\", any());\r\n    PowerMockito.doReturn(autoSizingConfig).when(CoordinatorStreamUtil.class, \"readLaunchConfigFromCoordinatorStream\", any(), any());\r\n    PowerMockito.whenNew(CoordinatorStreamStore.class).withAnyArguments().thenReturn(coordinatorStreamStore);\r\n    PowerMockito.whenNew(RemoteJobPlanner.class).withAnyArguments().thenReturn(remoteJobPlanner);\r\n    when(remoteJobPlanner.prepareJobs()).thenReturn(Collections.singletonList(fullConfig));\r\n    PowerMockito.mockStatic(ReflectionUtil.class);\r\n    PowerMockito.doReturn(jobCoordinatorFactory).when(ReflectionUtil.class, \"getObj\", jobCoordinatorFactoryClass, JobCoordinatorFactory.class);\r\n    when(jobCoordinatorFactory.getJobCoordinator(eq(\"samza-job-coordinator\"), eq(finalConfig), any(), eq(coordinatorStreamStore))).thenReturn(jobCoordinator);\r\n    // use a latch to keep track of when shutdown hook was added to know when we should start verifications\r\n    CountDownLatch addShutdownHookLatch = new CountDownLatch(1);\r\n    PowerMockito.spy(JobCoordinatorLaunchUtil.class);\r\n    PowerMockito.doAnswer(invocation -> {\r\n        addShutdownHookLatch.countDown();\r\n        return null;\r\n    }).when(JobCoordinatorLaunchUtil.class, \"addShutdownHook\", any());\r\n    MetricsReporter metricsReporter = mock(MetricsReporter.class);\r\n    Map<String, MetricsReporter> metricsReporterMap = ImmutableMap.of(\"reporter\", metricsReporter);\r\n    PowerMockito.mockStatic(MetricsReporterLoader.class);\r\n    PowerMockito.doReturn(metricsReporterMap).when(MetricsReporterLoader.class, \"getMetricsReporters\", new MetricsConfig(finalConfig), \"JobCoordinator\");\r\n    NoProcessorJobCoordinatorListener jobCoordinatorListener = mock(NoProcessorJobCoordinatorListener.class);\r\n    PowerMockito.whenNew(NoProcessorJobCoordinatorListener.class).withAnyArguments().thenReturn(jobCoordinatorListener);\r\n    Thread runThread = new Thread(() -> JobCoordinatorLaunchUtil.run(new MockStreamApplication(), originalConfig));\r\n    runThread.start();\r\n    // last thing before waiting for shutdown is to add shutdown hook, so do verifications once hook is added\r\n    addShutdownHookLatch.await();\r\n    verifyStatic();\r\n    CoordinatorStreamUtil.createCoordinatorStream(fullConfig);\r\n    verifyStatic();\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(finalConfig, true);\r\n    verifyStatic();\r\n    JobCoordinatorLaunchUtil.addShutdownHook(jobCoordinator);\r\n    InOrder inOrder = Mockito.inOrder(metricsReporter, jobCoordinator);\r\n    inOrder.verify(metricsReporter).register(eq(\"JobCoordinator\"), any());\r\n    inOrder.verify(metricsReporter).start();\r\n    ArgumentCaptor<CountDownLatch> countDownLatchArgumentCaptor = ArgumentCaptor.forClass(CountDownLatch.class);\r\n    verifyNew(NoProcessorJobCoordinatorListener.class).withArguments(countDownLatchArgumentCaptor.capture());\r\n    inOrder.verify(jobCoordinator).setListener(jobCoordinatorListener);\r\n    inOrder.verify(jobCoordinator).start();\r\n    // wait some time and then make sure the run thread is still alive\r\n    Thread.sleep(Duration.ofMillis(500).toMillis());\r\n    assertTrue(runThread.isAlive());\r\n    // trigger the count down latch so that the run thread can exit\r\n    countDownLatchArgumentCaptor.getValue().countDown();\r\n    runThread.join(Duration.ofSeconds(10).toMillis());\r\n    assertFalse(runThread.isAlive());\r\n    verify(metricsReporter).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestStandbyAllocator.java",
  "methodName" : "testWithNoStandby",
  "sourceCode" : "@Test\r\npublic void testWithNoStandby() {\r\n    JobModel jobModel = getJobModelWithStandby(1, 1, 1);\r\n    List<String> containerConstraints = StandbyTaskUtil.getStandbyContainerConstraints(\"0\", jobModel);\r\n    Assert.assertEquals(\"Constrained container count should be 0\", 0, containerConstraints.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\clustermanager\\TestStandbyAllocator.java",
  "methodName" : "testWithStandby",
  "sourceCode" : "@Test\r\npublic void testWithStandby() {\r\n    testWithStandby(2, 1, 2);\r\n    testWithStandby(10, 1, 2);\r\n    testWithStandby(2, 10, 2);\r\n    testWithStandby(2, 10, 4);\r\n    testWithStandby(10, 1, 4);\r\n    testWithStandby(10, 10, 4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\EnvironmentConfigRewriterTest.java",
  "methodName" : "testRewriteKeyValidKeys",
  "sourceCode" : "@Test\r\npublic void testRewriteKeyValidKeys() {\r\n    assertEquals(\"foo\", EnvironmentConfigRewriter.renameKey(\"SAMZA_FOO\"));\r\n    assertEquals(\"foo.bar\", EnvironmentConfigRewriter.renameKey(\"SAMZA_FOO_BAR\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\EnvironmentConfigRewriterTest.java",
  "methodName" : "testRewriteKeyInvalidKeyPrefix",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testRewriteKeyInvalidKeyPrefix() {\r\n    EnvironmentConfigRewriter.renameKey(\"SAMZA\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\EnvironmentConfigRewriterTest.java",
  "methodName" : "testRewriteKeyInvalidKeyNoSubkey",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testRewriteKeyInvalidKeyNoSubkey() {\r\n    EnvironmentConfigRewriter.renameKey(\"SAMZA\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\EnvironmentConfigRewriterTest.java",
  "methodName" : "testRewriteFailsOnDowncaseMatch",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testRewriteFailsOnDowncaseMatch() throws Exception {\r\n    Map<String, String> config = createMap(\"foo.Bar\", \"a\");\r\n    Map<String, String> env = createMap(\"SAMZA_FOO_BAR\", \"b\");\r\n    rewriter.rewrite(new MapConfig(config), env);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\EnvironmentConfigRewriterTest.java",
  "methodName" : "testRewriteOverridesConfig",
  "sourceCode" : "@Test\r\npublic void testRewriteOverridesConfig() throws Exception {\r\n    Map<String, String> config = createMap(\"foo.bar\", \"a\");\r\n    Map<String, String> env = createMap(\"SAMZA_FOO_BAR\", \"b\");\r\n    Config rewritten = rewriter.rewrite(new MapConfig(config), env);\r\n    assertEquals(\"b\", rewritten.get(\"foo.bar\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\loaders\\TestPropertiesConfigLoader.java",
  "methodName" : "testCanReadPropertiesConfigFiles",
  "sourceCode" : "@Test\r\npublic void testCanReadPropertiesConfigFiles() {\r\n    ConfigLoader loader = new PropertiesConfigLoaderFactory().getLoader(new MapConfig(Collections.singletonMap(\"path\", getClass().getResource(\"/test.properties\").getPath())));\r\n    Config config = loader.getConfig();\r\n    assertEquals(\"bar\", config.get(\"foo\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\loaders\\TestPropertiesConfigLoader.java",
  "methodName" : "testCanNotReadWithoutPath",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testCanNotReadWithoutPath() {\r\n    ConfigLoader loader = new PropertiesConfigLoaderFactory().getLoader(new MapConfig());\r\n    loader.getConfig();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestApplicationConfig.java",
  "methodName" : "isHighLevelJob",
  "sourceCode" : "@Test\r\npublic void isHighLevelJob() {\r\n    final Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_API_TYPE, ApplicationApiType.HIGH_LEVEL.name());\r\n    ApplicationConfig applicationConfig = new ApplicationConfig(new MapConfig(configMap));\r\n    assertTrue(applicationConfig.isHighLevelApiJob());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestApplicationConfig.java",
  "methodName" : "isHighLevelJobWithLowLevelJob",
  "sourceCode" : "@Test\r\npublic void isHighLevelJobWithLowLevelJob() {\r\n    final Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_API_TYPE, ApplicationApiType.LOW_LEVEL.name());\r\n    ApplicationConfig applicationConfig = new ApplicationConfig(new MapConfig(configMap));\r\n    assertFalse(applicationConfig.isHighLevelApiJob());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestApplicationConfig.java",
  "methodName" : "isHighLevelJobWithLegacyJob",
  "sourceCode" : "@Test\r\npublic void isHighLevelJobWithLegacyJob() {\r\n    final Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_API_TYPE, ApplicationApiType.LEGACY.name());\r\n    ApplicationConfig applicationConfig = new ApplicationConfig(new MapConfig(configMap));\r\n    assertFalse(applicationConfig.isHighLevelApiJob());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestFileSystemCheckpointManagerConfig.java",
  "methodName" : "testGetFileSystemCheckpointRoot",
  "sourceCode" : "@Test\r\npublic void testGetFileSystemCheckpointRoot() {\r\n    String checkpointManagerRoot = \"checkpointManagerRoot\";\r\n    // checkpoint path exists\r\n    Config config = new MapConfig(ImmutableMap.of(\"task.checkpoint.path\", checkpointManagerRoot));\r\n    FileSystemCheckpointManagerConfig fileSystemCheckpointManagerConfig = new FileSystemCheckpointManagerConfig(config);\r\n    assertEquals(checkpointManagerRoot, fileSystemCheckpointManagerConfig.getFileSystemCheckpointRoot().get());\r\n    // checkpoint path does not exist\r\n    config = new MapConfig();\r\n    fileSystemCheckpointManagerConfig = new FileSystemCheckpointManagerConfig(config);\r\n    assertFalse(fileSystemCheckpointManagerConfig.getFileSystemCheckpointRoot().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJavaTableConfig.java",
  "methodName" : "testGetTableIds",
  "sourceCode" : "@Test\r\npublic void testGetTableIds() {\r\n    Set<String> ids = Sets.newHashSet(\"t1\", \"t2\");\r\n    Map<String, String> map = ids.stream().map(id -> String.format(JavaTableConfig.TABLE_PROVIDER_FACTORY, id)).collect(Collectors.toMap(key -> key, key -> key + \"-provider-factory\"));\r\n    JavaTableConfig tableConfig = new JavaTableConfig(new MapConfig(map));\r\n    assertEquals(2, tableConfig.getTableIds().size());\r\n    ids.removeAll(tableConfig.getTableIds());\r\n    assertTrue(ids.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJavaTableConfig.java",
  "methodName" : "testGetTableProperties",
  "sourceCode" : "@Test\r\npublic void testGetTableProperties() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"stores.t1.key.serde\", \"key-serde\");\r\n    map.put(\"stores.t1.msg.serde\", \"msg-serde\");\r\n    map.put(\"tables.t1.provider.factory\", \"t1-provider-factory\");\r\n    JavaTableConfig tableConfig = new JavaTableConfig(new MapConfig(map));\r\n    assertEquals(\"t1-provider-factory\", tableConfig.getTableProviderFactory(\"t1\"));\r\n    assertEquals(\"key-serde\", tableConfig.getKeySerde(\"t1\"));\r\n    assertEquals(\"msg-serde\", tableConfig.getMsgSerde(\"t1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJavaTableConfig.java",
  "methodName" : "testBuildKey",
  "sourceCode" : "@Test\r\npublic void testBuildKey() {\r\n    String key = JavaTableConfig.buildKey(\"t1\", \"abc\");\r\n    Assert.assertEquals(\"tables.t1.abc\", key);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJavaTableConfig.java",
  "methodName" : "testGetForTable",
  "sourceCode" : "@Test\r\npublic void testGetForTable() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(JavaTableConfig.buildKey(\"t1\", \"abc\"), \"xyz\");\r\n    JavaTableConfig tableConfig = new JavaTableConfig(new MapConfig(map));\r\n    Assert.assertEquals(\"xyz\", tableConfig.getForTable(\"t1\", \"abc\"));\r\n    Assert.assertNull(tableConfig.getForTable(\"t1\", \"aaa\"));\r\n    Assert.assertEquals(\"xyz\", tableConfig.getForTable(\"t1\", \"aaa\", \"xyz\"));\r\n    Assert.assertNull(tableConfig.getForTable(\"tt\", \"abc\"));\r\n    Assert.assertEquals(\"xyz\", tableConfig.getForTable(\"tt\", \"abc\", \"xyz\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetName",
  "sourceCode" : "@Test\r\npublic void testGetName() {\r\n    String jobName = \"job-name\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_NAME, jobName)));\r\n    assertEquals(Optional.of(jobName), jobConfig.getName());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetCoordinatorSystemName",
  "sourceCode" : "@Test\r\npublic void testGetCoordinatorSystemName() {\r\n    String coordinatorSystemName = \"coordinator-system\", jobDefaultSystem = \"job-default-system\";\r\n    // has job coordinator system and default system; choose job coordinator system\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_COORDINATOR_SYSTEM, coordinatorSystemName, JobConfig.JOB_DEFAULT_SYSTEM, jobDefaultSystem)));\r\n    assertEquals(coordinatorSystemName, jobConfig.getCoordinatorSystemName());\r\n    // has job coordinator system only; choose job coordinator system\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_COORDINATOR_SYSTEM, coordinatorSystemName)));\r\n    assertEquals(coordinatorSystemName, jobConfig.getCoordinatorSystemName());\r\n    // has default system only; choose default system\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, jobDefaultSystem)));\r\n    assertEquals(jobDefaultSystem, jobConfig.getCoordinatorSystemName());\r\n    try {\r\n        new JobConfig(new MapConfig()).getCoordinatorSystemName();\r\n        fail(\"Should have gotten a ConfigException\");\r\n    } catch (ConfigException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetCoordinatorSystemNameOrNull",
  "sourceCode" : "@Test\r\npublic void testGetCoordinatorSystemNameOrNull() {\r\n    String coordinatorSystemName = \"coordinator-system\", jobDefaultSystem = \"job-default-system\";\r\n    // has job coordinator system and default system; choose job coordinator system\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_COORDINATOR_SYSTEM, coordinatorSystemName, JobConfig.JOB_DEFAULT_SYSTEM, jobDefaultSystem)));\r\n    assertEquals(coordinatorSystemName, jobConfig.getCoordinatorSystemNameOrNull());\r\n    // has job coordinator system only; choose job coordinator system\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_COORDINATOR_SYSTEM, coordinatorSystemName)));\r\n    assertEquals(coordinatorSystemName, jobConfig.getCoordinatorSystemNameOrNull());\r\n    // has default system only; choose default system\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, jobDefaultSystem)));\r\n    assertEquals(jobDefaultSystem, jobConfig.getCoordinatorSystemNameOrNull());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertNull(jobConfig.getCoordinatorSystemNameOrNull());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetDefaultSystem",
  "sourceCode" : "@Test\r\npublic void testGetDefaultSystem() {\r\n    String jobDefaultSystem = \"job-default-system\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, jobDefaultSystem)));\r\n    assertEquals(Optional.of(jobDefaultSystem), jobConfig.getDefaultSystem());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getDefaultSystem());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetContainerCount",
  "sourceCode" : "@Test\r\npublic void testGetContainerCount() {\r\n    int jobContainerCount = 10, yarnContainerCount = 5;\r\n    // has job container count and yarn container count; choose job container count\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_CONTAINER_COUNT, Integer.toString(jobContainerCount), \"yarn.container.count\", Integer.toString(yarnContainerCount))));\r\n    assertEquals(jobContainerCount, jobConfig.getContainerCount());\r\n    // has job container count only; choose job container count\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_CONTAINER_COUNT, Integer.toString(jobContainerCount))));\r\n    assertEquals(jobContainerCount, jobConfig.getContainerCount());\r\n    // has yarn container count only; choose yarn container count\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(\"yarn.container.count\", Integer.toString(yarnContainerCount))));\r\n    assertEquals(yarnContainerCount, jobConfig.getContainerCount());\r\n    // not specified; use default\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_JOB_CONTAINER_COUNT, jobConfig.getContainerCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMonitorRegexDisabled",
  "sourceCode" : "@Test\r\npublic void testGetMonitorRegexDisabled() {\r\n    // positive means enabled\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(100))));\r\n    assertFalse(jobConfig.getMonitorRegexDisabled());\r\n    // zero means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(0))));\r\n    assertTrue(jobConfig.getMonitorRegexDisabled());\r\n    // negative means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(-1))));\r\n    assertTrue(jobConfig.getMonitorRegexDisabled());\r\n    // not specified uses the default monitor partition change frequency, which means enabled\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertFalse(jobConfig.getMonitorRegexDisabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMonitorPartitionChangeFrequency",
  "sourceCode" : "@Test\r\npublic void testGetMonitorPartitionChangeFrequency() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_PARTITION_CHANGE_FREQUENCY_MS, Integer.toString(100))));\r\n    assertEquals(100, jobConfig.getMonitorPartitionChangeFrequency());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_PARTITION_CHANGE_FREQUENCY_MS, Integer.toString(0))));\r\n    assertEquals(0, jobConfig.getMonitorPartitionChangeFrequency());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_PARTITION_CHANGE_FREQUENCY_MS, Integer.toString(-1))));\r\n    assertEquals(-1, jobConfig.getMonitorPartitionChangeFrequency());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_MONITOR_PARTITION_CHANGE_FREQUENCY_MS, jobConfig.getMonitorPartitionChangeFrequency());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMonitorRegexFrequency",
  "sourceCode" : "@Test\r\npublic void testGetMonitorRegexFrequency() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(100))));\r\n    assertEquals(100, jobConfig.getMonitorRegexFrequency());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(0))));\r\n    assertEquals(0, jobConfig.getMonitorRegexFrequency());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.MONITOR_INPUT_REGEX_FREQUENCY_MS, Integer.toString(-1))));\r\n    assertEquals(-1, jobConfig.getMonitorRegexFrequency());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_MONITOR_INPUT_REGEX_FREQUENCY_MS, jobConfig.getMonitorRegexFrequency());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMonitorRegexPatternMap",
  "sourceCode" : "@Test\r\npublic void testGetMonitorRegexPatternMap() {\r\n    // 2 different rewriters to system0, 1 rewriter to system1, 1 rewriter which isn't in rewritersList\r\n    String system0 = \"system0\", system1 = \"system1\";\r\n    String system0Rewriter0 = \"system-0-rewriter-0\", system0Rewriter1 = \"system-0-rewriter-1\", system1Rewriter = \"system-1-rewriter\", systemOnlyRewriter = \"system-only-rewriter\";\r\n    String system0Rewriter0Streams = \"system-0-rewriter-0-.*\", system0Rewriter1Streams = \"system-0-rewriter-1-.*\", system1RewriterStreams = \"system-1-rewriter-.*\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(new ImmutableMap.Builder<String, String>().put(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, system0Rewriter0), system0).put(String.format(JobConfig.REGEX_RESOLVED_STREAMS, system0Rewriter0), system0Rewriter0Streams).put(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, system0Rewriter1), system0).put(String.format(JobConfig.REGEX_RESOLVED_STREAMS, system0Rewriter1), system0Rewriter1Streams).put(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, system1Rewriter), system1).put(String.format(JobConfig.REGEX_RESOLVED_STREAMS, system1Rewriter), system1RewriterStreams).// not passed in as a rewriter when calling getMonitorRegexPatternMap\r\n    put(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, \"unused-rewriter\"), system0).put(String.format(JobConfig.REGEX_RESOLVED_STREAMS, \"unused-rewriter\"), \"unused-rewriter-.*\").// should not be included since there is no regex\r\n    put(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, systemOnlyRewriter), system0).build()));\r\n    // Pattern.equals only checks that the references are the same, so can't compare maps directly\r\n    Map<String, Pattern> actual = jobConfig.getMonitorRegexPatternMap(String.join(\",\", ImmutableList.of(system0Rewriter0, system0Rewriter1, system1Rewriter, systemOnlyRewriter, \"not-a-regex-rewriter\")));\r\n    // only should have rewriters for system0 and system1\r\n    assertEquals(2, actual.size());\r\n    assertEquals(system0Rewriter0Streams + \"|\" + system0Rewriter1Streams, actual.get(system0).pattern());\r\n    assertEquals(system1RewriterStreams, actual.get(system1).pattern());\r\n    // empty configs should produce an empty map\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Collections.<String, Pattern>emptyMap(), jobConfig.getMonitorRegexPatternMap(system0Rewriter0));\r\n    assertEquals(Collections.<String, Pattern>emptyMap(), jobConfig.getMonitorRegexPatternMap(\"\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetRegexResolvedStreams",
  "sourceCode" : "@Test\r\npublic void testGetRegexResolvedStreams() {\r\n    String rewriterName = \"rewriter-name\", regex = \"my-stream-.*\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(String.format(JobConfig.REGEX_RESOLVED_STREAMS, rewriterName), regex)));\r\n    assertEquals(Optional.of(regex), jobConfig.getRegexResolvedStreams(rewriterName));\r\n    assertEquals(Optional.empty(), jobConfig.getRegexResolvedStreams(\"other-rewriter\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetRegexResolvedSystem",
  "sourceCode" : "@Test\r\npublic void testGetRegexResolvedSystem() {\r\n    String rewriterName = \"rewriter-name\", regex = \"my-system-.*\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(String.format(JobConfig.REGEX_RESOLVED_SYSTEM, rewriterName), regex)));\r\n    assertEquals(Optional.of(regex), jobConfig.getRegexResolvedSystem(rewriterName));\r\n    assertEquals(Optional.empty(), jobConfig.getRegexResolvedSystem(\"other-rewriter\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetRegexResolvedInheritedConfig",
  "sourceCode" : "@Test\r\npublic void testGetRegexResolvedInheritedConfig() {\r\n    String rewriterName = \"rewriter-name\";\r\n    String key0 = \"key0\", value0 = \"value0\", key1 = \"other.key1\", value1 = \"value1\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(String.format(JobConfig.REGEX_INHERITED_CONFIG + \".\" + key0, rewriterName), value0, String.format(JobConfig.REGEX_INHERITED_CONFIG + \".\" + key1, rewriterName), value1)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(key0, value0, key1, value1)), jobConfig.getRegexResolvedInheritedConfig(rewriterName));\r\n    assertEquals(new MapConfig(), jobConfig.getRegexResolvedInheritedConfig(\"other-rewriter\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetStreamJobFactoryClass",
  "sourceCode" : "@Test\r\npublic void testGetStreamJobFactoryClass() {\r\n    String jobFactoryClass = \"my.job.Factory.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STREAM_JOB_FACTORY_CLASS, jobFactoryClass)));\r\n    assertEquals(Optional.of(jobFactoryClass), jobConfig.getStreamJobFactoryClass());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getStreamJobFactoryClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetJobId",
  "sourceCode" : "@Test\r\npublic void testGetJobId() {\r\n    String jobId = \"job-id\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ID, jobId)));\r\n    assertEquals(jobId, jobConfig.getJobId());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_JOB_ID, jobConfig.getJobId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testFailOnCheckpointValidation",
  "sourceCode" : "@Test\r\npublic void testFailOnCheckpointValidation() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_FAIL_CHECKPOINT_VALIDATION, \"true\")));\r\n    assertTrue(jobConfig.failOnCheckpointValidation());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_FAIL_CHECKPOINT_VALIDATION, \"false\")));\r\n    assertFalse(jobConfig.failOnCheckpointValidation());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertTrue(jobConfig.failOnCheckpointValidation());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetConfigRewriters",
  "sourceCode" : "@Test\r\npublic void testGetConfigRewriters() {\r\n    String configRewriters = \"rewriter0,rewriter1\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.CONFIG_REWRITERS, configRewriters)));\r\n    assertEquals(Optional.of(configRewriters), jobConfig.getConfigRewriters());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getConfigRewriters());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetConfigRewriterClass",
  "sourceCode" : "@Test\r\npublic void testGetConfigRewriterClass() {\r\n    String rewriterName = \"rewriter-name\", className = \"my.Rewriter.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(String.format(JobConfig.CONFIG_REWRITER_CLASS, rewriterName), className)));\r\n    assertEquals(Optional.of(className), jobConfig.getConfigRewriterClass(rewriterName));\r\n    assertEquals(Optional.empty(), jobConfig.getConfigRewriterClass(\"other-rewriter\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSystemStreamPartitionGrouperFactory",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamPartitionGrouperFactory() {\r\n    String sspGrouperFactory = \"my.ssp.grouper.Factory.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SSP_GROUPER_FACTORY, sspGrouperFactory)));\r\n    assertEquals(sspGrouperFactory, jobConfig.getSystemStreamPartitionGrouperFactory());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(GroupByPartitionFactory.class.getName(), jobConfig.getSystemStreamPartitionGrouperFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetLocationIdProviderFactory",
  "sourceCode" : "@Test\r\npublic void testGetLocationIdProviderFactory() {\r\n    String locationIdProviderFactory = \"my.location.id.provider.Factory.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.LOCATION_ID_PROVIDER_FACTORY, locationIdProviderFactory)));\r\n    assertEquals(locationIdProviderFactory, jobConfig.getLocationIdProviderFactory());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(DefaultLocationIdProviderFactory.class.getName(), jobConfig.getLocationIdProviderFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSecurityManagerFactory",
  "sourceCode" : "@Test\r\npublic void testGetSecurityManagerFactory() {\r\n    String securityManagerFactory = \"my.security.manager.factory\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_SECURITY_MANAGER_FACTORY, securityManagerFactory)));\r\n    assertEquals(Optional.of(securityManagerFactory), jobConfig.getSecurityManagerFactory());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getSecurityManagerFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSSPMatcherClass",
  "sourceCode" : "@Test\r\npublic void testGetSSPMatcherClass() {\r\n    String sspMatcherClass = \"my.ssp.Matcher.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SSP_MATCHER_CLASS, sspMatcherClass)));\r\n    assertEquals(Optional.of(sspMatcherClass), jobConfig.getSSPMatcherClass());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getSSPMatcherClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSSPMatcherConfigRegex",
  "sourceCode" : "@Test\r\npublic void testGetSSPMatcherConfigRegex() {\r\n    String sspMatcherConfigRegex = \"ssp-.*\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SSP_MATCHER_CONFIG_REGEX, sspMatcherConfigRegex)));\r\n    assertEquals(sspMatcherConfigRegex, jobConfig.getSSPMatcherConfigRegex());\r\n    try {\r\n        new JobConfig(new MapConfig()).getSSPMatcherConfigRegex();\r\n        fail(\"Expected a SamzaException\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSSPMatcherConfigRanges",
  "sourceCode" : "@Test\r\npublic void testGetSSPMatcherConfigRanges() {\r\n    String sspMatcherConfigRanges = \"1,2,3\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SSP_MATCHER_CONFIG_RANGES, sspMatcherConfigRanges)));\r\n    assertEquals(sspMatcherConfigRanges, jobConfig.getSSPMatcherConfigRanges());\r\n    try {\r\n        new JobConfig(new MapConfig()).getSSPMatcherConfigRanges();\r\n        fail(\"Expected a SamzaException\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSSPMatcherConfigJobFactoryRegex",
  "sourceCode" : "@Test\r\npublic void testGetSSPMatcherConfigJobFactoryRegex() {\r\n    String sspMatcherConfigJobFactoryRegex = \".*JobFactory\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SSP_MATCHER_CONFIG_JOB_FACTORY_REGEX, sspMatcherConfigJobFactoryRegex)));\r\n    assertEquals(sspMatcherConfigJobFactoryRegex, jobConfig.getSSPMatcherConfigJobFactoryRegex());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_SSP_MATCHER_CONFIG_JOB_FACTORY_REGEX, jobConfig.getSSPMatcherConfigJobFactoryRegex());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetThreadPoolSize",
  "sourceCode" : "@Test\r\npublic void testGetThreadPoolSize() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_CONTAINER_THREAD_POOL_SIZE, \"10\")));\r\n    assertEquals(10, jobConfig.getThreadPoolSize());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(0, jobConfig.getThreadPoolSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetDebounceTimeMs",
  "sourceCode" : "@Test\r\npublic void testGetDebounceTimeMs() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEBOUNCE_TIME_MS, Integer.toString(100))));\r\n    assertEquals(100, jobConfig.getDebounceTimeMs());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEBOUNCE_TIME_MS, Integer.toString(0))));\r\n    assertEquals(0, jobConfig.getDebounceTimeMs());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEBOUNCE_TIME_MS, Integer.toString(-1))));\r\n    assertEquals(-1, jobConfig.getDebounceTimeMs());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_DEBOUNCE_TIME_MS, jobConfig.getDebounceTimeMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetNonLoggedStorePath",
  "sourceCode" : "@Test\r\npublic void testGetNonLoggedStorePath() {\r\n    String nonLoggedStorePath = \"/path/to/non/logged/store\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_NON_LOGGED_STORE_BASE_DIR, nonLoggedStorePath)));\r\n    assertEquals(Optional.of(nonLoggedStorePath), jobConfig.getNonLoggedStorePath());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getNonLoggedStorePath());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetLoggedStorePath",
  "sourceCode" : "@Test\r\npublic void testGetLoggedStorePath() {\r\n    String loggedStorePath = \"/path/to/logged/store\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_LOGGED_STORE_BASE_DIR, loggedStorePath)));\r\n    assertEquals(Optional.of(loggedStorePath), jobConfig.getLoggedStorePath());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(Optional.empty(), jobConfig.getLoggedStorePath());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMetadataStoreFactory",
  "sourceCode" : "@Test\r\npublic void testGetMetadataStoreFactory() {\r\n    String metadataStoreFactory = \"my.metadata.store.Factory.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.METADATA_STORE_FACTORY, metadataStoreFactory)));\r\n    assertEquals(metadataStoreFactory, jobConfig.getMetadataStoreFactory());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(CoordinatorStreamMetadataStoreFactory.class.getName(), jobConfig.getMetadataStoreFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetDiagnosticsEnabled",
  "sourceCode" : "@Test\r\npublic void testGetDiagnosticsEnabled() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DIAGNOSTICS_ENABLED, \"true\")));\r\n    assertTrue(jobConfig.getDiagnosticsEnabled());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DIAGNOSTICS_ENABLED, \"false\")));\r\n    assertFalse(jobConfig.getDiagnosticsEnabled());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertFalse(jobConfig.getDiagnosticsEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetJMXEnabled",
  "sourceCode" : "@Test\r\npublic void testGetJMXEnabled() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_JMX_ENABLED, \"true\")));\r\n    assertTrue(jobConfig.getJMXEnabled());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_JMX_ENABLED, \"false\")));\r\n    assertFalse(jobConfig.getJMXEnabled());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertTrue(jobConfig.getJMXEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetSystemStreamPartitionMapperFactoryName",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamPartitionMapperFactoryName() {\r\n    String sspMapperFactory = \"my.ssp.mapper.Factory.class\";\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.SYSTEM_STREAM_PARTITION_MAPPER_FACTORY, sspMapperFactory)));\r\n    assertEquals(sspMapperFactory, jobConfig.getSystemStreamPartitionMapperFactoryName());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(HashSystemStreamPartitionMapperFactory.class.getName(), jobConfig.getSystemStreamPartitionMapperFactoryName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetStandbyTasksEnabled",
  "sourceCode" : "@Test\r\npublic void testGetStandbyTasksEnabled() {\r\n    // greater than 1 means enabled\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(100))));\r\n    assertTrue(jobConfig.getStandbyTasksEnabled());\r\n    // zero means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(0))));\r\n    assertFalse(jobConfig.getStandbyTasksEnabled());\r\n    // negative means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(-1))));\r\n    assertFalse(jobConfig.getStandbyTasksEnabled());\r\n    // not specified uses the default standby count, which means disabled\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertFalse(jobConfig.getStandbyTasksEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetStandbyTaskReplicationFactor",
  "sourceCode" : "@Test\r\npublic void testGetStandbyTaskReplicationFactor() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(100))));\r\n    assertEquals(100, jobConfig.getStandbyTaskReplicationFactor());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(0))));\r\n    assertEquals(0, jobConfig.getStandbyTaskReplicationFactor());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, Integer.toString(-1))));\r\n    assertEquals(-1, jobConfig.getStandbyTaskReplicationFactor());\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_STANDBY_TASKS_REPLICATION_FACTOR, jobConfig.getStandbyTaskReplicationFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetMetadataFile",
  "sourceCode" : "@Test\r\npublic void testGetMetadataFile() {\r\n    String execEnvContainerId = \"container-id\";\r\n    String containerMetadataDirectory = \"/tmp/samza/log/dir\";\r\n    String containerMetadataFileNameFromEnv = \"container-metadata-file.metadata\";\r\n    PowerMockito.mockStatic(System.class);\r\n    PowerMockito.when(System.getProperty(JobConfig.CONTAINER_METADATA_DIRECTORY_SYS_PROPERTY)).thenReturn(null);\r\n    PowerMockito.when(System.getenv(EnvironmentVariables.ENV_CONTAINER_METADATA_FILENAME)).thenReturn(null);\r\n    // samza.log.dir not specified\r\n    assertEquals(Optional.empty(), JobConfig.getMetadataFile(execEnvContainerId));\r\n    // provide value for samza.log.dir for remainder of tests\r\n    PowerMockito.when(System.getProperty(JobConfig.CONTAINER_METADATA_DIRECTORY_SYS_PROPERTY)).thenReturn(containerMetadataDirectory);\r\n    // CONTAINER_METADATA_FILENAME not specified, execEnvContainerId specified\r\n    assertEquals(Optional.of(new File(containerMetadataDirectory, String.format(JobConfig.CONTAINER_METADATA_FILENAME_FORMAT, execEnvContainerId))), JobConfig.getMetadataFile(execEnvContainerId));\r\n    // CONTAINER_METADATA_FILENAME not specified, execEnvContainerId not specified\r\n    assertEquals(Optional.empty(), JobConfig.getMetadataFile(null));\r\n    // CONTAINER_METADATA_FILENAME specified\r\n    PowerMockito.when(System.getenv(EnvironmentVariables.ENV_CONTAINER_METADATA_FILENAME)).thenReturn(containerMetadataFileNameFromEnv);\r\n    assertEquals(Optional.of(new File(containerMetadataDirectory, containerMetadataFileNameFromEnv)), JobConfig.getMetadataFile(execEnvContainerId));\r\n    // CONTAINER_METADATA_FILENAME invalid\r\n    PowerMockito.when(System.getenv(EnvironmentVariables.ENV_CONTAINER_METADATA_FILENAME)).thenReturn(\"file/with/directories/file.txt\");\r\n    try {\r\n        JobConfig.getMetadataFile(execEnvContainerId);\r\n    } catch (IllegalStateException e) {\r\n        // expected to throw exception for having directories in file name\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetCoordinatorStreamFactory",
  "sourceCode" : "@Test\r\npublic void testGetCoordinatorStreamFactory() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(\"test\", \"\")));\r\n    assertEquals(jobConfig.getCoordinatorStreamFactory(), JobConfig.DEFAULT_COORDINATOR_STREAM_CONFIG_FACTORY);\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.COORDINATOR_STREAM_FACTORY, \"specific_coordinator_stream\")));\r\n    assertEquals(jobConfig.getCoordinatorStreamFactory(), \"specific_coordinator_stream\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testAutosizingConfig",
  "sourceCode" : "@Test\r\npublic void testAutosizingConfig() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(\"job.autosizing.enabled\", \"true\");\r\n    config.put(\"job.container.count\", \"1\");\r\n    config.put(\"job.autosizing.container.count\", \"2\");\r\n    config.put(\"job.container.thread.pool.size\", \"1\");\r\n    config.put(\"job.autosizing.container.thread.pool.size\", \"3\");\r\n    config.put(\"job.autosizing.container.maxheap.mb\", \"500\");\r\n    config.put(\"cluster-manager.container.memory.mb\", \"500\");\r\n    config.put(\"job.autosizing.container.memory.mb\", \"900\");\r\n    config.put(\"cluster-manager.container.cpu.cores\", \"1\");\r\n    config.put(\"job.autosizing.container.cpu.cores\", \"2\");\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(config));\r\n    Assert.assertTrue(jobConfig.getAutosizingEnabled());\r\n    Assert.assertEquals(2, jobConfig.getContainerCount());\r\n    Assert.assertEquals(3, jobConfig.getThreadPoolSize());\r\n    ClusterManagerConfig clusterManagerConfig = new ClusterManagerConfig(new MapConfig(config));\r\n    Assert.assertEquals(900, clusterManagerConfig.getContainerMemoryMb());\r\n    Assert.assertEquals(2, clusterManagerConfig.getNumCores());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetContainerHeartbeatMonitorEnabled",
  "sourceCode" : "@Test\r\npublic void testGetContainerHeartbeatMonitorEnabled() {\r\n    assertTrue(new JobConfig(new MapConfig()).getContainerHeartbeatMonitorEnabled());\r\n    assertTrue(new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.CONTAINER_HEARTBEAT_MONITOR_ENABLED, \"true\"))).getContainerHeartbeatMonitorEnabled());\r\n    assertFalse(new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.CONTAINER_HEARTBEAT_MONITOR_ENABLED, \"false\"))).getContainerHeartbeatMonitorEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetElastictyEnabled",
  "sourceCode" : "@Test\r\npublic void testGetElastictyEnabled() {\r\n    // greater than 1 means enabled\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(2))));\r\n    assertTrue(jobConfig.getElasticityEnabled());\r\n    // one means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(1))));\r\n    assertFalse(jobConfig.getElasticityEnabled());\r\n    // zero means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(0))));\r\n    boolean exceptionCaught = false;\r\n    try {\r\n        jobConfig.getElasticityEnabled();\r\n    } catch (ConfigException e) {\r\n        exceptionCaught = true;\r\n    }\r\n    assertTrue(exceptionCaught);\r\n    // negative means disabled\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(-1))));\r\n    exceptionCaught = false;\r\n    try {\r\n        jobConfig.getElasticityEnabled();\r\n    } catch (ConfigException e) {\r\n        exceptionCaught = true;\r\n    }\r\n    assertTrue(exceptionCaught);\r\n    // not specified uses the default standby count, which means disabled\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertFalse(jobConfig.getElasticityEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetElasticityFactor",
  "sourceCode" : "@Test\r\npublic void testGetElasticityFactor() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(2))));\r\n    assertEquals(2, jobConfig.getElasticityFactor());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(1))));\r\n    assertEquals(1, jobConfig.getElasticityFactor());\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(0))));\r\n    boolean exceptionCaught = false;\r\n    try {\r\n        jobConfig.getElasticityFactor();\r\n    } catch (ConfigException e) {\r\n        exceptionCaught = true;\r\n    }\r\n    assertTrue(exceptionCaught);\r\n    jobConfig = new JobConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_ELASTICITY_FACTOR, Integer.toString(-1))));\r\n    exceptionCaught = false;\r\n    try {\r\n        jobConfig.getElasticityFactor();\r\n    } catch (ConfigException e) {\r\n        exceptionCaught = true;\r\n    }\r\n    assertTrue(exceptionCaught);\r\n    jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_JOB_ELASTICITY_FACTOR, jobConfig.getElasticityFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobConfig.java",
  "methodName" : "testGetCoordinatorExecuteCommand",
  "sourceCode" : "@Test\r\npublic void testGetCoordinatorExecuteCommand() {\r\n    JobConfig jobConfig = new JobConfig(new MapConfig());\r\n    assertEquals(JobConfig.DEFAULT_COORDINATOR_EXECUTE_COMMAND, jobConfig.getCoordinatorExecuteCommand());\r\n    String myJcCmd = \"bin/run-my-jc.sh\";\r\n    jobConfig = new JobConfig(new MapConfig(Collections.singletonMap(JobConfig.COORDINATOR_EXECUTE_COMMAND, myJcCmd)));\r\n    assertEquals(myJcCmd, jobConfig.getCoordinatorExecuteCommand());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobCoordinatorConfig.java",
  "methodName" : "getJobCoordinatorFactoryClassName",
  "sourceCode" : "@Test\r\npublic void getJobCoordinatorFactoryClassName() {\r\n    assertEquals(ZkJobCoordinatorFactory.class.getName(), new JobCoordinatorConfig(new MapConfig()).getJobCoordinatorFactoryClassName());\r\n    JobCoordinatorConfig jobCoordinatorConfig = new JobCoordinatorConfig(new MapConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"\")));\r\n    assertEquals(ZkJobCoordinatorFactory.class.getName(), jobCoordinatorConfig.getJobCoordinatorFactoryClassName());\r\n    jobCoordinatorConfig = new JobCoordinatorConfig(new MapConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.custom.MyJobCoordinatorFactory\")));\r\n    assertEquals(\"org.custom.MyJobCoordinatorFactory\", jobCoordinatorConfig.getJobCoordinatorFactoryClassName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobCoordinatorConfig.java",
  "methodName" : "getOptionalJobCoordinatorFactoryClassName",
  "sourceCode" : "@Test\r\npublic void getOptionalJobCoordinatorFactoryClassName() {\r\n    assertFalse(new JobCoordinatorConfig(new MapConfig()).getOptionalJobCoordinatorFactoryClassName().isPresent());\r\n    JobCoordinatorConfig jobCoordinatorConfig = new JobCoordinatorConfig(new MapConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"\")));\r\n    assertFalse(jobCoordinatorConfig.getOptionalJobCoordinatorFactoryClassName().isPresent());\r\n    jobCoordinatorConfig = new JobCoordinatorConfig(new MapConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.custom.MyJobCoordinatorFactory\")));\r\n    assertEquals(Optional.of(\"org.custom.MyJobCoordinatorFactory\"), jobCoordinatorConfig.getOptionalJobCoordinatorFactoryClassName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestJobCoordinatorConfig.java",
  "methodName" : "testGetJobRestartSignalFactory",
  "sourceCode" : "@Test\r\npublic void testGetJobRestartSignalFactory() {\r\n    assertEquals(NoOpJobRestartSignalFactory.class.getName(), new JobCoordinatorConfig(new MapConfig()).getJobRestartSignalFactory());\r\n    JobCoordinatorConfig jobCoordinatorConfig = new JobCoordinatorConfig(new MapConfig(ImmutableMap.of(JobCoordinatorConfig.JOB_RESTART_SIGNAL_FACTORY, \"org.apache.samza.MyJobRestartSignalFactory\")));\r\n    assertEquals(\"org.apache.samza.MyJobRestartSignalFactory\", jobCoordinatorConfig.getJobRestartSignalFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsFactoryClass",
  "sourceCode" : "@Test\r\npublic void testGetMetricsFactoryClass() {\r\n    String metricsReporterName = \"metricReporterName\";\r\n    String metricsReporterValue = \"metrics.reporter.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(MetricsConfig.METRICS_REPORTER_FACTORY, metricsReporterName), metricsReporterValue));\r\n    assertEquals(Optional.of(metricsReporterValue), new MetricsConfig(config).getMetricsFactoryClass(metricsReporterName));\r\n    assertFalse(metricsReporterValue, new MetricsConfig(new MapConfig()).getMetricsFactoryClass(\"someName\").isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsSnapshotReporterStream",
  "sourceCode" : "@Test\r\npublic void testGetMetricsSnapshotReporterStream() {\r\n    String metricsReporterName = \"metricReporterName\";\r\n    String value = \"reporter-stream\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(MetricsConfig.METRICS_SNAPSHOT_REPORTER_STREAM, metricsReporterName), value));\r\n    assertEquals(Optional.of(value), new MetricsConfig(config).getMetricsSnapshotReporterStream(metricsReporterName));\r\n    assertFalse(value, new MetricsConfig(new MapConfig()).getMetricsSnapshotReporterStream(\"someName\").isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsSnapshotReporterInterval",
  "sourceCode" : "@Test\r\npublic void testGetMetricsSnapshotReporterInterval() {\r\n    String metricsReporterName = \"metricReporterName\";\r\n    String value = \"10\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(MetricsConfig.METRICS_SNAPSHOT_REPORTER_INTERVAL, metricsReporterName), value));\r\n    assertEquals(10, new MetricsConfig(config).getMetricsSnapshotReporterInterval(metricsReporterName));\r\n    assertEquals(MetricsConfig.DEFAULT_METRICS_SNAPSHOT_REPORTER_INTERVAL, new MetricsConfig(new MapConfig()).getMetricsSnapshotReporterInterval(\"someName\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsSnapshotReporterBlacklist",
  "sourceCode" : "@Test\r\npublic void testGetMetricsSnapshotReporterBlacklist() {\r\n    String metricsReporterName = \"metricReporterName\";\r\n    String value = \"metric0|metric1\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(MetricsConfig.METRICS_SNAPSHOT_REPORTER_BLACKLIST, metricsReporterName), value));\r\n    assertEquals(Optional.of(value), new MetricsConfig(config).getMetricsSnapshotReporterBlacklist(metricsReporterName));\r\n    assertFalse(value, new MetricsConfig(new MapConfig()).getMetricsSnapshotReporterBlacklist(\"someName\").isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricReporterNames",
  "sourceCode" : "@Test\r\npublic void testGetMetricReporterNames() {\r\n    Config config = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_REPORTERS, \"reporter0.class, reporter1.class, reporter2.class \"));\r\n    assertEquals(ImmutableList.of(\"reporter0.class\", \"reporter1.class\", \"reporter2.class\"), new MetricsConfig(config).getMetricReporterNames());\r\n    Config configEmptyValue = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_REPORTERS, \"\"));\r\n    assertEquals(Collections.emptyList(), new MetricsConfig(configEmptyValue).getMetricReporterNames());\r\n    assertEquals(Collections.emptyList(), new MetricsConfig(new MapConfig()).getMetricReporterNames());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsTimerEnabled",
  "sourceCode" : "@Test\r\npublic void testGetMetricsTimerEnabled() {\r\n    Config config = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_TIMER_ENABLED, \"true\"));\r\n    assertTrue(new MetricsConfig(config).getMetricsTimerEnabled());\r\n    config = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_TIMER_ENABLED, \"false\"));\r\n    assertFalse(new MetricsConfig(config).getMetricsTimerEnabled());\r\n    assertTrue(new MetricsConfig(new MapConfig()).getMetricsTimerEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestMetricsConfig.java",
  "methodName" : "testGetMetricsTimerDebugEnabled",
  "sourceCode" : "@Test\r\npublic void testGetMetricsTimerDebugEnabled() {\r\n    Config config = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_TIMER_DEBUG_ENABLED, \"true\"));\r\n    assertTrue(new MetricsConfig(config).getMetricsTimerDebugEnabled());\r\n    config = new MapConfig(ImmutableMap.of(MetricsConfig.METRICS_TIMER_DEBUG_ENABLED, \"false\"));\r\n    assertFalse(new MetricsConfig(config).getMetricsTimerDebugEnabled());\r\n    assertFalse(new MetricsConfig(new MapConfig()).getMetricsTimerDebugEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestRunLoopConfig.java",
  "methodName" : "testWatermarkCallbackTimeoutDefaultsToTaskCallbackTimeout",
  "sourceCode" : "@Test\r\npublic void testWatermarkCallbackTimeoutDefaultsToTaskCallbackTimeout() {\r\n    long taskCallbackTimeout = 10L;\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.CALLBACK_TIMEOUT_MS, Long.toString(taskCallbackTimeout)));\r\n    RunLoopConfig runLoopConfig = new RunLoopConfig(config);\r\n    assertEquals(\"Watermark callback timeout should default to task callback timeout\", taskCallbackTimeout, runLoopConfig.getWatermarkCallbackTimeoutMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestRunLoopConfig.java",
  "methodName" : "testWatermarkCallbackTimeout",
  "sourceCode" : "@Test\r\npublic void testWatermarkCallbackTimeout() {\r\n    long taskCallbackTimeout = 10L;\r\n    long watermarkCallbackTimeout = 20L;\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.CALLBACK_TIMEOUT_MS, Long.toString(taskCallbackTimeout), TaskConfig.WATERMARK_CALLBACK_TIMEOUT_MS, Long.toString(watermarkCallbackTimeout)));\r\n    RunLoopConfig runLoopConfig = new RunLoopConfig(config);\r\n    assertEquals(\"Mismatch in watermark callback timeout\", watermarkCallbackTimeout, runLoopConfig.getWatermarkCallbackTimeoutMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSerializerConfig.java",
  "methodName" : "testGetPredefinedSerdeFactoryName",
  "sourceCode" : "@Test\r\npublic void testGetPredefinedSerdeFactoryName() {\r\n    assertEquals(ByteSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"byte\"));\r\n    assertEquals(ByteBufferSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"bytebuffer\"));\r\n    assertEquals(IntegerSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"integer\"));\r\n    assertEquals(JsonSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"json\"));\r\n    assertEquals(LongSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"long\"));\r\n    assertEquals(SerializableSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"serializable\"));\r\n    assertEquals(StringSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"string\"));\r\n    assertEquals(DoubleSerdeFactory.class.getName(), SerializerConfig.getPredefinedSerdeFactoryName(\"double\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSerializerConfig.java",
  "methodName" : "testGetSerdeFactoryNameUnknown",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetSerdeFactoryNameUnknown() {\r\n    SerializerConfig.getPredefinedSerdeFactoryName(\"otherName\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSerializerConfig.java",
  "methodName" : "testGetSerdeFactoryClass",
  "sourceCode" : "@Test\r\npublic void testGetSerdeFactoryClass() {\r\n    String serdeClassName = \"my.class.serde.name\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(SerializerConfig.SERDE_FACTORY_CLASS, SERDE_NAME), serdeClassName));\r\n    assertEquals(serdeClassName, new SerializerConfig(config).getSerdeFactoryClass(SERDE_NAME).get());\r\n    assertFalse(new SerializerConfig(config).getSerdeFactoryClass(\"otherSerdeName\").isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSerializerConfig.java",
  "methodName" : "testGetSerdeNames",
  "sourceCode" : "@Test\r\npublic void testGetSerdeNames() {\r\n    String otherSerdeName = \"otherSerdeName\";\r\n    String serdeClassName = \"my.class.serde.name\";\r\n    Config config = new MapConfig(ImmutableMap.of(String.format(SerializerConfig.SERDE_FACTORY_CLASS, SERDE_NAME), serdeClassName, String.format(SerializerConfig.SERDE_FACTORY_CLASS, otherSerdeName), serdeClassName));\r\n    List<String> serdeNames = new SerializerConfig(config).getSerdeNames();\r\n    ImmutableSet<String> expectedSerdeNames = ImmutableSet.of(SERDE_NAME, otherSerdeName);\r\n    // ensures no duplicates\r\n    assertEquals(expectedSerdeNames.size(), serdeNames.size());\r\n    // can't check ordering in a stable way since values come from the key set of a map, so just check entries\r\n    assertEquals(expectedSerdeNames, ImmutableSet.copyOf(serdeNames));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetCommand",
  "sourceCode" : "@Test\r\npublic void testGetCommand() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig());\r\n    assertEquals(\"bin/run-container.sh\", shellCommandConfig.getCommand());\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(ShellCommandConfig.COMMAND_SHELL_EXECUTE, \"my-run-container.sh\")));\r\n    assertEquals(\"my-run-container.sh\", shellCommandConfig.getCommand());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetTaskOptsAutosizingDisabled",
  "sourceCode" : "@Test\r\npublic void testGetTaskOptsAutosizingDisabled() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"false\")));\r\n    assertEquals(Optional.empty(), shellCommandConfig.getTaskOpts());\r\n    String taskOpts = \"-Dproperty=value\";\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(ShellCommandConfig.TASK_JVM_OPTS, taskOpts, JobConfig.JOB_AUTOSIZING_ENABLED, \"false\")));\r\n    assertEquals(Optional.of(taskOpts), shellCommandConfig.getTaskOpts());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetTaskOptsAutosizingEnabled",
  "sourceCode" : "@Test\r\npublic void testGetTaskOptsAutosizingEnabled() {\r\n    // opts not set, autosizing max heap not set\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\")));\r\n    assertEquals(Optional.empty(), shellCommandConfig.getTaskOpts());\r\n    // opts set, autosizing max heap not set\r\n    String taskOpts = \"-Dproperty=value\";\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(ShellCommandConfig.TASK_JVM_OPTS, taskOpts, JobConfig.JOB_AUTOSIZING_ENABLED, \"true\")));\r\n    assertEquals(Optional.of(taskOpts), shellCommandConfig.getTaskOpts());\r\n    // opts not set, autosizing max heap set\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\", JobConfig.JOB_AUTOSIZING_CONTAINER_MAX_HEAP_MB, \"1024\")));\r\n    assertEquals(Optional.of(\"-Xmx1024m\"), shellCommandConfig.getTaskOpts());\r\n    // opts set with Xmx, autosizing max heap set\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\", JobConfig.JOB_AUTOSIZING_CONTAINER_MAX_HEAP_MB, \"1024\", \"task.opts\", \"-Xmx10m -Dproperty=value\")));\r\n    assertEquals(Optional.of(\"-Xmx1024m -Dproperty=value\"), shellCommandConfig.getTaskOpts());\r\n    // opts set without -Xmx, autosizing max heap set\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\", JobConfig.JOB_AUTOSIZING_CONTAINER_MAX_HEAP_MB, \"1024\", \"task.opts\", \"-Dproperty=value\")));\r\n    assertEquals(Optional.of(\"-Dproperty=value -Xmx1024m\"), shellCommandConfig.getTaskOpts());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetWorkerOptsAutosizingDisabled",
  "sourceCode" : "@Test\r\npublic void testGetWorkerOptsAutosizingDisabled() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_WORKER_MAX_HEAP_MB, \"1024\", \"worker.opts\", \"-Xmx10m -Dproperty=value\")));\r\n    String workerOpts = shellCommandConfig.getWorkerOpts().orElse(null);\r\n    String expectedOpts = \"-Xmx10m -Dproperty=value\";\r\n    assertNotNull(workerOpts);\r\n    assertEquals(expectedOpts, workerOpts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetWorkerOptsAutosizingEnabled",
  "sourceCode" : "@Test\r\npublic void testGetWorkerOptsAutosizingEnabled() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\", JobConfig.JOB_AUTOSIZING_WORKER_MAX_HEAP_MB, \"1024\", \"worker.opts\", \"-Xmx10m -Dproperty=value\")));\r\n    String workerOpts = shellCommandConfig.getWorkerOpts().orElse(null);\r\n    String expectedOpts = \"-Xmx1024m -Dproperty=value\";\r\n    assertNotNull(workerOpts);\r\n    assertEquals(expectedOpts, workerOpts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetFinalJvmOptionsAutosizingDisabled",
  "sourceCode" : "@Test\r\npublic void testGetFinalJvmOptionsAutosizingDisabled() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"false\")));\r\n    String jvmOptions = \"\";\r\n    String expectedJvmOptions = \"\";\r\n    // no override passed\r\n    assertEquals(expectedJvmOptions, shellCommandConfig.getFinalJvmOptions(jvmOptions, \"\"));\r\n    // ignore override since autosizing is disabled\r\n    assertEquals(expectedJvmOptions, shellCommandConfig.getFinalJvmOptions(jvmOptions, \"2048\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetFinalJvmOptionsAutosizingEnabled",
  "sourceCode" : "@Test\r\npublic void testGetFinalJvmOptionsAutosizingEnabled() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_AUTOSIZING_ENABLED, \"true\")));\r\n    String jvmOptions = \"-Xmx1024m\";\r\n    String expectedJvmOptions = \"-Xmx1024m\";\r\n    assertEquals(expectedJvmOptions, shellCommandConfig.getFinalJvmOptions(jvmOptions, \"\"));\r\n    // override should take effect with autosizing enabled\r\n    expectedJvmOptions = \"-Xmx2048m\";\r\n    assertEquals(expectedJvmOptions, shellCommandConfig.getFinalJvmOptions(jvmOptions, \"2048\"));\r\n    // override should take effect even if xmx is not set\r\n    jvmOptions = \"-Dproperty=value\";\r\n    expectedJvmOptions = \"-Dproperty=value -Xmx2048m\";\r\n    assertEquals(expectedJvmOptions, shellCommandConfig.getFinalJvmOptions(jvmOptions, \"2048\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetJavaHome",
  "sourceCode" : "@Test\r\npublic void testGetJavaHome() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig());\r\n    assertFalse(shellCommandConfig.getJavaHome().isPresent());\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(ShellCommandConfig.TASK_JAVA_HOME, \"/location/java\")));\r\n    assertEquals(Optional.of(\"/location/java\"), shellCommandConfig.getJavaHome());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestShellCommandConfig.java",
  "methodName" : "testGetAdditionalClasspathDir",
  "sourceCode" : "@Test\r\npublic void testGetAdditionalClasspathDir() {\r\n    ShellCommandConfig shellCommandConfig = new ShellCommandConfig(new MapConfig());\r\n    assertFalse(shellCommandConfig.getAdditionalClasspathDir().isPresent());\r\n    shellCommandConfig = new ShellCommandConfig(new MapConfig(ImmutableMap.of(ShellCommandConfig.ADDITIONAL_CLASSPATH_DIR, \"/location/classpath\")));\r\n    assertEquals(Optional.of(\"/location/classpath\"), shellCommandConfig.getAdditionalClasspathDir());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStoreNames",
  "sourceCode" : "@Test\r\npublic void testGetStoreNames() {\r\n    // empty config, so no stores\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig()).getStoreNames());\r\n    Set<String> expectedStoreNames = ImmutableSet.of(STORE_NAME0, STORE_NAME1);\r\n    // has stores\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(StorageConfig.FACTORY, STORE_NAME1), \"store1.factory.class\")));\r\n    List<String> actual = storageConfig.getStoreNames();\r\n    // ordering shouldn't matter\r\n    assertEquals(2, actual.size());\r\n    assertEquals(expectedStoreNames, ImmutableSet.copyOf(actual));\r\n    //has side input stores\r\n    StorageConfig config = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(StorageConfig.SIDE_INPUTS_PROCESSOR_FACTORY, STORE_NAME1), \"store1.factory.class\")));\r\n    actual = config.getStoreNames();\r\n    assertEquals(2, actual.size());\r\n    assertEquals(expectedStoreNames, ImmutableSet.copyOf(actual));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStoreNamesDoesNotReturnDuplicatesForSideInputs",
  "sourceCode" : "@Test\r\npublic void testGetStoreNamesDoesNotReturnDuplicatesForSideInputs() {\r\n    // empty config, so no stores\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig()).getStoreNames());\r\n    Set<String> expectedStoreNames = ImmutableSet.of(STORE_NAME0, STORE_NAME1);\r\n    // has stores\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(StorageConfig.FACTORY, STORE_NAME1), \"store1.factory.class\", String.format(StorageConfig.SIDE_INPUTS_PROCESSOR_FACTORY, STORE_NAME1), \"store1.side.inputs.processor\")));\r\n    List<String> actual = storageConfig.getStoreNames();\r\n    // ordering shouldn't matter\r\n    assertEquals(2, actual.size());\r\n    assertEquals(expectedStoreNames, ImmutableSet.copyOf(actual));\r\n    //has side input stores\r\n    StorageConfig config = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(StorageConfig.SIDE_INPUTS_PROCESSOR_FACTORY, STORE_NAME1), \"store1.factory.class\")));\r\n    actual = storageConfig.getStoreNames();\r\n    assertEquals(2, actual.size());\r\n    assertEquals(expectedStoreNames, ImmutableSet.copyOf(actual));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStoreNamesIgnoreStateRestoreFactory",
  "sourceCode" : "/**\r\n * Test verifies that the {@link StorageConfig#STORE_RESTORE_FACTORIES} which matches pattern for store.%s.factory\r\n * is not picked up as in store names list\r\n */\r\n@Test\r\npublic void testGetStoreNamesIgnoreStateRestoreFactory() {\r\n    // empty config, so no stores\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig()).getStoreNames());\r\n    Set<String> expectedStoreNames = ImmutableSet.of(STORE_NAME0, STORE_NAME1);\r\n    // has stores\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(StorageConfig.FACTORY, STORE_NAME1), \"store1.factory.class\", STORE_RESTORE_FACTORIES, \"org.apache.class\")));\r\n    List<String> actual = storageConfig.getStoreNames();\r\n    // ordering shouldn't matter\r\n    assertEquals(2, actual.size());\r\n    assertEquals(expectedStoreNames, ImmutableSet.copyOf(actual));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetChangelogStream",
  "sourceCode" : "@Test\r\npublic void testGetChangelogStream() {\r\n    // empty config, so no changelog stream\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getChangelogStream(STORE_NAME0));\r\n    // store has empty string for changelog stream\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_STREAM, STORE_NAME0), \"\")));\r\n    assertEquals(Optional.empty(), storageConfig.getChangelogStream(STORE_NAME0));\r\n    assertEquals(Collections.emptyMap(), storageConfig.getStoreChangelogs());\r\n    // store has full changelog system-stream defined\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_STREAM, STORE_NAME0), \"changelog-system.changelog-stream0\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(Optional.of(\"changelog-system.changelog-stream0\"), storageConfig.getChangelogStream(STORE_NAME0));\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"changelog-system\", \"changelog-stream0\")), storageConfig.getStoreChangelogs());\r\n    // store has changelog stream defined, but system comes from job.changelog.system\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_STREAM, STORE_NAME0), \"changelog-stream0\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\")));\r\n    assertEquals(Optional.of(\"changelog-system.changelog-stream0\"), storageConfig.getChangelogStream(STORE_NAME0));\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"changelog-system\", \"changelog-stream0\")), storageConfig.getStoreChangelogs());\r\n    // batch mode: create unique stream name\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_STREAM, STORE_NAME0), \"changelog-system.changelog-stream0\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name().toLowerCase(), ApplicationConfig.APP_RUN_ID, \"run-id\")));\r\n    assertEquals(Optional.of(\"changelog-system.changelog-stream0-run-id\"), storageConfig.getChangelogStream(STORE_NAME0));\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"changelog-system\", \"changelog-stream0-run-id\")), storageConfig.getStoreChangelogs());\r\n    // job has no changelog stream defined\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\", JobConfig.JOB_DEFAULT_SYSTEM, \"should-not-be-used\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(Optional.empty(), storageConfig.getChangelogStream(STORE_NAME0));\r\n    assertEquals(Collections.emptyMap(), storageConfig.getStoreChangelogs());\r\n    // job.changelog.system takes precedence over job.default.system when changelog is specified as just streamName\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\", JobConfig.JOB_DEFAULT_SYSTEM, \"should-not-be-used\", String.format(CHANGELOG_STREAM, STORE_NAME0), \"streamName\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(\"changelog-system.streamName\", storageConfig.getChangelogStream(STORE_NAME0).get());\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"changelog-system\", \"streamName\")), storageConfig.getStoreChangelogs());\r\n    // job.changelog.system takes precedence over job.default.system when changelog is specified as {systemName}.{streamName}\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\", JobConfig.JOB_DEFAULT_SYSTEM, \"should-not-be-used\", String.format(CHANGELOG_STREAM, STORE_NAME0), \"changelog-system.streamName\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(\"changelog-system.streamName\", storageConfig.getChangelogStream(STORE_NAME0).get());\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"changelog-system\", \"streamName\")), storageConfig.getStoreChangelogs());\r\n    // systemName specified using stores.{storeName}.changelog = {systemName}.{streamName} should take precedence even\r\n    // when job.changelog.system and job.default.system are specified\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(StorageConfig.CHANGELOG_SYSTEM, \"default-changelog-system\", JobConfig.JOB_DEFAULT_SYSTEM, \"default-system\", String.format(CHANGELOG_STREAM, STORE_NAME0), \"nondefault-changelog-system.streamName\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(\"nondefault-changelog-system.streamName\", storageConfig.getChangelogStream(STORE_NAME0).get());\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"nondefault-changelog-system\", \"streamName\")), storageConfig.getStoreChangelogs());\r\n    // fall back to job.default.system if job.changelog.system is not specified\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, \"default-system\", String.format(CHANGELOG_STREAM, STORE_NAME0), \"streamName\", String.format(FACTORY, STORE_NAME0), \"store0.factory.class\")));\r\n    assertEquals(\"default-system.streamName\", storageConfig.getChangelogStream(STORE_NAME0).get());\r\n    assertEquals(ImmutableMap.of(STORE_NAME0, new SystemStream(\"default-system\", \"streamName\")), storageConfig.getStoreChangelogs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetChangelogStreamMissingSystem",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetChangelogStreamMissingSystem() {\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_STREAM, STORE_NAME0), \"changelog-stream0\")));\r\n    storageConfig.getChangelogStream(STORE_NAME0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetBackupManagerFactories",
  "sourceCode" : "@Test\r\npublic void testGetBackupManagerFactories() {\r\n    String factory1 = \"factory1\";\r\n    String factory2 = \"factory2\";\r\n    String factory3 = \"factory3\";\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(STORE_BACKUP_FACTORIES, STORE_NAME0), factory1 + \",\" + factory2, String.format(STORE_BACKUP_FACTORIES, STORE_NAME1), factory1, String.format(STORE_BACKUP_FACTORIES, STORE_NAME2), factory3, // store_name3 should use DEFAULT_STATE_BACKEND_FACTORY due to changelog presence\r\n    String.format(CHANGELOG_STREAM, STORE_NAME3), \"nondefault-changelog-system.streamName\"), ImmutableMap.of(String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(FACTORY, STORE_NAME1), \"store1.factory.class\", String.format(FACTORY, STORE_NAME2), \"store2.factory.class\", String.format(FACTORY, STORE_NAME3), \"store3.factory.class\", // this store should have no backend factory configured\r\n    String.format(FACTORY, \"noFactoryStore\"), \"noFactory.factory.class\")));\r\n    Set<String> factories = storageConfig.getBackupFactories();\r\n    assertTrue(factories.contains(factory1));\r\n    assertTrue(factories.contains(factory2));\r\n    assertTrue(factories.contains(factory3));\r\n    assertTrue(factories.contains(KAFKA_STATE_BACKEND_FACTORY));\r\n    assertEquals(4, factories.size());\r\n    assertEquals(ImmutableList.of(factory1, factory2), storageConfig.getStoreBackupFactories(STORE_NAME0));\r\n    assertEquals(ImmutableList.of(factory1), storageConfig.getStoreBackupFactories(STORE_NAME1));\r\n    assertEquals(ImmutableList.of(factory3), storageConfig.getStoreBackupFactories(STORE_NAME2));\r\n    assertEquals(DEFAULT_BACKUP_FACTORIES, storageConfig.getStoreBackupFactories(STORE_NAME3));\r\n    assertTrue(storageConfig.getStoreBackupFactories(\"emptyStore\").isEmpty());\r\n    assertTrue(storageConfig.getStoreBackupFactories(\"noFactoryStore\").isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStoreToBackup",
  "sourceCode" : "@Test\r\npublic void testGetStoreToBackup() {\r\n    String targetFactory = \"target.class\";\r\n    StorageConfig config = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.STORE_BACKUP_FACTORIES, STORE_NAME0), targetFactory, String.format(StorageConfig.STORE_BACKUP_FACTORIES, STORE_NAME1), targetFactory + \",\" + KAFKA_STATE_BACKEND_FACTORY, String.format(StorageConfig.STORE_BACKUP_FACTORIES, STORE_NAME2), KAFKA_STATE_BACKEND_FACTORY), ImmutableMap.of(String.format(FACTORY, STORE_NAME0), \"store0.factory.class\", String.format(FACTORY, STORE_NAME1), \"store1.factory.class\", String.format(FACTORY, STORE_NAME2), \"store2.factory.class\", String.format(FACTORY, STORE_NAME3), \"store3.factory.class\", String.format(CHANGELOG_STREAM, STORE_NAME3), \"nondefault-changelog-system.streamName\")));\r\n    List<String> targetStoreNames = config.getStoresWithBackupFactory(targetFactory);\r\n    List<String> defaultStoreNames = config.getStoresWithBackupFactory(KAFKA_STATE_BACKEND_FACTORY);\r\n    assertTrue(targetStoreNames.containsAll(ImmutableList.of(STORE_NAME0, STORE_NAME1)));\r\n    assertEquals(2, targetStoreNames.size());\r\n    assertTrue(defaultStoreNames.containsAll(ImmutableList.of(STORE_NAME2, STORE_NAME1, STORE_NAME3)));\r\n    assertEquals(3, defaultStoreNames.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetAccessLogEnabled",
  "sourceCode" : "@Test\r\npublic void testGetAccessLogEnabled() {\r\n    // empty config, access log disabled\r\n    assertFalse(new StorageConfig(new MapConfig()).getAccessLogEnabled(STORE_NAME0));\r\n    assertFalse(new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.ACCESSLOG_ENABLED, STORE_NAME0), \"false\"))).getAccessLogEnabled(STORE_NAME0));\r\n    assertTrue(new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.ACCESSLOG_ENABLED, STORE_NAME0), \"true\"))).getAccessLogEnabled(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetAccessLogStream",
  "sourceCode" : "@Test\r\npublic void testGetAccessLogStream() {\r\n    String changelogStream = \"changelog-stream\";\r\n    assertEquals(changelogStream + \"-\" + StorageConfig.ACCESSLOG_STREAM_SUFFIX, new StorageConfig(new MapConfig()).getAccessLogStream(changelogStream));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetAccessLogSamplingRatio",
  "sourceCode" : "@Test\r\npublic void testGetAccessLogSamplingRatio() {\r\n    // empty config, return default sampling ratio\r\n    assertEquals(StorageConfig.DEFAULT_ACCESSLOG_SAMPLING_RATIO, new StorageConfig(new MapConfig()).getAccessLogSamplingRatio(STORE_NAME0));\r\n    assertEquals(40, new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.ACCESSLOG_SAMPLING_RATIO, STORE_NAME0), \"40\"))).getAccessLogSamplingRatio(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStorageFactoryClassName",
  "sourceCode" : "@Test\r\npublic void testGetStorageFactoryClassName() {\r\n    // empty config, so no factory\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getStorageFactoryClassName(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"my.factory.class\")));\r\n    assertEquals(Optional.of(\"my.factory.class\"), storageConfig.getStorageFactoryClassName(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStorageKeySerde",
  "sourceCode" : "@Test\r\npublic void testGetStorageKeySerde() {\r\n    // empty config, so no key serde\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getStorageKeySerde(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.KEY_SERDE, STORE_NAME0), \"my.key.serde.class\")));\r\n    assertEquals(Optional.of(\"my.key.serde.class\"), storageConfig.getStorageKeySerde(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetStorageMsgSerde",
  "sourceCode" : "@Test\r\npublic void testGetStorageMsgSerde() {\r\n    // empty config, so no msg serde\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getStorageMsgSerde(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.MSG_SERDE, STORE_NAME0), \"my.msg.serde.class\")));\r\n    assertEquals(Optional.of(\"my.msg.serde.class\"), storageConfig.getStorageMsgSerde(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetSideInputs",
  "sourceCode" : "@Test\r\npublic void testGetSideInputs() {\r\n    // empty config, so no system\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig()).getSideInputs(STORE_NAME0));\r\n    // single side input\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.SIDE_INPUTS, STORE_NAME0), \"side-input\")));\r\n    assertEquals(Collections.singletonList(\"side-input\"), storageConfig.getSideInputs(STORE_NAME0));\r\n    // multiple side inputs\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.SIDE_INPUTS, STORE_NAME0), \"side-input0,side-input1\")));\r\n    assertEquals(ImmutableList.of(\"side-input0\", \"side-input1\"), storageConfig.getSideInputs(STORE_NAME0));\r\n    // ignore whitespace\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.SIDE_INPUTS, STORE_NAME0), \", side-input0 ,,side-input1,\")));\r\n    assertEquals(ImmutableList.of(\"side-input0\", \"side-input1\"), storageConfig.getSideInputs(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetSideInputsProcessorFactory",
  "sourceCode" : "@Test\r\npublic void testGetSideInputsProcessorFactory() {\r\n    // empty config, so no factory\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getSideInputsProcessorFactory(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.SIDE_INPUTS_PROCESSOR_FACTORY, STORE_NAME0), \"my.side.inputs.factory.class\")));\r\n    assertEquals(Optional.of(\"my.side.inputs.factory.class\"), storageConfig.getSideInputsProcessorFactory(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetSideInputsProcessorSerializedInstance",
  "sourceCode" : "@Test\r\npublic void testGetSideInputsProcessorSerializedInstance() {\r\n    // empty config, so no factory\r\n    assertEquals(Optional.empty(), new StorageConfig(new MapConfig()).getSideInputsProcessorSerializedInstance(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.SIDE_INPUTS_PROCESSOR_SERIALIZED_INSTANCE, STORE_NAME0), \"serialized_instance\")));\r\n    assertEquals(Optional.of(\"serialized_instance\"), storageConfig.getSideInputsProcessorSerializedInstance(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetChangeLogDeleteRetentionInMs",
  "sourceCode" : "@Test\r\npublic void testGetChangeLogDeleteRetentionInMs() {\r\n    // empty config, return default sampling ratio\r\n    assertEquals(StorageConfig.DEFAULT_CHANGELOG_DELETE_RETENTION_MS, new StorageConfig(new MapConfig()).getChangeLogDeleteRetentionInMs(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_DELETE_RETENTION_MS, STORE_NAME0), Long.toString(StorageConfig.DEFAULT_CHANGELOG_DELETE_RETENTION_MS * 2))));\r\n    assertEquals(StorageConfig.DEFAULT_CHANGELOG_DELETE_RETENTION_MS * 2, storageConfig.getChangeLogDeleteRetentionInMs(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testHasDurableStores",
  "sourceCode" : "@Test\r\npublic void testHasDurableStores() {\r\n    // no changelog, which means no durable stores\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"factory.class\")));\r\n    assertFalse(storageConfig.hasDurableStores());\r\n    storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME0), \"factory.class\", String.format(CHANGELOG_STREAM, STORE_NAME0), \"system0.changelog-stream\")));\r\n    assertTrue(storageConfig.hasDurableStores());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetChangelogMaxMsgSizeBytes",
  "sourceCode" : "@Test\r\npublic void testGetChangelogMaxMsgSizeBytes() {\r\n    // empty config, return default size\r\n    assertEquals(StorageConfig.DEFAULT_CHANGELOG_MAX_MSG_SIZE_BYTES, new StorageConfig(new MapConfig()).getChangelogMaxMsgSizeBytes(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.CHANGELOG_MAX_MSG_SIZE_BYTES, STORE_NAME0), \"10\")));\r\n    assertEquals(10, storageConfig.getChangelogMaxMsgSizeBytes(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetDisallowLargeMessages",
  "sourceCode" : "@Test\r\npublic void testGetDisallowLargeMessages() {\r\n    // empty config, return default size\r\n    assertEquals(StorageConfig.DEFAULT_DISALLOW_LARGE_MESSAGES, new StorageConfig(new MapConfig()).getDisallowLargeMessages(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.DISALLOW_LARGE_MESSAGES, STORE_NAME0), \"true\")));\r\n    assertEquals(true, storageConfig.getDisallowLargeMessages(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetDropLargeMessages",
  "sourceCode" : "@Test\r\npublic void testGetDropLargeMessages() {\r\n    // empty config, return default size\r\n    assertEquals(StorageConfig.DEFAULT_DROP_LARGE_MESSAGES, new StorageConfig(new MapConfig()).getDropLargeMessages(STORE_NAME0));\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(StorageConfig.DROP_LARGE_MESSAGES, STORE_NAME0), \"true\")));\r\n    assertEquals(true, storageConfig.getDropLargeMessages(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetChangelogMinCompactionLagMs",
  "sourceCode" : "@Test\r\npublic void testGetChangelogMinCompactionLagMs() {\r\n    // empty config, return default lag ms\r\n    Map<String, String> configMap = new HashMap<>();\r\n    assertEquals(DEFAULT_CHANGELOG_MIN_COMPACTION_LAG_MS, new StorageConfig(new MapConfig(configMap)).getChangelogMinCompactionLagMs(STORE_NAME0));\r\n    // override with configured default\r\n    long defaultLagOverride = TimeUnit.HOURS.toMillis(8);\r\n    configMap.put(String.format(CHANGELOG_MIN_COMPACTION_LAG_MS, \"default\"), String.valueOf(defaultLagOverride));\r\n    assertEquals(defaultLagOverride, new StorageConfig(new MapConfig(configMap)).getChangelogMinCompactionLagMs(STORE_NAME0));\r\n    // override for specific store\r\n    long storeSpecificLagOverride = TimeUnit.HOURS.toMillis(6);\r\n    configMap.put(String.format(CHANGELOG_MIN_COMPACTION_LAG_MS, STORE_NAME0), String.valueOf(storeSpecificLagOverride));\r\n    assertEquals(storeSpecificLagOverride, new StorageConfig(new MapConfig(configMap)).getChangelogMinCompactionLagMs(STORE_NAME0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetRestoreManagers",
  "sourceCode" : "@Test\r\npublic void testGetRestoreManagers() {\r\n    String storeName = \"store1\";\r\n    String storeName2 = \"store2\";\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(FACTORY, storeName), \"store1.factory.class\");\r\n    configMap.put(String.format(FACTORY, storeName2), \"store2.factory.class\");\r\n    // empty config, return no restore managers\r\n    assertEquals(Collections.emptySet(), new StorageConfig(new MapConfig(configMap)).getRestoreFactories());\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName2));\r\n    // changelog set, should default to kafka state backend restore\r\n    String changelogStreamOverride = \"changelogStream\";\r\n    configMap.put(String.format(CHANGELOG_STREAM, storeName), changelogStreamOverride);\r\n    configMap.put(String.format(CHANGELOG_STREAM, storeName2), changelogStreamOverride);\r\n    configMap.put(StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\");\r\n    configMap.put(String.format(StorageConfig.CHANGELOG_STREAM, storeName), \"changelog-stream0\");\r\n    assertEquals(ImmutableSet.of(KAFKA_STATE_BACKEND_FACTORY), new StorageConfig(new MapConfig(configMap)).getRestoreFactories());\r\n    assertEquals(DEFAULT_RESTORE_FACTORIES, new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName));\r\n    assertEquals(DEFAULT_RESTORE_FACTORIES, new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName2));\r\n    // job restore manager config set should override to job backend factory\r\n    String jobRestoreFactory1 = \"jobBackendRestoreFactory1\";\r\n    String jobRestoreFactory2 = \"jobBackendRestoreFactory2\";\r\n    String jobRestoreFactoryOverride = jobRestoreFactory1 + \",\" + jobRestoreFactory2;\r\n    configMap.put(JOB_RESTORE_FACTORIES, jobRestoreFactoryOverride);\r\n    assertEquals(ImmutableSet.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getRestoreFactories());\r\n    assertEquals(ImmutableList.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName));\r\n    assertEquals(ImmutableList.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName2));\r\n    // store specific restore managers set\r\n    String storeRestoreFactory1 = \"storeBackendRestoreFactory1\";\r\n    String storeRestoreFactory2 = \"storeBackendRestoreFactory2\";\r\n    String storeRestoreFactoryOverride = storeRestoreFactory1 + \",\" + storeRestoreFactory2;\r\n    configMap.put(String.format(STORE_RESTORE_FACTORIES, storeName), storeRestoreFactoryOverride);\r\n    assertEquals(ImmutableSet.of(jobRestoreFactory1, jobRestoreFactory2, storeRestoreFactory1, storeRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getRestoreFactories());\r\n    assertEquals(ImmutableList.of(storeRestoreFactory1, storeRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName));\r\n    assertEquals(ImmutableList.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName2));\r\n    String emptyBackupFactory = \"\";\r\n    configMap.put(String.format(STORE_RESTORE_FACTORIES, storeName), emptyBackupFactory);\r\n    assertEquals(ImmutableSet.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getRestoreFactories());\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName));\r\n    assertEquals(ImmutableList.of(jobRestoreFactory1, jobRestoreFactory2), new StorageConfig(new MapConfig(configMap)).getStoreRestoreFactories(storeName2));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithRestoreFactory(KAFKA_STATE_BACKEND_FACTORY));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithRestoreFactory(storeRestoreFactory1));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithRestoreFactory(storeRestoreFactory2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetBackupManagers",
  "sourceCode" : "@Test\r\npublic void testGetBackupManagers() {\r\n    String storeName = \"store1\";\r\n    String storeName2 = \"store2\";\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(FACTORY, storeName), \"store1.factory.class\");\r\n    configMap.put(String.format(FACTORY, storeName2), \"store2.factory.class\");\r\n    // empty config, return no restore managers\r\n    assertEquals(Collections.emptySet(), new StorageConfig(new MapConfig(configMap)).getBackupFactories());\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName2));\r\n    // changelog set, should default to kafka state backend restore\r\n    String changelogStreamOverride = \"changelogStream\";\r\n    configMap.put(String.format(CHANGELOG_STREAM, storeName), changelogStreamOverride);\r\n    configMap.put(String.format(CHANGELOG_STREAM, storeName2), changelogStreamOverride);\r\n    configMap.put(StorageConfig.CHANGELOG_SYSTEM, \"changelog-system\");\r\n    configMap.put(String.format(StorageConfig.CHANGELOG_STREAM, storeName), \"changelog-stream0\");\r\n    assertEquals(ImmutableSet.of(KAFKA_STATE_BACKEND_FACTORY), new StorageConfig(new MapConfig(configMap)).getBackupFactories());\r\n    assertEquals(DEFAULT_BACKUP_FACTORIES, new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName));\r\n    assertEquals(DEFAULT_BACKUP_FACTORIES, new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName2));\r\n    assertEquals(ImmutableSet.of(storeName2, storeName), ImmutableSet.copyOf(new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(KAFKA_STATE_BACKEND_FACTORY)));\r\n    // job restore manager config set should override to job backend factory\r\n    String jobBackupFactory1 = \"jobBackendBackupFactory1\";\r\n    String jobBackupFactory2 = \"jobBackendBackupFactory2\";\r\n    String jobBackupFactoryOverride = jobBackupFactory1 + \",\" + jobBackupFactory2;\r\n    configMap.put(JOB_BACKUP_FACTORIES, jobBackupFactoryOverride);\r\n    assertEquals(ImmutableSet.of(jobBackupFactory1, jobBackupFactory2), new StorageConfig(new MapConfig(configMap)).getBackupFactories());\r\n    assertEquals(ImmutableSet.of(jobBackupFactory1, jobBackupFactory2), ImmutableSet.copyOf(new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName)));\r\n    assertEquals(ImmutableSet.of(jobBackupFactory1, jobBackupFactory2), ImmutableSet.copyOf(new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName2)));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(KAFKA_STATE_BACKEND_FACTORY));\r\n    assertEquals(ImmutableSet.of(storeName2, storeName), ImmutableSet.copyOf(new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(jobBackupFactory1)));\r\n    assertEquals(ImmutableSet.of(storeName2, storeName), ImmutableSet.copyOf(new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(jobBackupFactory2)));\r\n    // store specific restore managers set\r\n    String storeBackupFactory1 = \"storeBackendBackupFactory1\";\r\n    String storeBackupFactory2 = \"storeBackendBackupFactory2\";\r\n    String storeBackupFactoryOverride = storeBackupFactory1 + \",\" + storeBackupFactory2;\r\n    configMap.put(String.format(STORE_BACKUP_FACTORIES, storeName), storeBackupFactoryOverride);\r\n    assertEquals(ImmutableSet.of(jobBackupFactory1, jobBackupFactory2, storeBackupFactory1, storeBackupFactory2), new StorageConfig(new MapConfig(configMap)).getBackupFactories());\r\n    assertEquals(ImmutableList.of(storeBackupFactory1, storeBackupFactory2), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName));\r\n    assertEquals(ImmutableList.of(jobBackupFactory1, jobBackupFactory2), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName2));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(KAFKA_STATE_BACKEND_FACTORY));\r\n    assertEquals(ImmutableList.of(storeName2), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(jobBackupFactory1));\r\n    assertEquals(ImmutableList.of(storeName2), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(jobBackupFactory2));\r\n    assertEquals(ImmutableList.of(storeName), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(storeBackupFactory1));\r\n    assertEquals(ImmutableList.of(storeName), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(storeBackupFactory2));\r\n    String emptyBackupFactory = \"\";\r\n    configMap.put(String.format(STORE_BACKUP_FACTORIES, storeName), emptyBackupFactory);\r\n    assertEquals(ImmutableSet.of(jobBackupFactory1, jobBackupFactory2), new StorageConfig(new MapConfig(configMap)).getBackupFactories());\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName));\r\n    assertEquals(ImmutableList.of(jobBackupFactory1, jobBackupFactory2), new StorageConfig(new MapConfig(configMap)).getStoreBackupFactories(storeName2));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(KAFKA_STATE_BACKEND_FACTORY));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(storeBackupFactory1));\r\n    assertEquals(Collections.emptyList(), new StorageConfig(new MapConfig(configMap)).getStoresWithBackupFactory(storeBackupFactory2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStorageConfig.java",
  "methodName" : "testGetMaxManifestFileSize",
  "sourceCode" : "@Test\r\npublic void testGetMaxManifestFileSize() {\r\n    // empty config, return default size, which is 1GB\r\n    assertEquals(DEFAULT_ROCKSDB_MAX_MANIFEST_FILE_SIZE_IN_BYTES, new StorageConfig(new MapConfig()).getDefaultMaxManifestFileSizeBytes());\r\n    StorageConfig storageConfig = new StorageConfig(new MapConfig(ImmutableMap.of(String.format(DEFAULT_ROCKSDB_MAX_MANIFEST_FILE_SIZE), \"1024\")));\r\n    assertEquals(1024, storageConfig.getDefaultMaxManifestFileSizeBytes());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetStreamMsgSerde",
  "sourceCode" : "@Test\r\npublic void testGetStreamMsgSerde() {\r\n    String value = \"my.msg.serde\";\r\n    doTestSamzaProperty(StreamConfig.MSG_SERDE, value, (config, systemStream) -> assertEquals(Optional.of(value), config.getStreamMsgSerde(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.MSG_SERDE, \"\", (config, systemStream) -> assertEquals(Optional.empty(), config.getStreamMsgSerde(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.MSG_SERDE, (config, systemStream) -> assertEquals(Optional.empty(), config.getStreamMsgSerde(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getStreamMsgSerde);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetStreamKeySerde",
  "sourceCode" : "@Test\r\npublic void testGetStreamKeySerde() {\r\n    String value = \"my.key.serde\";\r\n    doTestSamzaProperty(StreamConfig.KEY_SERDE, value, (config, systemStream) -> assertEquals(Optional.of(value), config.getStreamKeySerde(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.KEY_SERDE, \"\", (config, systemStream) -> assertEquals(Optional.empty(), config.getStreamKeySerde(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.KEY_SERDE, (config, systemStream) -> assertEquals(Optional.empty(), config.getStreamKeySerde(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getStreamKeySerde);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetResetOffset",
  "sourceCode" : "@Test\r\npublic void testGetResetOffset() {\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"true\", (config, systemStream) -> assertTrue(config.getResetOffset(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"false\", (config, systemStream) -> assertFalse(config.getResetOffset(systemStream)));\r\n    // if not true/false, then use false\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"unknown_value\", (config, systemStream) -> assertFalse(config.getResetOffset(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.CONSUMER_RESET_OFFSET, (config, systemStream) -> assertFalse(config.getResetOffset(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getResetOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testIsResetOffsetConfigured",
  "sourceCode" : "@Test\r\npublic void testIsResetOffsetConfigured() {\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"true\", (config, systemStream) -> assertTrue(config.isResetOffsetConfigured(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"false\", (config, systemStream) -> assertTrue(config.isResetOffsetConfigured(systemStream)));\r\n    // if not true/false, then use false\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_RESET_OFFSET, \"unknown_value\", (config, systemStream) -> assertTrue(config.isResetOffsetConfigured(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.CONSUMER_RESET_OFFSET, (config, systemStream) -> assertFalse(config.isResetOffsetConfigured(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::isResetOffsetConfigured);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetDefaultStreamOffset",
  "sourceCode" : "@Test\r\npublic void testGetDefaultStreamOffset() {\r\n    String value = \"my_offset_default\";\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_OFFSET_DEFAULT, value, (config, systemStream) -> assertEquals(Optional.of(value), config.getDefaultStreamOffset(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_OFFSET_DEFAULT, \"\", (config, systemStream) -> assertEquals(Optional.of(\"\"), config.getDefaultStreamOffset(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.CONSUMER_OFFSET_DEFAULT, (config, systemStream) -> assertEquals(Optional.empty(), new StreamConfig(config).getDefaultStreamOffset(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getDefaultStreamOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testIsDefaultStreamOffsetConfigured",
  "sourceCode" : "@Test\r\npublic void testIsDefaultStreamOffsetConfigured() {\r\n    String value = \"my_offset_default\";\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_OFFSET_DEFAULT, value, (config, systemStream) -> assertTrue(config.isDefaultStreamOffsetConfigured(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.CONSUMER_OFFSET_DEFAULT, \"\", (config, systemStream) -> assertTrue(config.isDefaultStreamOffsetConfigured(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.CONSUMER_OFFSET_DEFAULT, (config, systemStream) -> assertFalse(config.isDefaultStreamOffsetConfigured(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::isDefaultStreamOffsetConfigured);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetBootstrapEnabled",
  "sourceCode" : "@Test\r\npublic void testGetBootstrapEnabled() {\r\n    doTestSamzaProperty(StreamConfig.BOOTSTRAP, \"true\", (config, systemStream) -> assertTrue(config.getBootstrapEnabled(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.BOOTSTRAP, \"false\", (config, systemStream) -> assertFalse(config.getBootstrapEnabled(systemStream)));\r\n    // if not true/false, then use false\r\n    doTestSamzaProperty(StreamConfig.BOOTSTRAP, \"unknown_value\", (config, systemStream) -> assertFalse(config.getBootstrapEnabled(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.BOOTSTRAP, (config, systemStream) -> assertFalse(config.getBootstrapEnabled(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getBootstrapEnabled);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetBroadcastEnabled",
  "sourceCode" : "@Test\r\npublic void testGetBroadcastEnabled() {\r\n    doTestSamzaProperty(StreamConfig.BROADCAST, \"true\", (config, systemStream) -> assertTrue(config.getBroadcastEnabled(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.BROADCAST, \"false\", (config, systemStream) -> assertFalse(config.getBroadcastEnabled(systemStream)));\r\n    // if not true/false, then use false\r\n    doTestSamzaProperty(StreamConfig.BROADCAST, \"unknown_value\", (config, systemStream) -> assertFalse(config.getBroadcastEnabled(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.BROADCAST, (config, systemStream) -> assertFalse(config.getBroadcastEnabled(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getBroadcastEnabled);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetPriority",
  "sourceCode" : "@Test\r\npublic void testGetPriority() {\r\n    doTestSamzaProperty(StreamConfig.PRIORITY, \"0\", (config, systemStream) -> assertEquals(0, config.getPriority(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.PRIORITY, \"100\", (config, systemStream) -> assertEquals(100, config.getPriority(systemStream)));\r\n    doTestSamzaProperty(StreamConfig.PRIORITY, \"-1\", (config, systemStream) -> assertEquals(-1, config.getPriority(systemStream)));\r\n    doTestSamzaPropertyDoesNotExist(StreamConfig.PRIORITY, (config, systemStream) -> assertEquals(-1, config.getPriority(systemStream)));\r\n    doTestSamzaPropertyInvalidConfig(StreamConfig::getPriority);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetSerdeStreams",
  "sourceCode" : "@Test\r\npublic void testGetSerdeStreams() {\r\n    assertEquals(Collections.emptySet(), new StreamConfig(new MapConfig()).getSerdeStreams(SYSTEM));\r\n    // not key/msg serde property for \"streams.\"\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + SAMZA_IGNORED_PROPERTY, UNUSED_VALUE)));\r\n    assertEquals(Collections.emptySet(), streamConfig.getSerdeStreams(SYSTEM));\r\n    // not matching system for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.KEY_SERDE, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), \"otherSystem\")));\r\n    assertEquals(Collections.emptySet(), streamConfig.getSerdeStreams(SYSTEM));\r\n    // not key/msg serde property for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + SAMZA_IGNORED_PROPERTY, UNUSED_VALUE)));\r\n    assertEquals(Collections.emptySet(), streamConfig.getSerdeStreams(SYSTEM));\r\n    // not matching system for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, \"otherSystem\", STREAM_ID) + StreamConfig.KEY_SERDE, UNUSED_VALUE)));\r\n    assertEquals(Collections.emptySet(), streamConfig.getSerdeStreams(SYSTEM));\r\n    // not matching system for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.KEY_SERDE, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), \"otherSystem\")));\r\n    assertEquals(Collections.emptySet(), streamConfig.getSerdeStreams(SYSTEM));\r\n    String serdeValue = \"my.serde.class\";\r\n    // key serde for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.KEY_SERDE, serdeValue)));\r\n    assertEquals(Collections.singleton(new SystemStream(SYSTEM, STREAM_ID)), streamConfig.getSerdeStreams(SYSTEM));\r\n    // msg serde for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.MSG_SERDE, serdeValue)));\r\n    assertEquals(Collections.singleton(new SystemStream(SYSTEM, STREAM_ID)), streamConfig.getSerdeStreams(SYSTEM));\r\n    // serde for \"streams.\" with physical stream name mapping\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM, String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.KEY_SERDE, serdeValue)));\r\n    assertEquals(Collections.singleton(new SystemStream(SYSTEM, PHYSICAL_STREAM)), streamConfig.getSerdeStreams(SYSTEM));\r\n    // key serde for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + StreamConfig.KEY_SERDE, serdeValue)));\r\n    assertEquals(Collections.singleton(new SystemStream(SYSTEM, STREAM_ID)), streamConfig.getSerdeStreams(SYSTEM));\r\n    // msg serde for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + StreamConfig.MSG_SERDE, serdeValue)));\r\n    assertEquals(Collections.singleton(new SystemStream(SYSTEM, STREAM_ID)), streamConfig.getSerdeStreams(SYSTEM));\r\n    // merge several different ways of providing serdes\r\n    String streamIdWithPhysicalName = \"streamIdWithPhysicalName\";\r\n    streamConfig = new StreamConfig(new MapConfig(new ImmutableMap.Builder<String, String>().// need to map the stream ids to the system\r\n    put(JobConfig.JOB_DEFAULT_SYSTEM, SYSTEM).// key and msg serde for \"streams.\"\r\n    put(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.KEY_SERDE, serdeValue).put(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + StreamConfig.MSG_SERDE, serdeValue).// key serde for \"streams.\" with physical stream name mapping\r\n    put(String.format(StreamConfig.STREAM_ID_PREFIX, streamIdWithPhysicalName) + StreamConfig.KEY_SERDE, serdeValue).put(String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, streamIdWithPhysicalName), PHYSICAL_STREAM).// key serde for \"systems.<system>.streams.\"\r\n    put(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, OTHER_STREAM_ID) + StreamConfig.KEY_SERDE, serdeValue).build()));\r\n    assertEquals(Sets.newHashSet(new SystemStream(SYSTEM, STREAM_ID), new SystemStream(SYSTEM, PHYSICAL_STREAM), new SystemStream(SYSTEM, OTHER_STREAM_ID)), streamConfig.getSerdeStreams(SYSTEM));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetStreamProperties",
  "sourceCode" : "@Test\r\npublic void testGetStreamProperties() {\r\n    assertEquals(new MapConfig(), new StreamConfig(new MapConfig()).getStreamProperties(STREAM_ID));\r\n    String propertyName = \"stream.property.name\";\r\n    // BEGIN: tests in which properties cannot be found in the config\r\n    // not matching stream id for \"streams.\"\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, OTHER_STREAM_ID) + propertyName, UNUSED_VALUE)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(STREAM_ID));\r\n    // not matching stream id for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, OTHER_STREAM_ID) + propertyName, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, OTHER_STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(STREAM_ID));\r\n    // no system mapping when using \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, UNUSED_VALUE)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(STREAM_ID));\r\n    // ignore property with \"samza\" prefix for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + SAMZA_IGNORED_PROPERTY, UNUSED_VALUE)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(STREAM_ID));\r\n    // ignore property with \"samza\" prefix for \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + SAMZA_IGNORED_PROPERTY, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(STREAM_ID));\r\n    // should not map physical name back to stream id if physical name is passed as stream id\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertEquals(new MapConfig(), streamConfig.getStreamProperties(PHYSICAL_STREAM));\r\n    // BEGIN: tests in which properties can be found in the config\r\n    String propertyValue = \"value\";\r\n    // \"streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + propertyName, propertyValue)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, propertyValue, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // \"systems.<system>.default.stream.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(SystemConfig.SYSTEM_DEFAULT_STREAMS_PREFIX_FORMAT, SYSTEM) + propertyName, propertyValue, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // use physical name mapping for \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, PHYSICAL_STREAM) + propertyName, propertyValue, // should not use stream id since there is physical stream\r\n    String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // \"streams.\" should override \"systems.<system>.streams.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + propertyName, propertyValue, // should not use \"systems.<system>.streams.\" since there is a \"streams.\" config\r\n    String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // \"systems.<system>.streams.\" should override \"systems.<system>.default.stream.\"\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + propertyName, propertyValue, // should not use \"systems.<system>.default.stream.\" since there is a \"systems.<system>.streams.\"\r\n    String.format(SystemConfig.SYSTEM_DEFAULT_STREAMS_PREFIX_FORMAT, SYSTEM) + propertyName, UNUSED_VALUE, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(propertyName, propertyValue)), streamConfig.getStreamProperties(STREAM_ID));\r\n    // merge multiple ways of specifying configs\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(// \"streams.\"\r\n    String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \"from.stream.id.property\", \"fromStreamIdValue\", // second \"streams.\" property\r\n    String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \"from.stream.id.other.property\", \"fromStreamIdOtherValue\", // \"systems.<system>.streams.\"\r\n    String.format(StreamConfig.STREAM_PREFIX, SYSTEM, STREAM_ID) + \"from.system.stream.property\", \"fromSystemStreamValue\", // need to map the stream id to a system\r\n    String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM)));\r\n    assertEquals(new MapConfig(ImmutableMap.of(\"from.stream.id.property\", \"fromStreamIdValue\", \"from.stream.id.other.property\", \"fromStreamIdOtherValue\", \"from.system.stream.property\", \"fromSystemStreamValue\")), streamConfig.getStreamProperties(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetSystem",
  "sourceCode" : "@Test\r\npublic void testGetSystem() {\r\n    assertNull(new StreamConfig(new MapConfig()).getSystem(STREAM_ID));\r\n    // system is specified directly\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID), SYSTEM, JobConfig.JOB_DEFAULT_SYSTEM, \"otherSystem\", String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, OTHER_STREAM_ID), \"otherSystem\")));\r\n    assertEquals(SYSTEM, streamConfig.getSystem(STREAM_ID));\r\n    // fall back to job default system\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, SYSTEM, String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, OTHER_STREAM_ID), \"otherSystem\")));\r\n    assertEquals(SYSTEM, streamConfig.getSystem(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetPhysicalName",
  "sourceCode" : "@Test\r\npublic void testGetPhysicalName() {\r\n    assertEquals(STREAM_ID, new StreamConfig(new MapConfig()).getPhysicalName(STREAM_ID));\r\n    // ignore mapping for other stream ids\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, OTHER_STREAM_ID), PHYSICAL_STREAM)));\r\n    assertEquals(STREAM_ID, streamConfig.getPhysicalName(STREAM_ID));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertEquals(PHYSICAL_STREAM, streamConfig.getPhysicalName(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetIsIntermediateStream",
  "sourceCode" : "@Test\r\npublic void testGetIsIntermediateStream() {\r\n    assertFalse(new StreamConfig(new MapConfig()).getIsIntermediateStream(STREAM_ID));\r\n    // ignore mapping for other stream ids\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_INTERMEDIATE_FOR_STREAM_ID, OTHER_STREAM_ID), \"true\")));\r\n    assertFalse(streamConfig.getIsIntermediateStream(STREAM_ID));\r\n    // do not use stream id property if physical name is passed as input\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_INTERMEDIATE_FOR_STREAM_ID, STREAM_ID), \"true\", String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertFalse(streamConfig.getIsIntermediateStream(PHYSICAL_STREAM));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_INTERMEDIATE_FOR_STREAM_ID, STREAM_ID), \"true\")));\r\n    assertTrue(streamConfig.getIsIntermediateStream(STREAM_ID));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_INTERMEDIATE_FOR_STREAM_ID, STREAM_ID), \"false\")));\r\n    assertFalse(streamConfig.getIsIntermediateStream(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetDeleteCommittedMessages",
  "sourceCode" : "@Test\r\npublic void testGetDeleteCommittedMessages() {\r\n    assertFalse(new StreamConfig(new MapConfig()).getDeleteCommittedMessages(STREAM_ID));\r\n    // ignore mapping for other stream ids\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.DELETE_COMMITTED_MESSAGES_FOR_STREAM_ID, OTHER_STREAM_ID), \"true\")));\r\n    assertFalse(streamConfig.getDeleteCommittedMessages(STREAM_ID));\r\n    // do not use stream id property if physical name is passed as input\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.DELETE_COMMITTED_MESSAGES_FOR_STREAM_ID, STREAM_ID), \"true\", String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertFalse(streamConfig.getDeleteCommittedMessages(PHYSICAL_STREAM));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.DELETE_COMMITTED_MESSAGES_FOR_STREAM_ID, STREAM_ID), \"true\")));\r\n    assertTrue(streamConfig.getDeleteCommittedMessages(STREAM_ID));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.DELETE_COMMITTED_MESSAGES_FOR_STREAM_ID, STREAM_ID), \"false\")));\r\n    assertFalse(streamConfig.getDeleteCommittedMessages(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetIsBounded",
  "sourceCode" : "@Test\r\npublic void testGetIsBounded() {\r\n    assertFalse(new StreamConfig(new MapConfig()).getIsBounded(STREAM_ID));\r\n    // ignore mapping for other stream ids\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_BOUNDED_FOR_STREAM_ID, OTHER_STREAM_ID), \"true\")));\r\n    assertFalse(streamConfig.getIsBounded(STREAM_ID));\r\n    // do not use stream id property if physical name is passed as input\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_BOUNDED_FOR_STREAM_ID, STREAM_ID), \"true\", String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID), PHYSICAL_STREAM)));\r\n    assertFalse(streamConfig.getIsBounded(PHYSICAL_STREAM));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_BOUNDED_FOR_STREAM_ID, STREAM_ID), \"true\")));\r\n    assertTrue(streamConfig.getIsBounded(STREAM_ID));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.IS_BOUNDED_FOR_STREAM_ID, STREAM_ID), \"false\")));\r\n    assertFalse(streamConfig.getIsBounded(STREAM_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestStreamConfig.java",
  "methodName" : "testGetStreamIds",
  "sourceCode" : "@Test\r\npublic void testGetStreamIds() {\r\n    assertEquals(ImmutableList.of(), ImmutableList.copyOf(new StreamConfig(new MapConfig()).getStreamIds()));\r\n    StreamConfig streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property\", \"value\")));\r\n    assertEquals(ImmutableSet.of(STREAM_ID), ImmutableSet.copyOf(streamConfig.getStreamIds()));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property.subProperty\", \"value\")));\r\n    assertEquals(ImmutableSet.of(STREAM_ID), ImmutableSet.copyOf(streamConfig.getStreamIds()));\r\n    streamConfig = new StreamConfig(new MapConfig(ImmutableMap.of(String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property0\", \"value\", String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property1\", \"value\", String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property.subProperty0\", \"value\", String.format(StreamConfig.STREAM_ID_PREFIX, STREAM_ID) + \".property.subProperty1\", \"value\", String.format(StreamConfig.STREAM_ID_PREFIX, OTHER_STREAM_ID) + \".property\", \"value\")));\r\n    assertEquals(ImmutableSet.of(STREAM_ID, OTHER_STREAM_ID), ImmutableSet.copyOf(streamConfig.getStreamIds()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemFactory",
  "sourceCode" : "@Test\r\npublic void testGetSystemFactory() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(MOCK_SYSTEM_FACTORY_NAME1, MOCK_SYSTEM_FACTORY_CLASSNAME1);\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    assertEquals(MOCK_SYSTEM_FACTORY_CLASSNAME1, systemConfig.getSystemFactory(MOCK_SYSTEM_NAME1).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemFactoryEmptyClassName",
  "sourceCode" : "@Test\r\npublic void testGetSystemFactoryEmptyClassName() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(MOCK_SYSTEM_FACTORY_NAME1, \"\");\r\n    map.put(MOCK_SYSTEM_FACTORY_NAME2, \" \");\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    assertFalse(systemConfig.getSystemFactory(MOCK_SYSTEM_NAME1).isPresent());\r\n    assertFalse(systemConfig.getSystemFactory(MOCK_SYSTEM_NAME2).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemNames",
  "sourceCode" : "@Test\r\npublic void testGetSystemNames() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(MOCK_SYSTEM_FACTORY_NAME1, MOCK_SYSTEM_FACTORY_CLASSNAME1);\r\n    map.put(MOCK_SYSTEM_FACTORY_NAME2, MOCK_SYSTEM_FACTORY_CLASSNAME2);\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    assertEquals(2, systemConfig.getSystemNames().size());\r\n    assertTrue(systemConfig.getSystemNames().contains(MOCK_SYSTEM_NAME1));\r\n    assertTrue(systemConfig.getSystemNames().contains(MOCK_SYSTEM_NAME2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemAdmins",
  "sourceCode" : "@Test\r\npublic void testGetSystemAdmins() {\r\n    Map<String, String> map = ImmutableMap.of(MOCK_SYSTEM_FACTORY_NAME1, MockSystemFactory.class.getName());\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    Map<String, SystemAdmin> expected = ImmutableMap.of(MOCK_SYSTEM_NAME1, SYSTEM_ADMIN1);\r\n    assertEquals(expected, systemConfig.getSystemAdmins());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemAdmin",
  "sourceCode" : "@Test\r\npublic void testGetSystemAdmin() {\r\n    Map<String, String> map = ImmutableMap.of(MOCK_SYSTEM_FACTORY_NAME1, MockSystemFactory.class.getName());\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    assertEquals(SYSTEM_ADMIN1, systemConfig.getSystemAdmin(MOCK_SYSTEM_NAME1));\r\n    assertNull(systemConfig.getSystemAdmin(MOCK_SYSTEM_NAME2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemFactories",
  "sourceCode" : "@Test\r\npublic void testGetSystemFactories() {\r\n    Map<String, String> map = ImmutableMap.of(MOCK_SYSTEM_FACTORY_NAME1, MockSystemFactory.class.getName());\r\n    SystemConfig systemConfig = new SystemConfig(new MapConfig(map));\r\n    Map<String, SystemFactory> actual = systemConfig.getSystemFactories();\r\n    assertEquals(actual.size(), 1);\r\n    assertTrue(actual.get(MOCK_SYSTEM_NAME1) instanceof MockSystemFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetDefaultStreamProperties",
  "sourceCode" : "@Test\r\npublic void testGetDefaultStreamProperties() {\r\n    String defaultStreamPrefix = buildDefaultStreamPropertiesPrefix(MOCK_SYSTEM_NAME1);\r\n    String system1ConfigKey = \"config1-key\";\r\n    String system1ConfigValue = \"config1-value\";\r\n    String system2ConfigKey = \"config2-key\";\r\n    String system2ConfigValue = \"config2-value\";\r\n    Config config = new MapConfig(ImmutableMap.of(defaultStreamPrefix + system1ConfigKey, system1ConfigValue, defaultStreamPrefix + system2ConfigKey, system2ConfigValue));\r\n    Config expected = new MapConfig(ImmutableMap.of(system1ConfigKey, system1ConfigValue, system2ConfigKey, system2ConfigValue));\r\n    SystemConfig systemConfig = new SystemConfig(config);\r\n    assertEquals(expected, systemConfig.getDefaultStreamProperties(MOCK_SYSTEM_NAME1));\r\n    assertEquals(new MapConfig(), systemConfig.getDefaultStreamProperties(MOCK_SYSTEM_NAME2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemOffsetDefault",
  "sourceCode" : "@Test\r\npublic void testGetSystemOffsetDefault() {\r\n    String system1OffsetDefault = \"offset-system-1\";\r\n    String system2OffsetDefault = \"offset-system-2\";\r\n    Config config = new MapConfig(ImmutableMap.of(// default.stream.samza.offset.default set\r\n    String.format(SystemConfig.SYSTEM_DEFAULT_STREAMS_PREFIX_FORMAT, MOCK_SYSTEM_NAME1) + SAMZA_OFFSET_DEFAULT, system1OffsetDefault, // should not use this value since default.stream.samza.offset.default is set\r\n    String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + SAMZA_OFFSET_DEFAULT, \"wrong-value\", // only samza.offset.default set\r\n    String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME2) + SAMZA_OFFSET_DEFAULT, system2OffsetDefault));\r\n    SystemConfig systemConfig = new SystemConfig(config);\r\n    assertEquals(system1OffsetDefault, systemConfig.getSystemOffsetDefault(MOCK_SYSTEM_NAME1));\r\n    assertEquals(system2OffsetDefault, systemConfig.getSystemOffsetDefault(MOCK_SYSTEM_NAME2));\r\n    assertEquals(SystemConfig.SAMZA_SYSTEM_OFFSET_UPCOMING, systemConfig.getSystemOffsetDefault(\"other-system\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemKeySerde",
  "sourceCode" : "@Test\r\npublic void testGetSystemKeySerde() {\r\n    String system1KeySerde = \"system1-key-serde\";\r\n    String defaultStreamPrefixSystem1 = buildDefaultStreamPropertiesPrefix(MOCK_SYSTEM_NAME1);\r\n    // value specified explicitly\r\n    Config config = new MapConfig(ImmutableMap.of(defaultStreamPrefixSystem1 + StreamConfig.KEY_SERDE, system1KeySerde));\r\n    SystemConfig systemConfig = new SystemConfig(config);\r\n    assertEquals(system1KeySerde, systemConfig.getSystemKeySerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is unspecified, try fall back config key\r\n    config = new MapConfig(ImmutableMap.of(String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.KEY_SERDE, system1KeySerde));\r\n    systemConfig = new SystemConfig(config);\r\n    assertEquals(system1KeySerde, systemConfig.getSystemKeySerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is empty string, try fall back config key\r\n    config = new MapConfig(ImmutableMap.of(// default stream property is empty\r\n    defaultStreamPrefixSystem1 + StreamConfig.KEY_SERDE, \"\", // fall back entry\r\n    String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.KEY_SERDE, system1KeySerde));\r\n    systemConfig = new SystemConfig(config);\r\n    assertEquals(system1KeySerde, systemConfig.getSystemKeySerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is unspecified, fall back is also empty\r\n    config = new MapConfig(ImmutableMap.of(String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.KEY_SERDE, \"\"));\r\n    systemConfig = new SystemConfig(config);\r\n    assertFalse(systemConfig.getSystemKeySerde(MOCK_SYSTEM_NAME1).isPresent());\r\n    // default stream property is unspecified, fall back is also unspecified\r\n    config = new MapConfig();\r\n    systemConfig = new SystemConfig(config);\r\n    assertFalse(systemConfig.getSystemKeySerde(MOCK_SYSTEM_NAME1).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testGetSystemMsgSerde",
  "sourceCode" : "@Test\r\npublic void testGetSystemMsgSerde() {\r\n    String system1MsgSerde = \"system1-msg-serde\";\r\n    String defaultStreamPrefixSystem1 = buildDefaultStreamPropertiesPrefix(MOCK_SYSTEM_NAME1);\r\n    // value specified explicitly\r\n    Config config = new MapConfig(ImmutableMap.of(defaultStreamPrefixSystem1 + StreamConfig.MSG_SERDE, system1MsgSerde));\r\n    SystemConfig systemConfig = new SystemConfig(config);\r\n    assertEquals(system1MsgSerde, systemConfig.getSystemMsgSerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is unspecified, try fall back config msg\r\n    config = new MapConfig(ImmutableMap.of(String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.MSG_SERDE, system1MsgSerde));\r\n    systemConfig = new SystemConfig(config);\r\n    assertEquals(system1MsgSerde, systemConfig.getSystemMsgSerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is empty string, try fall back config msg\r\n    config = new MapConfig(ImmutableMap.of(// default stream property is empty\r\n    defaultStreamPrefixSystem1 + StreamConfig.MSG_SERDE, \"\", // fall back entry\r\n    String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.MSG_SERDE, system1MsgSerde));\r\n    systemConfig = new SystemConfig(config);\r\n    assertEquals(system1MsgSerde, systemConfig.getSystemMsgSerde(MOCK_SYSTEM_NAME1).get());\r\n    // default stream property is unspecified, fall back is also empty\r\n    config = new MapConfig(ImmutableMap.of(String.format(SystemConfig.SYSTEM_ID_PREFIX, MOCK_SYSTEM_NAME1) + StreamConfig.MSG_SERDE, \"\"));\r\n    systemConfig = new SystemConfig(config);\r\n    assertFalse(systemConfig.getSystemMsgSerde(MOCK_SYSTEM_NAME1).isPresent());\r\n    // default stream property is unspecified, fall back is also unspecified\r\n    config = new MapConfig();\r\n    systemConfig = new SystemConfig(config);\r\n    assertFalse(systemConfig.getSystemMsgSerde(MOCK_SYSTEM_NAME1).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestSystemConfig.java",
  "methodName" : "testDeleteCommittedMessages",
  "sourceCode" : "@Test\r\npublic void testDeleteCommittedMessages() {\r\n    Config config = new MapConfig(ImmutableMap.of(// value is \"true\"\r\n    String.format(SystemConfig.DELETE_COMMITTED_MESSAGES, MOCK_SYSTEM_NAME1), \"true\", // value is explicitly \"false\"\r\n    String.format(SystemConfig.DELETE_COMMITTED_MESSAGES, MOCK_SYSTEM_NAME2), \"false\"));\r\n    SystemConfig systemConfig = new SystemConfig(config);\r\n    assertTrue(systemConfig.deleteCommittedMessages(MOCK_SYSTEM_NAME1));\r\n    assertFalse(systemConfig.deleteCommittedMessages(MOCK_SYSTEM_NAME2));\r\n    // value is not specified\r\n    assertFalse(systemConfig.deleteCommittedMessages(\"other-system\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetInputStreams",
  "sourceCode" : "@Test\r\npublic void testGetInputStreams() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.INPUT_STREAMS, \"kafka.foo, kafka.bar, otherKafka.bar, otherKafka.foo.bar\"));\r\n    Set<SystemStream> expected = ImmutableSet.of(new SystemStream(\"kafka\", \"foo\"), new SystemStream(\"kafka\", \"bar\"), new SystemStream(\"otherKafka\", \"bar\"), new SystemStream(\"otherKafka\", \"foo.bar\"));\r\n    assertEquals(expected, new TaskConfig(config).getAllInputStreams());\r\n    // empty string for value\r\n    MapConfig configEmptyInput = new MapConfig(ImmutableMap.of(TaskConfig.INPUT_STREAMS, \"\"));\r\n    assertTrue(new TaskConfig(configEmptyInput).getInputStreams().isEmpty());\r\n    // config not specified\r\n    assertTrue(new TaskConfig(new MapConfig()).getInputStreams().isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetWindowMs",
  "sourceCode" : "@Test\r\npublic void testGetWindowMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.WINDOW_MS, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getWindowMs());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.WINDOW_MS, \"-1\"));\r\n    assertEquals(-1, new TaskConfig(config).getWindowMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_WINDOW_MS, new TaskConfig(new MapConfig()).getWindowMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetCommitMs",
  "sourceCode" : "@Test\r\npublic void testGetCommitMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.COMMIT_MS, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getCommitMs());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.COMMIT_MS, \"-1\"));\r\n    assertEquals(-1, new TaskConfig(config).getCommitMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_COMMIT_MS, new TaskConfig(new MapConfig()).getCommitMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetTaskClass",
  "sourceCode" : "@Test\r\npublic void testGetTaskClass() {\r\n    String taskClass = \"some.task.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.TASK_CLASS, taskClass));\r\n    assertEquals(Optional.of(taskClass), new TaskConfig(config).getTaskClass());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getTaskClass().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetCommandClass",
  "sourceCode" : "@Test\r\npublic void testGetCommandClass() {\r\n    String commandClass = \"some.command.class\";\r\n    String defaultCommandClass = \"default.command.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.COMMAND_BUILDER, commandClass));\r\n    assertEquals(commandClass, new TaskConfig(config).getCommandClass(defaultCommandClass));\r\n    // config not specified\r\n    assertEquals(defaultCommandClass, new TaskConfig(new MapConfig()).getCommandClass(defaultCommandClass));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetMessageChooserClass",
  "sourceCode" : "@Test\r\npublic void testGetMessageChooserClass() {\r\n    String messageChooserClassValue = \"some.message.chooser.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.MESSAGE_CHOOSER_CLASS_NAME, messageChooserClassValue));\r\n    assertEquals(messageChooserClassValue, new TaskConfig(config).getMessageChooserClass());\r\n    // config not specified\r\n    assertEquals(RoundRobinChooserFactory.class.getName(), new TaskConfig(new MapConfig()).getMessageChooserClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetDropDeserializationErrors",
  "sourceCode" : "@Test\r\npublic void testGetDropDeserializationErrors() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_DESERIALIZATION_ERRORS, \"true\"));\r\n    assertTrue(new TaskConfig(config).getDropDeserializationErrors());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_DESERIALIZATION_ERRORS, \"false\"));\r\n    assertFalse(new TaskConfig(config).getDropDeserializationErrors());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getDropDeserializationErrors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetDropSerializationErrors",
  "sourceCode" : "@Test\r\npublic void testGetDropSerializationErrors() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_SERIALIZATION_ERRORS, \"true\"));\r\n    assertTrue(new TaskConfig(config).getDropSerializationErrors());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_SERIALIZATION_ERRORS, \"false\"));\r\n    assertFalse(new TaskConfig(config).getDropSerializationErrors());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getDropSerializationErrors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetDropProducerErrors",
  "sourceCode" : "@Test\r\npublic void testGetDropProducerErrors() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_PRODUCER_ERRORS, \"true\"));\r\n    assertTrue(new TaskConfig(config).getDropProducerErrors());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.DROP_PRODUCER_ERRORS, \"false\"));\r\n    assertFalse(new TaskConfig(config).getDropProducerErrors());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getDropProducerErrors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetPollIntervalMs",
  "sourceCode" : "@Test\r\npublic void testGetPollIntervalMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.POLL_INTERVAL_MS, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getPollIntervalMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_POLL_INTERVAL_MS, new TaskConfig(new MapConfig()).getPollIntervalMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetIgnoredExceptions",
  "sourceCode" : "@Test\r\npublic void testGetIgnoredExceptions() {\r\n    String ignoredExceptionsValue = \"exception0.class, exception1.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.IGNORED_EXCEPTIONS, ignoredExceptionsValue));\r\n    assertEquals(Optional.of(ignoredExceptionsValue), new TaskConfig(config).getIgnoredExceptions());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getIgnoredExceptions().isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetTaskNameGrouperFactory",
  "sourceCode" : "@Test\r\npublic void testGetTaskNameGrouperFactory() {\r\n    String taskNameGrouperFactoryValue = \"task.name.grouper.factory.class\";\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.GROUPER_FACTORY, taskNameGrouperFactoryValue));\r\n    assertEquals(taskNameGrouperFactoryValue, new TaskConfig(config).getTaskNameGrouperFactory());\r\n    // config not specified\r\n    assertEquals(GroupByContainerCountFactory.class.getName(), new TaskConfig(new MapConfig()).getTaskNameGrouperFactory());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetMaxConcurrency",
  "sourceCode" : "@Test\r\npublic void testGetMaxConcurrency() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.MAX_CONCURRENCY, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getMaxConcurrency());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_MAX_CONCURRENCY, new TaskConfig(new MapConfig()).getMaxConcurrency());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetCallbackTimeoutMs",
  "sourceCode" : "@Test\r\npublic void testGetCallbackTimeoutMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.CALLBACK_TIMEOUT_MS, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getCallbackTimeoutMs());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.CALLBACK_TIMEOUT_MS, \"-1\"));\r\n    assertEquals(-1, new TaskConfig(config).getCallbackTimeoutMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_CALLBACK_TIMEOUT_MS, new TaskConfig(new MapConfig()).getCallbackTimeoutMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetDrainCallbackTimeoutMs",
  "sourceCode" : "@Test\r\npublic void testGetDrainCallbackTimeoutMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.DRAIN_CALLBACK_TIMEOUT_MS, \"100\"));\r\n    assertEquals(100, new TaskConfig(config).getDrainCallbackTimeoutMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_DRAIN_CALLBACK_TIMEOUT_MS, new TaskConfig(new MapConfig()).getDrainCallbackTimeoutMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetAsyncCommit",
  "sourceCode" : "@Test\r\npublic void testGetAsyncCommit() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.ASYNC_COMMIT, \"true\"));\r\n    assertTrue(new TaskConfig(config).getAsyncCommit());\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.ASYNC_COMMIT, \"false\"));\r\n    assertFalse(new TaskConfig(config).getAsyncCommit());\r\n    // config not specified\r\n    assertFalse(new TaskConfig(new MapConfig()).getAsyncCommit());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetMaxIdleMs",
  "sourceCode" : "@Test\r\npublic void testGetMaxIdleMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.MAX_IDLE_MS, \"20\"));\r\n    assertEquals(20, new TaskConfig(config).getMaxIdleMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_MAX_IDLE_MS, new TaskConfig(new MapConfig()).getMaxIdleMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetCheckpointManager",
  "sourceCode" : "@Test\r\npublic void testGetCheckpointManager() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.CHECKPOINT_MANAGER_FACTORY, MockCheckpointManagerFactory.class.getName()));\r\n    assertTrue(new TaskConfig(config).getCheckpointManager(null).get() instanceof MockCheckpointManager);\r\n    Config configEmptyString = new MapConfig(ImmutableMap.of(TaskConfig.CHECKPOINT_MANAGER_FACTORY, \"\"));\r\n    assertFalse(new TaskConfig(configEmptyString).getCheckpointManager(null).isPresent());\r\n    assertFalse(new TaskConfig(new MapConfig()).getCheckpointManager(null).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetBroadcastSystemStreamPartitions",
  "sourceCode" : "@Test\r\npublic void testGetBroadcastSystemStreamPartitions() {\r\n    // no entry for \"task.broadcast.inputs\"\r\n    assertEquals(Collections.emptySet(), new TaskConfig(new MapConfig()).getBroadcastSystemStreamPartitions());\r\n    HashMap<String, String> map = new HashMap<>();\r\n    map.put(\"task.broadcast.inputs\", \"kafka.foo#4, kafka.boo#5, kafka.z-o-o#[12-14], kafka.foo.bar#[3-4]\");\r\n    Config config = new MapConfig(map);\r\n    TaskConfig taskConfig = new TaskConfig(config);\r\n    Set<SystemStreamPartition> systemStreamPartitionSet = taskConfig.getBroadcastSystemStreamPartitions();\r\n    HashSet<SystemStreamPartition> expected = new HashSet<>();\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"foo\", new Partition(4)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"boo\", new Partition(5)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"z-o-o\", new Partition(12)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"z-o-o\", new Partition(13)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"z-o-o\", new Partition(14)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"foo.bar\", new Partition(3)));\r\n    expected.add(new SystemStreamPartition(\"kafka\", \"foo.bar\", new Partition(4)));\r\n    assertEquals(expected, systemStreamPartitionSet);\r\n    map.put(\"task.broadcast.inputs\", \"kafka.foo\");\r\n    taskConfig = new TaskConfig(new MapConfig(map));\r\n    boolean catchCorrectException = false;\r\n    try {\r\n        taskConfig.getBroadcastSystemStreamPartitions();\r\n    } catch (IllegalArgumentException e) {\r\n        catchCorrectException = true;\r\n    }\r\n    assertTrue(catchCorrectException);\r\n    map.put(\"task.broadcast.inputs\", \"kafka.org.apache.events.WhitelistedIps#1-2\");\r\n    taskConfig = new TaskConfig(new MapConfig(map));\r\n    boolean invalidFormatException = false;\r\n    try {\r\n        taskConfig.getBroadcastSystemStreamPartitions();\r\n    } catch (IllegalArgumentException e) {\r\n        invalidFormatException = true;\r\n    }\r\n    assertTrue(invalidFormatException);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetBroadcastSystemStreams",
  "sourceCode" : "@Test\r\npublic void testGetBroadcastSystemStreams() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.BROADCAST_INPUT_STREAMS, \"kafka.foo#4, kafka.bar#5, otherKafka.foo#4, otherKafka.foo.bar#5\"));\r\n    Set<SystemStream> expected = ImmutableSet.of(new SystemStream(\"kafka\", \"foo\"), new SystemStream(\"kafka\", \"bar\"), new SystemStream(\"otherKafka\", \"foo\"), new SystemStream(\"otherKafka\", \"foo.bar\"));\r\n    assertEquals(expected, new TaskConfig(config).getBroadcastSystemStreams());\r\n    assertTrue(new TaskConfig(new MapConfig()).getBroadcastSystemStreams().isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetAllInputStreams",
  "sourceCode" : "@Test\r\npublic void testGetAllInputStreams() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.INPUT_STREAMS, \"kafka.foo, otherKafka.bar\", TaskConfig.BROADCAST_INPUT_STREAMS, \"kafka.bar#4, otherKafka.foo#5\"));\r\n    Set<SystemStream> expected = ImmutableSet.of(new SystemStream(\"kafka\", \"foo\"), new SystemStream(\"otherKafka\", \"bar\"), new SystemStream(\"kafka\", \"bar\"), new SystemStream(\"otherKafka\", \"foo\"));\r\n    assertEquals(expected, new TaskConfig(config).getAllInputStreams());\r\n    Config configOnlyBroadcast = new MapConfig(ImmutableMap.of(TaskConfig.BROADCAST_INPUT_STREAMS, \"kafka.bar#4, otherKafka.foo#5\"));\r\n    Set<SystemStream> expectedOnlyBroadcast = ImmutableSet.of(new SystemStream(\"kafka\", \"bar\"), new SystemStream(\"otherKafka\", \"foo\"));\r\n    assertEquals(expectedOnlyBroadcast, new TaskConfig(configOnlyBroadcast).getAllInputStreams());\r\n    Config configOnlyInputs = new MapConfig(ImmutableMap.of(TaskConfig.INPUT_STREAMS, \"kafka.foo, otherKafka.bar\"));\r\n    Set<SystemStream> expectedOnlyInputs = ImmutableSet.of(new SystemStream(\"kafka\", \"foo\"), new SystemStream(\"otherKafka\", \"bar\"));\r\n    assertEquals(expectedOnlyInputs, new TaskConfig(configOnlyInputs).getAllInputStreams());\r\n    assertTrue(new TaskConfig(new MapConfig()).getAllInputStreams().isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetShutdownMs",
  "sourceCode" : "@Test\r\npublic void testGetShutdownMs() {\r\n    Config config = new MapConfig(ImmutableMap.of(TaskConfig.TASK_SHUTDOWN_MS, \"10\"));\r\n    assertEquals(10, new TaskConfig(config).getShutdownMs());\r\n    // unable to parse value into number\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.TASK_SHUTDOWN_MS, \"not a number\"));\r\n    assertEquals(TaskConfig.DEFAULT_TASK_SHUTDOWN_MS, new TaskConfig(config).getShutdownMs());\r\n    // config not specified\r\n    assertEquals(TaskConfig.DEFAULT_TASK_SHUTDOWN_MS, new TaskConfig(new MapConfig()).getShutdownMs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\config\\TestTaskConfig.java",
  "methodName" : "testGetTransactionalStateRestoreEnabled",
  "sourceCode" : "@Test\r\npublic void testGetTransactionalStateRestoreEnabled() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RESTORE_ENABLED, \"true\");\r\n    // standby and async commit both off; transactional state restore returned as enabled\r\n    assertTrue(new TaskConfig(new MapConfig(configMap)).getTransactionalStateRestoreEnabled());\r\n    // standby off and async commit on; transactional state restore returned as disabled\r\n    configMap.put(TaskConfig.ASYNC_COMMIT, \"true\");\r\n    configMap.put(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, \"1\");\r\n    assertFalse(new TaskConfig(new MapConfig(configMap)).getTransactionalStateRestoreEnabled());\r\n    // standby on and async commit off; transactional state restore returned as disabled\r\n    configMap.put(TaskConfig.ASYNC_COMMIT, \"false\");\r\n    configMap.put(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, \"2\");\r\n    assertFalse(new TaskConfig(new MapConfig(configMap)).getTransactionalStateRestoreEnabled());\r\n    // standby on and async commit on; transactional state restore returned as disabled\r\n    configMap.put(TaskConfig.ASYNC_COMMIT, \"true\");\r\n    configMap.put(JobConfig.STANDBY_TASKS_REPLICATION_FACTOR, \"2\");\r\n    assertFalse(new TaskConfig(new MapConfig(configMap)).getTransactionalStateRestoreEnabled());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testConstruction",
  "sourceCode" : "@Test\r\npublic void testConstruction() {\r\n    final WatermarkDiskQuotaPolicy.Entry policy = new WatermarkDiskQuotaPolicy.Entry(ARBITRARY_LOW_WATER_MARK, ARBITRARY_HIGH_WATER_MARK, ARBITRARY_WORK_FACTOR);\r\n    assertEquals(ARBITRARY_LOW_WATER_MARK, policy.getLowWaterMarkPercent());\r\n    assertEquals(ARBITRARY_HIGH_WATER_MARK, policy.getHighWaterMarkPercent());\r\n    assertEquals(ARBITRARY_WORK_FACTOR, policy.getWorkFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testLowWaterMarkGreaterThanHighWaterMark",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testLowWaterMarkGreaterThanHighWaterMark() {\r\n    new WatermarkDiskQuotaPolicy.Entry(1.0, ARBITRARY_HIGH_WATER_MARK, ARBITRARY_WORK_FACTOR);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testLowWaterMarkBelowZero",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testLowWaterMarkBelowZero() {\r\n    new WatermarkDiskQuotaPolicy.Entry(-1.0, ARBITRARY_HIGH_WATER_MARK, ARBITRARY_WORK_FACTOR);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testHighWaterMarkAboveOne",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testHighWaterMarkAboveOne() {\r\n    new WatermarkDiskQuotaPolicy.Entry(ARBITRARY_LOW_WATER_MARK, 2.0, ARBITRARY_WORK_FACTOR);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testWorkFactorOfZero",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWorkFactorOfZero() {\r\n    new WatermarkDiskQuotaPolicy.Entry(ARBITRARY_LOW_WATER_MARK, ARBITRARY_HIGH_WATER_MARK, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestDiskQuotaPolicyEntry.java",
  "methodName" : "testWorkFactorGreaterThanOne",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWorkFactorGreaterThanOne() {\r\n    new WatermarkDiskQuotaPolicy.Entry(ARBITRARY_LOW_WATER_MARK, ARBITRARY_HIGH_WATER_MARK, 2.0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfSingleFile",
  "sourceCode" : "@Test\r\npublic void testSizeOfSingleFile() throws IOException {\r\n    writeFile(testDir, \"single-file\", new byte[1024]);\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(testDir)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfDisjointDirectoriesFromRoot",
  "sourceCode" : "@Test\r\npublic void testSizeOfDisjointDirectoriesFromRoot() throws IOException {\r\n    Path child1Dir = createDirectory(testDir, \"child1\");\r\n    writeFile(child1Dir, \"foo\", new byte[1024]);\r\n    Path child2Dir = createDirectory(testDir, \"child2\");\r\n    writeFile(child2Dir, \"bar\", new byte[4096]);\r\n    assertEquals(1024 + 4096, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(testDir)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfDisjointDirectoriesFromChildDirs",
  "sourceCode" : "@Test\r\npublic void testSizeOfDisjointDirectoriesFromChildDirs() throws IOException {\r\n    Path child1Dir = createDirectory(testDir, \"child1\");\r\n    writeFile(child1Dir, \"foo\", new byte[1024]);\r\n    Path child2Dir = createDirectory(testDir, \"child2\");\r\n    writeFile(child2Dir, \"bar\", new byte[4096]);\r\n    Set<Path> pathSet = new HashSet<>(Arrays.asList(child1Dir, child2Dir));\r\n    assertEquals(1024 + 4096, PollingScanDiskSpaceMonitor.getSpaceUsed(pathSet));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfOverlappedDirectories",
  "sourceCode" : "@Test\r\npublic void testSizeOfOverlappedDirectories() throws IOException {\r\n    Path childDir = createDirectory(testDir, \"child\");\r\n    writeFile(childDir, \"foo\", new byte[1024]);\r\n    Path grandchildDir = createDirectory(childDir, \"grandchild\");\r\n    writeFile(grandchildDir, \"bar\", new byte[4096]);\r\n    // If getSpaceUsed were not handling overlapping directories we would expect to count\r\n    // grandchild twice, which would give us the erroneous total `1024 + 4096 * 2`.\r\n    Set<Path> pathSet = new HashSet<>(Arrays.asList(childDir, grandchildDir));\r\n    assertEquals(1024 + 4096, PollingScanDiskSpaceMonitor.getSpaceUsed(pathSet));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfDirectoryAccessedWithDifferentPaths",
  "sourceCode" : "@Test\r\npublic void testSizeOfDirectoryAccessedWithDifferentPaths() throws IOException {\r\n    Path childDir = createDirectory(testDir, \"child1\");\r\n    writeFile(childDir, \"foo\", new byte[1024]);\r\n    Path otherPath = childDir.resolve(\"..\").resolve(childDir.getFileName());\r\n    Set<Path> pathSet = new HashSet<>(Arrays.asList(childDir, otherPath));\r\n    // This test actually verifies that !childDir.equals(otherPath) and ensures that we properly\r\n    // handle duplicate paths to the same directory.\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(pathSet));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfAlreadyCountedSymlinkedFile",
  "sourceCode" : "@Test\r\npublic void testSizeOfAlreadyCountedSymlinkedFile() throws IOException {\r\n    writeFile(testDir, \"regular-file\", new byte[1024]);\r\n    Files.createSymbolicLink(testDir.resolve(\"symlink\"), testDir.resolve(\"regular-file\"));\r\n    // We should not double count a symlinked file\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(testDir)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testSizeOfUncountedSymlinkedFile",
  "sourceCode" : "@Test\r\npublic void testSizeOfUncountedSymlinkedFile() throws IOException {\r\n    Path childDir = createDirectory(testDir, \"child\");\r\n    writeFile(testDir, \"regular-file\", new byte[1024]);\r\n    Files.createSymbolicLink(childDir.resolve(\"symlink\"), testDir.resolve(\"regular-file\"));\r\n    // We should count the space of the symlinked file even thought it is outside of the root\r\n    // from which we started.\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(testDir)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testFollowSymlinkedDirectory",
  "sourceCode" : "@Test\r\npublic void testFollowSymlinkedDirectory() throws IOException {\r\n    Path childDir = createDirectory(testDir, \"child\");\r\n    writeFile(childDir, \"regular-file\", new byte[1024]);\r\n    Path dirSymlink = testDir.resolve(\"symlink\");\r\n    Files.createSymbolicLink(dirSymlink, childDir);\r\n    // We should follow the symlink and read the symlinked file\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(dirSymlink)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testHandleCyclicalSymlink",
  "sourceCode" : "@Test\r\npublic void testHandleCyclicalSymlink() throws IOException {\r\n    Path childDir = createDirectory(testDir, \"child\");\r\n    writeFile(childDir, \"regular-file\", new byte[1024]);\r\n    Files.createSymbolicLink(childDir.resolve(\"symlink\"), testDir);\r\n    // We have testDir/childDir/symlink -> testDir, which effectively creates a cycle.\r\n    assertEquals(1024, PollingScanDiskSpaceMonitor.getSpaceUsed(Collections.singleton(childDir)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testMissingDirectory",
  "sourceCode" : "@Test\r\npublic void testMissingDirectory() throws IOException {\r\n    Set<Path> pathSet = Collections.singleton(testDir.resolve(\"non-existant-child\"));\r\n    assertEquals(0, PollingScanDiskSpaceMonitor.getSpaceUsed(pathSet));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testGetSamplesFromListener",
  "sourceCode" : "@Test\r\npublic void testGetSamplesFromListener() throws IOException, InterruptedException {\r\n    writeFile(testDir, \"single-file\", new byte[1024]);\r\n    final AtomicLong sample = new AtomicLong();\r\n    final CountDownLatch sampleReady = new CountDownLatch(1);\r\n    final PollingScanDiskSpaceMonitor monitor = new PollingScanDiskSpaceMonitor(Collections.singleton(testDir), 50);\r\n    monitor.registerListener(new DiskSpaceMonitor.Listener() {\r\n\r\n        @Override\r\n        public void onUpdate(long diskUsageSample) {\r\n            sample.set(diskUsageSample);\r\n            sampleReady.countDown();\r\n        }\r\n    });\r\n    monitor.start();\r\n    try {\r\n        if (!sampleReady.await(5, TimeUnit.SECONDS)) {\r\n            fail(\"Timed out waiting for listener to be provide disk usage sample\");\r\n        }\r\n        assertEquals(1024, sample.get());\r\n    } finally {\r\n        monitor.stop();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestPollingScanDiskSpaceMonitor.java",
  "methodName" : "testStartStop",
  "sourceCode" : "@Test\r\npublic void testStartStop() throws IOException, InterruptedException {\r\n    writeFile(testDir, \"single-file\", new byte[1024]);\r\n    final int numSamplesToCollect = 5;\r\n    final AtomicInteger numCallbackInvocations = new AtomicInteger();\r\n    final CountDownLatch doneLatch = new CountDownLatch(1);\r\n    final PollingScanDiskSpaceMonitor monitor = new PollingScanDiskSpaceMonitor(Collections.singleton(testDir), 50);\r\n    monitor.registerListener(new DiskSpaceMonitor.Listener() {\r\n\r\n        @Override\r\n        public void onUpdate(long diskUsageSample) {\r\n            if (numCallbackInvocations.incrementAndGet() == numSamplesToCollect) {\r\n                monitor.stop();\r\n                doneLatch.countDown();\r\n            }\r\n        }\r\n    });\r\n    monitor.start();\r\n    try {\r\n        if (!doneLatch.await(5, TimeUnit.SECONDS)) {\r\n            fail(String.format(\"Timed out waiting for listener to be give %d updates\", numSamplesToCollect));\r\n        }\r\n        if (!monitor.awaitTermination(5, TimeUnit.SECONDS)) {\r\n            fail(\"Timed out waiting for monitor to terminate\");\r\n        }\r\n        // A number larger than numSamplesToCollect indicates that we got a callback after we stopped\r\n        // the monitor. We should safely be able to assert this will not happen as we stopped the\r\n        // monitor in the the thread on which it is delivering notifications.\r\n        assertEquals(numSamplesToCollect, numCallbackInvocations.get());\r\n    } finally {\r\n        monitor.stop();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testNoEntries",
  "sourceCode" : "@Test\r\npublic void testNoEntries() {\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(Collections.<WatermarkDiskQuotaPolicy.Entry>emptyList());\r\n    assertEquals(1.0, policy.apply(1.0));\r\n    assertEquals(1.0, policy.apply(0.5));\r\n    assertEquals(1.0, policy.apply(0.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testOneEntryUntriggered",
  "sourceCode" : "@Test\r\npublic void testOneEntryUntriggered() {\r\n    double workFactor = 0.5f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Collections.singletonList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(1.0, policy.apply(1.0));\r\n    assertEquals(1.0, policy.apply(0.75));\r\n    assertEquals(1.0, policy.apply(0.5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testOneEntryTriggered",
  "sourceCode" : "@Test\r\npublic void testOneEntryTriggered() {\r\n    double workFactor = 0.5f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Collections.singletonList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(workFactor, policy.apply(0.25));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testTwoEntriesTriggered",
  "sourceCode" : "@Test\r\npublic void testTwoEntriesTriggered() {\r\n    double workFactor1 = 0.5f;\r\n    double workFactor2 = 0.25f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor1), new WatermarkDiskQuotaPolicy.Entry(0.2, 0.4, workFactor2));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(1.0, policy.apply(0.5));\r\n    assertEquals(workFactor1, policy.apply(0.4));\r\n    assertEquals(workFactor2, policy.apply(0.1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testTwoEntriesTriggeredSkipFirst",
  "sourceCode" : "@Test\r\npublic void testTwoEntriesTriggeredSkipFirst() {\r\n    double workFactor1 = 0.5f;\r\n    double workFactor2 = 0.25f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor1), new WatermarkDiskQuotaPolicy.Entry(0.2, 0.4, workFactor2));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(workFactor2, policy.apply(0.1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testTwoEntriesReversedOrder",
  "sourceCode" : "@Test\r\npublic void testTwoEntriesReversedOrder() {\r\n    // Results should be the same regardless of order as we sort policies at construction time.\r\n    double workFactor1 = 0.5f;\r\n    double workFactor2 = 0.25f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.2, 0.4, workFactor2), new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor1));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(workFactor2, policy.apply(0.1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testTriggerEntriesAndRecover",
  "sourceCode" : "@Test\r\npublic void testTriggerEntriesAndRecover() {\r\n    double workFactor1 = 0.5f;\r\n    double workFactor2 = 0.25f;\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, workFactor1), new WatermarkDiskQuotaPolicy.Entry(0.2, 0.4, workFactor2));\r\n    WatermarkDiskQuotaPolicy policy = new WatermarkDiskQuotaPolicy(entries);\r\n    assertEquals(workFactor2, policy.apply(0.1));\r\n    assertEquals(workFactor1, policy.apply(0.4));\r\n    assertEquals(1.0, policy.apply(1.0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testWorkFactorTooHigh",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWorkFactorTooHigh() {\r\n    new WatermarkDiskQuotaPolicy(Collections.singletonList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, 1.5)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testWorkFactorTooLow",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWorkFactorTooLow() {\r\n    new WatermarkDiskQuotaPolicy(Collections.singletonList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, -1.0)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testRaiseWorkFactorWithLowerThreshold",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testRaiseWorkFactorWithLowerThreshold() {\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, 0.5), new WatermarkDiskQuotaPolicy.Entry(0.2, 0.4, 0.75));\r\n    new WatermarkDiskQuotaPolicy(entries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testDuplicatedRange",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testDuplicatedRange() {\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, 0.5), new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, 0.75));\r\n    new WatermarkDiskQuotaPolicy(entries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testFullyOverlappedRange1",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFullyOverlappedRange1() {\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 1.0, 0.5), new WatermarkDiskQuotaPolicy.Entry(0.25, 1.0, 0.75));\r\n    new WatermarkDiskQuotaPolicy(entries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\disk\\TestWatermarkDiskQuotaPolicy.java",
  "methodName" : "testFullyOverlappedRange2",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFullyOverlappedRange2() {\r\n    List<WatermarkDiskQuotaPolicy.Entry> entries = Arrays.asList(new WatermarkDiskQuotaPolicy.Entry(0.5, 0.75, 0.75), new WatermarkDiskQuotaPolicy.Entry(0.25, 1.0, 0.5));\r\n    new WatermarkDiskQuotaPolicy(entries);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestAllSspToSingleTaskGrouper.java",
  "methodName" : "testLocalStreamGroupedCorrectlyForYarn",
  "sourceCode" : "@Test\r\npublic void testLocalStreamGroupedCorrectlyForYarn() {\r\n    HashSet<SystemStreamPartition> allSSPs = new HashSet<>();\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"job.container.count\", \"2\");\r\n    configMap.put(\"processor.list\", \"0,1\");\r\n    Config config = new MapConfig(configMap);\r\n    SystemStreamPartitionGrouper grouper = grouperFactory.getSystemStreamPartitionGrouper(config);\r\n    Collections.addAll(allSSPs, aa0, aa1, aa2, ab0);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(allSSPs);\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = new HashMap<>();\r\n    HashSet<SystemStreamPartition> partitions = new HashSet<>();\r\n    partitions.add(aa0);\r\n    partitions.add(aa1);\r\n    partitions.add(aa2);\r\n    partitions.add(ab0);\r\n    expectedResult.put(new TaskName(\"Task-0\"), partitions);\r\n    expectedResult.put(new TaskName(\"Task-1\"), partitions);\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestAllSspToSingleTaskGrouper.java",
  "methodName" : "testLocalStreamGroupedCorrectlyForPassthru",
  "sourceCode" : "@Test\r\npublic void testLocalStreamGroupedCorrectlyForPassthru() {\r\n    HashSet<SystemStreamPartition> allSSPs = new HashSet<>();\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"job.coordinator.factory\", \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configMap.put(\"processor.id\", \"1\");\r\n    configMap.put(\"processor.list\", configMap.get(\"processor.id\"));\r\n    Config config = new MapConfig(configMap);\r\n    SystemStreamPartitionGrouper grouper = grouperFactory.getSystemStreamPartitionGrouper(config);\r\n    Collections.addAll(allSSPs, aa0, aa1, aa2, ab0);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(allSSPs);\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = new HashMap<>();\r\n    HashSet<SystemStreamPartition> partitions = new HashSet<>();\r\n    partitions.add(aa0);\r\n    partitions.add(aa1);\r\n    partitions.add(aa2);\r\n    partitions.add(ab0);\r\n    expectedResult.put(new TaskName(\"Task-1\"), partitions);\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestAllSspToSingleTaskGrouper.java",
  "methodName" : "testLocalStreamWithEmptySsps",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testLocalStreamWithEmptySsps() {\r\n    HashSet<SystemStreamPartition> allSSPs = new HashSet<>();\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"job.coordinator.factory\", \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configMap.put(\"processor.list\", \"1\");\r\n    Config config = new MapConfig(configMap);\r\n    SystemStreamPartitionGrouper grouper = grouperFactory.getSystemStreamPartitionGrouper(config);\r\n    grouper.group(allSSPs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestAllSspToSingleTaskGrouper.java",
  "methodName" : "testLocalStreamWithBroadcastStream",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testLocalStreamWithBroadcastStream() {\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"task.broadcast.inputs\", \"test.stream#0\");\r\n    configMap.put(\"processor.list\", \"1\");\r\n    Config config = new MapConfig(configMap);\r\n    grouperFactory.getSystemStreamPartitionGrouper(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartition.java",
  "methodName" : "testLocalStreamsGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testLocalStreamsGroupedCorrectly() {\r\n    GroupByPartition grouper = new GroupByPartition(new MapConfig());\r\n    Map<TaskName, Set<SystemStreamPartition>> emptyResult = grouper.group(new HashSet<>());\r\n    assertTrue(emptyResult.isEmpty());\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ab1, ab2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableSet.of(aa0, ac0)).put(new TaskName(\"Partition 1\"), ImmutableSet.of(aa1, ab1)).put(new TaskName(\"Partition 2\"), ImmutableSet.of(aa2, ab2)).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartition.java",
  "methodName" : "testBroadcastStreamsGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testBroadcastStreamsGroupedCorrectly() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"task.broadcast.inputs\", \"SystemA.StreamA#0, SystemA.StreamB#1\"));\r\n    GroupByPartition grouper = new GroupByPartition(config);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ab1, ab2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableSet.of(aa0, ac0, ab1)).put(new TaskName(\"Partition 1\"), ImmutableSet.of(aa1, aa0, ab1)).put(new TaskName(\"Partition 2\"), ImmutableSet.of(aa2, aa0, ab2, ab1)).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartition.java",
  "methodName" : "testNoTaskOnlyContainsBroadcastStreams",
  "sourceCode" : "@Test\r\npublic void testNoTaskOnlyContainsBroadcastStreams() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"task.broadcast.inputs\", \"SystemA.StreamA#0, SystemA.StreamB#1\"));\r\n    GroupByPartition grouper = new GroupByPartition(config);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, ab1, ab2));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 2\"), ImmutableSet.of(aa0, ab1, ab2)).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartition.java",
  "methodName" : "testElastictyEnabledLocalStreamGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testElastictyEnabledLocalStreamGroupedCorrectly() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.elasticity.factor\", \"2\"));\r\n    GroupByPartition grouper = new GroupByPartition(config);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ab1, ab2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0 0\"), ImmutableSet.of(new SystemStreamPartition(aa0, 0), new SystemStreamPartition(ac0, 0))).put(new TaskName(\"Partition 0 1\"), ImmutableSet.of(new SystemStreamPartition(aa0, 1), new SystemStreamPartition(ac0, 1))).put(new TaskName(\"Partition 1 0\"), ImmutableSet.of(new SystemStreamPartition(aa1, 0), new SystemStreamPartition(ab1, 0))).put(new TaskName(\"Partition 1 1\"), ImmutableSet.of(new SystemStreamPartition(aa1, 1), new SystemStreamPartition(ab1, 1))).put(new TaskName(\"Partition 2 0\"), ImmutableSet.of(new SystemStreamPartition(aa2, 0), new SystemStreamPartition(ab2, 0))).put(new TaskName(\"Partition 2 1\"), ImmutableSet.of(new SystemStreamPartition(aa2, 1), new SystemStreamPartition(ab2, 1))).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartitionWithGrouperProxy.java",
  "methodName" : "testSingleStreamRepartitioning",
  "sourceCode" : "@Test\r\npublic void testSingleStreamRepartitioning() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithSingleStream = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupByPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithSingleStream, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupByPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartitionWithGrouperProxy.java",
  "methodName" : "testMultipleStreamsWithSingleStreamRepartitioning",
  "sourceCode" : "@Test\r\npublic void testMultipleStreamsWithSingleStreamRepartitioning() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    IntStream.range(0, 4).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(partitionId))));\r\n    // New stream added to previous streams\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupByPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupByPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartitionWithGrouperProxy.java",
  "methodName" : "testRemovalOfPreviousStreamAndThenAddNewStream",
  "sourceCode" : "@Test\r\npublic void testRemovalOfPreviousStreamAndThenAddNewStream() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    // expected Grouping\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStatefulAndStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupByPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStatefulAndStateless, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupByPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStatefulAndStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartitionWithGrouperProxy.java",
  "methodName" : "testRemovalAndAdditionOfStreamsWithExpansion",
  "sourceCode" : "@Test\r\npublic void testRemovalAndAdditionOfStreamsWithExpansion() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))));\r\n    // expected grouping\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).build();\r\n    // expected grouping\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupByPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupByPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupByPartitionWithGrouperProxy.java",
  "methodName" : "testMultipleStreamRepartitioningWithNewStreams",
  "sourceCode" : "@Test\r\npublic void testMultipleStreamRepartitioningWithNewStreams() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"Partition 0\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"Partition 1\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"Partition 2\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"Partition 3\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = new HashSet<>();\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))));\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(partitionId))));\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"Partition 1\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"Partition 3\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"Partition 2\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"Partition 5\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(5)))).put(new TaskName(\"Partition 4\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(4)))).put(new TaskName(\"Partition 7\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(7)))).put(new TaskName(\"Partition 6\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(6)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupByPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupByPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupByPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartition.java",
  "methodName" : "testLocalStreamGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testLocalStreamGroupedCorrectly() {\r\n    SystemStreamPartitionGrouper grouper = grouperFactory.getSystemStreamPartitionGrouper(new MapConfig());\r\n    Map<TaskName, Set<SystemStreamPartition>> emptyResult = grouper.group(new HashSet<>());\r\n    assertTrue(emptyResult.isEmpty());\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(aa0.toString()), ImmutableSet.of(aa0)).put(new TaskName(aa1.toString()), ImmutableSet.of(aa1)).put(new TaskName(aa2.toString()), ImmutableSet.of(aa2)).put(new TaskName(ac0.toString()), ImmutableSet.of(ac0)).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartition.java",
  "methodName" : "testBroadcastStreamGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testBroadcastStreamGroupedCorrectly() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"task.broadcast.inputs\", \"SystemA.StreamA#0\"));\r\n    SystemStreamPartitionGrouper grouper = new GroupBySystemStreamPartition(config);\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(aa1.toString()), ImmutableSet.of(aa1, aa0)).put(new TaskName(aa2.toString()), ImmutableSet.of(aa2, aa0)).put(new TaskName(ac0.toString()), ImmutableSet.of(ac0, aa0)).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartition.java",
  "methodName" : "testElasticityEnabledLocalStreamGroupedCorrectly",
  "sourceCode" : "@Test\r\npublic void testElasticityEnabledLocalStreamGroupedCorrectly() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.elasticity.factor\", \"2\"));\r\n    SystemStreamPartitionGrouper grouper = grouperFactory.getSystemStreamPartitionGrouper(config);\r\n    Map<TaskName, Set<SystemStreamPartition>> emptyResult = grouper.group(new HashSet<>());\r\n    assertTrue(emptyResult.isEmpty());\r\n    Map<TaskName, Set<SystemStreamPartition>> result = grouper.group(ImmutableSet.of(aa0, aa1, aa2, ac0));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedResult = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(new SystemStreamPartition(aa0, 0).toString()), ImmutableSet.of(new SystemStreamPartition(aa0, 0))).put(new TaskName(new SystemStreamPartition(aa0, 1).toString()), ImmutableSet.of(new SystemStreamPartition(aa0, 1))).put(new TaskName(new SystemStreamPartition(aa1, 0).toString()), ImmutableSet.of(new SystemStreamPartition(aa1, 0))).put(new TaskName(new SystemStreamPartition(aa1, 1).toString()), ImmutableSet.of(new SystemStreamPartition(aa1, 1))).put(new TaskName(new SystemStreamPartition(aa2, 0).toString()), ImmutableSet.of(new SystemStreamPartition(aa2, 0))).put(new TaskName(new SystemStreamPartition(aa2, 1).toString()), ImmutableSet.of(new SystemStreamPartition(aa2, 1))).put(new TaskName(new SystemStreamPartition(ac0, 0).toString()), ImmutableSet.of(new SystemStreamPartition(ac0, 0))).put(new TaskName(new SystemStreamPartition(ac0, 1).toString()), ImmutableSet.of(new SystemStreamPartition(ac0, 1))).build();\r\n    assertEquals(expectedResult, result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartitionWithGrouperProxy.java",
  "methodName" : "testSingleStreamRepartitioning",
  "sourceCode" : "@Test\r\npublic void testSingleStreamRepartitioning() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithSingleStream = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupBySystemStreamPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithSingleStream, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupBySystemStreamPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartitionWithGrouperProxy.java",
  "methodName" : "testMultipleStreamsWithSingleStreamExpansionAndNewStream",
  "sourceCode" : "@Test\r\npublic void testMultipleStreamsWithSingleStreamExpansionAndNewStream() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))));\r\n    IntStream.range(0, 4).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(partitionId))));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupBySystemStreamPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupBySystemStreamPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartitionWithGrouperProxy.java",
  "methodName" : "testRemovalOfPreviousStreamsAndThenAddNewStream",
  "sourceCode" : "@Test\r\npublic void testRemovalOfPreviousStreamsAndThenAddNewStream() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStatefulAndStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupBySystemStreamPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStatefulAndStateless, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupBySystemStreamPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStatefulAndStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartitionWithGrouperProxy.java",
  "methodName" : "testRemovalAndAdditionOfStreamsWithExpansion",
  "sourceCode" : "@Test\r\npublic void testRemovalAndAdditionOfStreamsWithExpansion() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupBySystemStreamPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupBySystemStreamPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\stream\\TestGroupBySystemStreamPartitionWithGrouperProxy.java",
  "methodName" : "testMultipleStreamExpansionWithNewStreams",
  "sourceCode" : "@Test\r\npublic void testMultipleStreamExpansionWithNewStreams() {\r\n    Map<TaskName, List<SystemStreamPartition>> prevGroupingWithMultipleStreams = ImmutableMap.<TaskName, List<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableList.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).build();\r\n    Set<SystemStreamPartition> currSsps = IntStream.range(0, 8).mapToObj(partitionId -> new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(partitionId))).collect(Collectors.toSet());\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(partitionId))));\r\n    IntStream.range(0, 8).forEach(partitionId -> currSsps.add(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(partitionId))));\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateful = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)), new SystemStreamPartition(\"kafka\", \"URE\", new Partition(4)))).build();\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedGroupingForStateless = ImmutableMap.<TaskName, Set<SystemStreamPartition>>builder().put(new TaskName(\"SystemStreamPartition [kafka, BOB, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, BOB, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"BOB\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, PVE, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"PVE\", new Partition(7)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 1]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(1)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 2]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(2)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 3]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(3)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 0]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(0)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 4]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(4)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 5]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(5)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 6]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(6)))).put(new TaskName(\"SystemStreamPartition [kafka, URE, 7]\"), ImmutableSet.of(new SystemStreamPartition(\"kafka\", \"URE\", new Partition(7)))).build();\r\n    // SSPGrouperProxy for stateful job\r\n    SSPGrouperProxy groupBySystemStreamPartition = buildSspGrouperProxy(true);\r\n    GrouperMetadata grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), prevGroupingWithMultipleStreams, new HashMap<>());\r\n    Map<TaskName, Set<SystemStreamPartition>> finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateful, finalGrouping);\r\n    // SSPGrouperProxy for stateless job\r\n    groupBySystemStreamPartition = buildSspGrouperProxy(false);\r\n    finalGrouping = groupBySystemStreamPartition.group(currSsps, grouperMetadata);\r\n    Assert.assertEquals(expectedGroupingForStateless, finalGrouping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupEmptyTasks",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGroupEmptyTasks() {\r\n    new GroupByContainerCount(1).group(new HashSet<>());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupFewerTasksThanContainers",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGroupFewerTasksThanContainers() {\r\n    Set<TaskModel> taskModels = new HashSet<>();\r\n    taskModels.add(getTaskModel(1));\r\n    new GroupByContainerCount(2).group(taskModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGrouperResultImmutable",
  "sourceCode" : "@Test(expected = UnsupportedOperationException.class)\r\npublic void testGrouperResultImmutable() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(3).group(taskModels);\r\n    containers.remove(containers.iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupHappyPath",
  "sourceCode" : "@Test\r\npublic void testGroupHappyPath() {\r\n    Set<TaskModel> taskModels = generateTaskModels(5);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupManyTasks",
  "sourceCode" : "@Test\r\npublic void testGroupManyTasks() {\r\n    Set<TaskModel> taskModels = generateTaskModels(21);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(11, container0.getTasks().size());\r\n    assertEquals(10, container1.getTasks().size());\r\n    // NOTE: tasks are sorted lexicographically, so the container assignment\r\n    // can seem odd, but the consistency is the key focus\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(10)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(12)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(14)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(16)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(18)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(3)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(7)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(9)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(11)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(13)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(15)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(17)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(19)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(20)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerAfterContainerIncrease",
  "sourceCode" : "/**\r\n * Before:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *  T2  T3\r\n *  T4  T5\r\n *  T6  T7\r\n *  T8\r\n *\r\n * After:\r\n *  C0  C1  C2  C3\r\n * ----------------\r\n *  T0  T1  T6  T5\r\n *  T2  T3  T8  T7\r\n *  T4\r\n *\r\n *  NOTE for host affinity, it would help to have some additional logic to reassign tasks\r\n *  from C0 and C1 to containers that were on the same respective hosts, it wasn't implemented\r\n *  because the scenario is infrequent, the benefits are not guaranteed, and the code complexity\r\n *  wasn't worth it. It certainly could be implemented in the future.\r\n */\r\n@Test\r\npublic void testBalancerAfterContainerIncrease() {\r\n    Set<TaskModel> taskModels = generateTaskModels(9);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(4).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(4, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    ContainerModel container2 = containersMap.get(\"2\");\r\n    ContainerModel container3 = containersMap.get(\"3\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertNotNull(container2);\r\n    assertNotNull(container3);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertEquals(2, container2.getTasks().size());\r\n    assertEquals(2, container3.getTasks().size());\r\n    // Tasks 0-4 should stay on the same original containers\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n    // Tasks 5-8 should be reassigned to the new containers.\r\n    // Consistency is the goal with these reassignments\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(8)));\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container3.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container3.getTasks().containsKey(getTaskName(7)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerAfterContainerDecrease",
  "sourceCode" : "/**\r\n * Before:\r\n *  C0  C1  C2  C3\r\n * ----------------\r\n *  T0  T1  T2  T3\r\n *  T4  T5  T6  T7\r\n *  T8\r\n *\r\n * After:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *  T4  T5\r\n *  T8  T7\r\n *  T6  T3\r\n *  T2\r\n *\r\n *  NOTE for host affinity, it would help to have some additional logic to reassign tasks\r\n *  from C2 and C3 to containers that were on the same respective hosts, it wasn't implemented\r\n *  because the scenario is infrequent, the benefits are not guaranteed, and the code complexity\r\n *  wasn't worth it. It certainly could be implemented in the future.\r\n */\r\n@Test\r\npublic void testBalancerAfterContainerDecrease() {\r\n    Set<TaskModel> taskModels = generateTaskModels(9);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(4).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(5, container0.getTasks().size());\r\n    assertEquals(4, container1.getTasks().size());\r\n    // Tasks 0,4,8 and 1,5 should stay on the same original containers\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(8)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(5)));\r\n    // Tasks 2,6 and 3,7 should be reassigned to the new containers.\r\n    // Consistency is the goal with these reassignments\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(7)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerMultipleReblances",
  "sourceCode" : "/**\r\n * Before:\r\n *  C0  C1  C2  C3\r\n * ----------------\r\n *  T0  T1  T2  T3\r\n *  T4  T5  T6  T7\r\n *  T8\r\n *\r\n * Intermediate:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *  T4  T5\r\n *  T8  T7\r\n *  T6  T3\r\n *  T2\r\n *\r\n *  After:\r\n *  C0  C1  C2\r\n * ------------\r\n *  T0  T1  T6\r\n *  T4  T5  T2\r\n *  T8  T7  T3\r\n */\r\n@Test\r\npublic void testBalancerMultipleReblances() {\r\n    // Before\r\n    Set<TaskModel> taskModels = generateTaskModels(9);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(4).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    // First balance\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(5, container0.getTasks().size());\r\n    assertEquals(4, container1.getTasks().size());\r\n    // Tasks 0,4,8 and 1,5 should stay on the same original containers\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(8)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(5)));\r\n    // Tasks 2,6 and 3,7 should be reassigned to the new containers.\r\n    // Consistency is the goal with these reassignments\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(7)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n    // Second balance\r\n    prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata1 = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    containers = new GroupByContainerCount(3).group(taskModels, grouperMetadata1);\r\n    containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(3, containers.size());\r\n    container0 = containersMap.get(\"0\");\r\n    container1 = containersMap.get(\"1\");\r\n    ContainerModel container2 = containersMap.get(\"2\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertNotNull(container2);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(\"2\", container2.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(3, container1.getTasks().size());\r\n    assertEquals(3, container2.getTasks().size());\r\n    // Tasks 0,4,8 and 1,5,7 should stay on the same original containers\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(8)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(7)));\r\n    // Tasks 2,6 and 3 should be reassigned to the new container.\r\n    // Consistency is the goal with these reassignments\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerAfterContainerSame",
  "sourceCode" : "/**\r\n * Before:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *  T2  T3\r\n *  T4  T5\r\n *  T6  T7\r\n *  T8\r\n *\r\n *  After:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *  T2  T3\r\n *  T4  T5\r\n *  T6  T7\r\n *  T8\r\n */\r\n@Test\r\npublic void testBalancerAfterContainerSame() {\r\n    Set<TaskModel> taskModels = generateTaskModels(9);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(5, container0.getTasks().size());\r\n    assertEquals(4, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(8)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(7)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerAfterContainerSameCustomAssignment",
  "sourceCode" : "/**\r\n * Verifies the ability to have a custom task-container mapping that is *deliberately* unbalanced.\r\n *\r\n * Before:\r\n *  C0  C1\r\n * --------\r\n *  T0  T6\r\n *  T1  T7\r\n *  T2  T8\r\n *  T3\r\n *  T4\r\n *  T5\r\n *\r\n *  After:\r\n *  C0  C1\r\n * --------\r\n *  T0  T6\r\n *  T1  T7\r\n *  T2  T8\r\n *  T3\r\n *  T4\r\n *  T5\r\n */\r\n@Test\r\npublic void testBalancerAfterContainerSameCustomAssignment() {\r\n    Set<TaskModel> taskModels = generateTaskModels(9);\r\n    Map<TaskName, String> prevTaskToContainerMapping = new HashMap<>();\r\n    prevTaskToContainerMapping.put(getTaskName(0), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(1), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(2), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(3), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(4), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(5), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(6), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(7), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(8), \"1\");\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(6, container0.getTasks().size());\r\n    assertEquals(3, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(3)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(7)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerAfterContainerSameCustomAssignmentAndContainerIncrease",
  "sourceCode" : "/**\r\n * Verifies the ability to have a custom task-container mapping that is *deliberately* unbalanced.\r\n *\r\n * Before:\r\n *  C0  C1\r\n * --------\r\n *  T0  T1\r\n *      T2\r\n *      T3\r\n *      T4\r\n *      T5\r\n *\r\n *  After:\r\n *  C0  C1  C2\r\n * ------------\r\n *  T0  T1  T4\r\n *  T5  T2  T3\r\n *\r\n *  The key here is that C0, which is not one of the new containers was under-allocated.\r\n *  This is an important case because this scenario, while impossible with GroupByContainerCount.group()\r\n *  could occur when the grouper class is switched or if there is a custom mapping.\r\n */\r\n@Test\r\npublic void testBalancerAfterContainerSameCustomAssignmentAndContainerIncrease() {\r\n    Set<TaskModel> taskModels = generateTaskModels(6);\r\n    Map<TaskName, String> prevTaskToContainerMapping = new HashMap<>();\r\n    prevTaskToContainerMapping.put(getTaskName(0), \"0\");\r\n    prevTaskToContainerMapping.put(getTaskName(1), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(2), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(3), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(4), \"1\");\r\n    prevTaskToContainerMapping.put(getTaskName(5), \"1\");\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(3).group(taskModels, grouperMetadata);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(3, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    ContainerModel container2 = containersMap.get(\"2\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertNotNull(container2);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(\"2\", container2.getId());\r\n    assertEquals(2, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container2.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerOldContainerCountOne",
  "sourceCode" : "@Test\r\npublic void testBalancerOldContainerCountOne() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(1).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(3).group(taskModels, grouperMetadata);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerNewContainerCountOne",
  "sourceCode" : "@Test\r\npublic void testBalancerNewContainerCountOne() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).group(taskModels, grouperMetadata);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerEmptyTaskMapping",
  "sourceCode" : "@Test\r\npublic void testBalancerEmptyTaskMapping() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), new HashMap<>());\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).group(taskModels, grouperMetadata);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupTaskCountIncrease",
  "sourceCode" : "@Test\r\npublic void testGroupTaskCountIncrease() {\r\n    int taskCount = 3;\r\n    Set<TaskModel> taskModels = generateTaskModels(taskCount);\r\n    // Here's the key step\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(2).group(generateTaskModels(taskCount - 1));\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).group(taskModels, grouperMetadata);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testGroupTaskCountDecrease",
  "sourceCode" : "@Test\r\npublic void testGroupTaskCountDecrease() {\r\n    int taskCount = 3;\r\n    Set<TaskModel> taskModels = generateTaskModels(taskCount);\r\n    // Here's the key step\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(generateTaskModels(taskCount + 1));\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(1).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(1).group(taskModels, grouperMetadata);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerNewContainerCountGreaterThanTasks",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testBalancerNewContainerCountGreaterThanTasks() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    // Should throw\r\n    new GroupByContainerCount(5).group(taskModels, grouperMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerEmptyTasks",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testBalancerEmptyTasks() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    new GroupByContainerCount(5).group(new HashSet<>(), grouperMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerResultImmutable",
  "sourceCode" : "@Test(expected = UnsupportedOperationException.class)\r\npublic void testBalancerResultImmutable() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> prevContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Map<TaskName, String> prevTaskToContainerMapping = generateTaskContainerMapping(prevContainers);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), prevTaskToContainerMapping);\r\n    Set<ContainerModel> containers = new GroupByContainerCount(2).group(taskModels, grouperMetadata);\r\n    containers.remove(containers.iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerCount.java",
  "methodName" : "testBalancerWithNullLocalityManager",
  "sourceCode" : "@Test\r\npublic void testBalancerWithNullLocalityManager() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> groupContainers = new GroupByContainerCount(3).group(taskModels);\r\n    Set<ContainerModel> balanceContainers = new GroupByContainerCount(3).balance(taskModels, null);\r\n    // Results should be the same as calling group()\r\n    assertEquals(groupContainers, balanceContainers);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupEmptyTasks",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGroupEmptyTasks() {\r\n    buildSimpleGrouper(1).group(new HashSet());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGrouperResultImmutable",
  "sourceCode" : "@Test(expected = UnsupportedOperationException.class)\r\npublic void testGrouperResultImmutable() {\r\n    Set<TaskModel> taskModels = generateTaskModels(3);\r\n    Set<ContainerModel> containers = buildSimpleGrouper(2).group(taskModels);\r\n    containers.remove(containers.iterator().next());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupHappyPath",
  "sourceCode" : "@Test\r\npublic void testGroupHappyPath() {\r\n    Set<TaskModel> taskModels = generateTaskModels(5);\r\n    Set<ContainerModel> containers = buildSimpleGrouper(2).group(taskModels);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupWithNullContainerIds",
  "sourceCode" : "@Test\r\npublic void testGroupWithNullContainerIds() {\r\n    Set<TaskModel> taskModels = generateTaskModels(5);\r\n    List<String> containerIds = null;\r\n    Set<ContainerModel> containers = buildSimpleGrouper(2).group(taskModels, containerIds);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"0\");\r\n    ContainerModel container1 = containersMap.get(\"1\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"0\", container0.getId());\r\n    assertEquals(\"1\", container1.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupWithEmptyContainerIds",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGroupWithEmptyContainerIds() {\r\n    Set<TaskModel> taskModels = generateTaskModels(5);\r\n    buildSimpleGrouper(2).group(taskModels, Collections.emptyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupHappyPathWithListOfContainers",
  "sourceCode" : "@Test\r\npublic void testGroupHappyPathWithListOfContainers() {\r\n    Set<TaskModel> taskModels = generateTaskModels(5);\r\n    List<String> containerIds = new ArrayList<String>() {\r\n\r\n        {\r\n            add(\"4\");\r\n            add(\"2\");\r\n        }\r\n    };\r\n    Set<ContainerModel> containers = buildSimpleGrouper().group(taskModels, containerIds);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"4\");\r\n    ContainerModel container1 = containersMap.get(\"2\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"4\", container0.getId());\r\n    assertEquals(\"2\", container1.getId());\r\n    assertEquals(3, container0.getTasks().size());\r\n    assertEquals(2, container1.getTasks().size());\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(3)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGroupManyTasks",
  "sourceCode" : "@Test\r\npublic void testGroupManyTasks() {\r\n    Set<TaskModel> taskModels = generateTaskModels(21);\r\n    List<String> containerIds = new ArrayList<String>() {\r\n\r\n        {\r\n            add(\"4\");\r\n            add(\"2\");\r\n        }\r\n    };\r\n    Set<ContainerModel> containers = buildSimpleGrouper().group(taskModels, containerIds);\r\n    Map<String, ContainerModel> containersMap = new HashMap<>();\r\n    for (ContainerModel container : containers) {\r\n        containersMap.put(container.getId(), container);\r\n    }\r\n    assertEquals(2, containers.size());\r\n    ContainerModel container0 = containersMap.get(\"4\");\r\n    ContainerModel container1 = containersMap.get(\"2\");\r\n    assertNotNull(container0);\r\n    assertNotNull(container1);\r\n    assertEquals(\"4\", container0.getId());\r\n    assertEquals(\"2\", container1.getId());\r\n    assertEquals(11, container0.getTasks().size());\r\n    assertEquals(10, container1.getTasks().size());\r\n    // NOTE: tasks are sorted lexicographically, so the container assignment\r\n    // can seem odd, but the consistency is the key focus\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(0)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(10)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(12)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(14)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(16)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(18)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(2)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(3)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(5)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(7)));\r\n    assertTrue(container0.getTasks().containsKey(getTaskName(9)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(1)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(11)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(13)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(15)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(17)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(19)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(20)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(4)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(6)));\r\n    assertTrue(container1.getTasks().containsKey(getTaskName(8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testFewerTasksThanContainers",
  "sourceCode" : "@Test\r\npublic void testFewerTasksThanContainers() {\r\n    final String testContainerId1 = \"1\";\r\n    final String testContainerId2 = \"2\";\r\n    Set<TaskModel> taskModels = generateTaskModels(1);\r\n    List<String> containerIds = ImmutableList.of(testContainerId1, testContainerId2);\r\n    Map<TaskName, TaskModel> expectedTasks = taskModels.stream().collect(Collectors.toMap(TaskModel::getTaskName, x -> x));\r\n    ContainerModel expectedContainerModel = new ContainerModel(testContainerId1, expectedTasks);\r\n    Set<ContainerModel> actualContainerModels = buildSimpleGrouper().group(taskModels, containerIds);\r\n    assertEquals(1, actualContainerModels.size());\r\n    assertEquals(ImmutableSet.of(expectedContainerModel), actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testShouldUseTaskLocalityWhenGeneratingContainerModels",
  "sourceCode" : "@Test\r\npublic void testShouldUseTaskLocalityWhenGeneratingContainerModels() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(3);\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    String testProcessorId2 = \"testProcessorId2\";\r\n    String testProcessorId3 = \"testProcessorId3\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    TaskModel testTaskModel1 = new TaskModel(testTaskName1, new HashSet<>(), new Partition(0));\r\n    TaskModel testTaskModel2 = new TaskModel(testTaskName2, new HashSet<>(), new Partition(1));\r\n    TaskModel testTaskModel3 = new TaskModel(testTaskName3, new HashSet<>(), new Partition(2));\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2, testProcessorId3, testLocationId3);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1, testTaskName2, testLocationId2, testTaskName3, testLocationId3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = ImmutableSet.of(testTaskModel1, testTaskModel2, testTaskModel3);\r\n    Set<ContainerModel> expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1)), new ContainerModel(testProcessorId2, ImmutableMap.of(testTaskName2, testTaskModel2)), new ContainerModel(testProcessorId3, ImmutableMap.of(testTaskName3, testTaskModel3)));\r\n    Set<ContainerModel> actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testGenerateContainerModelForSingleContainer",
  "sourceCode" : "@Test\r\npublic void testGenerateContainerModelForSingleContainer() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(1);\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    TaskModel testTaskModel1 = new TaskModel(testTaskName1, new HashSet<>(), new Partition(0));\r\n    TaskModel testTaskModel2 = new TaskModel(testTaskName2, new HashSet<>(), new Partition(1));\r\n    TaskModel testTaskModel3 = new TaskModel(testTaskName3, new HashSet<>(), new Partition(2));\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1, testTaskName2, testLocationId2, testTaskName3, testLocationId3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = ImmutableSet.of(testTaskModel1, testTaskModel2, testTaskModel3);\r\n    Set<ContainerModel> expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1, testTaskName2, testTaskModel2, testTaskName3, testTaskModel3)));\r\n    Set<ContainerModel> actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testShouldGenerateCorrectContainerModelWhenTaskLocalityIsEmpty",
  "sourceCode" : "@Test\r\npublic void testShouldGenerateCorrectContainerModelWhenTaskLocalityIsEmpty() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(3);\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    String testProcessorId2 = \"testProcessorId2\";\r\n    String testProcessorId3 = \"testProcessorId3\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    TaskModel testTaskModel1 = new TaskModel(testTaskName1, new HashSet<>(), new Partition(0));\r\n    TaskModel testTaskModel2 = new TaskModel(testTaskName2, new HashSet<>(), new Partition(1));\r\n    TaskModel testTaskModel3 = new TaskModel(testTaskName3, new HashSet<>(), new Partition(2));\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2, testProcessorId3, testLocationId3);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = ImmutableSet.of(testTaskModel1, testTaskModel2, testTaskModel3);\r\n    Set<ContainerModel> expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1)), new ContainerModel(testProcessorId2, ImmutableMap.of(testTaskName2, testTaskModel2)), new ContainerModel(testProcessorId3, ImmutableMap.of(testTaskName3, testTaskModel3)));\r\n    Set<ContainerModel> actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testShouldFailWhenProcessorLocalityIsEmpty",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testShouldFailWhenProcessorLocalityIsEmpty() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(new HashMap<>(), new HashMap<>(), new HashMap<>(), new HashMap<>());\r\n    taskNameGrouper.group(new HashSet<>(), grouperMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testShouldGenerateIdenticalTaskDistributionWhenNoChangeInProcessorGroup",
  "sourceCode" : "@Test\r\npublic void testShouldGenerateIdenticalTaskDistributionWhenNoChangeInProcessorGroup() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(3);\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    String testProcessorId2 = \"testProcessorId2\";\r\n    String testProcessorId3 = \"testProcessorId3\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    TaskModel testTaskModel1 = new TaskModel(testTaskName1, new HashSet<>(), new Partition(0));\r\n    TaskModel testTaskModel2 = new TaskModel(testTaskName2, new HashSet<>(), new Partition(1));\r\n    TaskModel testTaskModel3 = new TaskModel(testTaskName3, new HashSet<>(), new Partition(2));\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2, testProcessorId3, testLocationId3);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1, testTaskName2, testLocationId2, testTaskName3, testLocationId3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = ImmutableSet.of(testTaskModel1, testTaskModel2, testTaskModel3);\r\n    Set<ContainerModel> expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1)), new ContainerModel(testProcessorId2, ImmutableMap.of(testTaskName2, testTaskModel2)), new ContainerModel(testProcessorId3, ImmutableMap.of(testTaskName3, testTaskModel3)));\r\n    Set<ContainerModel> actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n    actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testShouldMinimizeTaskShuffleWhenAvailableProcessorInGroupChanges",
  "sourceCode" : "@Test\r\npublic void testShouldMinimizeTaskShuffleWhenAvailableProcessorInGroupChanges() {\r\n    TaskNameGrouper taskNameGrouper = buildSimpleGrouper(3);\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    String testProcessorId2 = \"testProcessorId2\";\r\n    String testProcessorId3 = \"testProcessorId3\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    TaskModel testTaskModel1 = new TaskModel(testTaskName1, new HashSet<>(), new Partition(0));\r\n    TaskModel testTaskModel2 = new TaskModel(testTaskName2, new HashSet<>(), new Partition(1));\r\n    TaskModel testTaskModel3 = new TaskModel(testTaskName3, new HashSet<>(), new Partition(2));\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2, testProcessorId3, testLocationId3);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1, testTaskName2, testLocationId2, testTaskName3, testLocationId3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = ImmutableSet.of(testTaskModel1, testTaskModel2, testTaskModel3);\r\n    Set<ContainerModel> expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1)), new ContainerModel(testProcessorId2, ImmutableMap.of(testTaskName2, testTaskModel2)), new ContainerModel(testProcessorId3, ImmutableMap.of(testTaskName3, testTaskModel3)));\r\n    Set<ContainerModel> actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n    processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2);\r\n    grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    actualContainerModels = taskNameGrouper.group(taskModels, grouperMetadata);\r\n    expectedContainerModels = ImmutableSet.of(new ContainerModel(testProcessorId1, ImmutableMap.of(testTaskName1, testTaskModel1, testTaskName3, testTaskModel3)), new ContainerModel(testProcessorId2, ImmutableMap.of(testTaskName2, testTaskModel2)));\r\n    assertEquals(expectedContainerModels, actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestGroupByContainerIds.java",
  "methodName" : "testMoreTasksThanProcessors",
  "sourceCode" : "@Test\r\npublic void testMoreTasksThanProcessors() {\r\n    String testProcessorId1 = \"testProcessorId1\";\r\n    String testProcessorId2 = \"testProcessorId2\";\r\n    LocationId testLocationId1 = new LocationId(\"testLocationId1\");\r\n    LocationId testLocationId2 = new LocationId(\"testLocationId2\");\r\n    LocationId testLocationId3 = new LocationId(\"testLocationId3\");\r\n    TaskName testTaskName1 = new TaskName(\"testTasKId1\");\r\n    TaskName testTaskName2 = new TaskName(\"testTaskId2\");\r\n    TaskName testTaskName3 = new TaskName(\"testTaskId3\");\r\n    Map<String, LocationId> processorLocality = ImmutableMap.of(testProcessorId1, testLocationId1, testProcessorId2, testLocationId2);\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(testTaskName1, testLocationId1, testTaskName2, testLocationId2, testTaskName3, testLocationId3);\r\n    GrouperMetadataImpl grouperMetadata = new GrouperMetadataImpl(processorLocality, taskLocality, new HashMap<>(), new HashMap<>());\r\n    Set<TaskModel> taskModels = generateTaskModels(1);\r\n    List<String> containerIds = ImmutableList.of(testProcessorId1, testProcessorId2);\r\n    Map<TaskName, TaskModel> expectedTasks = taskModels.stream().collect(Collectors.toMap(TaskModel::getTaskName, x -> x));\r\n    ContainerModel expectedContainerModel = new ContainerModel(testProcessorId1, expectedTasks);\r\n    Set<ContainerModel> actualContainerModels = buildSimpleGrouper().group(taskModels, grouperMetadata);\r\n    assertEquals(1, actualContainerModels.size());\r\n    assertEquals(ImmutableSet.of(expectedContainerModel), actualContainerModels);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskAssignmentManager.java",
  "methodName" : "testTaskAssignmentManager",
  "sourceCode" : "@Test\r\npublic void testTaskAssignmentManager() {\r\n    Map<String, String> expectedMap = ImmutableMap.of(\"Task0\", \"0\", \"Task1\", \"1\", \"Task2\", \"2\", \"Task3\", \"0\", \"Task4\", \"1\");\r\n    Map<String, Map<String, TaskMode>> taskContainerMappings = ImmutableMap.of(\"0\", ImmutableMap.of(\"Task0\", TaskMode.Active, \"Task3\", TaskMode.Active), \"1\", ImmutableMap.of(\"Task1\", TaskMode.Active, \"Task4\", TaskMode.Active), \"2\", ImmutableMap.of(\"Task2\", TaskMode.Active));\r\n    taskAssignmentManager.writeTaskContainerMappings(taskContainerMappings);\r\n    Map<String, String> localMap = taskAssignmentManager.readTaskAssignment();\r\n    assertEquals(expectedMap, localMap);\r\n    taskAssignmentManager.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskAssignmentManager.java",
  "methodName" : "testDeleteMappings",
  "sourceCode" : "@Test\r\npublic void testDeleteMappings() {\r\n    Map<String, String> expectedMap = ImmutableMap.of(\"Task0\", \"0\", \"Task1\", \"1\");\r\n    Map<String, Map<String, TaskMode>> taskContainerMappings = ImmutableMap.of(\"0\", ImmutableMap.of(\"Task0\", TaskMode.Active), \"1\", ImmutableMap.of(\"Task1\", TaskMode.Active));\r\n    taskAssignmentManager.writeTaskContainerMappings(taskContainerMappings);\r\n    Map<String, String> localMap = taskAssignmentManager.readTaskAssignment();\r\n    assertEquals(expectedMap, localMap);\r\n    taskAssignmentManager.deleteTaskContainerMappings(localMap.keySet());\r\n    Map<String, String> deletedMap = taskAssignmentManager.readTaskAssignment();\r\n    assertTrue(deletedMap.isEmpty());\r\n    taskAssignmentManager.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskAssignmentManager.java",
  "methodName" : "testTaskAssignmentManagerEmptyCoordinatorStream",
  "sourceCode" : "@Test\r\npublic void testTaskAssignmentManagerEmptyCoordinatorStream() {\r\n    Map<String, String> expectedMap = new HashMap<>();\r\n    Map<String, String> localMap = taskAssignmentManager.readTaskAssignment();\r\n    assertEquals(expectedMap, localMap);\r\n    taskAssignmentManager.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskNameGrouperProxy.java",
  "methodName" : "testBuddyContainerBasedGenerationIdentity",
  "sourceCode" : "@Test\r\npublic void testBuddyContainerBasedGenerationIdentity() {\r\n    this.standbyTaskGenerator = new TaskNameGrouperProxy(Mockito.mock(TaskNameGrouper.class), true, 2);\r\n    Assert.assertEquals(\"Shouldnt add standby tasks to empty container map\", Collections.emptySet(), this.standbyTaskGenerator.generateStandbyTasks(Collections.emptySet(), 1));\r\n    Assert.assertEquals(\"Shouldnt add standby tasks when repl factor = 1\", getContainerMap(), this.standbyTaskGenerator.generateStandbyTasks(getContainerMap(), 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskNameGrouperProxy.java",
  "methodName" : "testBuddyContainerBasedGenerationForVaryingRF",
  "sourceCode" : "@Test\r\npublic void testBuddyContainerBasedGenerationForVaryingRF() {\r\n    testBuddyContainerBasedGeneration(1);\r\n    testBuddyContainerBasedGeneration(2);\r\n    testBuddyContainerBasedGeneration(3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskPartitionAssignmentManager.java",
  "methodName" : "testReadAfterWrite",
  "sourceCode" : "@Test\r\npublic void testReadAfterWrite() {\r\n    List<String> testTaskNames = ImmutableList.of(\"test-task1\", \"test-task2\", \"test-task3\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition, testTaskNames));\r\n    Map<SystemStreamPartition, List<String>> expectedMapping = ImmutableMap.of(testSystemStreamPartition, testTaskNames);\r\n    Map<SystemStreamPartition, List<String>> actualMapping = taskPartitionAssignmentManager.readTaskPartitionAssignments();\r\n    Assert.assertEquals(expectedMapping, actualMapping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskPartitionAssignmentManager.java",
  "methodName" : "testDeleteAfterWrite",
  "sourceCode" : "@Test\r\npublic void testDeleteAfterWrite() {\r\n    List<String> testTaskNames = ImmutableList.of(\"test-task1\", \"test-task2\", \"test-task3\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition, testTaskNames));\r\n    Map<SystemStreamPartition, List<String>> actualMapping = taskPartitionAssignmentManager.readTaskPartitionAssignments();\r\n    Assert.assertEquals(1, actualMapping.size());\r\n    taskPartitionAssignmentManager.delete(ImmutableList.of(testSystemStreamPartition));\r\n    actualMapping = taskPartitionAssignmentManager.readTaskPartitionAssignments();\r\n    Assert.assertEquals(0, actualMapping.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskPartitionAssignmentManager.java",
  "methodName" : "testReadPartitionAssignments",
  "sourceCode" : "@Test\r\npublic void testReadPartitionAssignments() {\r\n    SystemStreamPartition testSystemStreamPartition1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, PARTITION);\r\n    List<String> testTaskNames1 = ImmutableList.of(\"test-task1\", \"test-task2\", \"test-task3\");\r\n    SystemStreamPartition testSystemStreamPartition2 = new SystemStreamPartition(TEST_SYSTEM, \"stream-2\", PARTITION);\r\n    List<String> testTaskNames2 = ImmutableList.of(\"test-task4\", \"test-task5\");\r\n    SystemStreamPartition testSystemStreamPartition3 = new SystemStreamPartition(TEST_SYSTEM, \"stream-3\", PARTITION);\r\n    List<String> testTaskNames3 = ImmutableList.of(\"test-task6\", \"test-task7\", \"test-task8\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition1, testTaskNames1, testSystemStreamPartition2, testTaskNames2, testSystemStreamPartition3, testTaskNames3));\r\n    Map<SystemStreamPartition, List<String>> expectedMapping = ImmutableMap.of(testSystemStreamPartition1, testTaskNames1, testSystemStreamPartition2, testTaskNames2, testSystemStreamPartition3, testTaskNames3);\r\n    Map<SystemStreamPartition, List<String>> actualMapping = taskPartitionAssignmentManager.readTaskPartitionAssignments();\r\n    Assert.assertEquals(expectedMapping, actualMapping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\grouper\\task\\TestTaskPartitionAssignmentManager.java",
  "methodName" : "testMultipleUpdatesReturnsTheMostRecentValue",
  "sourceCode" : "@Test\r\npublic void testMultipleUpdatesReturnsTheMostRecentValue() {\r\n    List<String> testTaskNames1 = ImmutableList.of(\"test-task1\", \"test-task2\", \"test-task3\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition, testTaskNames1));\r\n    List<String> testTaskNames2 = ImmutableList.of(\"test-task4\", \"test-task5\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition, testTaskNames2));\r\n    List<String> testTaskNames3 = ImmutableList.of(\"test-task6\", \"test-task7\", \"test-task8\");\r\n    taskPartitionAssignmentManager.writeTaskPartitionAssignments(ImmutableMap.of(testSystemStreamPartition, testTaskNames3));\r\n    Map<SystemStreamPartition, List<String>> expectedMapping = ImmutableMap.of(testSystemStreamPartition, testTaskNames3);\r\n    Map<SystemStreamPartition, List<String>> actualMapping = taskPartitionAssignmentManager.readTaskPartitionAssignments();\r\n    Assert.assertEquals(expectedMapping, actualMapping);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestDefaultSystemStatisticsGetter.java",
  "methodName" : "testGetSystemMemoryStatistics",
  "sourceCode" : "@Test\r\npublic void testGetSystemMemoryStatistics() {\r\n    defaultSystemStatisticsGetter.getSystemMemoryStatistics();\r\n    verify(posixCommandBasedStatisticsGetter).getSystemMemoryStatistics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestDefaultSystemStatisticsGetter.java",
  "methodName" : "testGetProcessCPUStatistics",
  "sourceCode" : "@Test\r\npublic void testGetProcessCPUStatistics() {\r\n    defaultSystemStatisticsGetter.getProcessCPUStatistics();\r\n    verify(oshiBasedStatisticsGetter).getProcessCPUStatistics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestOshiBasedStatisticsGetter.java",
  "methodName" : "testGetProcessCPUStatistics",
  "sourceCode" : "@Test\r\npublic void testGetProcessCPUStatistics() {\r\n    // first time to get cpu usage info\r\n    double processCpuUsage1 = 0.5;\r\n    double childProcessCPUUsage1 = 0.3;\r\n    when(os.getProcess(pid)).thenReturn(processSnapshot);\r\n    when(os.getProcess(childPid)).thenReturn(childProcessSnapshot);\r\n    when(os.getChildProcesses(pid, OperatingSystem.ProcessFiltering.ALL_PROCESSES, OperatingSystem.ProcessSorting.NO_SORTING, 0)).thenReturn(Lists.newArrayList(childProcessSnapshot));\r\n    when(processSnapshot.getProcessCpuLoadBetweenTicks(null)).thenReturn(processCpuUsage1);\r\n    when(childProcessSnapshot.getProcessCpuLoadBetweenTicks(null)).thenReturn(childProcessCPUUsage1);\r\n    assertEquals(new ProcessCPUStatistics(100d * (processCpuUsage1 + childProcessCPUUsage1) / cpuCount), oshiBasedStatisticsGetter.getProcessCPUStatistics());\r\n    // second time to get cpu usage info\r\n    double processCpuUsage2 = 0.2;\r\n    double childProcessCPUUsage2 = 2;\r\n    when(os.getProcess(pid)).thenReturn(process);\r\n    when(os.getProcess(childPid)).thenReturn(childProcess);\r\n    when(os.getChildProcesses(pid, OperatingSystem.ProcessFiltering.ALL_PROCESSES, OperatingSystem.ProcessSorting.NO_SORTING, 0)).thenReturn(Lists.newArrayList(childProcess));\r\n    when(process.getProcessCpuLoadBetweenTicks(processSnapshot)).thenReturn(processCpuUsage2);\r\n    when(childProcess.getProcessCpuLoadBetweenTicks(childProcessSnapshot)).thenReturn(childProcessCPUUsage2);\r\n    assertEquals(new ProcessCPUStatistics(100d * (processCpuUsage2 + childProcessCPUUsage2) / cpuCount), oshiBasedStatisticsGetter.getProcessCPUStatistics());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestOshiBasedStatisticsGetter.java",
  "methodName" : "testGetSystemMemoryStatistics",
  "sourceCode" : "@Test(expected = UnsupportedOperationException.class)\r\npublic void testGetSystemMemoryStatistics() {\r\n    oshiBasedStatisticsGetter.getSystemMemoryStatistics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestStatisticsMonitorImpl.java",
  "methodName" : "testSystemStatisticsReporting",
  "sourceCode" : "@Test\r\npublic void testSystemStatisticsReporting() throws Exception {\r\n    final int numSamplesToCollect = 5;\r\n    final CountDownLatch latch = new CountDownLatch(numSamplesToCollect);\r\n    final StatisticsMonitorImpl monitor = new StatisticsMonitorImpl(10, new DefaultSystemStatisticsGetter());\r\n    monitor.start();\r\n    boolean result = monitor.registerListener(new SystemStatisticsMonitor.Listener() {\r\n\r\n        @Override\r\n        public void onUpdate(SystemStatistics sample) {\r\n            SystemMemoryStatistics memorySample = sample.getMemoryStatistics();\r\n            // assert memory is greater than 10 bytes, as a sanity check\r\n            Assert.assertTrue(memorySample.getPhysicalMemoryBytes() > 10);\r\n            ProcessCPUStatistics cpuSample = sample.getCpuStatistics();\r\n            // assert cpu usage is greater than 0, as a sanity check\r\n            Assert.assertTrue(cpuSample.getProcessCPUUsagePercentage() > 0);\r\n            latch.countDown();\r\n        }\r\n    });\r\n    if (!latch.await(5, TimeUnit.SECONDS)) {\r\n        fail(String.format(\"Timed out waiting for listener to be give %d updates\", numSamplesToCollect));\r\n    }\r\n    // assert that the registration for the listener was successful\r\n    Assert.assertTrue(result);\r\n    monitor.stop();\r\n    // assert that attempting to register a listener after monitor stop results in failure of registration\r\n    boolean registrationFailsAfterStop = monitor.registerListener(new SystemStatisticsMonitor.Listener() {\r\n\r\n        @Override\r\n        public void onUpdate(SystemStatistics sample) {\r\n        }\r\n    });\r\n    Assert.assertFalse(registrationFailsAfterStop);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\host\\TestStatisticsMonitorImpl.java",
  "methodName" : "testStopBehavior",
  "sourceCode" : "@Test\r\npublic void testStopBehavior() throws Exception {\r\n    final int numSamplesToCollect = 5;\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final AtomicInteger numCallbacks = new AtomicInteger(0);\r\n    final StatisticsMonitorImpl monitor = new StatisticsMonitorImpl(10, new DefaultSystemStatisticsGetter());\r\n    monitor.start();\r\n    monitor.registerListener(new SystemStatisticsMonitor.Listener() {\r\n\r\n        @Override\r\n        public void onUpdate(SystemStatistics sample) {\r\n            SystemMemoryStatistics memorySample = sample.getMemoryStatistics();\r\n            Assert.assertTrue(memorySample.getPhysicalMemoryBytes() > 10);\r\n            if (numCallbacks.incrementAndGet() == numSamplesToCollect) {\r\n                //monitor.stop() is invoked from the same thread. So, there's no race between a stop() call and the\r\n                //callback invocation for the next sample.\r\n                monitor.stop();\r\n                latch.countDown();\r\n            }\r\n        }\r\n    });\r\n    if (!latch.await(5, TimeUnit.SECONDS)) {\r\n        fail(String.format(\"Timed out waiting for listener to be give %d updates\", numSamplesToCollect));\r\n    }\r\n    // Ensure that we only receive as many callbacks\r\n    Assert.assertEquals(numCallbacks.get(), numSamplesToCollect);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatClient.java",
  "methodName" : "testClientResponseForHeartbeatAlive",
  "sourceCode" : "@Test\r\npublic void testClientResponseForHeartbeatAlive() throws IOException {\r\n    client.setHttpOutput(\"{\\\"alive\\\": true}\");\r\n    ContainerHeartbeatResponse response = client.requestHeartbeat();\r\n    Assert.assertTrue(response.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatClient.java",
  "methodName" : "testClientResponseForHeartbeatDead",
  "sourceCode" : "@Test\r\npublic void testClientResponseForHeartbeatDead() throws IOException {\r\n    client.setHttpOutput(\"{\\\"alive\\\": false}\");\r\n    ContainerHeartbeatResponse response = client.requestHeartbeat();\r\n    Assert.assertFalse(response.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatClient.java",
  "methodName" : "testClientResponseOnBadRequest",
  "sourceCode" : "@Test\r\npublic void testClientResponseOnBadRequest() throws IOException {\r\n    client.shouldThrowException(true);\r\n    ContainerHeartbeatResponse response = client.requestHeartbeat();\r\n    Assert.assertFalse(response.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testCallbackWhenHeartbeatDead",
  "sourceCode" : "@Test\r\npublic void testCallbackWhenHeartbeatDead() throws InterruptedException {\r\n    ContainerHeartbeatResponse response = new ContainerHeartbeatResponse(false);\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(response);\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(2, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    // check that the shutdown task got submitted, but don't actually execute it since it will shut down the process\r\n    assertEquals(\"Heartbeat expired count should be 1\", 1, containerHeartbeatMonitor.getMetrics().getHeartbeatExpiredCount().getCount());\r\n    verify(this.scheduler).schedule(any(Runnable.class), eq((long) ContainerHeartbeatMonitor.SHUTDOWN_TIMOUT_MS), eq(TimeUnit.MILLISECONDS));\r\n    verify(this.onExpired).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testDoesNotCallbackWhenHeartbeatAlive",
  "sourceCode" : "@Test\r\npublic void testDoesNotCallbackWhenHeartbeatAlive() throws InterruptedException {\r\n    ContainerHeartbeatResponse response = new ContainerHeartbeatResponse(true);\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(response);\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(2, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    assertEquals(\"Heartbeat expired count should be 0\", 0, containerHeartbeatMonitor.getMetrics().getHeartbeatExpiredCount().getCount());\r\n    // shutdown task should not have been submitted\r\n    verify(this.scheduler, never()).schedule(any(Runnable.class), anyLong(), any());\r\n    verify(this.onExpired, never()).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testReestablishConnectionWithNewAM",
  "sourceCode" : "@Test\r\npublic void testReestablishConnectionWithNewAM() throws InterruptedException {\r\n    String newCoordinatorUrl = \"http://some-host-2.prod.linkedin.com\";\r\n    this.containerHeartbeatMonitor = spy(buildContainerHeartbeatMonitor(true));\r\n    CoordinatorStreamValueSerde serde = new CoordinatorStreamValueSerde(SetConfig.TYPE);\r\n    ContainerHeartbeatMonitor.ContainerHeartbeatMetrics metrics = this.containerHeartbeatMonitor.getMetrics();\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(FAILURE_RESPONSE).thenReturn(SUCCESS_RESPONSE);\r\n    when(this.containerHeartbeatMonitor.createContainerHeartbeatClient(newCoordinatorUrl, CONTAINER_EXECUTION_ID)).thenReturn(this.containerHeartbeatClient);\r\n    when(this.coordinatorStreamStore.get(CoordinationConstants.YARN_COORDINATOR_URL)).thenReturn(serde.toBytes(newCoordinatorUrl));\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(2, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    assertEquals(\"Heartbeat expired count should be 1\", 1, metrics.getHeartbeatExpiredCount().getCount());\r\n    assertEquals(\"Heartbeat established failure count should be 0\", 0, metrics.getHeartbeatEstablishedFailureCount().getCount());\r\n    assertEquals(\"Heartbeat established with new AM should be 1\", 1, metrics.getHeartbeatEstablishedWithNewAmCount().getCount());\r\n    // shutdown task should not have been submitted\r\n    verify(this.scheduler, never()).schedule(any(Runnable.class), anyLong(), any());\r\n    verify(this.onExpired, never()).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testFailedToFetchNewAMCoordinatorUrl",
  "sourceCode" : "@Test\r\npublic void testFailedToFetchNewAMCoordinatorUrl() throws InterruptedException {\r\n    this.containerHeartbeatMonitor = spy(buildContainerHeartbeatMonitor(true));\r\n    CoordinatorStreamValueSerde serde = new CoordinatorStreamValueSerde(SetConfig.TYPE);\r\n    ContainerHeartbeatMonitor.ContainerHeartbeatMetrics metrics = this.containerHeartbeatMonitor.getMetrics();\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(FAILURE_RESPONSE);\r\n    when(this.coordinatorStreamStore.get(CoordinationConstants.YARN_COORDINATOR_URL)).thenReturn(serde.toBytes(COORDINATOR_URL));\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(2, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    assertEquals(\"Heartbeat expired count should be 1\", 1, metrics.getHeartbeatExpiredCount().getCount());\r\n    assertEquals(\"Heartbeat established failure count should be 1\", 1, metrics.getHeartbeatEstablishedFailureCount().getCount());\r\n    // shutdown task should have been submitted\r\n    verify(this.scheduler).schedule(any(Runnable.class), eq((long) ContainerHeartbeatMonitor.SHUTDOWN_TIMOUT_MS), eq(TimeUnit.MILLISECONDS));\r\n    verify(this.onExpired).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testConnectToNewAMFailed",
  "sourceCode" : "@Test\r\npublic void testConnectToNewAMFailed() throws InterruptedException {\r\n    String newCoordinatorUrl = \"http://some-host-2.prod.linkedin.com\";\r\n    this.containerHeartbeatMonitor = spy(buildContainerHeartbeatMonitor(true));\r\n    CoordinatorStreamValueSerde serde = new CoordinatorStreamValueSerde(SetConfig.TYPE);\r\n    ContainerHeartbeatMonitor.ContainerHeartbeatMetrics metrics = this.containerHeartbeatMonitor.getMetrics();\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(FAILURE_RESPONSE);\r\n    when(this.containerHeartbeatMonitor.createContainerHeartbeatClient(newCoordinatorUrl, CONTAINER_EXECUTION_ID)).thenReturn(this.containerHeartbeatClient);\r\n    when(this.coordinatorStreamStore.get(CoordinationConstants.YARN_COORDINATOR_URL)).thenReturn(serde.toBytes(newCoordinatorUrl));\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(2, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    assertEquals(\"Heartbeat expired count should be 1\", 1, metrics.getHeartbeatExpiredCount().getCount());\r\n    assertEquals(\"Heartbeat established failure count should be 1\", 1, metrics.getHeartbeatEstablishedFailureCount().getCount());\r\n    // shutdown task should have been submitted\r\n    verify(this.scheduler).schedule(any(Runnable.class), eq((long) ContainerHeartbeatMonitor.SHUTDOWN_TIMOUT_MS), eq(TimeUnit.MILLISECONDS));\r\n    verify(this.onExpired).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestContainerHeartbeatMonitor.java",
  "methodName" : "testConnectToNewAMSerdeException",
  "sourceCode" : "@Test\r\npublic void testConnectToNewAMSerdeException() throws InterruptedException {\r\n    String newCoordinatorUrl = \"http://some-host-2.prod.linkedin.com\";\r\n    this.containerHeartbeatMonitor = spy(buildContainerHeartbeatMonitor(true));\r\n    CoordinatorStreamValueSerde serde = new CoordinatorStreamValueSerde(SetConfig.TYPE);\r\n    ContainerHeartbeatMonitor.ContainerHeartbeatMetrics metrics = this.containerHeartbeatMonitor.getMetrics();\r\n    when(this.containerHeartbeatClient.requestHeartbeat()).thenReturn(FAILURE_RESPONSE);\r\n    when(this.containerHeartbeatMonitor.createContainerHeartbeatClient(newCoordinatorUrl, CONTAINER_EXECUTION_ID)).thenReturn(this.containerHeartbeatClient);\r\n    when(this.coordinatorStreamStore.get(CoordinationConstants.YARN_COORDINATOR_URL)).thenThrow(new NullPointerException(\"serde failed\"));\r\n    this.containerHeartbeatMonitor.start();\r\n    // wait for the executor to finish the heartbeat check task\r\n    boolean fixedRateTaskCompleted = this.schedulerFixedRateExecutionLatch.await(10, TimeUnit.SECONDS);\r\n    assertTrue(\"Did not complete heartbeat check\", fixedRateTaskCompleted);\r\n    assertEquals(\"Heartbeat expired count should be 1\", 1, metrics.getHeartbeatExpiredCount().getCount());\r\n    assertEquals(\"Heartbeat established failure count should be 1\", 1, metrics.getHeartbeatEstablishedFailureCount().getCount());\r\n    // shutdown task should have been submitted\r\n    verify(this.scheduler).schedule(any(Runnable.class), eq((long) ContainerHeartbeatMonitor.SHUTDOWN_TIMOUT_MS), eq(TimeUnit.MILLISECONDS));\r\n    verify(this.onExpired).run();\r\n    this.containerHeartbeatMonitor.stop();\r\n    verify(this.scheduler).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestExecutionContainerIdManager.java",
  "methodName" : "testExecutionContainerIdManager",
  "sourceCode" : "@Test\r\npublic void testExecutionContainerIdManager() {\r\n    String physicalId = \"container_123_123_123\";\r\n    String processorId = \"0\";\r\n    executionContainerIdManager.writeExecutionEnvironmentContainerIdMapping(processorId, physicalId);\r\n    Map<String, String> localMap = executionContainerIdManager.readExecutionEnvironmentContainerIdMapping();\r\n    Map<String, String> expectedMap = ImmutableMap.of(processorId, physicalId);\r\n    assertEquals(expectedMap, localMap);\r\n    executionContainerIdManager.close();\r\n    MockCoordinatorStreamSystemFactory.MockCoordinatorStreamSystemProducer producer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemProducer();\r\n    MockCoordinatorStreamSystemFactory.MockCoordinatorStreamSystemConsumer consumer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemConsumer();\r\n    assertTrue(producer.isStopped());\r\n    assertTrue(consumer.isStopped());\r\n    ArgumentCaptor<byte[]> argument1 = ArgumentCaptor.forClass(byte[].class);\r\n    Mockito.verify(store).put(Mockito.eq(processorId), argument1.capture());\r\n    CoordinatorStreamValueSerde valueSerde = new CoordinatorStreamValueSerde(SetExecutionEnvContainerIdMapping.TYPE);\r\n    assertEquals(physicalId, valueSerde.fromBytes(argument1.getValue()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestExecutionContainerIdManager.java",
  "methodName" : "testInvalidKeyExecutionContainerIdManager",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testInvalidKeyExecutionContainerIdManager() {\r\n    String physicalId = \"container_123_123_123\";\r\n    String processorId = null;\r\n    executionContainerIdManager.writeExecutionEnvironmentContainerIdMapping(processorId, physicalId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestExecutionContainerIdManager.java",
  "methodName" : "testInvalidValueExecutionContainerIdManager",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testInvalidValueExecutionContainerIdManager() {\r\n    String physicalId = null;\r\n    String processorId = \"0\";\r\n    executionContainerIdManager.writeExecutionEnvironmentContainerIdMapping(processorId, physicalId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestLocalityManager.java",
  "methodName" : "testLocalityManager",
  "sourceCode" : "@Test\r\npublic void testLocalityManager() {\r\n    LocalityManager localityManager = new LocalityManager(new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetContainerHostMapping.TYPE));\r\n    localityManager.writeContainerToHostMapping(\"0\", \"localhost\");\r\n    Map<String, Map<String, String>> localMap = readContainerLocality(localityManager);\r\n    Map<String, Map<String, String>> expectedMap = new HashMap<String, Map<String, String>>() {\r\n\r\n        {\r\n            this.put(\"0\", new HashMap<String, String>() {\r\n\r\n                {\r\n                    this.put(SetContainerHostMapping.HOST_KEY, \"localhost\");\r\n                }\r\n            });\r\n        }\r\n    };\r\n    assertEquals(expectedMap, localMap);\r\n    localityManager.close();\r\n    MockCoordinatorStreamSystemProducer producer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemProducer();\r\n    MockCoordinatorStreamSystemConsumer consumer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemConsumer();\r\n    assertTrue(producer.isStopped());\r\n    assertTrue(consumer.isStopped());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestLocalityManager.java",
  "methodName" : "testWriteOnlyLocalityManager",
  "sourceCode" : "@Test\r\npublic void testWriteOnlyLocalityManager() {\r\n    LocalityManager localityManager = new LocalityManager(new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, SetContainerHostMapping.TYPE));\r\n    localityManager.writeContainerToHostMapping(\"1\", \"localhost\");\r\n    assertEquals(readContainerLocality(localityManager).size(), 1);\r\n    assertEquals(ImmutableMap.of(\"1\", ImmutableMap.of(\"host\", \"localhost\")), readContainerLocality(localityManager));\r\n    localityManager.close();\r\n    MockCoordinatorStreamSystemProducer producer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemProducer();\r\n    MockCoordinatorStreamSystemConsumer consumer = coordinatorStreamStoreTestUtil.getMockCoordinatorStreamSystemConsumer();\r\n    assertTrue(producer.isStopped());\r\n    assertTrue(consumer.isStopped());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testProcessMultipleTasks",
  "sourceCode" : "@Test\r\npublic void testProcessMultipleTasks() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    tasks.put(taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(sspA0EndOfStream).thenReturn(sspA1EndOfStream).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task0).process(eq(envelopeA00), any(), any());\r\n    verify(task1).process(eq(envelopeA11), any(), any());\r\n    assertEquals(4L, containerMetrics.envelopes().getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testProcessInOrder",
  "sourceCode" : "@Test\r\npublic void testProcessInOrder() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA01).thenReturn(sspA0EndOfStream).thenReturn(null);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    runLoop.run();\r\n    InOrder inOrder = inOrder(task0);\r\n    inOrder.verify(task0).process(eq(envelopeA00), any(), any());\r\n    inOrder.verify(task0).process(eq(envelopeA01), any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testProcessCallbacksCompletedOutOfOrder",
  "sourceCode" : "@Test\r\npublic void testProcessCallbacksCompletedOutOfOrder() {\r\n    int maxMessagesInFlight = 2;\r\n    ExecutorService taskExecutor = Executors.newFixedThreadPool(1);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    OffsetManager offsetManager = mock(OffsetManager.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    when(task0.offsetManager()).thenReturn(offsetManager);\r\n    CountDownLatch firstMessageBarrier = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        taskExecutor.submit(() -> {\r\n            firstMessageBarrier.await();\r\n            coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n            coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n            callback.complete();\r\n            return null;\r\n        });\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(1, task0.metrics().messagesInFlight().getValue());\r\n        assertEquals(0, task0.metrics().asyncCallbackCompleted().getCount());\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        callback.complete();\r\n        firstMessageBarrier.countDown();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA01), any(), any());\r\n    when(mockRunLoopConfig.getMaxConcurrency()).thenReturn(maxMessagesInFlight);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA01).thenReturn(null);\r\n    runLoop.run();\r\n    InOrder inOrder = inOrder(task0);\r\n    inOrder.verify(task0).process(eq(envelopeA00), any(), any());\r\n    inOrder.verify(task0).process(eq(envelopeA01), any(), any());\r\n    verify(offsetManager).update(eq(taskName0), eq(sspA0), eq(envelopeA00.getOffset()));\r\n    assertEquals(2L, containerMetrics.processes().getCount());\r\n    assertEquals(1L, containerMetrics.containerRunning().getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testProcessElasticityEnabled",
  "sourceCode" : "@Test\r\npublic void testProcessElasticityEnabled() {\r\n    TaskName taskName0 = new TaskName(p0.toString() + \" 0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"testSystem\", \"testStreamA\", p0);\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(\"testSystem\", \"testStreamA\", p0, 0);\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"testSystem\", \"testStreamA\", p0, 1);\r\n    // create two IME such that one of their ssp keybucket maps to ssp0 and the other one maps to ssp1\r\n    // task in the runloop should process only the first ime (aka the one whose ssp keybucket is ssp0)\r\n    IncomingMessageEnvelope envelope00 = spy(new IncomingMessageEnvelope(ssp, \"0\", \"key0\", \"value0\"));\r\n    IncomingMessageEnvelope envelope01 = spy(new IncomingMessageEnvelope(ssp, \"1\", \"key0\", \"value0\"));\r\n    when(envelope00.getSystemStreamPartition(2)).thenReturn(ssp0);\r\n    when(envelope01.getSystemStreamPartition(2)).thenReturn(ssp1);\r\n    // have a single task in the run loop that processes ssp0 -> 0th keybucket of ssp\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, ssp0);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelope00), any(), any());\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelope01), any(), any());\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelope00).thenReturn(envelope01).thenReturn(sspA0EndOfStream).thenReturn(null);\r\n    when(mockRunLoopConfig.getElasticityFactor()).thenReturn(2);\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    runLoop.run();\r\n    verify(task0).process(eq(envelope00), any(), any());\r\n    verify(task0, never()).process(eq(envelope01), any(), any());\r\n    // envelop00 and end of stream\r\n    assertEquals(2, containerMetrics.envelopes().getCount());\r\n    // only envelope00 and not envelope01 and not end of stream\r\n    assertEquals(1, containerMetrics.processes().getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testDrainForTasksWithSingleSSP",
  "sourceCode" : "@Test\r\npublic void testDrainForTasksWithSingleSSP() {\r\n    TaskName taskName0 = new TaskName(p0.toString() + \" 0\");\r\n    TaskName taskName1 = new TaskName(p1.toString() + \" 1\");\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    // insert all envelopes followed by drain messages\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(sspA0Drain).thenReturn(sspA1Drain);\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0, taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    runLoop.run();\r\n    // check if process was called once for each task\r\n    verify(task0, times(1)).process(any(), any(), any());\r\n    verify(task1, times(1)).process(any(), any(), any());\r\n    // check if drain was called once for each task followed by commit\r\n    verify(task0, times(1)).drain(any());\r\n    verify(task1, times(1)).drain(any());\r\n    verify(task0, times(1)).commit();\r\n    verify(task1, times(1)).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testDrainForTasksWithMultipleSSP",
  "sourceCode" : "@Test\r\npublic void testDrainForTasksWithMultipleSSP() {\r\n    TaskName taskName0 = new TaskName(p0.toString() + \" 0\");\r\n    TaskName taskName1 = new TaskName(p1.toString() + \" 1\");\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0, sspB0);\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1, sspB1);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    // insert all envelopes followed by drain messages\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(envelopeB00).thenReturn(envelopeB11).thenReturn(sspA0Drain).thenReturn(sspA1Drain).thenReturn(sspB0Drain).thenReturn(sspB1Drain);\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0, taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    runLoop.run();\r\n    // check if process was called twice for each task\r\n    verify(task0, times(2)).process(any(), any(), any());\r\n    verify(task1, times(2)).process(any(), any(), any());\r\n    // check if drain was called once for each task followed by commit\r\n    verify(task0, times(1)).drain(any());\r\n    verify(task1, times(1)).drain(any());\r\n    verify(task0, times(1)).commit();\r\n    verify(task1, times(1)).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testWindow",
  "sourceCode" : "@Test\r\npublic void testWindow() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    long windowMs = 1;\r\n    RunLoopTask task = getMockRunLoopTask(taskName0, sspA0);\r\n    when(task.isWindowableTask()).thenReturn(true);\r\n    final AtomicInteger windowCount = new AtomicInteger(0);\r\n    doAnswer(x -> {\r\n        windowCount.incrementAndGet();\r\n        if (windowCount.get() == 4) {\r\n            x.getArgumentAt(0, ReadableCoordinator.class).shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n        }\r\n        return null;\r\n    }).when(task).window(any());\r\n    when(mockRunLoopConfig.getWindowMs()).thenReturn(windowMs);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task, times(4)).window(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testCommitSingleTask",
  "sourceCode" : "@Test\r\npublic void testCommitSingleTask() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n        coordinator.shutdown(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(this.taskName0, task0);\r\n    tasks.put(taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    //have a null message in between to make sure task0 finishes processing and invoke the commit\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task0).process(any(), any(), any());\r\n    verify(task1).process(any(), any(), any());\r\n    verify(task0).commit();\r\n    verify(task1, never()).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testCommitAllTasks",
  "sourceCode" : "@Test\r\npublic void testCommitAllTasks() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        coordinator.commit(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n        coordinator.shutdown(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(this.taskName0, task0);\r\n    tasks.put(taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    //have a null message in between to make sure task0 finishes processing and invoke the commit\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task0).process(any(), any(), any());\r\n    verify(task1).process(any(), any(), any());\r\n    verify(task0).commit();\r\n    verify(task1).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testShutdownOnConsensus",
  "sourceCode" : "@Test\r\npublic void testShutdownOnConsensus() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task1).process(eq(envelopeA11), any(), any());\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    tasks.put(taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    // consensus is reached after envelope1 is processed.\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task0).process(any(), any(), any());\r\n    verify(task1).process(any(), any(), any());\r\n    assertEquals(2L, containerMetrics.envelopes().getCount());\r\n    assertEquals(2L, containerMetrics.processes().getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testEndOfStreamWithMultipleTasks",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamWithMultipleTasks() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0, sspB0);\r\n    RunLoopTask task1 = getMockRunLoopTask(taskName1, sspA1, sspB1);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    tasks.put(taskName1, task1);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA11).thenReturn(envelopeB00).thenReturn(envelopeB11).thenReturn(sspA0EndOfStream).thenReturn(sspB0EndOfStream).thenReturn(sspB1EndOfStream).thenReturn(sspA1EndOfStream).thenReturn(null);\r\n    runLoop.run();\r\n    verify(task0).process(eq(envelopeA00), any(), any());\r\n    verify(task0).process(eq(envelopeB00), any(), any());\r\n    verify(task0).endOfStream(any());\r\n    verify(task1).process(eq(envelopeA11), any(), any());\r\n    verify(task1).process(eq(envelopeB11), any(), any());\r\n    verify(task1).endOfStream(any());\r\n    assertEquals(8L, containerMetrics.envelopes().getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testEndOfStreamWaitsForInFlightMessages",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamWaitsForInFlightMessages() {\r\n    int maxMessagesInFlight = 2;\r\n    ExecutorService taskExecutor = Executors.newFixedThreadPool(1);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    OffsetManager offsetManager = mock(OffsetManager.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    when(task0.offsetManager()).thenReturn(offsetManager);\r\n    CountDownLatch firstMessageBarrier = new CountDownLatch(2);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        taskExecutor.submit(() -> {\r\n            firstMessageBarrier.await();\r\n            callback.complete();\r\n            return null;\r\n        });\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(1, task0.metrics().messagesInFlight().getValue());\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        callback.complete();\r\n        firstMessageBarrier.countDown();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA01), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(0, task0.metrics().messagesInFlight().getValue());\r\n        assertEquals(2, task0.metrics().asyncCallbackCompleted().getCount());\r\n        return null;\r\n    }).when(task0).endOfStream(any());\r\n    when(mockRunLoopConfig.getMaxConcurrency()).thenReturn(maxMessagesInFlight);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA01).thenReturn(sspA0EndOfStream).thenAnswer(invocation -> {\r\n        // this ensures that the end of stream message has passed through run loop BEFORE the last remaining in flight message completes\r\n        firstMessageBarrier.countDown();\r\n        return null;\r\n    });\r\n    runLoop.run();\r\n    verify(task0).endOfStream(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testDrainWaitsForInFlightMessages",
  "sourceCode" : "@Test\r\npublic void testDrainWaitsForInFlightMessages() {\r\n    int maxMessagesInFlight = 2;\r\n    ExecutorService taskExecutor = Executors.newFixedThreadPool(1);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    OffsetManager offsetManager = mock(OffsetManager.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    when(task0.offsetManager()).thenReturn(offsetManager);\r\n    CountDownLatch firstMessageBarrier = new CountDownLatch(2);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        taskExecutor.submit(() -> {\r\n            firstMessageBarrier.await();\r\n            callback.complete();\r\n            return null;\r\n        });\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(1, task0.metrics().messagesInFlight().getValue());\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        callback.complete();\r\n        firstMessageBarrier.countDown();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA01), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(0, task0.metrics().messagesInFlight().getValue());\r\n        assertEquals(2, task0.metrics().asyncCallbackCompleted().getCount());\r\n        return null;\r\n    }).when(task0).drain(any());\r\n    when(mockRunLoopConfig.getMaxConcurrency()).thenReturn(maxMessagesInFlight);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA01).thenReturn(sspA0Drain).thenAnswer(invocation -> {\r\n        // this ensures that the drain message has passed through run loop BEFORE the flight message\r\n        // completes\r\n        firstMessageBarrier.countDown();\r\n        return null;\r\n    });\r\n    runLoop.run();\r\n    verify(task0).drain(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testEndOfStreamCommitBehavior",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamCommitBehavior() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(0, ReadableCoordinator.class);\r\n        coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n        return null;\r\n    }).when(task0).endOfStream(any());\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(sspA0EndOfStream).thenReturn(null);\r\n    runLoop.run();\r\n    InOrder inOrder = inOrder(task0);\r\n    inOrder.verify(task0).endOfStream(any());\r\n    inOrder.verify(task0).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testDrainCommitBehavior",
  "sourceCode" : "@Test\r\npublic void testDrainCommitBehavior() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(sspA0Drain).thenReturn(null);\r\n    runLoop.run();\r\n    InOrder inOrder = inOrder(task0);\r\n    inOrder.verify(task0).drain(any());\r\n    inOrder.verify(task0).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testCommitWithMessageInFlightWhenAsyncCommitIsEnabled",
  "sourceCode" : "@Test\r\npublic void testCommitWithMessageInFlightWhenAsyncCommitIsEnabled() {\r\n    int maxMessagesInFlight = 2;\r\n    ExecutorService taskExecutor = Executors.newFixedThreadPool(2);\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    OffsetManager offsetManager = mock(OffsetManager.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    when(task0.offsetManager()).thenReturn(offsetManager);\r\n    CountDownLatch firstMessageBarrier = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        taskExecutor.submit(() -> {\r\n            firstMessageBarrier.await();\r\n            coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n            callback.complete();\r\n            return null;\r\n        });\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    CountDownLatch secondMessageBarrier = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        ReadableCoordinator coordinator = invocation.getArgumentAt(1, ReadableCoordinator.class);\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        taskExecutor.submit(() -> {\r\n            // let the first message proceed to ask for a commit\r\n            firstMessageBarrier.countDown();\r\n            // block this message until commit is executed\r\n            secondMessageBarrier.await();\r\n            coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n            callback.complete();\r\n            return null;\r\n        });\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA01), any(), any());\r\n    doAnswer(invocation -> {\r\n        assertEquals(1, task0.metrics().asyncCallbackCompleted().getCount());\r\n        assertEquals(1, task0.metrics().messagesInFlight().getValue());\r\n        secondMessageBarrier.countDown();\r\n        return null;\r\n    }).when(task0).commit();\r\n    when(mockRunLoopConfig.getMaxConcurrency()).thenReturn(maxMessagesInFlight);\r\n    when(mockRunLoopConfig.asyncCommitEnabled()).thenReturn(true);\r\n    Map<TaskName, RunLoopTask> tasks = new HashMap<>();\r\n    tasks.put(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(envelopeA01).thenReturn(null);\r\n    runLoop.run();\r\n    InOrder inOrder = inOrder(task0);\r\n    inOrder.verify(task0).process(eq(envelopeA00), any(), any());\r\n    inOrder.verify(task0).process(eq(envelopeA01), any(), any());\r\n    inOrder.verify(task0).commit();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testExceptionIsPropagated",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testExceptionIsPropagated() {\r\n    SystemConsumers consumerMultiplexer = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        callbackFactory.createCallback().failure(new Exception(\"Intentional failure\"));\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumerMultiplexer, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumerMultiplexer.choose(false)).thenReturn(envelopeA00).thenReturn(sspA0EndOfStream).thenReturn(null);\r\n    runLoop.run();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testWatermarkCallbackTimeout",
  "sourceCode" : "@Test\r\npublic void testWatermarkCallbackTimeout() throws InterruptedException {\r\n    final CountDownLatch watermarkProcessLatch = new CountDownLatch(1);\r\n    when(mockRunLoopConfig.getTaskCallbackTimeoutMs()).thenReturn(5L);\r\n    when(mockRunLoopConfig.getWatermarkCallbackTimeoutMs()).thenReturn(15L);\r\n    SystemConsumers consumers = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        Thread.sleep(10);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(watermarkA0), any(), any());\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        callbackFactory.createCallback().complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        watermarkProcessLatch.countDown();\r\n        callbackFactory.createCallback().complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA01), any(), any());\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumers, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumers.choose(false)).thenReturn(envelopeA00).thenReturn(watermarkA0).thenReturn(envelopeA01).thenReturn(sspA0EndOfStream).thenReturn(null);\r\n    runLoop.run();\r\n    assertTrue(watermarkProcessLatch.await(15L, TimeUnit.MILLISECONDS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestRunLoop.java",
  "methodName" : "testWatermarkCallbackTimeoutThrowsException",
  "sourceCode" : "@Test\r\npublic void testWatermarkCallbackTimeoutThrowsException() {\r\n    when(mockRunLoopConfig.getTaskCallbackTimeoutMs()).thenReturn(10L);\r\n    when(mockRunLoopConfig.getWatermarkCallbackTimeoutMs()).thenReturn(1L);\r\n    SystemConsumers consumers = mock(SystemConsumers.class);\r\n    RunLoopTask task0 = getMockRunLoopTask(taskName0, sspA0);\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        TaskCallback callback = callbackFactory.createCallback();\r\n        Thread.sleep(5);\r\n        callback.complete();\r\n        return null;\r\n    }).when(task0).process(eq(watermarkA0), any(), any());\r\n    doAnswer(invocation -> {\r\n        TaskCallbackFactory callbackFactory = invocation.getArgumentAt(2, TaskCallbackFactory.class);\r\n        callbackFactory.createCallback().complete();\r\n        return null;\r\n    }).when(task0).process(eq(envelopeA00), any(), any());\r\n    Map<TaskName, RunLoopTask> tasks = ImmutableMap.of(taskName0, task0);\r\n    RunLoop runLoop = new RunLoop(tasks, executor, consumers, containerMetrics, () -> 0L, mockRunLoopConfig);\r\n    when(consumers.choose(false)).thenReturn(envelopeA00).thenReturn(watermarkA0).thenReturn(null);\r\n    try {\r\n        runLoop.run();\r\n        fail(\"Watermark callback should have timed out and failed run loop\");\r\n    } catch (SamzaException e) {\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestSamzaContainerMonitorListener.java",
  "methodName" : "testOnUpdate",
  "sourceCode" : "@Test\r\npublic void testOnUpdate() {\r\n    samzaContainerMonitorListener.onUpdate(sample);\r\n    assertEquals(cpuUsage, containerMetrics.totalProcessCpuUsage().getValue());\r\n    float physicalMemoryMb = physicalMemoryBytes / 1024.0F / 1024.0F;\r\n    assertEquals(physicalMemoryMb, containerMetrics.physicalMemoryMb().getValue());\r\n    assertEquals(physicalMemoryMb / containerMemoryMb, containerMetrics.physicalMemoryUtilization().getValue());\r\n    assertEquals(activeThreadCount, containerMetrics.containerActiveThreads().getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\container\\TestSamzaUncaughtExceptionHandler.java",
  "methodName" : "testExceptionHandler",
  "sourceCode" : "@Test\r\npublic void testExceptionHandler() {\r\n    final AtomicBoolean exitCalled = new AtomicBoolean(false);\r\n    Thread.UncaughtExceptionHandler exceptionHandler = new SamzaUncaughtExceptionHandler(() -> exitCalled.getAndSet(true));\r\n    exceptionHandler.uncaughtException(Thread.currentThread(), new SamzaException());\r\n    assertTrue(exitCalled.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetApplicationContainerContext",
  "sourceCode" : "/**\r\n * Given a concrete context, getApplicationContainerContext should return it.\r\n */\r\n@Test\r\npublic void testGetApplicationContainerContext() {\r\n    ApplicationContainerContext applicationContainerContext = mock(ApplicationContainerContext.class);\r\n    Context context = buildWithApplicationContainerContext(applicationContainerContext);\r\n    assertEquals(applicationContainerContext, context.getApplicationContainerContext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetMissingApplicationContainerContext",
  "sourceCode" : "/**\r\n * Given no concrete context, getApplicationContainerContext should throw an exception.\r\n */\r\n@Test(expected = IllegalStateException.class)\r\npublic void testGetMissingApplicationContainerContext() {\r\n    Context context = buildWithApplicationContainerContext(null);\r\n    context.getApplicationContainerContext();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetApplicationTaskContext",
  "sourceCode" : "/**\r\n * Given a concrete context, getApplicationTaskContext should return it.\r\n */\r\n@Test\r\npublic void testGetApplicationTaskContext() {\r\n    ApplicationTaskContext applicationTaskContext = mock(ApplicationTaskContext.class);\r\n    Context context = buildWithApplicationTaskContext(applicationTaskContext);\r\n    assertEquals(applicationTaskContext, context.getApplicationTaskContext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetMissingApplicationTaskContext",
  "sourceCode" : "/**\r\n * Given no concrete context, getApplicationTaskContext should throw an exception.\r\n */\r\n@Test(expected = IllegalStateException.class)\r\npublic void testGetMissingApplicationTaskContext() {\r\n    Context context = buildWithApplicationTaskContext(null);\r\n    context.getApplicationTaskContext();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetExternalContext",
  "sourceCode" : "/**\r\n * Given a concrete context, getExternalContext should return it.\r\n */\r\n@Test\r\npublic void testGetExternalContext() {\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    Context context = buildWithExternalContext(externalContext);\r\n    assertEquals(externalContext, context.getExternalContext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestContextImpl.java",
  "methodName" : "testGetMissingExternalContext",
  "sourceCode" : "/**\r\n * Given no concrete context, getExternalContext should throw an exception.\r\n */\r\n@Test(expected = IllegalStateException.class)\r\npublic void testGetMissingExternalContext() {\r\n    Context context = buildWithExternalContext(null);\r\n    context.getExternalContext();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestInternalTaskContext.java",
  "methodName" : "testRegisterAndFetchObject",
  "sourceCode" : "/**\r\n * Given a registered object, fetchObject should get it. If an object is not registered at a key, then fetchObject\r\n * should return null.\r\n */\r\n@Test\r\npublic void testRegisterAndFetchObject() {\r\n    String value = \"hello world\";\r\n    internalTaskContext.registerObject(\"key\", value);\r\n    assertEquals(value, internalTaskContext.fetchObject(\"key\"));\r\n    assertNull(internalTaskContext.fetchObject(\"not a key\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestTaskContextImpl.java",
  "methodName" : "testGetStore",
  "sourceCode" : "/**\r\n * Given that there is a store corresponding to the storeName, getStore should return the store.\r\n */\r\n@Test\r\npublic void testGetStore() {\r\n    KeyValueStore store = mock(KeyValueStore.class);\r\n    when(keyValueStoreProvider.apply(\"myStore\")).thenReturn(store);\r\n    assertEquals(store, taskContext.getStore(\"myStore\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestTaskContextImpl.java",
  "methodName" : "testGetMissingStore",
  "sourceCode" : "/**\r\n * Given that there is not a store corresponding to the storeName, getStore should throw an exception.\r\n */\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetMissingStore() {\r\n    KeyValueStore store = mock(KeyValueStore.class);\r\n    when(keyValueStoreProvider.apply(\"myStore\")).thenReturn(null);\r\n    assertEquals(store, taskContext.getStore(\"myStore\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\context\\TestTaskContextImpl.java",
  "methodName" : "testSetStartingOffset",
  "sourceCode" : "/**\r\n * Given an SSP and offset, setStartingOffset should delegate to the offset manager.\r\n */\r\n@Test\r\npublic void testSetStartingOffset() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"mySystem\", \"myStream\", new Partition(0));\r\n    taskContext.setStartingOffset(ssp, \"123\");\r\n    verify(offsetManager).setStartingOffset(TASK_NAME, ssp, \"123\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestHttpCoordinatorCommunication.java",
  "methodName" : "testStartStop",
  "sourceCode" : "@Test\r\npublic void testStartStop() {\r\n    this.httpCoordinatorCommunication.start();\r\n    verify(this.httpServer).start();\r\n    this.httpCoordinatorCommunication.start();\r\n    // consecutive stops should still only result in one start\r\n    verify(this.httpServer).start();\r\n    this.httpCoordinatorCommunication.stop();\r\n    verify(this.httpServer).stop();\r\n    this.httpCoordinatorCommunication.stop();\r\n    // consecutive stops should still only result in one stop\r\n    verify(this.httpServer).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestHttpCoordinatorCommunication.java",
  "methodName" : "testStopOnly",
  "sourceCode" : "@Test\r\npublic void testStopOnly() {\r\n    this.httpCoordinatorCommunication.stop();\r\n    verify(this.httpServer, never()).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestJobModelHttpServlet.java",
  "methodName" : "testDoGet",
  "sourceCode" : "@Test\r\npublic void testDoGet() throws IOException {\r\n    JobModel jobModel = jobModel();\r\n    when(this.jobInfoProvider.getSerializedJobModel()).thenReturn(Optional.of(OBJECT_MAPPER.writeValueAsBytes(jobModel)));\r\n    when(this.httpServletResponse.getOutputStream()).thenReturn(this.servletOutputStream);\r\n    this.jobModelHttpServlet.doGet(mock(HttpServletRequest.class), this.httpServletResponse);\r\n    verify(this.httpServletResponse).setContentType(\"application/json;charset=UTF-8\");\r\n    verify(this.httpServletResponse).setStatus(HttpServletResponse.SC_OK);\r\n    verify(this.servletOutputStream).write(aryEq(OBJECT_MAPPER.writeValueAsBytes(jobModel)));\r\n    assertEquals(1, this.metrics.incomingRequests.getCount());\r\n    assertEquals(1, this.metrics.successfulResponses.getCount());\r\n    assertEquals(0, this.metrics.failedResponses.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestJobModelHttpServlet.java",
  "methodName" : "testDoGetMissingJobModel",
  "sourceCode" : "@Test\r\npublic void testDoGetMissingJobModel() throws IOException {\r\n    when(this.jobInfoProvider.getSerializedJobModel()).thenReturn(Optional.empty());\r\n    this.jobModelHttpServlet.doGet(mock(HttpServletRequest.class), this.httpServletResponse);\r\n    verify(this.httpServletResponse).setStatus(HttpServletResponse.SC_NOT_FOUND);\r\n    verify(this.httpServletResponse, never()).setContentType(any());\r\n    verify(this.httpServletResponse, never()).getOutputStream();\r\n    assertEquals(1, this.metrics.incomingRequests.getCount());\r\n    assertEquals(0, this.metrics.successfulResponses.getCount());\r\n    assertEquals(1, this.metrics.failedResponses.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestJobModelHttpServlet.java",
  "methodName" : "testDoGetFailureToWriteResponse",
  "sourceCode" : "@Test\r\npublic void testDoGetFailureToWriteResponse() throws IOException {\r\n    when(this.jobInfoProvider.getSerializedJobModel()).thenReturn(Optional.of(OBJECT_MAPPER.writeValueAsBytes(jobModel())));\r\n    when(this.httpServletResponse.getOutputStream()).thenReturn(this.servletOutputStream);\r\n    doThrow(new IOException(\"failure to write to output stream\")).when(this.servletOutputStream).write(any());\r\n    this.jobModelHttpServlet.doGet(mock(HttpServletRequest.class), this.httpServletResponse);\r\n    verify(this.httpServletResponse).setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\r\n    verify(this.httpServletResponse, never()).setContentType(any());\r\n    assertEquals(1, this.metrics.incomingRequests.getCount());\r\n    assertEquals(0, this.metrics.successfulResponses.getCount());\r\n    assertEquals(1, this.metrics.failedResponses.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\communication\\TestJobModelServingContext.java",
  "methodName" : "testSetGet",
  "sourceCode" : "@Test\r\npublic void testSetGet() throws IOException {\r\n    // return empty if no job model has been set\r\n    assertFalse(this.jobModelServingContext.getSerializedJobModel().isPresent());\r\n    Config config = new MapConfig(ImmutableMap.of(\"samza.user.config\", \"config-value\"));\r\n    Map<String, ContainerModel> containerModelMap = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(new TaskName(\"Partition 0\"), new TaskModel(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"system\", \"stream\", new Partition(0))), new Partition(0)))));\r\n    JobModel jobModel = new JobModel(config, containerModelMap);\r\n    this.jobModelServingContext.setJobModel(jobModel);\r\n    Optional<byte[]> serializedJobModel = this.jobModelServingContext.getSerializedJobModel();\r\n    assertTrue(serializedJobModel.isPresent());\r\n    assertEquals(jobModel, SamzaObjectMapper.getObjectMapper().readValue(serializedJobModel.get(), JobModel.class));\r\n    config = new MapConfig(ImmutableMap.of(\"samza.user.config0\", \"config-value0\"));\r\n    containerModelMap = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(new TaskName(\"Partition 0\"), new TaskModel(new TaskName(\"Partition 0\"), ImmutableSet.of(new SystemStreamPartition(\"system0\", \"stream0\", new Partition(0))), new Partition(0)))));\r\n    jobModel = new JobModel(config, containerModelMap);\r\n    this.jobModelServingContext.setJobModel(jobModel);\r\n    serializedJobModel = this.jobModelServingContext.getSerializedJobModel();\r\n    assertTrue(serializedJobModel.isPresent());\r\n    assertEquals(jobModel, SamzaObjectMapper.getObjectMapper().readValue(serializedJobModel.get(), JobModel.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testReadAfterWrite",
  "sourceCode" : "@Test\r\npublic void testReadAfterWrite() {\r\n    String key = getCoordinatorMessageKey(\"test-key1\");\r\n    byte[] value = getValue(\"test-value1\");\r\n    Assert.assertNull(coordinatorStreamStore.get(key));\r\n    coordinatorStreamStore.put(key, value);\r\n    Assert.assertEquals(value, coordinatorStreamStore.get(key));\r\n    Assert.assertEquals(1, namespaceAwareCoordinatorStreamStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testReadAfterDelete",
  "sourceCode" : "@Test\r\npublic void testReadAfterDelete() {\r\n    String key = getCoordinatorMessageKey(\"test-key1\");\r\n    byte[] value = getValue(\"test-value1\");\r\n    Assert.assertNull(coordinatorStreamStore.get(key));\r\n    coordinatorStreamStore.put(key, value);\r\n    Assert.assertEquals(value, coordinatorStreamStore.get(key));\r\n    coordinatorStreamStore.delete(key);\r\n    Assert.assertNull(coordinatorStreamStore.get(key));\r\n    Assert.assertEquals(0, namespaceAwareCoordinatorStreamStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testReadOfNonExistentKey",
  "sourceCode" : "@Test\r\npublic void testReadOfNonExistentKey() {\r\n    Assert.assertNull(coordinatorStreamStore.get(\"randomKey\"));\r\n    Assert.assertEquals(0, namespaceAwareCoordinatorStreamStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testMultipleUpdatesForSameKey",
  "sourceCode" : "@Test\r\npublic void testMultipleUpdatesForSameKey() {\r\n    String key = getCoordinatorMessageKey(\"test-key1\");\r\n    byte[] value = getValue(\"test-value1\");\r\n    byte[] value1 = getValue(\"test-value2\");\r\n    coordinatorStreamStore.put(key, value);\r\n    coordinatorStreamStore.put(key, value1);\r\n    Assert.assertEquals(value1, coordinatorStreamStore.get(key));\r\n    Assert.assertEquals(1, namespaceAwareCoordinatorStreamStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testPutAll",
  "sourceCode" : "@Test\r\npublic void testPutAll() {\r\n    CoordinatorStreamStore spyCoordinatorStreamStore = Mockito.spy(coordinatorStreamStore);\r\n    String key1 = getCoordinatorMessageKey(\"test-key1\");\r\n    String key2 = getCoordinatorMessageKey(\"test-key2\");\r\n    String key3 = getCoordinatorMessageKey(\"test-key3\");\r\n    String key4 = getCoordinatorMessageKey(\"test-key4\");\r\n    String key5 = getCoordinatorMessageKey(\"test-key5\");\r\n    byte[] value1 = getValue(\"test-value1\");\r\n    byte[] value2 = getValue(\"test-value2\");\r\n    byte[] value3 = getValue(\"test-value3\");\r\n    byte[] value4 = getValue(\"test-value4\");\r\n    byte[] value5 = getValue(\"test-value5\");\r\n    ImmutableMap<String, byte[]> map = ImmutableMap.of(key1, value1, key2, value2, key3, value3, key4, value4, key5, value5);\r\n    spyCoordinatorStreamStore.putAll(map);\r\n    Assert.assertEquals(value1, spyCoordinatorStreamStore.get(key1));\r\n    Assert.assertEquals(value2, spyCoordinatorStreamStore.get(key2));\r\n    Assert.assertEquals(value3, spyCoordinatorStreamStore.get(key3));\r\n    Assert.assertEquals(value4, spyCoordinatorStreamStore.get(key4));\r\n    Assert.assertEquals(value5, spyCoordinatorStreamStore.get(key5));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestCoordinatorStreamStore.java",
  "methodName" : "testAllEntries",
  "sourceCode" : "@Test\r\npublic void testAllEntries() {\r\n    String key = getCoordinatorMessageKey(\"test-key1\");\r\n    String key1 = getCoordinatorMessageKey(\"test-key2\");\r\n    String key2 = getCoordinatorMessageKey(\"test-key3\");\r\n    byte[] value = getValue(\"test-value1\");\r\n    byte[] value1 = getValue(\"test-value2\");\r\n    byte[] value2 = getValue(\"test-value3\");\r\n    coordinatorStreamStore.put(key, value);\r\n    coordinatorStreamStore.put(key1, value1);\r\n    coordinatorStreamStore.put(key2, value2);\r\n    ImmutableMap<String, byte[]> expected = ImmutableMap.of(\"test-key1\", value, \"test-key2\", value1, \"test-key3\", value2);\r\n    Assert.assertEquals(expected, namespaceAwareCoordinatorStreamStore.all());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestNamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "testGetShouldDelegateTheInvocationToUnderlyingStore",
  "sourceCode" : "@Test\r\npublic void testGetShouldDelegateTheInvocationToUnderlyingStore() {\r\n    String value = RandomStringUtils.randomAlphabetic(5);\r\n    String namespacedKey = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, KEY1);\r\n    Mockito.when(coordinatorStreamStore.all()).thenReturn(ImmutableMap.of(namespacedKey, value.getBytes(StandardCharsets.UTF_8)));\r\n    Assert.assertArrayEquals(value.getBytes(StandardCharsets.UTF_8), namespaceAwareCoordinatorStreamStore.get(KEY1));\r\n    Mockito.verify(coordinatorStreamStore).all();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestNamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "testPutShouldDelegateTheInvocationToUnderlyingStore",
  "sourceCode" : "@Test\r\npublic void testPutShouldDelegateTheInvocationToUnderlyingStore() {\r\n    String value = RandomStringUtils.randomAlphabetic(5);\r\n    String namespacedKey = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, KEY1);\r\n    Mockito.doNothing().when(coordinatorStreamStore).put(namespacedKey, value.getBytes(StandardCharsets.UTF_8));\r\n    namespaceAwareCoordinatorStreamStore.put(KEY1, value.getBytes(StandardCharsets.UTF_8));\r\n    Mockito.verify(coordinatorStreamStore).put(namespacedKey, value.getBytes(StandardCharsets.UTF_8));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestNamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "testPutAllShouldDelegateTheInvocationToUnderlyingStore",
  "sourceCode" : "@Test\r\npublic void testPutAllShouldDelegateTheInvocationToUnderlyingStore() {\r\n    byte[] value = RandomStringUtils.randomAlphabetic(5).getBytes(StandardCharsets.UTF_8);\r\n    ImmutableMap<String, byte[]> map = ImmutableMap.of(\"key1\", value, \"key2\", value, \"key3\", value, \"key4\", value, \"key5\", value);\r\n    ImmutableMap<String, byte[]> namespacedMap = ImmutableMap.of(CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, \"key1\"), value, CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, \"key2\"), value, CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, \"key3\"), value, CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, \"key4\"), value, CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, \"key5\"), value);\r\n    Mockito.doNothing().when(coordinatorStreamStore).putAll(namespacedMap);\r\n    namespaceAwareCoordinatorStreamStore.putAll(map);\r\n    Mockito.verify(coordinatorStreamStore).putAll(namespacedMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestNamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "testDeleteShouldDelegateTheInvocationToUnderlyingStore",
  "sourceCode" : "@Test\r\npublic void testDeleteShouldDelegateTheInvocationToUnderlyingStore() {\r\n    String namespacedKey = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, KEY1);\r\n    namespaceAwareCoordinatorStreamStore.delete(KEY1);\r\n    Mockito.verify(coordinatorStreamStore).delete(namespacedKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\metadatastore\\TestNamespaceAwareCoordinatorStreamStore.java",
  "methodName" : "testAllShouldDelegateToUnderlyingMetadaStore",
  "sourceCode" : "@Test\r\npublic void testAllShouldDelegateToUnderlyingMetadaStore() {\r\n    String value = RandomStringUtils.randomAlphabetic(5);\r\n    String key2 = \"key2\";\r\n    String key3 = \"key3\";\r\n    byte[] valueAsBytes = value.getBytes(StandardCharsets.UTF_8);\r\n    String namespacedKey1 = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(namespace, KEY1);\r\n    String namespacedKey2 = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(\"namespace1\", key2);\r\n    String namespacedKey3 = CoordinatorStreamStore.serializeCoordinatorMessageKeyToJson(\"namespace1\", key3);\r\n    Mockito.when(coordinatorStreamStore.all()).thenReturn(ImmutableMap.of(namespacedKey1, valueAsBytes, namespacedKey2, new byte[0], namespacedKey3, new byte[0]));\r\n    Assert.assertEquals(ImmutableMap.of(KEY1, valueAsBytes), namespaceAwareCoordinatorStreamStore.all());\r\n    Mockito.verify(coordinatorStreamStore).all();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testNoExistingJobModel",
  "sourceCode" : "@Test\r\npublic void testNoExistingJobModel() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    JobCoordinatorMetadata newMetadata = setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.copyOf(Arrays.asList(JobMetadataChange.values())), false);\r\n    setUpDiagnosticsManager(jobModel);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    assertEquals(jobModel, this.staticResourceJobCoordinator.getJobModel());\r\n    verifyStartLifecycle();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.diagnosticsManager).start();\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, streamRegexMonitor, newMetadata, SINGLE_SSP_FANOUT);\r\n    verify(this.jobCoordinatorListener).onNewJobModel(PROCESSOR_ID, jobModel);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testSameJobModelAsPrevious",
  "sourceCode" : "@Test\r\npublic void testSameJobModelAsPrevious() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.of(), true);\r\n    setUpDiagnosticsManager(jobModel);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    assertEquals(jobModel, this.staticResourceJobCoordinator.getJobModel());\r\n    verifyStartLifecycle();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.diagnosticsManager).start();\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, streamRegexMonitor, null, null);\r\n    verify(this.jobCoordinatorListener).onNewJobModel(PROCESSOR_ID, jobModel);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testSameDeploymentWithNewJobModel",
  "sourceCode" : "@Test\r\npublic void testSameDeploymentWithNewJobModel() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.of(JobMetadataChange.JOB_MODEL), true);\r\n    setUpDiagnosticsManager(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    verifyStartLifecycle();\r\n    verify(this.jobRestartSignal).restartJob();\r\n    assertNull(this.staticResourceJobCoordinator.getJobModel());\r\n    verifyNoSideEffects(streamPartitionCountMonitor, streamRegexMonitor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testNewDeploymentNewJobModel",
  "sourceCode" : "@Test\r\npublic void testNewDeploymentNewJobModel() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    JobCoordinatorMetadata newMetadata = setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT, JobMetadataChange.JOB_MODEL), true);\r\n    setUpDiagnosticsManager(jobModel);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    assertEquals(jobModel, this.staticResourceJobCoordinator.getJobModel());\r\n    verifyStartLifecycle();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.diagnosticsManager).start();\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, streamRegexMonitor, newMetadata, SINGLE_SSP_FANOUT);\r\n    verify(this.jobCoordinatorListener).onNewJobModel(PROCESSOR_ID, jobModel);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testStartMissingOptionalComponents",
  "sourceCode" : "/**\r\n * Missing {@link StartpointManager}, {@link JobCoordinatorListener}, {@link StreamRegexMonitor}\r\n */\r\n@Test\r\npublic void testStartMissingOptionalComponents() throws IOException {\r\n    this.staticResourceJobCoordinator = spy(new StaticResourceJobCoordinator(PROCESSOR_ID, this.jobModelHelper, this.jobModelServingContext, this.coordinatorCommunication, this.jobCoordinatorMetadataManager, this.streamPartitionCountMonitorFactory, this.streamRegexMonitorFactory, Optional.empty(), this.changelogStreamManager, this.jobRestartSignal, this.metrics, this.systemAdmins, Optional.empty(), Optional.empty(), this.config));\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    when(this.streamRegexMonitorFactory.build(any(), any(), any())).thenReturn(Optional.empty());\r\n    JobCoordinatorMetadata newMetadata = setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.copyOf(Arrays.asList(JobMetadataChange.values())), false);\r\n    doReturn(Optional.empty()).when(this.staticResourceJobCoordinator).buildDiagnosticsManager(JOB_NAME, JOB_ID, jobModel, CoordinationConstants.JOB_COORDINATOR_CONTAINER_NAME, Optional.empty(), Optional.empty(), this.config);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    assertEquals(jobModel, this.staticResourceJobCoordinator.getJobModel());\r\n    verify(this.systemAdmins).start();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.staticResourceJobCoordinator).buildDiagnosticsManager(JOB_NAME, JOB_ID, jobModel, CoordinationConstants.JOB_COORDINATOR_CONTAINER_NAME, Optional.empty(), Optional.empty(), this.config);\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, null, newMetadata, null);\r\n    verifyZeroInteractions(this.jobCoordinatorListener, this.startpointManager);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testStopAfterStart",
  "sourceCode" : "@Test\r\npublic void testStopAfterStart() throws InterruptedException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.copyOf(Arrays.asList(JobMetadataChange.values())), false);\r\n    setUpDiagnosticsManager(jobModel);\r\n    metadataResourceUtil(jobModel);\r\n    // call start in order to set up monitors\r\n    this.staticResourceJobCoordinator.start();\r\n    // call stop to check that the expected components get shut down\r\n    this.staticResourceJobCoordinator.stop();\r\n    verify(this.jobCoordinatorListener).onJobModelExpired();\r\n    verify(this.diagnosticsManager).stop();\r\n    verify(streamPartitionCountMonitor).stop();\r\n    verify(streamRegexMonitor).stop();\r\n    verify(this.coordinatorCommunication).stop();\r\n    verify(this.startpointManager).stop();\r\n    verify(this.systemAdmins).stop();\r\n    verify(this.jobCoordinatorListener).onCoordinatorStop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testStopMissingOptionalComponents",
  "sourceCode" : "@Test\r\npublic void testStopMissingOptionalComponents() {\r\n    this.staticResourceJobCoordinator = spy(new StaticResourceJobCoordinator(PROCESSOR_ID, this.jobModelHelper, this.jobModelServingContext, this.coordinatorCommunication, this.jobCoordinatorMetadataManager, this.streamPartitionCountMonitorFactory, this.streamRegexMonitorFactory, Optional.empty(), this.changelogStreamManager, this.jobRestartSignal, this.metrics, this.systemAdmins, Optional.empty(), Optional.empty(), this.config));\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    when(this.streamRegexMonitorFactory.build(any(), any(), any())).thenReturn(Optional.empty());\r\n    setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.copyOf(Arrays.asList(JobMetadataChange.values())), false);\r\n    doReturn(Optional.empty()).when(this.staticResourceJobCoordinator).buildDiagnosticsManager(JOB_NAME, JOB_ID, jobModel, CoordinationConstants.JOB_COORDINATOR_CONTAINER_NAME, Optional.empty(), Optional.empty(), this.config);\r\n    metadataResourceUtil(jobModel);\r\n    // call start in order to set up monitors\r\n    this.staticResourceJobCoordinator.start();\r\n    this.staticResourceJobCoordinator.stop();\r\n    verify(streamPartitionCountMonitor).stop();\r\n    verify(this.coordinatorCommunication).stop();\r\n    verify(this.systemAdmins).stop();\r\n    verifyZeroInteractions(this.jobCoordinatorListener);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testStopWithoutStart",
  "sourceCode" : "@Test\r\npublic void testStopWithoutStart() {\r\n    this.staticResourceJobCoordinator.stop();\r\n    verify(this.jobCoordinatorListener).onJobModelExpired();\r\n    verify(this.startpointManager).stop();\r\n    verify(this.systemAdmins).stop();\r\n    verify(this.jobCoordinatorListener).onCoordinatorStop();\r\n    verifyZeroInteractions(this.coordinatorCommunication, this.streamPartitionCountMonitorFactory, this.streamRegexMonitorFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testPartitionCountChange",
  "sourceCode" : "@Test\r\npublic void testPartitionCountChange() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = mock(StreamPartitionCountMonitor.class);\r\n    ArgumentCaptor<StreamPartitionCountMonitor.Callback> callbackArgumentCaptor = ArgumentCaptor.forClass(StreamPartitionCountMonitor.Callback.class);\r\n    when(this.streamPartitionCountMonitorFactory.build(eq(jobModelConfig), callbackArgumentCaptor.capture())).thenReturn(streamPartitionCountMonitor);\r\n    StreamRegexMonitor streamRegexMonitor = setupStreamRegexMonitor(jobModel, jobModelConfig);\r\n    JobCoordinatorMetadata newMetadata = setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT, JobMetadataChange.JOB_MODEL), true);\r\n    setUpDiagnosticsManager(jobModel);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    verifyStartLifecycle();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.diagnosticsManager).start();\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, streamRegexMonitor, newMetadata, SINGLE_SSP_FANOUT);\r\n    // call the callback from the monitor\r\n    callbackArgumentCaptor.getValue().onSystemStreamPartitionChange(ImmutableSet.of(SYSTEM_STREAM));\r\n    verify(this.jobRestartSignal).restartJob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\staticresource\\TestStaticResourceJobCoordinator.java",
  "methodName" : "testStreamRegexChange",
  "sourceCode" : "@Test\r\npublic void testStreamRegexChange() throws IOException {\r\n    Config jobModelConfig = mock(Config.class);\r\n    JobModel jobModel = setupJobModel(jobModelConfig);\r\n    StreamPartitionCountMonitor streamPartitionCountMonitor = setupStreamPartitionCountMonitor(jobModelConfig);\r\n    StreamRegexMonitor streamRegexMonitor = mock(StreamRegexMonitor.class);\r\n    ArgumentCaptor<StreamRegexMonitor.Callback> callbackArgumentCaptor = ArgumentCaptor.forClass(StreamRegexMonitor.Callback.class);\r\n    when(this.streamRegexMonitorFactory.build(eq(jobModel), eq(jobModelConfig), callbackArgumentCaptor.capture())).thenReturn(Optional.of(streamRegexMonitor));\r\n    JobCoordinatorMetadata newMetadata = setupJobCoordinatorMetadata(jobModel, jobModelConfig, ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT, JobMetadataChange.JOB_MODEL), true);\r\n    setUpDiagnosticsManager(jobModel);\r\n    MetadataResourceUtil metadataResourceUtil = metadataResourceUtil(jobModel);\r\n    this.staticResourceJobCoordinator.start();\r\n    verifyStartLifecycle();\r\n    verify(this.staticResourceJobCoordinator).doSetLoggingContextConfig(jobModelConfig);\r\n    verify(this.diagnosticsManager).start();\r\n    verifyPrepareWorkerExecutionAndMonitor(jobModel, metadataResourceUtil, streamPartitionCountMonitor, streamRegexMonitor, newMetadata, SINGLE_SSP_FANOUT);\r\n    // call the callback from the monitor\r\n    callbackArgumentCaptor.getValue().onInputStreamsChanged(ImmutableSet.of(SYSTEM_STREAM), ImmutableSet.of(SYSTEM_STREAM, new SystemStream(\"system\", \"stream1\")), ImmutableMap.of(\"system\", Pattern.compile(\"stream.*\")));\r\n    verify(this.jobRestartSignal).restartJob();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testCoordinatorStreamMessage",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStreamMessage() {\r\n    CoordinatorStreamMessage message = new CoordinatorStreamMessage(\"source\");\r\n    assertEquals(\"source\", message.getSource());\r\n    assertEquals(CoordinatorStreamMessage.VERSION, message.getVersion());\r\n    assertNotNull(message.getUsername());\r\n    assertTrue(message.getTimestamp() > 0);\r\n    assertTrue(!message.isDelete());\r\n    CoordinatorStreamMessage secondMessage = new CoordinatorStreamMessage(message.getKeyArray(), message.getMessageMap());\r\n    assertEquals(secondMessage, message);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testCoordinatorStreamMessageIsDelete",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStreamMessageIsDelete() {\r\n    CoordinatorStreamMessage message = new CoordinatorStreamMessage(new Object[] {}, null);\r\n    assertTrue(message.isDelete());\r\n    assertNull(message.getMessageMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testSetConfig",
  "sourceCode" : "@Test\r\npublic void testSetConfig() {\r\n    SetConfig setConfig = new SetConfig(\"source\", \"key\", \"value\");\r\n    assertEquals(SetConfig.TYPE, setConfig.getType());\r\n    assertEquals(\"key\", setConfig.getKey());\r\n    assertEquals(\"value\", setConfig.getConfigValue());\r\n    assertFalse(setConfig.isDelete());\r\n    assertEquals(CoordinatorStreamMessage.VERSION, setConfig.getVersion());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testGetConfigValueShouldReturnNullForDeletedConfigs",
  "sourceCode" : "@Test\r\npublic void testGetConfigValueShouldReturnNullForDeletedConfigs() {\r\n    // Null in passed in the message map to indicate that the message has been deleted.\r\n    CoordinatorStreamMessage message = new CoordinatorStreamMessage(new Object[] { Integer.valueOf(0), SetConfig.TYPE, \"random-key\" }, null);\r\n    SetConfig setConfig = new SetConfig(message);\r\n    assertEquals(null, setConfig.getConfigValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testDelete",
  "sourceCode" : "@Test\r\npublic void testDelete() {\r\n    Delete delete = new Delete(\"source2\", \"key\", \"delete-type\");\r\n    assertEquals(\"delete-type\", delete.getType());\r\n    assertEquals(\"key\", delete.getKey());\r\n    assertNull(delete.getMessageMap());\r\n    assertTrue(delete.isDelete());\r\n    assertEquals(CoordinatorStreamMessage.VERSION, delete.getVersion());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testHashCodeAndEquality",
  "sourceCode" : "@Test\r\npublic void testHashCodeAndEquality() {\r\n    SetConfig message = new SetConfig(\"source\", \"key1\", \"value1\");\r\n    SetConfig message1 = new SetConfig(\"source\", \"key1\", \"value1\");\r\n    SetConfig message2 = new SetConfig(\"source\", \"key2\", \"value1\");\r\n    assertEquals(message.hashCode(), message1.hashCode());\r\n    assertEquals(message, message1);\r\n    assertTrue(!message.equals(message2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamMessage.java",
  "methodName" : "testEqualsNPEonNullValues",
  "sourceCode" : "@Test\r\npublic void testEqualsNPEonNullValues() {\r\n    String[] testKeys = { \"key1\", \"key2\" };\r\n    HashMap<String, Object> messageMap = new HashMap<>();\r\n    messageMap.put(\"values\", new HashMap<String, String>());\r\n    HashMap<String, Object> messageMapWithNullValues = new HashMap<>();\r\n    messageMapWithNullValues.put(\"values\", null);\r\n    CoordinatorStreamMessage message = new CoordinatorStreamMessage(testKeys, messageMap);\r\n    CoordinatorStreamMessage messageWithNullValue = new CoordinatorStreamMessage(testKeys, messageMapWithNullValues);\r\n    assertFalse(\"Should not throw NPE and should not be equal to each other.\", messageWithNullValue.equals(message));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamSystemConsumer.java",
  "methodName" : "testCoordinatorStreamSystemConsumer",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStreamSystemConsumer() {\r\n    Map<String, String> expectedConfig = new LinkedHashMap<String, String>();\r\n    expectedConfig.put(\"job.id\", \"1234\");\r\n    SystemStream systemStream = new SystemStream(\"system\", \"stream\");\r\n    MockSystemConsumer systemConsumer = new MockSystemConsumer(new SystemStreamPartition(systemStream, new Partition(0)));\r\n    CoordinatorStreamSystemConsumer consumer = new CoordinatorStreamSystemConsumer(systemStream, systemConsumer, new SinglePartitionWithoutOffsetsSystemAdmin());\r\n    assertEquals(0, systemConsumer.getRegisterCount());\r\n    consumer.register();\r\n    assertEquals(1, systemConsumer.getRegisterCount());\r\n    assertFalse(systemConsumer.isStarted());\r\n    consumer.start();\r\n    assertTrue(systemConsumer.isStarted());\r\n    try {\r\n        consumer.getConfig();\r\n        fail(\"Should have failed when retrieving config before bootstrapping.\");\r\n    } catch (SamzaException e) {\r\n        // Expected.\r\n    }\r\n    consumer.bootstrap();\r\n    assertEquals(expectedConfig, consumer.getConfig());\r\n    assertFalse(systemConsumer.isStopped());\r\n    consumer.stop();\r\n    assertTrue(systemConsumer.isStopped());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamSystemConsumer.java",
  "methodName" : "testCoordinatorStreamSystemConsumerRegisterOnceOnly",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStreamSystemConsumerRegisterOnceOnly() throws Exception {\r\n    Map<String, String> expectedConfig = new LinkedHashMap<String, String>();\r\n    expectedConfig.put(\"job.id\", \"1234\");\r\n    SystemStream systemStream = new SystemStream(\"system\", \"stream\");\r\n    MockSystemConsumer systemConsumer = new MockSystemConsumer(new SystemStreamPartition(systemStream, new Partition(0)));\r\n    CoordinatorStreamSystemConsumer consumer = new CoordinatorStreamSystemConsumer(systemStream, systemConsumer, new SinglePartitionWithoutOffsetsSystemAdmin());\r\n    assertEquals(0, systemConsumer.getRegisterCount());\r\n    consumer.register();\r\n    assertEquals(1, systemConsumer.getRegisterCount());\r\n    assertFalse(systemConsumer.isStarted());\r\n    consumer.start();\r\n    assertTrue(systemConsumer.isStarted());\r\n    consumer.register();\r\n    assertEquals(1, systemConsumer.getRegisterCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamSystemConsumer.java",
  "methodName" : "testOrderKeyRewrite",
  "sourceCode" : "/**\r\n * Verify that if a particular key-value is written, then another, then the original again,\r\n * that the original occurs last in the set.\r\n */\r\n@Test\r\npublic void testOrderKeyRewrite() throws InterruptedException {\r\n    final SystemStream systemStream = new SystemStream(\"system\", \"stream\");\r\n    final SystemStreamPartition ssp = new SystemStreamPartition(systemStream, new Partition(0));\r\n    final SystemConsumer systemConsumer = mock(SystemConsumer.class);\r\n    final List<IncomingMessageEnvelope> list = new ArrayList<>();\r\n    SetConfig setConfig1 = new SetConfig(\"source\", \"key1\", \"value1\");\r\n    SetConfig setConfig2 = new SetConfig(\"source\", \"key1\", \"value2\");\r\n    SetConfig setConfig3 = new SetConfig(\"source\", \"key1\", \"value1\");\r\n    list.add(createIncomingMessageEnvelope(setConfig1, ssp));\r\n    list.add(createIncomingMessageEnvelope(setConfig2, ssp));\r\n    list.add(createIncomingMessageEnvelope(setConfig3, ssp));\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> messages = new HashMap<SystemStreamPartition, List<IncomingMessageEnvelope>>() {\r\n\r\n        {\r\n            put(ssp, list);\r\n        }\r\n    };\r\n    when(systemConsumer.poll(anySet(), anyLong())).thenReturn(messages, Collections.<SystemStreamPartition, List<IncomingMessageEnvelope>>emptyMap());\r\n    CoordinatorStreamSystemConsumer consumer = new CoordinatorStreamSystemConsumer(systemStream, systemConsumer, new SinglePartitionWithoutOffsetsSystemAdmin());\r\n    consumer.bootstrap();\r\n    Set<CoordinatorStreamMessage> bootstrappedMessages = consumer.getBootstrappedStream(SetConfig.TYPE);\r\n    // First message should have been removed as a duplicate\r\n    assertEquals(2, bootstrappedMessages.size());\r\n    CoordinatorStreamMessage[] coordinatorStreamMessages = bootstrappedMessages.toArray(new CoordinatorStreamMessage[2]);\r\n    assertEquals(setConfig2, coordinatorStreamMessages[0]);\r\n    //Config 3 MUST be the last message, not config 2\r\n    assertEquals(setConfig3, coordinatorStreamMessages[1]);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamSystemProducer.java",
  "methodName" : "testCoordinatorStreamSystemProducer",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStreamSystemProducer() {\r\n    MockCoordinatorStreamSystemFactory.enableMockConsumerCache();\r\n    String source = \"source\";\r\n    SystemStream systemStream = new SystemStream(\"system\", \"stream\");\r\n    MockCoordinatorSystemProducer systemProducer = new MockCoordinatorSystemProducer(source);\r\n    MockSystemAdmin systemAdmin = new MockSystemAdmin();\r\n    CoordinatorStreamSystemProducer producer = new CoordinatorStreamSystemProducer(systemStream, systemProducer, systemAdmin);\r\n    SetConfig setConfig1 = new SetConfig(source, \"job.name\", \"my-job-name\");\r\n    SetConfig setConfig2 = new SetConfig(source, \"job.id\", \"1234\");\r\n    Delete delete = new Delete(source, \"job.name\", SetConfig.TYPE);\r\n    assertFalse(systemProducer.isRegistered());\r\n    producer.register(source);\r\n    assertTrue(systemProducer.isRegistered());\r\n    assertFalse(systemProducer.isStarted());\r\n    producer.start();\r\n    assertTrue(systemProducer.isStarted());\r\n    producer.send(setConfig1);\r\n    producer.send(setConfig2);\r\n    producer.send(delete);\r\n    assertFalse(systemProducer.isStopped());\r\n    producer.stop();\r\n    assertTrue(systemProducer.isStopped());\r\n    List<OutgoingMessageEnvelope> envelopes = systemProducer.getEnvelopes();\r\n    OutgoingMessageEnvelope envelope0 = envelopes.get(0);\r\n    OutgoingMessageEnvelope envelope1 = envelopes.get(1);\r\n    OutgoingMessageEnvelope envelope2 = envelopes.get(2);\r\n    TypeReference<Object[]> keyRef = new TypeReference<Object[]>() {\r\n    };\r\n    TypeReference<Map<String, Object>> msgRef = new TypeReference<Map<String, Object>>() {\r\n    };\r\n    assertEquals(3, envelopes.size());\r\n    assertEquals(new CoordinatorStreamMessage(setConfig1), new CoordinatorStreamMessage(deserialize((byte[]) envelope0.getKey(), keyRef), deserialize((byte[]) envelope0.getMessage(), msgRef)));\r\n    assertEquals(new CoordinatorStreamMessage(setConfig2), new CoordinatorStreamMessage(deserialize((byte[]) envelope1.getKey(), keyRef), deserialize((byte[]) envelope1.getMessage(), msgRef)));\r\n    assertEquals(new CoordinatorStreamMessage(delete), new CoordinatorStreamMessage(deserialize((byte[]) envelope2.getKey(), keyRef), deserialize((byte[]) envelope2.getMessage(), msgRef)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\stream\\TestCoordinatorStreamWriter.java",
  "methodName" : "testCoordinatorStream",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStream() {\r\n    Map<String, String> userConfigs = new HashMap<>();\r\n    userConfigs.put(\"systems.coordinatorStreamWriter.samza.factory\", \"org.apache.samza.coordinator.stream.MockCoordinatorStreamSystemFactory\");\r\n    userConfigs.put(\"app.name\", \"coordinator-stream-writer-test\");\r\n    userConfigs.put(\"job.coordinator.system\", \"coordinatorStreamWriter\");\r\n    Config generatedConfig = JobPlanner.generateSingleJobConfig(userConfigs);\r\n    coordinatorStreamWriter = new CoordinatorStreamWriter(generatedConfig);\r\n    boolean exceptionHappened = false;\r\n    try {\r\n        //get coordinator system producer\r\n        Field coordinatorProducerField = coordinatorStreamWriter.getClass().getDeclaredField(\"coordinatorStreamSystemProducer\");\r\n        coordinatorProducerField.setAccessible(true);\r\n        assertNotNull(coordinatorProducerField.get(coordinatorStreamWriter));\r\n        CoordinatorStreamSystemProducer coordinatorStreamSystemProducer = (CoordinatorStreamSystemProducer) coordinatorProducerField.get(coordinatorStreamWriter);\r\n        //get mock system producer\r\n        Field systemProducerField = coordinatorStreamSystemProducer.getClass().getDeclaredField(\"systemProducer\");\r\n        systemProducerField.setAccessible(true);\r\n        systemProducer = (MockCoordinatorStreamSystemFactory.MockSystemProducer) systemProducerField.get(coordinatorStreamSystemProducer);\r\n        testStart();\r\n        testSendMessage();\r\n        testStop();\r\n    } catch (NoSuchFieldException | IllegalAccessException | NoSuchMethodException | InvocationTargetException e) {\r\n        e.printStackTrace();\r\n        exceptionHappened = true;\r\n    }\r\n    assertFalse(exceptionHappened);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testBasicSingleStream",
  "sourceCode" : "@Test\r\npublic void testBasicSingleStream() {\r\n    addStreamMetadataCacheMetadata(this.streamMetadataCache, ImmutableMap.of(SYSTEM_STREAM0, buildSystemStreamMetadata(4)));\r\n    Map<TaskName, Integer> changeLogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0), ImmutableMap.of());\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0), taskName(2), taskModel(2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changeLogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testBasicMultipleStreams",
  "sourceCode" : "@Test\r\npublic void testBasicMultipleStreams() {\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of());\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testCustomSSPGrouper",
  "sourceCode" : "@Test\r\npublic void testCustomSSPGrouper() {\r\n    // custom grouper only groups into two tasks, so only need 2 changelog partitions\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(2);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of(JobConfig.SSP_GROUPER_FACTORY, Partition0SeparateFactory.class.getName()));\r\n    when(this.grouperMetadata.getProcessorLocality()).thenReturn(ImmutableMap.of(\"0\", mock(LocationId.class), \"1\", mock(LocationId.class)));\r\n    Set<SystemStreamPartition> sspsForTask1 = new ImmutableSet.Builder<SystemStreamPartition>().add(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1))).add(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(2))).add(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(3))).add(new SystemStreamPartition(SYSTEM_STREAM1, new Partition(1))).add(new SystemStreamPartition(SYSTEM_STREAM1, new Partition(2))).build();\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), new TaskModel(taskName(1), sspsForTask1, new Partition(1)))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testCustomTaskNameGrouper",
  "sourceCode" : "@Test\r\npublic void testCustomTaskNameGrouper() {\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of(TaskConfig.GROUPER_FACTORY, Task0SeparateFactory.class.getName()));\r\n    when(this.grouperMetadata.getProcessorLocality()).thenReturn(ImmutableMap.of(\"0\", mock(LocationId.class), \"1\", mock(LocationId.class)));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(2), taskModel(2, 2, 2), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testWithRegexTopicRewriters",
  "sourceCode" : "@Test\r\npublic void testWithRegexTopicRewriters() {\r\n    // this is the SystemStream that is directly in the config\r\n    SystemStream existingSystemStream = new SystemStream(\"existingSystem\", \"existingStream\");\r\n    addStreamMetadataCacheMetadata(this.streamMetadataCache, ImmutableMap.of(SYSTEM_STREAM0, buildSystemStreamMetadata(4), SYSTEM_STREAM1, buildSystemStreamMetadata(3), existingSystemStream, buildSystemStreamMetadata(1)));\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    PowerMockito.mockStatic(ConfigUtil.class);\r\n    // add SYSTEM_STREAM0 for one rewriter\r\n    PowerMockito.when(ConfigUtil.applyRewriter(any(), eq(REGEX_REWRITER0))).thenAnswer(invocation -> addSystemStreamInput(SYSTEM_STREAM0, invocation.getArgumentAt(0, Config.class)));\r\n    // add SYSTEM_STREAM1 for another rewriter\r\n    PowerMockito.when(ConfigUtil.applyRewriter(any(), eq(REGEX_REWRITER1))).thenAnswer(invocation -> addSystemStreamInput(SYSTEM_STREAM1, invocation.getArgumentAt(0, Config.class)));\r\n    Config config = config(ImmutableList.of(existingSystemStream), ImmutableMap.of(JobConfig.CONFIG_REWRITERS, String.format(\"%s,%s\", REGEX_REWRITER0, REGEX_REWRITER1), String.format(JobConfig.CONFIG_REWRITER_CLASS, REGEX_REWRITER0), RegExTopicGenerator.class.getName(), String.format(JobConfig.CONFIG_REWRITER_CLASS, REGEX_REWRITER1), RegExTopicGenerator.class.getName()));\r\n    Set<SystemStreamPartition> sspsForTask0 = ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0)), new SystemStreamPartition(existingSystemStream, new Partition(0)));\r\n    TaskModel taskModel0 = new TaskModel(taskName(0), sspsForTask0, new Partition(0));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel0, taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    Map<String, String> expectedConfigMap = new HashMap<>(config);\r\n    expectedConfigMap.put(TaskConfig.INPUT_STREAMS, String.format(\"%s,%s,%s\", taskInputString(existingSystemStream), taskInputString(SYSTEM_STREAM0), taskInputString(SYSTEM_STREAM1)));\r\n    JobModel expected = new JobModel(new MapConfig(expectedConfigMap), containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testWithSSPFilter",
  "sourceCode" : "@Test\r\npublic void testWithSSPFilter() {\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of(JobConfig.SSP_MATCHER_CLASS, Partition0Or1Filter.class.getName(), JobConfig.SSP_MATCHER_CONFIG_JOB_FACTORY_REGEX, \".*MyJobFactory\", // this needs to match the regex in the line above\r\n    JobConfig.STREAM_JOB_FACTORY_CLASS, \"org.apache.samza.custom.MyJobFactory\"));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testSSPMatcherConfigJobFactoryRegexNotMatched",
  "sourceCode" : "@Test\r\npublic void testSSPMatcherConfigJobFactoryRegexNotMatched() {\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of(JobConfig.SSP_MATCHER_CLASS, Partition0Or1Filter.class.getName(), JobConfig.SSP_MATCHER_CONFIG_JOB_FACTORY_REGEX, \".*MyJobFactory\", // this needs to not match the regex in the line above\r\n    JobConfig.STREAM_JOB_FACTORY_CLASS, \"org.apache.samza.custom.OtherJobFactory\"));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testNoPreviousTasksAssignsNewChangelogPartitions",
  "sourceCode" : "@Test\r\npublic void testNoPreviousTasksAssignsNewChangelogPartitions() {\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of());\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, ImmutableMap.of(), this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testPreviousChangelogPartitionsMaintained",
  "sourceCode" : "@Test\r\npublic void testPreviousChangelogPartitionsMaintained() {\r\n    // existing changelog mapping has 2 tasks, but the job model ultimately will need 4 tasks\r\n    // intentionally using an \"out-of-order\" changelog mapping to make sure it gets maintained\r\n    Map<TaskName, Integer> changelogPartitionMapping = ImmutableMap.of(taskName(0), 1, taskName(1), 0);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of());\r\n    // these task models have special changelog partitions from the previous mapping\r\n    TaskModel taskModel0 = new TaskModel(taskName(0), ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0))), new Partition(1));\r\n    TaskModel taskModel1 = new TaskModel(taskName(1), ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(1))), new Partition(0));\r\n    // tasks 2 and 3 will get assigned new changelog partitions\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel0, taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel1, taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testSSPGrouperProxyUsed",
  "sourceCode" : "@Test\r\npublic void testSSPGrouperProxyUsed() {\r\n    addStreamMetadataCacheMetadata(this.streamMetadataCache, ImmutableMap.of(SYSTEM_STREAM0, buildSystemStreamMetadata(4)));\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(2);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0), ImmutableMap.of(JobConfig.SSP_GROUPER_FACTORY, Partition0SeparateFactory.class.getName(), // need this to trigger SSPGrouperProxy logic\r\n    String.format(StorageConfig.FACTORY, \"myStore\"), \"MyCustomStore\"));\r\n    // custom SSP grouper expects a certain processor locality for another test, so add the locality here too\r\n    when(this.grouperMetadata.getProcessorLocality()).thenReturn(ImmutableMap.of(\"0\", mock(LocationId.class), \"1\", mock(LocationId.class)));\r\n    /*\r\n     * Even though the custom grouper factory would normally send the additional SSPs to task 1, the SSP grouper proxy\r\n     * should give task 0 some of the SSPs.\r\n     */\r\n    when(this.grouperMetadata.getPreviousTaskToSSPAssignment()).thenReturn(ImmutableMap.of(taskName(0), ImmutableList.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0))), taskName(1), ImmutableList.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1)))));\r\n    Set<SystemStreamPartition> sspsForTask0 = ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0)), new SystemStreamPartition(SYSTEM_STREAM0, new Partition(2)));\r\n    Set<SystemStreamPartition> sspsForTask1 = ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1)), new SystemStreamPartition(SYSTEM_STREAM0, new Partition(3)));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), new TaskModel(taskName(0), sspsForTask0, new Partition(0)))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), new TaskModel(taskName(1), sspsForTask1, new Partition(1)))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelCalculator.java",
  "methodName" : "testHostAffinityEnabled",
  "sourceCode" : "@Test\r\npublic void testHostAffinityEnabled() {\r\n    Map<TaskName, Integer> changelogPartitionMapping = changelogPartitionMapping(4);\r\n    Config config = config(ImmutableList.of(SYSTEM_STREAM0, SYSTEM_STREAM1), ImmutableMap.of(ClusterManagerConfig.HOST_AFFINITY_ENABLED, \"true\", // make sure the group method which accepts GrouperMetadata is used\r\n    TaskConfig.GROUPER_FACTORY, GroupByContainerCountOverrideFactory.class.getName()));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    JobModel expected = new JobModel(config, containerModels);\r\n    JobModel actual = JobModelCalculator.INSTANCE.calculateJobModel(config, changelogPartitionMapping, this.streamMetadataCache, this.grouperMetadata);\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testNoPreviousJob",
  "sourceCode" : "@Test\r\npublic void testNoPreviousJob() {\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), ImmutableMap.of(), ImmutableMap.of(), ImmutableMap.of());\r\n    verifyNewTaskAssignments(ImmutableSet.of(), allSSPs(containerModels), containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testSameJobModelAsPrevious",
  "sourceCode" : "@Test\r\npublic void testSameJobModelAsPrevious() {\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    setupOldTaskAssignments(containerModels);\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(4), activeTaskToSSPs(containerModels), activeTaskToContainer(containerModels));\r\n    verifyNewTaskAssignments(null, null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testNewContainerWithProcessorLocality",
  "sourceCode" : "@Test\r\npublic void testNewContainerWithProcessorLocality() {\r\n    Map<String, ContainerModel> oldContainerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0))));\r\n    Map<String, ProcessorLocality> processorLocalities = ImmutableMap.of(\"0\", processorLocality(\"0\", HOST0));\r\n    when(this.localityModel.getProcessorLocalities()).thenReturn(processorLocalities);\r\n    setupOldTaskAssignments(oldContainerModels);\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(processorIdToLocationId(ImmutableMap.of(\"0\", HOST0)), ImmutableMap.of(taskName(0), new LocationId(HOST0)), activeTaskToSSPs(oldContainerModels), activeTaskToContainer(oldContainerModels));\r\n    verifyNewTaskAssignments(taskNameStrings(oldContainerModels, TaskMode.Active), allSSPs(containerModels), containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testAllProcessorLocalityExists",
  "sourceCode" : "@Test\r\npublic void testAllProcessorLocalityExists() {\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0, 0), taskName(2), taskModel(2, 2, 2))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1, 1), taskName(3), taskModel(3, 3))));\r\n    Map<String, ProcessorLocality> processorLocalities = ImmutableMap.of(\"0\", processorLocality(\"0\", HOST0), \"1\", processorLocality(\"1\", HOST1));\r\n    when(this.localityModel.getProcessorLocalities()).thenReturn(processorLocalities);\r\n    setupOldTaskAssignments(containerModels);\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(processorIdToLocationId(ImmutableMap.of(\"0\", HOST0, \"1\", HOST1)), ImmutableMap.of(taskName(0), new LocationId(HOST0), taskName(1), new LocationId(HOST1), taskName(2), new LocationId(HOST0), taskName(3), new LocationId(HOST1)), activeTaskToSSPs(containerModels), activeTaskToContainer(containerModels));\r\n    verifyNewTaskAssignments(null, null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testAddBroadcastInput",
  "sourceCode" : "@Test\r\npublic void testAddBroadcastInput() {\r\n    Map<String, ContainerModel> oldContainerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1))));\r\n    setupOldTaskAssignments(oldContainerModels);\r\n    TaskModel taskModel0 = taskModel(0, ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0))));\r\n    TaskModel taskModel1 = taskModel(1, ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0))));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel0)), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel1)));\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(2), activeTaskToSSPs(oldContainerModels), activeTaskToContainer(oldContainerModels));\r\n    verifyNewTaskAssignments(null, null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testExistingBroadcastInput",
  "sourceCode" : "@Test\r\npublic void testExistingBroadcastInput() {\r\n    TaskModel taskModel0 = taskModel(0, ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(0)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0))));\r\n    TaskModel taskModel1 = taskModel(1, ImmutableSet.of(new SystemStreamPartition(SYSTEM_STREAM0, new Partition(1)), new SystemStreamPartition(SYSTEM_STREAM1, new Partition(0))));\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel0)), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel1)));\r\n    setupOldTaskAssignments(containerModels);\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(2), activeTaskToSSPs(containerModels), activeTaskToContainer(containerModels));\r\n    verifyNewTaskAssignments(null, null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testNewStandbyContainers",
  "sourceCode" : "@Test\r\npublic void testNewStandbyContainers() {\r\n    Map<String, ContainerModel> oldContainerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1))));\r\n    setupOldTaskAssignments(oldContainerModels);\r\n    Map<String, ContainerModel> containerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1))), \"0-0\", new ContainerModel(\"0-0\", ImmutableMap.of(standbyTaskName(0, 0), standbyTaskModel(0, 0))), \"1-0\", new ContainerModel(\"1-0\", ImmutableMap.of(standbyTaskName(1, 0), standbyTaskModel(1, 0))));\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(2), activeTaskToSSPs(oldContainerModels), activeTaskToContainer(oldContainerModels));\r\n    // TaskAssignmentManager.deleteTaskContainerMappings is called once due to change in standby task count\r\n    verifyNewTaskAssignments(ImmutableSet.of(), null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testExistingStandbyContainers",
  "sourceCode" : "@Test\r\npublic void testExistingStandbyContainers() {\r\n    Map<String, ContainerModel> oldContainerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1))), \"0-0\", new ContainerModel(\"0-0\", ImmutableMap.of(standbyTaskName(0, 0), standbyTaskModel(0, 0))), \"1-0\", new ContainerModel(\"1-0\", ImmutableMap.of(standbyTaskName(1, 0), standbyTaskModel(1, 0))));\r\n    setupOldTaskAssignments(oldContainerModels);\r\n    Map<String, ContainerModel> containerModels = new ImmutableMap.Builder<String, ContainerModel>().put(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0)))).put(\"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1)))).put(\"0-0\", new ContainerModel(\"0-0\", ImmutableMap.of(standbyTaskName(0, 0), standbyTaskModel(0, 0)))).put(\"1-0\", new ContainerModel(\"1-0\", ImmutableMap.of(standbyTaskName(1, 0), standbyTaskModel(1, 0)))).put(\"0-1\", new ContainerModel(\"0-1\", ImmutableMap.of(standbyTaskName(0, 1), standbyTaskModel(0, 1)))).put(\"1-1\", new ContainerModel(\"1-1\", ImmutableMap.of(standbyTaskName(1, 1), standbyTaskModel(1, 1)))).build();\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(2), activeTaskToSSPs(oldContainerModels), activeTaskToContainer(oldContainerModels));\r\n    verifyNewTaskAssignments(taskNameStrings(oldContainerModels, TaskMode.Standby), null, containerToTaskToMode(containerModels), sspToTasks(containerModels));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelHelper.java",
  "methodName" : "testNewStandbyTasks",
  "sourceCode" : "@Test\r\npublic void testNewStandbyTasks() {\r\n    Map<String, ContainerModel> oldContainerModels = ImmutableMap.of(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0))), \"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1))), \"0-0\", new ContainerModel(\"0-0\", ImmutableMap.of(standbyTaskName(0, 0), standbyTaskModel(0, 0))), \"1-0\", new ContainerModel(\"1-0\", ImmutableMap.of(standbyTaskName(1, 0), standbyTaskModel(1, 0))));\r\n    setupOldTaskAssignments(oldContainerModels);\r\n    Map<String, ContainerModel> containerModels = new ImmutableMap.Builder<String, ContainerModel>().put(\"0\", new ContainerModel(\"0\", ImmutableMap.of(taskName(0), taskModel(0, 0)))).put(\"1\", new ContainerModel(\"1\", ImmutableMap.of(taskName(1), taskModel(1, 1)))).put(\"2\", new ContainerModel(\"2\", ImmutableMap.of(taskName(2), taskModel(2, 2)))).put(\"0-0\", new ContainerModel(\"0-0\", ImmutableMap.of(standbyTaskName(0, 0), standbyTaskModel(0, 0)))).put(\"1-0\", new ContainerModel(\"1-0\", ImmutableMap.of(standbyTaskName(1, 0), standbyTaskModel(1, 0)))).put(\"2-0\", new ContainerModel(\"2-0\", ImmutableMap.of(standbyTaskName(2, 0), standbyTaskModel(2, 0)))).build();\r\n    runAndCheckNewJobModel(config(), containerModels);\r\n    verifyGrouperMetadata(anyHostProcessorIdToLocationId(), anyHostTaskNameToLocationId(2), activeTaskToSSPs(oldContainerModels), activeTaskToContainer(oldContainerModels));\r\n    //noinspection unchecked: ArgumentCaptor doesn't ideally handle multiple levels of generics, so need cast\r\n    ArgumentCaptor<Iterable<String>> deleteTaskContainerMappingsCaptor = (ArgumentCaptor<Iterable<String>>) (ArgumentCaptor<?>) ArgumentCaptor.forClass(Iterable.class);\r\n    verify(this.taskAssignmentManager, times(2)).deleteTaskContainerMappings(deleteTaskContainerMappingsCaptor.capture());\r\n    assertEquals(taskNameStrings(oldContainerModels, TaskMode.Active), ImmutableSet.copyOf(deleteTaskContainerMappingsCaptor.getAllValues().get(0)));\r\n    assertEquals(taskNameStrings(oldContainerModels, TaskMode.Standby), ImmutableSet.copyOf(deleteTaskContainerMappingsCaptor.getAllValues().get(1)));\r\n    verify(this.taskPartitionAssignmentManager).delete(allSSPs(containerModels));\r\n    verify(this.taskPartitionAssignmentManager).delete(any());\r\n    verify(this.taskAssignmentManager).writeTaskContainerMappings(containerToTaskToMode(containerModels));\r\n    verify(this.taskAssignmentManager).writeTaskContainerMappings(any());\r\n    verify(this.taskPartitionAssignmentManager).writeTaskPartitionAssignments(sspToTasks(containerModels));\r\n    verify(this.taskPartitionAssignmentManager).writeTaskPartitionAssignments(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelMonitors.java",
  "methodName" : "testMonitorsExist",
  "sourceCode" : "@Test\r\npublic void testMonitorsExist() {\r\n    JobModelMonitors jobModelMonitors = new JobModelMonitors(this.streamPartitionCountMonitor, this.streamRegexMonitor);\r\n    jobModelMonitors.start();\r\n    verify(this.streamPartitionCountMonitor).start();\r\n    verify(this.streamRegexMonitor).start();\r\n    jobModelMonitors.stop();\r\n    verify(this.streamPartitionCountMonitor).stop();\r\n    verify(this.streamRegexMonitor).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelMonitors.java",
  "methodName" : "testMissingMonitors",
  "sourceCode" : "@Test\r\npublic void testMissingMonitors() {\r\n    JobModelMonitors jobModelMonitors = new JobModelMonitors(null, null);\r\n    // expect no failures\r\n    jobModelMonitors.start();\r\n    jobModelMonitors.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestJobModelMonitors.java",
  "methodName" : "testStopBeforeStart",
  "sourceCode" : "@Test\r\npublic void testStopBeforeStart() {\r\n    JobModelMonitors jobModelMonitors = new JobModelMonitors(this.streamPartitionCountMonitor, this.streamRegexMonitor);\r\n    jobModelMonitors.stop();\r\n    verifyZeroInteractions(this.streamPartitionCountMonitor, this.streamRegexMonitor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestMetadataResourceUtil.java",
  "methodName" : "testLoadWithCheckpointConfigured",
  "sourceCode" : "@Test\r\npublic void testLoadWithCheckpointConfigured() {\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(TaskConfig.CHECKPOINT_MANAGER_FACTORY, TestCheckpointManagerFactory.class.getName()));\r\n    MetadataResourceUtil metadataResourceUtil = Mockito.spy(new MetadataResourceUtil(mockJobModel, mockMetricsRegistry, mapConfig));\r\n    Mockito.doNothing().when(metadataResourceUtil).createChangelogStreams();\r\n    metadataResourceUtil.createResources();\r\n    // Never get config from job model. In standalone, this is empty.\r\n    Mockito.verify(mockJobModel, Mockito.never()).getConfig();\r\n    Mockito.verify(metadataResourceUtil.getCheckpointManager()).createResources();\r\n    Mockito.verify(metadataResourceUtil).createChangelogStreams();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestMetadataResourceUtil.java",
  "methodName" : "testLoadWithoutCheckpointConfigured",
  "sourceCode" : "@Test\r\npublic void testLoadWithoutCheckpointConfigured() {\r\n    MapConfig mapConfig = new MapConfig();\r\n    MetadataResourceUtil metadataResourceUtil = Mockito.spy(new MetadataResourceUtil(mockJobModel, mockMetricsRegistry, mapConfig));\r\n    Mockito.doNothing().when(metadataResourceUtil).createChangelogStreams();\r\n    metadataResourceUtil.createResources();\r\n    // Never get config from job model. In standalone, this is empty.\r\n    Mockito.verify(mockJobModel, Mockito.never()).getConfig();\r\n    Assert.assertNull(metadataResourceUtil.getCheckpointManager());\r\n    Mockito.verify(metadataResourceUtil).createChangelogStreams();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestNoProcessorJobCoordinatorListener.java",
  "methodName" : "testOnCoordinatorStop",
  "sourceCode" : "@Test\r\npublic void testOnCoordinatorStop() {\r\n    this.noProcessorJobCoordinatorListener.onCoordinatorStop();\r\n    verify(this.waitForShutdownLatch).countDown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestNoProcessorJobCoordinatorListener.java",
  "methodName" : "testOnCoordinatorFailure",
  "sourceCode" : "@Test\r\npublic void testOnCoordinatorFailure() {\r\n    this.noProcessorJobCoordinatorListener.onCoordinatorFailure(new RuntimeException());\r\n    verify(this.waitForShutdownLatch).countDown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestRunIdGenerator.java",
  "methodName" : "testSingleProcessorWriteRunId",
  "sourceCode" : "@Test\r\npublic void testSingleProcessorWriteRunId() throws Exception {\r\n    // When there is a single processor registered with ClusterMembership\r\n    // RunIdGenerator should write a new run id to the MetadataStore\r\n    prepareRunIdGenerator(1);\r\n    runIdGenerator.getRunId();\r\n    verify(coordinationUtils, Mockito.times(1)).getClusterMembership();\r\n    verify(coordinationUtils, Mockito.times(1)).getLock(anyString());\r\n    verify(distributedLock, Mockito.times(1)).lock(anyObject());\r\n    verify(distributedLock, Mockito.times(1)).unlock();\r\n    verify(membership, Mockito.times(1)).registerProcessor();\r\n    verify(membership, Mockito.times(1)).getNumberOfProcessors();\r\n    verify(metadataStore, Mockito.times(1)).put(eq(CoordinationConstants.RUNID_STORE_KEY), any(byte[].class));\r\n    verify(metadataStore, Mockito.times(1)).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\coordinator\\TestRunIdGenerator.java",
  "methodName" : "testTwoProcessorsReadRunId",
  "sourceCode" : "@Test\r\npublic void testTwoProcessorsReadRunId() throws Exception {\r\n    // When there are two processors registered with ClusterMembership\r\n    // RunIdGenerator should read run id from the MetadataStore\r\n    prepareRunIdGenerator(2);\r\n    String runId = runIdGenerator.getRunId().get();\r\n    assertEquals(\"Runid was not read from store\", runId, FAKE_RUNID);\r\n    verify(coordinationUtils, Mockito.times(1)).getClusterMembership();\r\n    verify(coordinationUtils, Mockito.times(1)).getLock(anyString());\r\n    verify(distributedLock, Mockito.times(1)).lock(anyObject());\r\n    verify(distributedLock, Mockito.times(1)).unlock();\r\n    verify(membership, Mockito.times(1)).registerProcessor();\r\n    verify(membership, Mockito.times(1)).getNumberOfProcessors();\r\n    verify(metadataStore, Mockito.times(1)).get(CoordinationConstants.RUNID_STORE_KEY);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testDiagnosticsManagerStart",
  "sourceCode" : "@Test\r\npublic void testDiagnosticsManagerStart() {\r\n    SystemProducer mockSystemProducer = Mockito.mock(SystemProducer.class);\r\n    DiagnosticsManager diagnosticsManager = new DiagnosticsManager(JOB_NAME, JOB_ID, containerModels, CONTAINER_MB, CONTAINER_NUM_CORES, NUM_PERSISTENT_STORES, MAX_HEAP_SIZE, CONTAINER_THREAD_POOL_SIZE, \"0\", EXECUTION_ENV_CONTAINER_ID, SAMZA_EPOCH_ID, TASK_CLASS_VERSION, SAMZA_VERSION, HOSTNAME, diagnosticsSystemStream, mockSystemProducer, Duration.ofSeconds(1), mockExecutorService, AUTOSIZING_ENABLED, config, this.clock);\r\n    diagnosticsManager.start();\r\n    Mockito.verify(mockSystemProducer, Mockito.times(1)).start();\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).scheduleWithFixedDelay(Mockito.any(Runnable.class), Mockito.anyLong(), Mockito.anyLong(), Mockito.any(TimeUnit.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testDiagnosticsManagerStop",
  "sourceCode" : "@Test\r\npublic void testDiagnosticsManagerStop() throws InterruptedException {\r\n    SystemProducer mockSystemProducer = Mockito.mock(SystemProducer.class);\r\n    Mockito.when(mockExecutorService.isTerminated()).thenReturn(true);\r\n    Duration terminationDuration = Duration.ofSeconds(1);\r\n    DiagnosticsManager diagnosticsManager = new DiagnosticsManager(JOB_NAME, JOB_ID, containerModels, CONTAINER_MB, CONTAINER_NUM_CORES, NUM_PERSISTENT_STORES, MAX_HEAP_SIZE, CONTAINER_THREAD_POOL_SIZE, \"0\", EXECUTION_ENV_CONTAINER_ID, SAMZA_EPOCH_ID, TASK_CLASS_VERSION, SAMZA_VERSION, HOSTNAME, diagnosticsSystemStream, mockSystemProducer, terminationDuration, mockExecutorService, AUTOSIZING_ENABLED, config, this.clock);\r\n    diagnosticsManager.stop();\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).shutdown();\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).awaitTermination(terminationDuration.toMillis(), TimeUnit.MILLISECONDS);\r\n    Mockito.verify(mockExecutorService, Mockito.never()).shutdownNow();\r\n    Mockito.verify(mockSystemProducer, Mockito.times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testDiagnosticsManagerForceStop",
  "sourceCode" : "@Test\r\npublic void testDiagnosticsManagerForceStop() throws InterruptedException {\r\n    SystemProducer mockSystemProducer = Mockito.mock(SystemProducer.class);\r\n    Mockito.when(mockExecutorService.isTerminated()).thenReturn(false);\r\n    Duration terminationDuration = Duration.ofSeconds(1);\r\n    DiagnosticsManager diagnosticsManager = new DiagnosticsManager(JOB_NAME, JOB_ID, containerModels, CONTAINER_MB, CONTAINER_NUM_CORES, NUM_PERSISTENT_STORES, MAX_HEAP_SIZE, CONTAINER_THREAD_POOL_SIZE, \"0\", EXECUTION_ENV_CONTAINER_ID, SAMZA_EPOCH_ID, TASK_CLASS_VERSION, SAMZA_VERSION, HOSTNAME, diagnosticsSystemStream, mockSystemProducer, terminationDuration, mockExecutorService, AUTOSIZING_ENABLED, config, this.clock);\r\n    diagnosticsManager.stop();\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).shutdown();\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).awaitTermination(terminationDuration.toMillis(), TimeUnit.MILLISECONDS);\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).shutdownNow();\r\n    Mockito.verify(mockSystemProducer, Mockito.times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testDiagnosticsStreamFirstMessagePublish",
  "sourceCode" : "@Test\r\npublic void testDiagnosticsStreamFirstMessagePublish() {\r\n    // invoking start will do a syncrhonous publish to the stream because of our mocked scheduled exec service\r\n    this.diagnosticsManager.start();\r\n    Assert.assertEquals(\"One message should have been published\", 1, mockSystemProducer.getEnvelopeList().size());\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(0);\r\n    validateOutgoingMessageEnvelope(outgoingMessageEnvelope);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testNoDualPublish",
  "sourceCode" : "@Test\r\npublic void testNoDualPublish() {\r\n    // Across two successive run() invocations only a single message should be published\r\n    this.diagnosticsManager.start();\r\n    this.diagnosticsManager.start();\r\n    Assert.assertEquals(\"One message should have been published\", 1, mockSystemProducer.getEnvelopeList().size());\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(0);\r\n    validateMetricsHeader(outgoingMessageEnvelope, FIRST_SEND_TIME);\r\n    validateOutgoingMessageEnvelope(outgoingMessageEnvelope);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testSecondPublishWithProcessorStopInSecondMessage",
  "sourceCode" : "@Test\r\npublic void testSecondPublishWithProcessorStopInSecondMessage() {\r\n    // Across two successive run() invocations two messages should be published if stop events are added\r\n    this.diagnosticsManager.start();\r\n    this.diagnosticsManager.addProcessorStopEvent(\"0\", EXECUTION_ENV_CONTAINER_ID, HOSTNAME, 102);\r\n    this.diagnosticsManager.start();\r\n    Assert.assertEquals(\"Two messages should have been published\", 2, mockSystemProducer.getEnvelopeList().size());\r\n    // Validate the first message\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(0);\r\n    validateMetricsHeader(outgoingMessageEnvelope, FIRST_SEND_TIME);\r\n    validateOutgoingMessageEnvelope(outgoingMessageEnvelope);\r\n    // Validate the second message's header\r\n    outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(1);\r\n    validateMetricsHeader(outgoingMessageEnvelope, SECOND_SEND_TIME);\r\n    // Validate the second message's body (should be all empty except for the processor-stop-event)\r\n    MetricsSnapshot metricsSnapshot = new MetricsSnapshotSerdeV2().fromBytes((byte[]) outgoingMessageEnvelope.getMessage());\r\n    DiagnosticsStreamMessage diagnosticsStreamMessage = DiagnosticsStreamMessage.convertToDiagnosticsStreamMessage(metricsSnapshot);\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerMb());\r\n    Assert.assertNull(diagnosticsStreamMessage.getExceptionEvents());\r\n    Assert.assertEquals(diagnosticsStreamMessage.getProcessorStopEvents(), Arrays.asList(new ProcessorStopEvent(\"0\", EXECUTION_ENV_CONTAINER_ID, HOSTNAME, 102)));\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerModels());\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerNumCores());\r\n    Assert.assertNull(diagnosticsStreamMessage.getNumPersistentStores());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsManager.java",
  "methodName" : "testSecondPublishWithExceptionInSecondMessage",
  "sourceCode" : "@Test\r\npublic void testSecondPublishWithExceptionInSecondMessage() {\r\n    // Across two successive run() invocations two messages should be published if stop events are added\r\n    this.diagnosticsManager.start();\r\n    DiagnosticsExceptionEvent diagnosticsExceptionEvent = new DiagnosticsExceptionEvent(System.currentTimeMillis(), new RuntimeException(\"exception\"), new HashMap());\r\n    this.diagnosticsManager.addExceptionEvent(diagnosticsExceptionEvent);\r\n    this.diagnosticsManager.start();\r\n    Assert.assertEquals(\"Two messages should have been published\", 2, mockSystemProducer.getEnvelopeList().size());\r\n    // Validate the first message\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(0);\r\n    validateMetricsHeader(outgoingMessageEnvelope, FIRST_SEND_TIME);\r\n    validateOutgoingMessageEnvelope(outgoingMessageEnvelope);\r\n    // Validate the second message's header\r\n    outgoingMessageEnvelope = mockSystemProducer.getEnvelopeList().get(1);\r\n    validateMetricsHeader(outgoingMessageEnvelope, SECOND_SEND_TIME);\r\n    // Validate the second message's body (should be all empty except for the processor-stop-event)\r\n    MetricsSnapshot metricsSnapshot = new MetricsSnapshotSerdeV2().fromBytes((byte[]) outgoingMessageEnvelope.getMessage());\r\n    DiagnosticsStreamMessage diagnosticsStreamMessage = DiagnosticsStreamMessage.convertToDiagnosticsStreamMessage(metricsSnapshot);\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerMb());\r\n    Assert.assertEquals(Arrays.asList(diagnosticsExceptionEvent), diagnosticsStreamMessage.getExceptionEvents());\r\n    Assert.assertNull(diagnosticsStreamMessage.getProcessorStopEvents());\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerModels());\r\n    Assert.assertNull(diagnosticsStreamMessage.getContainerNumCores());\r\n    Assert.assertNull(diagnosticsStreamMessage.getNumPersistentStores());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsStreamMessage.java",
  "methodName" : "basicTest",
  "sourceCode" : "/**\r\n * Tests basic operations on {@link DiagnosticsStreamMessage}.\r\n */\r\n@Test\r\npublic void basicTest() {\r\n    DiagnosticsStreamMessage diagnosticsStreamMessage = getDiagnosticsStreamMessage(Optional.of(SAMZA_EPOCH_ID));\r\n    Collection<DiagnosticsExceptionEvent> exceptionEventList = getExceptionList();\r\n    diagnosticsStreamMessage.addDiagnosticsExceptionEvents(exceptionEventList);\r\n    diagnosticsStreamMessage.addProcessorStopEvents(getProcessorStopEventList());\r\n    diagnosticsStreamMessage.addContainerModels(getSampleContainerModels());\r\n    Assert.assertEquals(1024, (int) diagnosticsStreamMessage.getContainerMb());\r\n    Assert.assertEquals(2, (int) diagnosticsStreamMessage.getContainerNumCores());\r\n    Assert.assertEquals(3, (int) diagnosticsStreamMessage.getNumPersistentStores());\r\n    Assert.assertEquals(config, diagnosticsStreamMessage.getConfig());\r\n    Assert.assertEquals(exceptionEventList, diagnosticsStreamMessage.getExceptionEvents());\r\n    Assert.assertEquals(getSampleContainerModels(), diagnosticsStreamMessage.getContainerModels());\r\n    Assert.assertEquals(diagnosticsStreamMessage.getProcessorStopEvents(), getProcessorStopEventList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsStreamMessage.java",
  "methodName" : "serdeTest",
  "sourceCode" : "/**\r\n * Tests serialization and deserialization of a {@link DiagnosticsStreamMessage}\r\n */\r\n@Test\r\npublic void serdeTest() {\r\n    DiagnosticsStreamMessage diagnosticsStreamMessage = getDiagnosticsStreamMessage(Optional.of(SAMZA_EPOCH_ID));\r\n    Collection<DiagnosticsExceptionEvent> exceptionEventList = getExceptionList();\r\n    diagnosticsStreamMessage.addDiagnosticsExceptionEvents(exceptionEventList);\r\n    diagnosticsStreamMessage.addProcessorStopEvents(getProcessorStopEventList());\r\n    diagnosticsStreamMessage.addContainerModels(getSampleContainerModels());\r\n    MetricsSnapshot metricsSnapshot = diagnosticsStreamMessage.convertToMetricsSnapshot();\r\n    MetricsHeader expectedHeader = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXECUTION_ENV_CONTAINER_ID, Optional.of(SAMZA_EPOCH_ID), DiagnosticsManager.class.getName(), TASK_CLASS_VERSION, SAMZA_VERSION, HOSTNAME, timestamp, resetTimestamp);\r\n    Assert.assertEquals(metricsSnapshot.getHeader(), expectedHeader);\r\n    Map<String, Map<String, Object>> metricsMap = metricsSnapshot.getMetrics().getAsMap();\r\n    Assert.assertTrue(metricsMap.get(\"org.apache.samza.container.SamzaContainerMetrics\").containsKey(\"exceptions\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"containerModels\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"numPersistentStores\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"containerNumCores\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"containerMemoryMb\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"stopEvents\"));\r\n    Assert.assertTrue(metricsMap.get(DiagnosticsManager.class.getName()).containsKey(\"config\"));\r\n    DiagnosticsStreamMessage convertedDiagnosticsStreamMessage = DiagnosticsStreamMessage.convertToDiagnosticsStreamMessage(metricsSnapshot);\r\n    Assert.assertEquals(convertedDiagnosticsStreamMessage, diagnosticsStreamMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\diagnostics\\TestDiagnosticsStreamMessage.java",
  "methodName" : "testSerdeEmptySamzaEpochIdInHeader",
  "sourceCode" : "@Test\r\npublic void testSerdeEmptySamzaEpochIdInHeader() {\r\n    DiagnosticsStreamMessage diagnosticsStreamMessage = getDiagnosticsStreamMessage(Optional.empty());\r\n    MetricsSnapshot metricsSnapshot = diagnosticsStreamMessage.convertToMetricsSnapshot();\r\n    MetricsHeader expectedHeader = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXECUTION_ENV_CONTAINER_ID, Optional.empty(), DiagnosticsManager.class.getName(), TASK_CLASS_VERSION, SAMZA_VERSION, HOSTNAME, timestamp, resetTimestamp);\r\n    Assert.assertEquals(metricsSnapshot.getHeader(), expectedHeader);\r\n    DiagnosticsStreamMessage convertedDiagnosticsStreamMessage = DiagnosticsStreamMessage.convertToDiagnosticsStreamMessage(metricsSnapshot);\r\n    Assert.assertEquals(convertedDiagnosticsStreamMessage, diagnosticsStreamMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testConstructorFailureWhenDrainManagerIsNull",
  "sourceCode" : "@Test()\r\npublic void testConstructorFailureWhenDrainManagerIsNull() {\r\n    exceptionRule.expect(NullPointerException.class);\r\n    exceptionRule.expectMessage(\"MetadataStore parameter cannot be null.\");\r\n    DrainMonitor unusedMonitor = new DrainMonitor(null, null, 100L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testConstructorFailureWhenConfigIsNull",
  "sourceCode" : "@Test()\r\npublic void testConstructorFailureWhenConfigIsNull() {\r\n    exceptionRule.expect(NullPointerException.class);\r\n    exceptionRule.expectMessage(\"Config parameter cannot be null.\");\r\n    DrainMonitor unusedMonitor = new DrainMonitor(Mockito.mock(MetadataStore.class), null, 100L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testConstructorFailureWithInvalidPollingInterval",
  "sourceCode" : "@Test()\r\npublic void testConstructorFailureWithInvalidPollingInterval() {\r\n    exceptionRule.expect(IllegalArgumentException.class);\r\n    exceptionRule.expectMessage(\"Polling interval specified is 0 ms. It should be greater than 0.\");\r\n    DrainMonitor unusedMonitor = new DrainMonitor(Mockito.mock(MetadataStore.class), Mockito.mock(Config.class), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testDrainMonitorStartFailureWhenCallbackIsNotSet",
  "sourceCode" : "@Test()\r\npublic void testDrainMonitorStartFailureWhenCallbackIsNotSet() {\r\n    exceptionRule.expect(IllegalStateException.class);\r\n    exceptionRule.expectMessage(\"Drain Callback needs to be set using registerCallback(callback) prior to \" + \"starting the DrainManager.\");\r\n    DrainMonitor drainMonitor = new DrainMonitor(coordinatorStreamStore, CONFIG, 100L);\r\n    drainMonitor.start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testSuccessfulCallbackRegistration",
  "sourceCode" : "@Test\r\npublic void testSuccessfulCallbackRegistration() {\r\n    DrainMonitor drainMonitor = new DrainMonitor(coordinatorStreamStore, CONFIG, 100L);\r\n    DrainMonitor.DrainCallback emptyCallback = () -> {\r\n    };\r\n    boolean callbackRegistrationResult1 = drainMonitor.registerDrainCallback(emptyCallback);\r\n    // first registration of callback should succeed\r\n    Assert.assertTrue(callbackRegistrationResult1);\r\n    boolean callbackRegistrationResult2 = drainMonitor.registerDrainCallback(emptyCallback);\r\n    // repeat registration of callback should fail\r\n    Assert.assertFalse(callbackRegistrationResult2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testCallbackCalledIfMonitorEncountersDrainOnStart",
  "sourceCode" : "@Test\r\npublic void testCallbackCalledIfMonitorEncountersDrainOnStart() throws InterruptedException {\r\n    final AtomicInteger numCallbacks = new AtomicInteger(0);\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    // write drain before monitor start\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore);\r\n    DrainMonitor drainMonitor = new DrainMonitor(coordinatorStreamStore, CONFIG);\r\n    drainMonitor.registerDrainCallback(() -> {\r\n        numCallbacks.incrementAndGet();\r\n        latch.countDown();\r\n    });\r\n    // monitor shouldn't go into RUNNING state as DrainNotification was already present and it shouldn't start poll\r\n    drainMonitor.start();\r\n    if (!latch.await(2, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"Timed out waiting for drain callback to complete\");\r\n    }\r\n    Assert.assertEquals(1, numCallbacks.get());\r\n    Assert.assertEquals(DrainMonitor.State.INIT, drainMonitor.getState());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testCallbackCalledOnDrain",
  "sourceCode" : "@Test\r\npublic void testCallbackCalledOnDrain() throws InterruptedException {\r\n    final AtomicInteger numCallbacks = new AtomicInteger(0);\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    DrainMonitor drainMonitor = new DrainMonitor(coordinatorStreamStore, CONFIG, 100L);\r\n    drainMonitor.registerDrainCallback(() -> {\r\n        numCallbacks.incrementAndGet();\r\n        latch.countDown();\r\n    });\r\n    drainMonitor.start();\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore);\r\n    if (!latch.await(2, TimeUnit.SECONDS)) {\r\n        Assert.fail(\"Timed out waiting for drain callback to complete\");\r\n    }\r\n    Assert.assertEquals(DrainMonitor.State.STOPPED, drainMonitor.getState());\r\n    Assert.assertEquals(1, numCallbacks.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testDrainMonitorStop",
  "sourceCode" : "@Test\r\npublic void testDrainMonitorStop() {\r\n    DrainMonitor drainMonitor = new DrainMonitor(coordinatorStreamStore, CONFIG, 100L);\r\n    drainMonitor.registerDrainCallback(() -> {\r\n    });\r\n    drainMonitor.start();\r\n    drainMonitor.stop();\r\n    Assert.assertEquals(drainMonitor.getState(), DrainMonitor.State.STOPPED);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainMonitorTests.java",
  "methodName" : "testShouldDrain",
  "sourceCode" : "@Test\r\npublic void testShouldDrain() {\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore);\r\n    NamespaceAwareCoordinatorStreamStore drainStore = new NamespaceAwareCoordinatorStreamStore(coordinatorStreamStore, DrainUtils.DRAIN_METADATA_STORE_NAMESPACE);\r\n    Assert.assertTrue(DrainMonitor.shouldDrain(drainStore, TEST_RUN_ID));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainNotificationObjectMapperTests.java",
  "methodName" : "testDrainNotificationSerde",
  "sourceCode" : "@Test\r\npublic void testDrainNotificationSerde() throws IOException {\r\n    UUID uuid = UUID.randomUUID();\r\n    DrainNotification originalMessage = DrainNotification.create(uuid, \"foobar\");\r\n    ObjectMapper objectMapper = DrainNotificationObjectMapper.getObjectMapper();\r\n    byte[] bytes = objectMapper.writeValueAsBytes(originalMessage);\r\n    DrainNotification deserializedMessage = objectMapper.readValue(bytes, DrainNotification.class);\r\n    assertEquals(originalMessage, deserializedMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainUtilsTests.java",
  "methodName" : "testWrites",
  "sourceCode" : "@Test\r\npublic void testWrites() {\r\n    String runId1 = \"foo1\";\r\n    DrainNotification drainNotification1 = DrainNotification.create(UUID.randomUUID(), runId1);\r\n    UUID uuid1 = DrainUtils.writeDrainNotification(coordinatorStreamStore, drainNotification1);\r\n    DrainNotification expectedDrainNotification1 = DrainNotification.create(uuid1, runId1);\r\n    Set<DrainNotification> expectedDrainNotifications = new HashSet<>(Arrays.asList(expectedDrainNotification1));\r\n    Optional<List<DrainNotification>> drainNotifications = readDrainNotificationMessages(coordinatorStreamStore);\r\n    Assert.assertTrue(drainNotifications.isPresent());\r\n    Assert.assertEquals(1, drainNotifications.get().size());\r\n    Assert.assertEquals(expectedDrainNotifications, new HashSet<>(drainNotifications.get()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainUtilsTests.java",
  "methodName" : "testCleanup",
  "sourceCode" : "@Test\r\npublic void testCleanup() {\r\n    DrainNotification drainNotification1 = DrainNotification.create(UUID.randomUUID(), TEST_RUN_ID);\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore, drainNotification1);\r\n    DrainUtils.cleanup(coordinatorStreamStore, CONFIG);\r\n    final Optional<List<DrainNotification>> drainNotifications1 = readDrainNotificationMessages(coordinatorStreamStore);\r\n    Assert.assertFalse(drainNotifications1.isPresent());\r\n    final String runId2 = \"bar\";\r\n    DrainNotification drainNotification2 = DrainNotification.create(UUID.randomUUID(), runId2);\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore, drainNotification2);\r\n    DrainUtils.cleanup(coordinatorStreamStore, CONFIG);\r\n    final Optional<List<DrainNotification>> drainNotifications2 = readDrainNotificationMessages(coordinatorStreamStore);\r\n    Assert.assertTrue(drainNotifications2.isPresent());\r\n    Assert.assertEquals(runId2, drainNotifications2.get().get(0).getRunId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\drain\\DrainUtilsTests.java",
  "methodName" : "testCleanupAll",
  "sourceCode" : "@Test\r\npublic void testCleanupAll() {\r\n    DrainNotification drainNotification1 = DrainNotification.create(UUID.randomUUID(), TEST_RUN_ID);\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore, drainNotification1);\r\n    final String runId2 = \"bar\";\r\n    DrainNotification drainNotification2 = DrainNotification.create(UUID.randomUUID(), runId2);\r\n    DrainUtils.writeDrainNotification(coordinatorStreamStore, drainNotification2);\r\n    DrainUtils.cleanupAll(coordinatorStreamStore);\r\n    final Optional<List<DrainNotification>> drainNotifications = readDrainNotificationMessages(coordinatorStreamStore);\r\n    Assert.assertFalse(drainNotifications.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCreateProcessorGraph",
  "sourceCode" : "@Test\r\npublic void testCreateProcessorGraph() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamStreamJoin();\r\n    JobGraph jobGraph = planner.createJobGraph(graphSpec);\r\n    assertTrue(jobGraph.getInputStreams().size() == 3);\r\n    assertTrue(jobGraph.getOutputStreams().size() == 2);\r\n    // two streams generated by partitionBy\r\n    assertTrue(jobGraph.getIntermediateStreams().size() == 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testFetchExistingStreamPartitions",
  "sourceCode" : "@Test\r\npublic void testFetchExistingStreamPartitions() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamStreamJoin();\r\n    JobGraph jobGraph = planner.createJobGraph(graphSpec);\r\n    planner.setInputAndOutputStreamPartitionCount(jobGraph);\r\n    assertTrue(jobGraph.getOrCreateStreamEdge(input1Spec).getPartitionCount() == 64);\r\n    assertTrue(jobGraph.getOrCreateStreamEdge(input2Spec).getPartitionCount() == 16);\r\n    assertTrue(jobGraph.getOrCreateStreamEdge(input3Spec).getPartitionCount() == 32);\r\n    assertTrue(jobGraph.getOrCreateStreamEdge(output1Spec).getPartitionCount() == 8);\r\n    assertTrue(jobGraph.getOrCreateStreamEdge(output2Spec).getPartitionCount() == 16);\r\n    jobGraph.getIntermediateStreamEdges().forEach(edge -> {\r\n        assertTrue(edge.getPartitionCount() == -1);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCalculateJoinInputPartitions",
  "sourceCode" : "@Test\r\npublic void testCalculateJoinInputPartitions() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamStreamJoin();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        assertEquals(64, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCalculateOrderSensitiveJoinInputPartitions",
  "sourceCode" : "@Test\r\npublic void testCalculateOrderSensitiveJoinInputPartitions() {\r\n    // This test ensures that the ExecutionPlanner can handle groups of joined stream edges\r\n    // in the correct order. It creates an example stream-stream join application that has\r\n    // the following sets of joined streams (notice the order):\r\n    //\r\n    //    a. e1 (16), e2` (?)\r\n    //    b. e3` (?), e4` (?)\r\n    //    c. e2` (?), e3` (?)\r\n    //\r\n    // If processed in the above order, the ExecutionPlanner will fail to assign the partitions\r\n    // correctly.\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithComplexStreamStreamJoin();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        assertEquals(64, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCalculateIntStreamPartitions",
  "sourceCode" : "@Test\r\npublic void testCalculateIntStreamPartitions() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createSimpleGraph();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        // max of input1 and output1\r\n        assertEquals(64, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCalculateInStreamPartitionsBehindTables",
  "sourceCode" : "@Test\r\npublic void testCalculateInStreamPartitionsBehindTables() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamTableJoin();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input3\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        assertEquals(32, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCalculateInStreamPartitionsBehindTablesWithSideInputs",
  "sourceCode" : "@Test\r\npublic void testCalculateInStreamPartitionsBehindTablesWithSideInputs() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamTableJoinWithSideInputs();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        assertEquals(64, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testHandlesVirtualStreamTableJoinCycles",
  "sourceCode" : "@Test\r\npublic void testHandlesVirtualStreamTableJoinCycles() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamTableJoinAndSendToSameTable();\r\n    // Just make sure planning terminates.\r\n    planner.plan(graphSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testDefaultPartitions",
  "sourceCode" : "@Test\r\npublic void testDefaultPartitions() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createSimpleGraph();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        assertTrue(edge.getPartitionCount() == DEFAULT_PARTITIONS);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testBroadcastConfig",
  "sourceCode" : "@Test\r\npublic void testBroadcastConfig() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(String.format(StreamConfig.BROADCAST_FOR_STREAM_ID, \"input1\"), \"true\");\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createSimpleGraph();\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    StreamEdge edge = jobGraph.getStreamEdge(\"input1\");\r\n    Assert.assertTrue(edge.isBroadcast());\r\n    Config jobConfig = jobGraph.getJobConfigs().get(0);\r\n    Assert.assertEquals(\"system1.input1#[0-63]\", jobConfig.get(\"task.broadcast.inputs\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testMaxPartitionLimitIsIgnoredForStreamingMode",
  "sourceCode" : "@Test\r\npublic void testMaxPartitionLimitIsIgnoredForStreamingMode() {\r\n    int expectedPartitionSize = IntermediateStreamManager.MAX_INFERRED_PARTITIONS * 2;\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream<KV<Object, Object>> input1 = appDesc.getInputStream(input4Descriptor);\r\n        OutputStream<KV<Object, Object>> output1 = appDesc.getOutputStream(output1Descriptor);\r\n        input1.partitionBy(m -> m.key, m -> m.value, mock(KVSerde.class), \"p1\").map(kv -> kv).sendTo(output1);\r\n    }, config);\r\n    JobGraph jobGraph = (JobGraph) planner.plan(graphSpec);\r\n    // Partitions should be the same as input1\r\n    jobGraph.getIntermediateStreams().forEach(edge -> {\r\n        // max of input1 and output1\r\n        assertEquals(expectedPartitionSize, edge.getPartitionCount());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testRejectsInvalidStreamStreamJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testRejectsInvalidStreamStreamJoin() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithInvalidStreamStreamJoin();\r\n    planner.plan(graphSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testRejectsInvalidStreamTableJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testRejectsInvalidStreamTableJoin() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithInvalidStreamTableJoin();\r\n    planner.plan(graphSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testRejectsInvalidStreamTableJoinWithSideInputs",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testRejectsInvalidStreamTableJoinWithSideInputs() {\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithInvalidStreamTableJoinWithSideInputs();\r\n    planner.plan(graphSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testTriggerIntervalForJoins",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalForJoins() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithStreamStreamJoin();\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    List<JobConfig> jobConfigs = plan.getJobConfigs();\r\n    for (JobConfig config : jobConfigs) {\r\n        System.out.println(config);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testTriggerIntervalForWindowsAndJoins",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalForWindowsAndJoins() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithJoinAndWindow();\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    List<JobConfig> jobConfigs = plan.getJobConfigs();\r\n    assertEquals(1, jobConfigs.size());\r\n    // GCD of 8, 16, 1600 and 252 is 4\r\n    assertEquals(\"4\", jobConfigs.get(0).get(TaskConfig.WINDOW_MS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testTriggerIntervalWithNoWindowMs",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalWithNoWindowMs() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createStreamGraphWithJoinAndWindow();\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    List<JobConfig> jobConfigs = plan.getJobConfigs();\r\n    assertEquals(1, jobConfigs.size());\r\n    // GCD of 8, 16, 1600 and 252 is 4\r\n    assertEquals(\"4\", jobConfigs.get(0).get(TaskConfig.WINDOW_MS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testTriggerIntervalForStatelessOperators",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalForStatelessOperators() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createSimpleGraph();\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    List<JobConfig> jobConfigs = plan.getJobConfigs();\r\n    assertEquals(1, jobConfigs.size());\r\n    assertFalse(jobConfigs.get(0).containsKey(TaskConfig.WINDOW_MS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testTriggerIntervalWhenWindowMsIsConfigured",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalWhenWindowMsIsConfigured() {\r\n    Map<String, String> map = new HashMap<>(config);\r\n    map.put(TaskConfig.WINDOW_MS, \"2000\");\r\n    map.put(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, String.valueOf(DEFAULT_PARTITIONS));\r\n    Config cfg = new MapConfig(map);\r\n    ExecutionPlanner planner = new ExecutionPlanner(cfg, streamManager);\r\n    StreamApplicationDescriptorImpl graphSpec = createSimpleGraph();\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    List<JobConfig> jobConfigs = plan.getJobConfigs();\r\n    assertEquals(1, jobConfigs.size());\r\n    assertEquals(\"2000\", jobConfigs.get(0).get(TaskConfig.WINDOW_MS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testMaxPartition",
  "sourceCode" : "@Test\r\npublic void testMaxPartition() {\r\n    Collection<StreamEdge> edges = new ArrayList<>();\r\n    StreamEdge edge = new StreamEdge(input1Spec, false, false, config);\r\n    edge.setPartitionCount(2);\r\n    edges.add(edge);\r\n    edge = new StreamEdge(input2Spec, false, false, config);\r\n    edge.setPartitionCount(32);\r\n    edges.add(edge);\r\n    edge = new StreamEdge(input3Spec, false, false, config);\r\n    edge.setPartitionCount(16);\r\n    edges.add(edge);\r\n    assertEquals(32, IntermediateStreamManager.maxPartitions(edges));\r\n    edges = Collections.emptyList();\r\n    assertEquals(StreamEdge.PARTITIONS_UNKNOWN, IntermediateStreamManager.maxPartitions(edges));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCreateJobGraphForTaskApplication",
  "sourceCode" : "@Test\r\npublic void testCreateJobGraphForTaskApplication() {\r\n    TaskApplicationDescriptorImpl taskAppDesc = mock(TaskApplicationDescriptorImpl.class);\r\n    // add interemediate streams\r\n    String intermediateStream1 = \"intermediate-stream1\";\r\n    String intermediateBroadcast = \"intermediate-broadcast1\";\r\n    // intermediate stream1, not broadcast\r\n    GenericInputDescriptor<KV<Object, Object>> intermediateInput1 = system1Descriptor.getInputDescriptor(intermediateStream1, new KVSerde<>(new NoOpSerde(), new NoOpSerde()));\r\n    GenericOutputDescriptor<KV<Object, Object>> intermediateOutput1 = system1Descriptor.getOutputDescriptor(intermediateStream1, new KVSerde<>(new NoOpSerde(), new NoOpSerde()));\r\n    // intermediate stream2, broadcast\r\n    GenericInputDescriptor<KV<Object, Object>> intermediateBroacastInput1 = system1Descriptor.getInputDescriptor(intermediateBroadcast, new KVSerde<>(new NoOpSerde<>(), new NoOpSerde<>()));\r\n    GenericOutputDescriptor<KV<Object, Object>> intermediateBroacastOutput1 = system1Descriptor.getOutputDescriptor(intermediateBroadcast, new KVSerde<>(new NoOpSerde<>(), new NoOpSerde<>()));\r\n    inputDescriptors.put(intermediateStream1, intermediateInput1);\r\n    outputDescriptors.put(intermediateStream1, intermediateOutput1);\r\n    inputDescriptors.put(intermediateBroadcast, intermediateBroacastInput1);\r\n    outputDescriptors.put(intermediateBroadcast, intermediateBroacastOutput1);\r\n    Set<String> broadcastStreams = new HashSet<>();\r\n    broadcastStreams.add(intermediateBroadcast);\r\n    when(taskAppDesc.getInputDescriptors()).thenReturn(inputDescriptors);\r\n    when(taskAppDesc.getInputStreamIds()).thenReturn(inputDescriptors.keySet());\r\n    when(taskAppDesc.getOutputDescriptors()).thenReturn(outputDescriptors);\r\n    when(taskAppDesc.getOutputStreamIds()).thenReturn(outputDescriptors.keySet());\r\n    when(taskAppDesc.getTableDescriptors()).thenReturn(Collections.emptySet());\r\n    when(taskAppDesc.getSystemDescriptors()).thenReturn(systemDescriptors);\r\n    when(taskAppDesc.getIntermediateBroadcastStreamIds()).thenReturn(broadcastStreams);\r\n    doReturn(MockTaskApplication.class).when(taskAppDesc).getAppClass();\r\n    Map<String, String> systemStreamConfigs = new HashMap<>();\r\n    inputDescriptors.forEach((key, value) -> systemStreamConfigs.putAll(value.toConfig()));\r\n    outputDescriptors.forEach((key, value) -> systemStreamConfigs.putAll(value.toConfig()));\r\n    systemDescriptors.forEach(sd -> systemStreamConfigs.putAll(sd.toConfig()));\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    JobGraph jobGraph = planner.createJobGraph(taskAppDesc);\r\n    assertEquals(1, jobGraph.getJobNodes().size());\r\n    assertTrue(jobGraph.getInputStreams().stream().map(edge -> edge.getName()).filter(streamId -> inputDescriptors.containsKey(streamId)).collect(Collectors.toList()).isEmpty());\r\n    Set<String> intermediateStreams = new HashSet<>(inputDescriptors.keySet());\r\n    jobGraph.getInputStreams().forEach(edge -> {\r\n        if (intermediateStreams.contains(edge.getStreamSpec().getId())) {\r\n            intermediateStreams.remove(edge.getStreamSpec().getId());\r\n        }\r\n    });\r\n    assertEquals(new HashSet<>(Arrays.asList(intermediateStream1, intermediateBroadcast)), intermediateStreams);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestExecutionPlanner.java",
  "methodName" : "testCreateJobGraphForLegacyTaskApplication",
  "sourceCode" : "@Test\r\npublic void testCreateJobGraphForLegacyTaskApplication() {\r\n    TaskApplicationDescriptorImpl taskAppDesc = mock(TaskApplicationDescriptorImpl.class);\r\n    when(taskAppDesc.getInputDescriptors()).thenReturn(new HashMap<>());\r\n    when(taskAppDesc.getOutputDescriptors()).thenReturn(new HashMap<>());\r\n    when(taskAppDesc.getTableDescriptors()).thenReturn(new HashSet<>());\r\n    when(taskAppDesc.getSystemDescriptors()).thenReturn(new HashSet<>());\r\n    when(taskAppDesc.getIntermediateBroadcastStreamIds()).thenReturn(new HashSet<>());\r\n    doReturn(LegacyTaskApplication.class).when(taskAppDesc).getAppClass();\r\n    Map<String, String> systemStreamConfigs = new HashMap<>();\r\n    inputDescriptors.forEach((key, value) -> systemStreamConfigs.putAll(value.toConfig()));\r\n    outputDescriptors.forEach((key, value) -> systemStreamConfigs.putAll(value.toConfig()));\r\n    systemDescriptors.forEach(sd -> systemStreamConfigs.putAll(sd.toConfig()));\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    JobGraph jobGraph = planner.createJobGraph(taskAppDesc);\r\n    assertEquals(1, jobGraph.getJobNodes().size());\r\n    JobNode jobNode = jobGraph.getJobNodes().get(0);\r\n    assertEquals(\"test-app\", jobNode.getJobName());\r\n    assertEquals(\"test-app-1\", jobNode.getJobNameAndId());\r\n    assertEquals(0, jobNode.getInEdges().size());\r\n    assertEquals(0, jobNode.getOutEdges().size());\r\n    assertEquals(0, jobNode.getTables().size());\r\n    assertEquals(config, jobNode.getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestIntermediateStreamManager.java",
  "methodName" : "setIntermediateStreamPartitions",
  "sourceCode" : "@Test\r\npublic void setIntermediateStreamPartitions() {\r\n    streamManager.setIntermediateStreamPartitions(jobGraph);\r\n    verify(intermediateStream).setPartitionCount(1024);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestIntermediateStreamManager.java",
  "methodName" : "setIntermediateStreamPartitionsForBatchMode",
  "sourceCode" : "@Test\r\npublic void setIntermediateStreamPartitionsForBatchMode() {\r\n    streamManager = spy(new IntermediateStreamManager(config));\r\n    doReturn(ApplicationMode.BATCH).when(streamManager).getAppMode();\r\n    streamManager.setIntermediateStreamPartitions(jobGraph);\r\n    verify(intermediateStream).setPartitionCount(256);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestIntermediateStreamManager.java",
  "methodName" : "setIntermediateStreamPartitionsWithDefaultConfigPropertySet",
  "sourceCode" : "@Test\r\npublic void setIntermediateStreamPartitionsWithDefaultConfigPropertySet() {\r\n    when(config.getInt(JobConfig.JOB_INTERMEDIATE_STREAM_PARTITIONS, StreamEdge.PARTITIONS_UNKNOWN)).thenReturn(128);\r\n    streamManager.setIntermediateStreamPartitions(jobGraph);\r\n    verify(intermediateStream).setPartitionCount(128);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraph.java",
  "methodName" : "testAddSource",
  "sourceCode" : "@Test\r\npublic void testAddSource() {\r\n    StreamApplicationDescriptorImpl appDesc = mock(StreamApplicationDescriptorImpl.class);\r\n    JobGraph graph = new JobGraph(new MapConfig(), appDesc);\r\n    /**\r\n     * s1 -> 1\r\n     * s2 ->|\r\n     *\r\n     * s3 -> 2\r\n     *   |-> 3\r\n     */\r\n    JobNode n1 = graph.getOrCreateJobNode(\"1\", \"1\");\r\n    JobNode n2 = graph.getOrCreateJobNode(\"2\", \"1\");\r\n    JobNode n3 = graph.getOrCreateJobNode(\"3\", \"1\");\r\n    StreamSpec s1 = genStream();\r\n    StreamSpec s2 = genStream();\r\n    StreamSpec s3 = genStream();\r\n    graph.addInputStream(s1, n1);\r\n    graph.addInputStream(s2, n1);\r\n    graph.addInputStream(s3, n2);\r\n    graph.addInputStream(s3, n3);\r\n    assertTrue(graph.getInputStreams().size() == 3);\r\n    assertTrue(graph.getOrCreateJobNode(\"1\", \"1\").getInEdges().size() == 2);\r\n    assertTrue(graph.getOrCreateJobNode(\"2\", \"1\").getInEdges().size() == 1);\r\n    assertTrue(graph.getOrCreateJobNode(\"3\", \"1\").getInEdges().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s1).getSourceNodes().size() == 0);\r\n    assertTrue(graph.getOrCreateStreamEdge(s1).getTargetNodes().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s2).getSourceNodes().size() == 0);\r\n    assertTrue(graph.getOrCreateStreamEdge(s2).getTargetNodes().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s3).getSourceNodes().size() == 0);\r\n    assertTrue(graph.getOrCreateStreamEdge(s3).getTargetNodes().size() == 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraph.java",
  "methodName" : "testAddSink",
  "sourceCode" : "@Test\r\npublic void testAddSink() {\r\n    /**\r\n     * 1 -> s1\r\n     * 2 -> s2\r\n     * 2 -> s3\r\n     */\r\n    StreamApplicationDescriptorImpl appDesc = mock(StreamApplicationDescriptorImpl.class);\r\n    JobGraph graph = new JobGraph(new MapConfig(), appDesc);\r\n    JobNode n1 = graph.getOrCreateJobNode(\"1\", \"1\");\r\n    JobNode n2 = graph.getOrCreateJobNode(\"2\", \"1\");\r\n    StreamSpec s1 = genStream();\r\n    StreamSpec s2 = genStream();\r\n    StreamSpec s3 = genStream();\r\n    graph.addOutputStream(s1, n1);\r\n    graph.addOutputStream(s2, n2);\r\n    graph.addOutputStream(s3, n2);\r\n    assertTrue(graph.getOutputStreams().size() == 3);\r\n    assertTrue(graph.getOrCreateJobNode(\"1\", \"1\").getOutEdges().size() == 1);\r\n    assertTrue(graph.getOrCreateJobNode(\"2\", \"1\").getOutEdges().size() == 2);\r\n    assertTrue(graph.getOrCreateStreamEdge(s1).getSourceNodes().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s1).getTargetNodes().size() == 0);\r\n    assertTrue(graph.getOrCreateStreamEdge(s2).getSourceNodes().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s2).getTargetNodes().size() == 0);\r\n    assertTrue(graph.getOrCreateStreamEdge(s3).getSourceNodes().size() == 1);\r\n    assertTrue(graph.getOrCreateStreamEdge(s3).getTargetNodes().size() == 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraph.java",
  "methodName" : "testReachable",
  "sourceCode" : "@Test\r\npublic void testReachable() {\r\n    Set<JobNode> reachable1 = graph1.findReachable();\r\n    assertTrue(reachable1.size() == 8);\r\n    Set<JobNode> reachable2 = graph2.findReachable();\r\n    assertTrue(reachable2.size() == 7);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraph.java",
  "methodName" : "testTopologicalSort",
  "sourceCode" : "@Test\r\npublic void testTopologicalSort() {\r\n    // test graph1\r\n    List<JobNode> sortedNodes1 = graph1.topologicalSort();\r\n    Map<String, Integer> idxMap1 = new HashMap<>();\r\n    for (int i = 0; i < sortedNodes1.size(); i++) {\r\n        idxMap1.put(sortedNodes1.get(i).getJobName(), i);\r\n    }\r\n    assertTrue(idxMap1.size() == 8);\r\n    assertTrue(idxMap1.get(\"11\") > idxMap1.get(\"5\"));\r\n    assertTrue(idxMap1.get(\"11\") > idxMap1.get(\"7\"));\r\n    assertTrue(idxMap1.get(\"8\") > idxMap1.get(\"7\"));\r\n    assertTrue(idxMap1.get(\"8\") > idxMap1.get(\"3\"));\r\n    assertTrue(idxMap1.get(\"2\") > idxMap1.get(\"11\"));\r\n    assertTrue(idxMap1.get(\"9\") > idxMap1.get(\"8\"));\r\n    assertTrue(idxMap1.get(\"9\") > idxMap1.get(\"11\"));\r\n    assertTrue(idxMap1.get(\"10\") > idxMap1.get(\"11\"));\r\n    assertTrue(idxMap1.get(\"10\") > idxMap1.get(\"3\"));\r\n    // test graph2\r\n    List<JobNode> sortedNodes2 = graph2.topologicalSort();\r\n    Map<String, Integer> idxMap2 = new HashMap<>();\r\n    for (int i = 0; i < sortedNodes2.size(); i++) {\r\n        idxMap2.put(sortedNodes2.get(i).getJobName(), i);\r\n    }\r\n    assertTrue(idxMap2.size() == 7);\r\n    assertTrue(idxMap2.get(\"2\") > idxMap2.get(\"1\"));\r\n    assertTrue(idxMap2.get(\"3\") > idxMap2.get(\"1\"));\r\n    assertTrue(idxMap2.get(\"4\") > idxMap2.get(\"1\"));\r\n    assertTrue(idxMap2.get(\"6\") > idxMap2.get(\"1\"));\r\n    assertTrue(idxMap2.get(\"5\") > idxMap2.get(\"4\"));\r\n    assertTrue(idxMap2.get(\"7\") > idxMap2.get(\"5\"));\r\n    //test graph3\r\n    List<JobNode> sortedNodes3 = graph3.topologicalSort();\r\n    assertTrue(sortedNodes3.size() == 2);\r\n    assertEquals(sortedNodes3.get(0).getJobName(), \"1\");\r\n    assertEquals(sortedNodes3.get(1).getJobName(), \"2\");\r\n    //test graph4\r\n    List<JobNode> sortedNodes4 = graph4.topologicalSort();\r\n    assertTrue(sortedNodes4.size() == 1);\r\n    assertEquals(sortedNodes4.get(0).getJobName(), \"1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraphJsonGenerator.java",
  "methodName" : "testRepartitionedJoinStreamApplication",
  "sourceCode" : "@Test\r\npublic void testRepartitionedJoinStreamApplication() throws Exception {\r\n    /**\r\n     * the graph looks like the following.\r\n     * number in parentheses () indicates number of stream partitions.\r\n     * number in parentheses in quotes (\"\") indicates expected partition count.\r\n     * number in square brackets [] indicates operator ID.\r\n     *\r\n     * input3 (32) -> filter [7] -> partitionBy [8] (\"64\") -> map [10] -> join [14] -> sendTo(output2) [15] (16)\r\n     *                                                                   |\r\n     *              input2 (16) -> partitionBy [3] (\"64\") -> filter [5] -| -> sink [13]\r\n     *                                                                   |\r\n     *                                         input1 (64) -> map [1] -> join [11] -> sendTo(output1) [12] (8)\r\n     */\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(JobConfig.JOB_NAME, \"test-app\");\r\n    configMap.put(JobConfig.JOB_DEFAULT_SYSTEM, \"test-system\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"input1\", \"system1\", \"input1\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"input2\", \"system2\", \"input2\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"input3\", \"system2\", \"input3\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"output1\", \"system1\", \"output1\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"output2\", \"system2\", \"output2\");\r\n    Config config = new MapConfig(configMap);\r\n    // set up external partition count\r\n    Map<String, Integer> system1Map = new HashMap<>();\r\n    system1Map.put(\"input1\", 64);\r\n    system1Map.put(\"output1\", 8);\r\n    Map<String, Integer> system2Map = new HashMap<>();\r\n    system2Map.put(\"input2\", 16);\r\n    system2Map.put(\"input3\", 32);\r\n    system2Map.put(\"output2\", 16);\r\n    SystemAdmin systemAdmin1 = createSystemAdmin(system1Map);\r\n    SystemAdmin systemAdmin2 = createSystemAdmin(system2Map);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    when(systemAdmins.getSystemAdmin(\"system1\")).thenReturn(systemAdmin1);\r\n    when(systemAdmins.getSystemAdmin(\"system2\")).thenReturn(systemAdmin2);\r\n    StreamManager streamManager = new StreamManager(systemAdmins);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        KVSerde<Object, Object> kvSerde = new KVSerde<>(new NoOpSerde(), new NoOpSerde());\r\n        String mockSystemFactoryClass = \"factory.class.name\";\r\n        GenericSystemDescriptor system1 = new GenericSystemDescriptor(\"system1\", mockSystemFactoryClass);\r\n        GenericSystemDescriptor system2 = new GenericSystemDescriptor(\"system2\", mockSystemFactoryClass);\r\n        GenericInputDescriptor<KV<Object, Object>> input1Descriptor = system1.getInputDescriptor(\"input1\", kvSerde);\r\n        GenericInputDescriptor<KV<Object, Object>> input2Descriptor = system2.getInputDescriptor(\"input2\", kvSerde);\r\n        GenericInputDescriptor<KV<Object, Object>> input3Descriptor = system2.getInputDescriptor(\"input3\", kvSerde);\r\n        GenericOutputDescriptor<KV<Object, Object>> output1Descriptor = system1.getOutputDescriptor(\"output1\", kvSerde);\r\n        GenericOutputDescriptor<KV<Object, Object>> output2Descriptor = system2.getOutputDescriptor(\"output2\", kvSerde);\r\n        MessageStream<KV<Object, Object>> messageStream1 = appDesc.getInputStream(input1Descriptor).map(m -> m);\r\n        MessageStream<KV<Object, Object>> messageStream2 = appDesc.getInputStream(input2Descriptor).partitionBy(m -> m.key, m -> m.value, mock(KVSerde.class), \"p1\").filter(m -> true);\r\n        MessageStream<KV<Object, Object>> messageStream3 = appDesc.getInputStream(input3Descriptor).filter(m -> true).partitionBy(m -> m.key, m -> m.value, mock(KVSerde.class), \"p2\").map(m -> m);\r\n        OutputStream<KV<Object, Object>> outputStream1 = appDesc.getOutputStream(output1Descriptor);\r\n        OutputStream<KV<Object, Object>> outputStream2 = appDesc.getOutputStream(output2Descriptor);\r\n        messageStream1.join(messageStream2, (JoinFunction<Object, KV<Object, Object>, KV<Object, Object>, KV<Object, Object>>) mock(JoinFunction.class), mock(Serde.class), mock(Serde.class), mock(Serde.class), Duration.ofHours(2), \"j1\").sendTo(outputStream1);\r\n        messageStream2.sink((message, collector, coordinator) -> {\r\n        });\r\n        messageStream3.join(messageStream2, (JoinFunction<Object, KV<Object, Object>, KV<Object, Object>, KV<Object, Object>>) mock(JoinFunction.class), mock(Serde.class), mock(Serde.class), mock(Serde.class), Duration.ofHours(1), \"j2\").sendTo(outputStream2);\r\n    }, config);\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    String json = plan.getPlanAsJson();\r\n    System.out.println(json);\r\n    // deserialize\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    JobGraphJsonGenerator.JobGraphJson nodes = mapper.readValue(json, JobGraphJsonGenerator.JobGraphJson.class);\r\n    assertEquals(5, nodes.jobs.get(0).operatorGraph.inputStreams.size());\r\n    assertEquals(11, nodes.jobs.get(0).operatorGraph.operators.size());\r\n    assertEquals(3, nodes.sourceStreams.size());\r\n    assertEquals(2, nodes.sinkStreams.size());\r\n    assertEquals(2, nodes.intermediateStreams.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraphJsonGenerator.java",
  "methodName" : "testRepartitionedWindowStreamApplication",
  "sourceCode" : "@Test\r\npublic void testRepartitionedWindowStreamApplication() throws Exception {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(JobConfig.JOB_NAME, \"test-app\");\r\n    configMap.put(JobConfig.JOB_DEFAULT_SYSTEM, \"test-system\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"PageView\", \"hdfs\", \"hdfs:/user/dummy/PageViewEvent\");\r\n    StreamTestUtils.addStreamConfigs(configMap, \"PageViewCount\", \"kafka\", \"PageViewCount\");\r\n    Config config = new MapConfig(configMap);\r\n    // set up external partition count\r\n    Map<String, Integer> system1Map = new HashMap<>();\r\n    system1Map.put(\"hdfs:/user/dummy/PageViewEvent\", 512);\r\n    Map<String, Integer> system2Map = new HashMap<>();\r\n    system2Map.put(\"PageViewCount\", 16);\r\n    SystemAdmin systemAdmin1 = createSystemAdmin(system1Map);\r\n    SystemAdmin systemAdmin2 = createSystemAdmin(system2Map);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    when(systemAdmins.getSystemAdmin(\"hdfs\")).thenReturn(systemAdmin1);\r\n    when(systemAdmins.getSystemAdmin(\"kafka\")).thenReturn(systemAdmin2);\r\n    StreamManager streamManager = new StreamManager(systemAdmins);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        KVSerde<String, PageViewEvent> pvSerde = KVSerde.of(new StringSerde(), new JsonSerdeV2<>(PageViewEvent.class));\r\n        GenericSystemDescriptor isd = new GenericSystemDescriptor(\"hdfs\", \"mockSystemFactoryClass\");\r\n        GenericInputDescriptor<KV<String, PageViewEvent>> pageView = isd.getInputDescriptor(\"PageView\", pvSerde);\r\n        KVSerde<String, Long> pvcSerde = KVSerde.of(new StringSerde(), new LongSerde());\r\n        GenericSystemDescriptor osd = new GenericSystemDescriptor(\"kafka\", \"mockSystemFactoryClass\");\r\n        GenericOutputDescriptor<KV<String, Long>> pageViewCount = osd.getOutputDescriptor(\"PageViewCount\", pvcSerde);\r\n        MessageStream<KV<String, PageViewEvent>> inputStream = appDesc.getInputStream(pageView);\r\n        OutputStream<KV<String, Long>> outputStream = appDesc.getOutputStream(pageViewCount);\r\n        inputStream.partitionBy(kv -> kv.getValue().getCountry(), kv -> kv.getValue(), pvSerde, \"keyed-by-country\").window(Windows.keyedTumblingWindow(kv -> kv.getValue().getCountry(), Duration.ofSeconds(10L), () -> 0L, (m, c) -> c + 1L, new StringSerde(), new LongSerde()), \"count-by-country\").map(pane -> new KV<>(pane.getKey().getKey(), pane.getMessage())).sendTo(outputStream);\r\n    }, config);\r\n    ExecutionPlanner planner = new ExecutionPlanner(config, streamManager);\r\n    ExecutionPlan plan = planner.plan(graphSpec);\r\n    String json = plan.getPlanAsJson();\r\n    System.out.println(json);\r\n    // deserialize\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    JobGraphJsonGenerator.JobGraphJson nodes = mapper.readValue(json, JobGraphJsonGenerator.JobGraphJson.class);\r\n    JobGraphJsonGenerator.OperatorGraphJson operatorGraphJson = nodes.jobs.get(0).operatorGraph;\r\n    assertEquals(2, operatorGraphJson.inputStreams.size());\r\n    assertEquals(4, operatorGraphJson.operators.size());\r\n    assertEquals(1, nodes.sourceStreams.size());\r\n    assertEquals(1, nodes.sinkStreams.size());\r\n    assertEquals(1, nodes.intermediateStreams.size());\r\n    // verify partitionBy op output to the intermdiate stream of the same id\r\n    assertEquals(operatorGraphJson.operators.get(\"test-app-1-partition_by-keyed-by-country\").get(\"outputStreamId\"), \"test-app-1-partition_by-keyed-by-country\");\r\n    assertEquals(operatorGraphJson.operators.get(\"test-app-1-send_to-5\").get(\"outputStreamId\"), \"PageViewCount\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraphJsonGenerator.java",
  "methodName" : "testTaskApplication",
  "sourceCode" : "@Test\r\npublic void testTaskApplication() throws Exception {\r\n    JobGraphJsonGenerator jsonGenerator = new JobGraphJsonGenerator();\r\n    JobGraph mockJobGraph = mock(JobGraph.class);\r\n    ApplicationConfig mockAppConfig = mock(ApplicationConfig.class);\r\n    when(mockAppConfig.getAppName()).thenReturn(\"testTaskApp\");\r\n    when(mockAppConfig.getAppId()).thenReturn(\"testTaskAppId\");\r\n    when(mockJobGraph.getApplicationConfig()).thenReturn(mockAppConfig);\r\n    // compute the three disjoint sets of the JobGraph: input only, output only, and intermediate streams\r\n    Set<StreamEdge> inEdges = new HashSet<>(mockJobNode.getInEdges().values());\r\n    Set<StreamEdge> outEdges = new HashSet<>(mockJobNode.getOutEdges().values());\r\n    Set<StreamEdge> intermediateEdges = new HashSet<>(inEdges);\r\n    // intermediate streams are the intersection between input and output\r\n    intermediateEdges.retainAll(outEdges);\r\n    // remove all intermediate streams from input\r\n    inEdges.removeAll(intermediateEdges);\r\n    // remove all intermediate streams from output\r\n    outEdges.removeAll(intermediateEdges);\r\n    // set the return values for mockJobGraph\r\n    when(mockJobGraph.getInputStreams()).thenReturn(inEdges);\r\n    when(mockJobGraph.getOutputStreams()).thenReturn(outEdges);\r\n    when(mockJobGraph.getIntermediateStreamEdges()).thenReturn(intermediateEdges);\r\n    Set<TableDescriptor> tables = new HashSet<>(mockJobNode.getTables().values());\r\n    when(mockJobGraph.getTables()).thenReturn(tables);\r\n    when(mockJobGraph.getJobNodes()).thenReturn(Collections.singletonList(mockJobNode));\r\n    String graphJson = jsonGenerator.toJson(mockJobGraph);\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    JobGraphJsonGenerator.JobGraphJson jsonObject = objectMapper.readValue(graphJson.getBytes(), JobGraphJsonGenerator.JobGraphJson.class);\r\n    assertEquals(\"testTaskAppId\", jsonObject.applicationId);\r\n    assertEquals(\"testTaskApp\", jsonObject.applicationName);\r\n    Set<String> inStreamIds = inEdges.stream().map(stream -> stream.getStreamSpec().getId()).collect(Collectors.toSet());\r\n    assertThat(jsonObject.sourceStreams.keySet(), Matchers.containsInAnyOrder(inStreamIds.toArray()));\r\n    Set<String> outStreamIds = outEdges.stream().map(stream -> stream.getStreamSpec().getId()).collect(Collectors.toSet());\r\n    assertThat(jsonObject.sinkStreams.keySet(), Matchers.containsInAnyOrder(outStreamIds.toArray()));\r\n    Set<String> intStreamIds = intermediateEdges.stream().map(stream -> stream.getStreamSpec().getId()).collect(Collectors.toSet());\r\n    assertThat(jsonObject.intermediateStreams.keySet(), Matchers.containsInAnyOrder(intStreamIds.toArray()));\r\n    Set<String> tableIds = tables.stream().map(t -> t.getTableId()).collect(Collectors.toSet());\r\n    assertThat(jsonObject.tables.keySet(), Matchers.containsInAnyOrder(tableIds.toArray()));\r\n    JobGraphJsonGenerator.JobNodeJson expectedNodeJson = new JobGraphJsonGenerator.JobNodeJson();\r\n    expectedNodeJson.jobId = mockJobNode.getJobId();\r\n    expectedNodeJson.jobName = mockJobNode.getJobName();\r\n    assertEquals(1, jsonObject.jobs.size());\r\n    JobGraphJsonGenerator.JobNodeJson actualNodeJson = jsonObject.jobs.get(0);\r\n    assertEquals(expectedNodeJson.jobId, actualNodeJson.jobId);\r\n    assertEquals(expectedNodeJson.jobName, actualNodeJson.jobName);\r\n    assertEquals(3, actualNodeJson.operatorGraph.inputStreams.size());\r\n    assertEquals(2, actualNodeJson.operatorGraph.outputStreams.size());\r\n    assertEquals(0, actualNodeJson.operatorGraph.operators.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraphJsonGenerator.java",
  "methodName" : "testLegacyTaskApplication",
  "sourceCode" : "@Test\r\npublic void testLegacyTaskApplication() throws Exception {\r\n    JobGraphJsonGenerator jsonGenerator = new JobGraphJsonGenerator();\r\n    JobGraph mockJobGraph = mock(JobGraph.class);\r\n    ApplicationConfig mockAppConfig = mock(ApplicationConfig.class);\r\n    when(mockAppConfig.getAppName()).thenReturn(\"testTaskApp\");\r\n    when(mockAppConfig.getAppId()).thenReturn(\"testTaskAppId\");\r\n    when(mockJobGraph.getApplicationConfig()).thenReturn(mockAppConfig);\r\n    String graphJson = jsonGenerator.toJson(mockJobGraph);\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    JobGraphJsonGenerator.JobGraphJson jsonObject = objectMapper.readValue(graphJson.getBytes(), JobGraphJsonGenerator.JobGraphJson.class);\r\n    assertEquals(\"testTaskAppId\", jsonObject.applicationId);\r\n    assertEquals(\"testTaskApp\", jsonObject.applicationName);\r\n    JobGraphJsonGenerator.JobNodeJson expectedNodeJson = new JobGraphJsonGenerator.JobNodeJson();\r\n    expectedNodeJson.jobId = mockJobNode.getJobId();\r\n    expectedNodeJson.jobName = mockJobNode.getJobName();\r\n    assertEquals(0, jsonObject.jobs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobGraphJsonGenerator.java",
  "methodName" : "testOperatorToMapForTable",
  "sourceCode" : "@Test\r\npublic void testOperatorToMapForTable() {\r\n    JobGraphJsonGenerator jsonGenerator = new JobGraphJsonGenerator();\r\n    Map<String, Object> map;\r\n    SendToTableOperatorSpec<Object, Object> sendToTableOperatorSpec = OperatorSpecs.createSendToTableOperatorSpec(\"test-sent-to-table\", \"test-sent-to\");\r\n    map = jsonGenerator.operatorToMap(sendToTableOperatorSpec);\r\n    assertTrue(map.containsKey(\"tableId\"));\r\n    assertEquals(map.get(\"tableId\"), \"test-sent-to-table\");\r\n    assertEquals(map.get(\"opCode\"), OperatorSpec.OpCode.SEND_TO.name());\r\n    assertEquals(map.get(\"opId\"), \"test-sent-to\");\r\n    StreamTableJoinOperatorSpec<String, String, String, String> streamTableJoinOperatorSpec = OperatorSpecs.createStreamTableJoinOperatorSpec(\"test-join-table\", mock(StreamTableJoinFunction.class), \"test-join\");\r\n    map = jsonGenerator.operatorToMap(streamTableJoinOperatorSpec);\r\n    assertTrue(map.containsKey(\"tableId\"));\r\n    assertEquals(map.get(\"tableId\"), \"test-join-table\");\r\n    assertEquals(map.get(\"opCode\"), OperatorSpec.OpCode.JOIN.name());\r\n    assertEquals(map.get(\"opId\"), \"test-join\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testConfigureSerdesWithRepartitionJoinApplication",
  "sourceCode" : "@Test\r\npublic void testConfigureSerdesWithRepartitionJoinApplication() {\r\n    mockStreamAppDesc = new StreamApplicationDescriptorImpl(getRepartitionJoinStreamApplication(), mockConfig);\r\n    configureJobNode(mockStreamAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    // Verify the results\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    // additional, check the computed window.ms for join\r\n    assertEquals(\"3600000\", jobConfig.get(TaskConfig.WINDOW_MS));\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 5);\r\n    validateStreamConfigures(jobConfig, deserializedSerdes);\r\n    validateJoinStoreConfigures(jobConfig, deserializedSerdes);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testConfigureSerdesForRepartitionWithNoDefaultSystem",
  "sourceCode" : "@Test\r\npublic void testConfigureSerdesForRepartitionWithNoDefaultSystem() {\r\n    // set the application to RepartitionOnlyStreamApplication\r\n    mockStreamAppDesc = new StreamApplicationDescriptorImpl(getRepartitionOnlyStreamApplication(), mockConfig);\r\n    configureJobNode(mockStreamAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    // Verify the results\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 2);\r\n    validateStreamConfigures(jobConfig, null);\r\n    String partitionByKeySerde = jobConfig.get(\"streams.jobName-jobId-partition_by-p1.samza.key.serde\");\r\n    String partitionByMsgSerde = jobConfig.get(\"streams.jobName-jobId-partition_by-p1.samza.msg.serde\");\r\n    assertTrue(\"Serialized serdes should not contain intermediate stream key serde\", !deserializedSerdes.containsKey(partitionByKeySerde));\r\n    assertTrue(\"Serialized serdes should not contain intermediate stream msg serde\", !deserializedSerdes.containsKey(partitionByMsgSerde));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testGenerateJobConfigWithTaskApplication",
  "sourceCode" : "@Test\r\npublic void testGenerateJobConfigWithTaskApplication() {\r\n    // set the application to TaskApplication, which still wire up all input/output/intermediate streams\r\n    TaskApplicationDescriptorImpl taskAppDesc = new TaskApplicationDescriptorImpl(getTaskApplication(), mockConfig);\r\n    configureJobNode(taskAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    // Verify the results\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 2);\r\n    validateStreamConfigures(jobConfig, deserializedSerdes);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testGenerateJobConfigWithLegacyTaskApplication",
  "sourceCode" : "@Test\r\npublic void testGenerateJobConfigWithLegacyTaskApplication() {\r\n    TaskApplicationDescriptorImpl taskAppDesc = new TaskApplicationDescriptorImpl(getLegacyTaskApplication(), mockConfig);\r\n    configureJobNode(taskAppDesc);\r\n    Map<String, String> originConfig = new HashMap<>(mockConfig);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"\");\r\n    // jobConfig should be exactly the same as original config\r\n    Map<String, String> generatedConfig = new HashMap<>(jobConfig);\r\n    assertEquals(originConfig, generatedConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testBroadcastStreamApplication",
  "sourceCode" : "@Test\r\npublic void testBroadcastStreamApplication() {\r\n    // set the application to BroadcastStreamApplication\r\n    mockStreamAppDesc = new StreamApplicationDescriptorImpl(getBroadcastOnlyStreamApplication(defaultSerde), mockConfig);\r\n    configureJobNode(mockStreamAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 2);\r\n    validateStreamSerdeConfigure(broadcastInputDesriptor.getStreamId(), jobConfig, deserializedSerdes);\r\n    validateIntermediateStreamConfigure(broadcastInputDesriptor.getStreamId(), broadcastInputDesriptor.getPhysicalName().get(), jobConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testStreamApplicationWithTableAndSideInput",
  "sourceCode" : "@Test\r\npublic void testStreamApplicationWithTableAndSideInput() {\r\n    mockStreamAppDesc = new StreamApplicationDescriptorImpl(getRepartitionJoinStreamApplication(), mockConfig);\r\n    // add table to the RepartitionJoinStreamApplication\r\n    GenericInputDescriptor<KV<String, Object>> sideInput1 = inputSystemDescriptor.getInputDescriptor(\"sideInput1\", defaultSerde);\r\n    BaseTableDescriptor mockTableDescriptor = new MockLocalTableDescriptor(\"testTable\", defaultSerde).withSideInputs(Arrays.asList(sideInput1.getStreamId())).withSideInputsProcessor(mock(SideInputsProcessor.class, withSettings().serializable())).withConfig(\"mock.table.provider.config\", \"mock.config.value\");\r\n    // add side input and terminate at table in the appplication\r\n    mockStreamAppDesc.getInputStream(sideInput1).sendTo(mockStreamAppDesc.getTable(mockTableDescriptor));\r\n    StreamEdge sideInputEdge = new StreamEdge(new StreamSpec(sideInput1.getStreamId(), \"sideInput1\", inputSystemDescriptor.getSystemName()), false, false, mockConfig);\r\n    // need to put the sideInput related stream configuration to the original config\r\n    // TODO: this is confusing since part of the system and stream related configuration is generated outside the JobGraphConfigureGenerator\r\n    // It would be nice if all system and stream related configuration is generated in one place and only intermediate stream\r\n    // configuration is generated by JobGraphConfigureGenerator\r\n    Map<String, String> configs = new HashMap<>(mockConfig);\r\n    configs.putAll(sideInputEdge.generateConfig());\r\n    mockConfig = spy(new MapConfig(configs));\r\n    configureJobNode(mockStreamAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 5);\r\n    validateTableConfigure(jobConfig, deserializedSerdes, mockTableDescriptor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testTaskApplicationWithTableAndSideInput",
  "sourceCode" : "@Test\r\npublic void testTaskApplicationWithTableAndSideInput() {\r\n    // add table to the RepartitionJoinStreamApplication\r\n    GenericInputDescriptor<KV<String, Object>> sideInput1 = inputSystemDescriptor.getInputDescriptor(\"sideInput1\", defaultSerde);\r\n    BaseTableDescriptor mockTableDescriptor = new MockLocalTableDescriptor(\"testTable\", defaultSerde).withSideInputs(Arrays.asList(sideInput1.getStreamId())).withSideInputsProcessor(mock(SideInputsProcessor.class, withSettings().serializable())).withConfig(\"mock.table.provider.config\", \"mock.config.value\");\r\n    StreamEdge sideInputEdge = new StreamEdge(new StreamSpec(sideInput1.getStreamId(), \"sideInput1\", inputSystemDescriptor.getSystemName()), false, false, mockConfig);\r\n    // need to put the sideInput related stream configuration to the original config\r\n    // TODO: this is confusing since part of the system and stream related configuration is generated outside the JobGraphConfigureGenerator\r\n    // It would be nice if all system and stream related configuration is generated in one place and only intermediate stream\r\n    // configuration is generated by JobGraphConfigureGenerator\r\n    Map<String, String> configs = new HashMap<>(mockConfig);\r\n    configs.putAll(sideInputEdge.generateConfig());\r\n    mockConfig = spy(new MapConfig(configs));\r\n    // set the application to TaskApplication, which still wire up all input/output/intermediate streams\r\n    TaskApplicationDescriptorImpl taskAppDesc = new TaskApplicationDescriptorImpl(getTaskApplication(), mockConfig);\r\n    // add table to the task application\r\n    taskAppDesc.withTable(mockTableDescriptor);\r\n    taskAppDesc.withInputStream(inputSystemDescriptor.getInputDescriptor(\"sideInput1\", defaultSerde));\r\n    configureJobNode(taskAppDesc);\r\n    // create the JobGraphConfigureGenerator and generate the jobConfig for the jobNode\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    // Verify the results\r\n    Config expectedJobConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedJobConfig, jobConfig);\r\n    Map<String, Serde> deserializedSerdes = validateAndGetDeserializedSerdes(jobConfig, 2);\r\n    validateStreamConfigures(jobConfig, deserializedSerdes);\r\n    validateTableConfigure(jobConfig, deserializedSerdes, mockTableDescriptor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testConfigRewriter",
  "sourceCode" : "@Test\r\npublic void testConfigRewriter() {\r\n    Map<String, String> configs = new HashMap<>(mockConfig);\r\n    String streamCfgToOverride = String.format(\"streams.%s.samza.system\", intermediateInputDescriptor.getStreamId());\r\n    configs.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, \"mock\"), MockConfigRewriter.class.getName());\r\n    configs.put(JobConfig.CONFIG_REWRITERS, \"mock\");\r\n    configs.put(String.format(\"job.config.rewriter.mock.%s\", streamCfgToOverride), \"rewritten-system\");\r\n    mockConfig = spy(new MapConfig(configs));\r\n    mockStreamAppDesc = new StreamApplicationDescriptorImpl(getRepartitionJoinStreamApplication(), mockConfig);\r\n    configureJobNode(mockStreamAppDesc);\r\n    JobNodeConfigurationGenerator configureGenerator = new JobNodeConfigurationGenerator();\r\n    JobConfig jobConfig = configureGenerator.generateJobConfig(mockJobNode, \"testJobGraphJson\");\r\n    Config expectedConfig = getExpectedJobConfig(mockConfig, mockJobNode.getInEdges());\r\n    validateJobConfig(expectedConfig, jobConfig);\r\n    assertEquals(\"rewritten-system\", jobConfig.get(streamCfgToOverride));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testJobNameConfigValidation",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testJobNameConfigValidation() {\r\n    ImmutableMap<String, String> userConfigs = ImmutableMap.of(\"job.name\", \"samza-job\", \"job.id\", \"1\", \"app.name\", \"samza-app\");\r\n    ImmutableMap<String, String> generatedConfigs = ImmutableMap.of(\"job.name\", \"samza-app\", \"job.id\", \"1\", \"app.name\", \"samza-app\");\r\n    JobNodeConfigurationGenerator.validateJobConfigs(userConfigs, generatedConfigs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobNodeConfigurationGenerator.java",
  "methodName" : "testJobIdConfigValidation",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testJobIdConfigValidation() {\r\n    ImmutableMap<String, String> userConfigs = ImmutableMap.of(\"job.id\", \"1\", \"app.id\", \"this-should-take-precedence\", \"app.name\", \"samza-app\");\r\n    ImmutableMap<String, String> generatedConfigs = ImmutableMap.of(\"job.name\", \"samza-app\", \"job.id\", \"this-should-take-precedence\", \"app.name\", \"samza-app\");\r\n    JobNodeConfigurationGenerator.validateJobConfigs(userConfigs, generatedConfigs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobPlanner.java",
  "methodName" : "testJobNameIdConfigGeneration",
  "sourceCode" : "@Test\r\npublic void testJobNameIdConfigGeneration() {\r\n    Map<String, String> testConfig = new HashMap<>();\r\n    testConfig.put(\"app.name\", \"samza-app\");\r\n    testConfig.put(\"app.id\", \"id\");\r\n    MapConfig generatedConfig = JobPlanner.generateSingleJobConfig(testConfig);\r\n    Assert.assertEquals(generatedConfig.get(\"job.name\"), \"samza-app\");\r\n    Assert.assertEquals(generatedConfig.get(\"job.id\"), \"id\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobPlanner.java",
  "methodName" : "testAppConfigPrecedence",
  "sourceCode" : "@Test\r\npublic void testAppConfigPrecedence() {\r\n    Map<String, String> testConfig = new HashMap<>();\r\n    testConfig.put(\"app.name\", \"samza-app\");\r\n    testConfig.put(\"app.id\", \"id\");\r\n    testConfig.put(\"job.id\", \"should-not-exist-id\");\r\n    testConfig.put(\"job.name\", \"should-not-exist-name\");\r\n    MapConfig generatedConfig = JobPlanner.generateSingleJobConfig(testConfig);\r\n    Assert.assertEquals(generatedConfig.get(\"job.name\"), \"samza-app\");\r\n    Assert.assertEquals(generatedConfig.get(\"job.id\"), \"id\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobPlanner.java",
  "methodName" : "testJobNameId",
  "sourceCode" : "@Test\r\npublic void testJobNameId() {\r\n    Map<String, String> testConfig = new HashMap<>();\r\n    testConfig.put(\"job.id\", \"should-exist-id\");\r\n    testConfig.put(\"job.name\", \"should-exist-name\");\r\n    MapConfig generatedConfig = JobPlanner.generateSingleJobConfig(testConfig);\r\n    Assert.assertEquals(generatedConfig.get(\"job.name\"), \"should-exist-name\");\r\n    Assert.assertEquals(generatedConfig.get(\"job.id\"), \"should-exist-id\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestJobPlanner.java",
  "methodName" : "testRunIdisConfiguredForAllTypesOfApps",
  "sourceCode" : "@Test\r\npublic void testRunIdisConfiguredForAllTypesOfApps() {\r\n    Map<String, String> testConfig = new HashMap<>();\r\n    testConfig.put(\"app.id\", \"should-exist-id\");\r\n    testConfig.put(\"app.name\", \"should-exist-name\");\r\n    ApplicationDescriptorImpl applicationDescriptor = Mockito.mock(ApplicationDescriptorImpl.class);\r\n    Mockito.when(applicationDescriptor.getConfig()).thenReturn(new MapConfig(testConfig));\r\n    Mockito.when(applicationDescriptor.getAppClass()).thenReturn(LegacyTaskApplication.class);\r\n    JobPlanner jobPlanner = new JobPlanner(applicationDescriptor) {\r\n\r\n        @Override\r\n        public List<JobConfig> prepareJobs() {\r\n            return null;\r\n        }\r\n    };\r\n    ExecutionPlan plan = jobPlanner.getExecutionPlan(\"custom-run-id\");\r\n    Assert.assertNotNull(plan.getApplicationConfig().getRunId(), \"custom-run-id\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestLocalJobPlanner.java",
  "methodName" : "testStreamCreation",
  "sourceCode" : "@Test\r\npublic void testStreamCreation() throws Exception {\r\n    StreamApplicationDescriptorImpl appDesc = mock(StreamApplicationDescriptorImpl.class);\r\n    doReturn(mock(Config.class)).when(appDesc).getConfig();\r\n    localPlanner = createLocalJobPlanner(appDesc);\r\n    StreamManager streamManager = mock(StreamManager.class);\r\n    doReturn(streamManager).when(localPlanner).buildAndStartStreamManager(any(Config.class));\r\n    ExecutionPlan plan = mock(ExecutionPlan.class);\r\n    when(plan.getIntermediateStreams()).thenReturn(Collections.singletonList(new StreamSpec(\"test-stream\", \"test-stream\", \"test-system\")));\r\n    when(plan.getPlanAsJson()).thenReturn(\"\");\r\n    when(plan.getJobConfigs()).thenReturn(Collections.singletonList(mock(JobConfig.class)));\r\n    doReturn(plan).when(localPlanner).getExecutionPlan(any());\r\n    CoordinationUtilsFactory coordinationUtilsFactory = mock(CoordinationUtilsFactory.class);\r\n    JobCoordinatorConfig mockJcConfig = mock(JobCoordinatorConfig.class);\r\n    when(mockJcConfig.getCoordinationUtilsFactory()).thenReturn(coordinationUtilsFactory);\r\n    PowerMockito.whenNew(JobCoordinatorConfig.class).withAnyArguments().thenReturn(mockJcConfig);\r\n    localPlanner.prepareJobs();\r\n    ArgumentCaptor<List> captor = ArgumentCaptor.forClass(List.class);\r\n    verify(streamManager).createStreams(captor.capture());\r\n    List<StreamSpec> streamSpecs = captor.getValue();\r\n    assertEquals(streamSpecs.size(), 1);\r\n    assertEquals(streamSpecs.get(0).getId(), \"test-stream\");\r\n    verify(streamManager).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestLocalJobPlanner.java",
  "methodName" : "testStreamCreationWithCoordination",
  "sourceCode" : "@Test\r\npublic void testStreamCreationWithCoordination() throws Exception {\r\n    StreamApplicationDescriptorImpl appDesc = mock(StreamApplicationDescriptorImpl.class);\r\n    doReturn(mock(Config.class)).when(appDesc).getConfig();\r\n    localPlanner = createLocalJobPlanner(appDesc);\r\n    StreamManager streamManager = mock(StreamManager.class);\r\n    doReturn(streamManager).when(localPlanner).buildAndStartStreamManager(any(Config.class));\r\n    ExecutionPlan plan = mock(ExecutionPlan.class);\r\n    when(plan.getIntermediateStreams()).thenReturn(Collections.singletonList(new StreamSpec(\"test-stream\", \"test-stream\", \"test-system\")));\r\n    when(plan.getPlanAsJson()).thenReturn(\"\");\r\n    when(plan.getJobConfigs()).thenReturn(Collections.singletonList(mock(JobConfig.class)));\r\n    doReturn(plan).when(localPlanner).getExecutionPlan(any());\r\n    CoordinationUtils coordinationUtils = mock(CoordinationUtils.class);\r\n    CoordinationUtilsFactory coordinationUtilsFactory = mock(CoordinationUtilsFactory.class);\r\n    JobCoordinatorConfig mockJcConfig = mock(JobCoordinatorConfig.class);\r\n    when(mockJcConfig.getCoordinationUtilsFactory()).thenReturn(coordinationUtilsFactory);\r\n    PowerMockito.whenNew(JobCoordinatorConfig.class).withAnyArguments().thenReturn(mockJcConfig);\r\n    DistributedLock lock = mock(DistributedLock.class);\r\n    when(lock.lock(anyObject())).thenReturn(true);\r\n    when(coordinationUtils.getLock(anyString())).thenReturn(lock);\r\n    when(coordinationUtilsFactory.getCoordinationUtils(anyString(), anyString(), anyObject())).thenReturn(coordinationUtils);\r\n    localPlanner.prepareJobs();\r\n    ArgumentCaptor<List> captor = ArgumentCaptor.forClass(List.class);\r\n    verify(streamManager).createStreams(captor.capture());\r\n    List<StreamSpec> streamSpecs = captor.getValue();\r\n    assertEquals(streamSpecs.size(), 1);\r\n    assertEquals(streamSpecs.get(0).getId(), \"test-stream\");\r\n    verify(streamManager).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestLocalJobPlanner.java",
  "methodName" : "testPlanIdWithShuffledStreamSpecs",
  "sourceCode" : "/**\r\n * A test case to verify if the plan results in different hash if there is change in topological sort order.\r\n * Note: the overall JOB PLAN remains the same outside the scope of intermediate streams the sake of these test cases.\r\n */\r\n@Test\r\npublic void testPlanIdWithShuffledStreamSpecs() {\r\n    List<StreamSpec> streamSpecs = ImmutableList.of(new StreamSpec(\"test-stream-1\", \"stream-1\", \"testStream\"), new StreamSpec(\"test-stream-2\", \"stream-2\", \"testStream\"), new StreamSpec(\"test-stream-3\", \"stream-3\", \"testStream\"));\r\n    String planIdBeforeShuffle = getExecutionPlanId(streamSpecs);\r\n    List<StreamSpec> shuffledStreamSpecs = ImmutableList.of(new StreamSpec(\"test-stream-2\", \"stream-2\", \"testStream\"), new StreamSpec(\"test-stream-1\", \"stream-1\", \"testStream\"), new StreamSpec(\"test-stream-3\", \"stream-3\", \"testStream\"));\r\n    assertFalse(\"Expected both of the latch ids to be different\", planIdBeforeShuffle.equals(getExecutionPlanId(shuffledStreamSpecs)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestLocalJobPlanner.java",
  "methodName" : "testGeneratePlanIdWithSameStreamSpecs",
  "sourceCode" : "/**\r\n * A test case to verify if the plan results in same hash in case of same plan.\r\n * Note: the overall JOB PLAN remains the same outside the scope of intermediate streams the sake of these test cases.\r\n */\r\n@Test\r\npublic void testGeneratePlanIdWithSameStreamSpecs() {\r\n    List<StreamSpec> streamSpecs = ImmutableList.of(new StreamSpec(\"test-stream-1\", \"stream-1\", \"testStream\"), new StreamSpec(\"test-stream-2\", \"stream-2\", \"testStream\"), new StreamSpec(\"test-stream-3\", \"stream-3\", \"testStream\"));\r\n    String planIdForFirstAttempt = getExecutionPlanId(streamSpecs);\r\n    String planIdForSecondAttempt = getExecutionPlanId(streamSpecs);\r\n    assertEquals(\"Expected latch ids to match!\", \"1447946713\", planIdForFirstAttempt);\r\n    assertEquals(\"Expected latch ids to match for the second attempt!\", planIdForFirstAttempt, planIdForSecondAttempt);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestLocalJobPlanner.java",
  "methodName" : "testGeneratePlanIdWithDifferentStreamSpecs",
  "sourceCode" : "/**\r\n * A test case to verify plan results in different hash in case of different intermediate stream.\r\n * Note: the overall JOB PLAN remains the same outside the scope of intermediate streams the sake of these test cases.\r\n */\r\n@Test\r\npublic void testGeneratePlanIdWithDifferentStreamSpecs() {\r\n    List<StreamSpec> streamSpecs = ImmutableList.of(new StreamSpec(\"test-stream-1\", \"stream-1\", \"testStream\"), new StreamSpec(\"test-stream-2\", \"stream-2\", \"testStream\"), new StreamSpec(\"test-stream-3\", \"stream-3\", \"testStream\"));\r\n    String planIdBeforeShuffle = getExecutionPlanId(streamSpecs);\r\n    List<StreamSpec> updatedStreamSpecs = ImmutableList.of(new StreamSpec(\"test-stream-1\", \"stream-1\", \"testStream\"), new StreamSpec(\"test-stream-4\", \"stream-4\", \"testStream\"), new StreamSpec(\"test-stream-3\", \"stream-3\", \"testStream\"));\r\n    assertFalse(\"Expected both of the latch ids to be different\", planIdBeforeShuffle.equals(getExecutionPlanId(updatedStreamSpecs)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestRemoteJobPlanner.java",
  "methodName" : "testStreamCreation",
  "sourceCode" : "@Test\r\npublic void testStreamCreation() throws Exception {\r\n    remotePlanner = createRemoteJobPlanner(mock(StreamApplicationDescriptorImpl.class));\r\n    StreamManager streamManager = mock(StreamManager.class);\r\n    doReturn(streamManager).when(remotePlanner).buildAndStartStreamManager(any(Config.class));\r\n    ExecutionPlan plan = mock(ExecutionPlan.class);\r\n    when(plan.getIntermediateStreams()).thenReturn(Collections.singletonList(new StreamSpec(\"test-stream\", \"test-stream\", \"test-system\")));\r\n    when(plan.getPlanAsJson()).thenReturn(\"\");\r\n    when(plan.getJobConfigs()).thenReturn(Collections.singletonList(mock(JobConfig.class)));\r\n    ApplicationConfig mockAppConfig = mock(ApplicationConfig.class);\r\n    when(mockAppConfig.getAppMode()).thenReturn(ApplicationConfig.ApplicationMode.STREAM);\r\n    when(plan.getApplicationConfig()).thenReturn(mockAppConfig);\r\n    doReturn(plan).when(remotePlanner).getExecutionPlan(any());\r\n    remotePlanner.prepareJobs();\r\n    verify(streamManager, times(0)).clearStreamsFromPreviousRun(any());\r\n    ArgumentCaptor<List> captor = ArgumentCaptor.forClass(List.class);\r\n    verify(streamManager).createStreams(captor.capture());\r\n    List<StreamSpec> streamSpecs = captor.getValue();\r\n    assertEquals(streamSpecs.size(), 1);\r\n    assertEquals(streamSpecs.get(0).getId(), \"test-stream\");\r\n    verify(streamManager).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamEdge.java",
  "methodName" : "testGetStreamSpec",
  "sourceCode" : "@Test\r\npublic void testGetStreamSpec() {\r\n    StreamEdge edge = new StreamEdge(spec, false, false, new MapConfig());\r\n    assertEquals(edge.getStreamSpec(), spec);\r\n    assertEquals(edge.getStreamSpec().getPartitionCount(), 1);\r\n    edge.setPartitionCount(10);\r\n    assertEquals(edge.getStreamSpec().getPartitionCount(), 10);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamEdge.java",
  "methodName" : "testGetStreamSpec_Batch",
  "sourceCode" : "@Test\r\npublic void testGetStreamSpec_Batch() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());\r\n    config.put(ApplicationConfig.APP_RUN_ID, \"123\");\r\n    // For batch, if the physical name hasn't been generated, it should append the unique id\r\n    StreamSpec batchSpec = new StreamSpec(\"stream-1\", \"stream-1\", \"system-1\");\r\n    StreamEdge edge = new StreamEdge(batchSpec, true, false, new MapConfig(config));\r\n    assertEquals(edge.getStreamSpec().getPhysicalName(), batchSpec.getPhysicalName() + \"-123\");\r\n    // if the physical name has already been geneated somehow, then don't change\r\n    edge = new StreamEdge(spec, true, false, new MapConfig(config));\r\n    assertEquals(edge.getStreamSpec().getPhysicalName(), spec.getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamEdge.java",
  "methodName" : "testGenerateConfig",
  "sourceCode" : "@Test\r\npublic void testGenerateConfig() {\r\n    // an example unbounded IO stream\r\n    StreamSpec spec = new StreamSpec(\"stream-1\", \"physical-stream-1\", \"system-1\", Collections.singletonMap(\"property1\", \"haha\"));\r\n    StreamEdge edge = new StreamEdge(spec, false, false, new MapConfig());\r\n    Config config = edge.generateConfig();\r\n    StreamConfig streamConfig = new StreamConfig(config);\r\n    assertEquals(streamConfig.getSystem(spec.getId()), \"system-1\");\r\n    assertEquals(streamConfig.getPhysicalName(spec.getId()), \"physical-stream-1\");\r\n    assertEquals(streamConfig.getIsIntermediateStream(spec.getId()), false);\r\n    assertEquals(streamConfig.getStreamProperties(spec.getId()).get(\"property1\"), \"haha\");\r\n    // bounded stream\r\n    spec = new StreamSpec(\"stream-1\", \"physical-stream-1\", \"system-1\", Collections.singletonMap(\"property1\", \"haha\"));\r\n    edge = new StreamEdge(spec, false, false, new MapConfig());\r\n    config = edge.generateConfig();\r\n    streamConfig = new StreamConfig(config);\r\n    // intermediate stream\r\n    edge = new StreamEdge(spec, true, false, new MapConfig());\r\n    config = edge.generateConfig();\r\n    streamConfig = new StreamConfig(config);\r\n    assertEquals(streamConfig.getIsIntermediateStream(spec.getId()), true);\r\n    assertEquals(streamConfig.getPriority(spec.toSystemStream()), Integer.MAX_VALUE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamManager.java",
  "methodName" : "testCreateStreams",
  "sourceCode" : "@Test\r\npublic void testCreateStreams() {\r\n    StreamSpec spec1 = new StreamSpec(STREAM1, STREAM1, SYSTEM1);\r\n    StreamSpec spec2 = new StreamSpec(STREAM2, STREAM2, SYSTEM2);\r\n    List<StreamSpec> specList = new ArrayList<>();\r\n    specList.add(spec1);\r\n    specList.add(spec2);\r\n    SystemAdmin admin1 = mock(SystemAdmin.class);\r\n    SystemAdmin admin2 = mock(SystemAdmin.class);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    when(systemAdmins.getSystemAdmin(SYSTEM1)).thenReturn(admin1);\r\n    when(systemAdmins.getSystemAdmin(SYSTEM2)).thenReturn(admin2);\r\n    StreamManager manager = new StreamManager(systemAdmins);\r\n    manager.createStreams(specList);\r\n    ArgumentCaptor<StreamSpec> captor = ArgumentCaptor.forClass(StreamSpec.class);\r\n    verify(admin1).createStream(captor.capture());\r\n    assertEquals(STREAM1, captor.getValue().getPhysicalName());\r\n    captor = ArgumentCaptor.forClass(StreamSpec.class);\r\n    verify(admin2).createStream(captor.capture());\r\n    assertEquals(STREAM2, captor.getValue().getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamManager.java",
  "methodName" : "testGetStreamPartitionCounts",
  "sourceCode" : "@Test\r\npublic void testGetStreamPartitionCounts() {\r\n    SystemAdmin admin1 = mock(SystemAdmin.class);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    when(systemAdmins.getSystemAdmin(SYSTEM1)).thenReturn(admin1);\r\n    Map<String, SystemStreamMetadata> map = new HashMap<>();\r\n    SystemStreamMetadata meta1 = mock(SystemStreamMetadata.class);\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitions = new HashMap<>();\r\n    partitions.put(new Partition(0), null);\r\n    when(meta1.getSystemStreamPartitionMetadata()).thenReturn(partitions);\r\n    map.put(STREAM1, meta1);\r\n    SystemStreamMetadata meta2 = mock(SystemStreamMetadata.class);\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitions2 = new HashMap<>();\r\n    partitions2.put(new Partition(0), null);\r\n    partitions2.put(new Partition(1), null);\r\n    when(meta2.getSystemStreamPartitionMetadata()).thenReturn(partitions2);\r\n    map.put(STREAM2, meta2);\r\n    when(admin1.getSystemStreamMetadata(anyObject())).thenReturn(map);\r\n    Set<String> streams = new HashSet<>();\r\n    streams.add(STREAM1);\r\n    streams.add(STREAM2);\r\n    StreamManager manager = new StreamManager(systemAdmins);\r\n    Map<String, Integer> counts = manager.getStreamPartitionCounts(SYSTEM1, streams);\r\n    assertTrue(counts.get(STREAM1).equals(1));\r\n    assertTrue(counts.get(STREAM2).equals(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\execution\\TestStreamManager.java",
  "methodName" : "testClearStreamsFromPreviousRun",
  "sourceCode" : "@Test\r\npublic void testClearStreamsFromPreviousRun() {\r\n    SystemAdmin admin1 = mock(SystemAdmin.class);\r\n    SystemAdmin admin2 = mock(SystemAdmin.class);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    when(systemAdmins.getSystemAdmin(SYSTEM1)).thenReturn(admin1);\r\n    when(systemAdmins.getSystemAdmin(SYSTEM2)).thenReturn(admin2);\r\n    String runId = \"123\";\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(ApplicationConfig.APP_RUN_ID, \"123\");\r\n    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());\r\n    config.put(\"streams.stream-1.samza.system\", SYSTEM1);\r\n    config.put(\"streams.stream-1.samza.physical.name\", STREAM1 + \"-\" + runId);\r\n    config.put(\"streams.stream-1.samza.intermediate\", \"true\");\r\n    config.put(\"task.checkpoint.factory\", MockCheckpointManagerFactory.class.getName());\r\n    config.put(\"stores.test-store.factory\", \"dummyfactory\");\r\n    config.put(\"stores.test-store.changelog\", SYSTEM2 + \".\" + STREAM2);\r\n    StreamManager manager = new StreamManager(systemAdmins);\r\n    manager.clearStreamsFromPreviousRun(new MapConfig(config));\r\n    ArgumentCaptor<StreamSpec> captor = ArgumentCaptor.forClass(StreamSpec.class);\r\n    verify(admin1).clearStream(captor.capture());\r\n    assertEquals(captor.getValue().getPhysicalName(), STREAM1 + \"-\" + runId);\r\n    captor = ArgumentCaptor.forClass(StreamSpec.class);\r\n    verify(admin2).clearStream(captor.capture());\r\n    assertEquals(captor.getValue().getPhysicalName(), STREAM2 + \"-\" + runId);\r\n    verify(checkpointManager, times(1)).clearCheckpoints();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\executors\\TestKeyBasedExecutorService.java",
  "methodName" : "testSubmitOrdered",
  "sourceCode" : "@Test\r\npublic void testSubmitOrdered() {\r\n    KeyBasedExecutorService executorService = new KeyBasedExecutorService(\"test\", 2);\r\n    ConcurrentLinkedQueue<Integer> resultFromThread0 = new ConcurrentLinkedQueue<>();\r\n    ConcurrentLinkedQueue<Integer> resultFromThread1 = new ConcurrentLinkedQueue<>();\r\n    final CountDownLatch shutdownLatch = new CountDownLatch(10);\r\n    for (int i = 0; i < 10; i++) {\r\n        final int currentStep = i;\r\n        executorService.submitOrdered(currentStep, new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                String threadName = Thread.currentThread().getName();\r\n                Pattern compiledPattern = Pattern.compile(\"test-(.+)-0\");\r\n                Matcher matcher = compiledPattern.matcher(threadName);\r\n                if (matcher.find()) {\r\n                    String threadPoolNumber = matcher.group(1);\r\n                    if (\"0\".equals(threadPoolNumber)) {\r\n                        resultFromThread0.add(currentStep);\r\n                    } else if (\"1\".equals(threadPoolNumber)) {\r\n                        resultFromThread1.add(currentStep);\r\n                    }\r\n                    shutdownLatch.countDown();\r\n                }\r\n            }\r\n        });\r\n    }\r\n    try {\r\n        shutdownLatch.await(2, TimeUnit.SECONDS);\r\n        executorService.shutdown();\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n    }\r\n    Assert.assertEquals(5, resultFromThread0.size());\r\n    Assert.assertEquals(5, resultFromThread1.size());\r\n    Iterator<Integer> iterator = resultFromThread0.iterator();\r\n    int i = 0;\r\n    while (iterator.hasNext()) {\r\n        Assert.assertEquals(i, iterator.next().intValue());\r\n        i += 2;\r\n    }\r\n    iterator = resultFromThread1.iterator();\r\n    i = 1;\r\n    while (iterator.hasNext()) {\r\n        Assert.assertEquals(i, iterator.next().intValue());\r\n        i += 2;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesNoPreviousMetadata",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesNoPreviousMetadata() {\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(NEW_EPOCH_ID, NEW_CONFIG_ID, NEW_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, null);\r\n    assertEquals(\"Metadata check should indicate all changes\", ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT, JobMetadataChange.JOB_MODEL, JobMetadataChange.CONFIG), metadataChanges);\r\n    assertEquals(\"New deployment should be 1 since Epoch ID changed\", 1, this.jobCoordinatorMetadataManager.getMetrics().getNewDeployment().getValue().intValue());\r\n    assertEquals(\"Job model changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getJobModelChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Config changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getConfigChangedAcrossApplicationAttempt().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesNewDeployment",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesNewDeployment() {\r\n    JobCoordinatorMetadata previousMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(NEW_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata);\r\n    assertEquals(\"Metadata check should indicate new deployment\", ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT), metadataChanges);\r\n    assertEquals(\"New deployment should be 1 since Epoch ID changed\", 1, this.jobCoordinatorMetadataManager.getMetrics().getNewDeployment().getValue().intValue());\r\n    assertEquals(\"Application attempt count should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getApplicationAttemptCount().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesJobModelChange",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesJobModelChange() {\r\n    JobCoordinatorMetadata previousMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, NEW_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata);\r\n    assertEquals(\"Metadata check should indicate new job model\", ImmutableSet.of(JobMetadataChange.JOB_MODEL), metadataChanges);\r\n    assertEquals(\"Job model changed across application attempts should be 1\", 1, this.jobCoordinatorMetadataManager.getMetrics().getJobModelChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Application attempt count should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getApplicationAttemptCount().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesConfigChange",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesConfigChange() {\r\n    JobCoordinatorMetadata previousMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, NEW_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata);\r\n    assertEquals(\"Metadata check should indicate new config\", ImmutableSet.of(JobMetadataChange.CONFIG), metadataChanges);\r\n    assertEquals(\"Config changed across application attempts should be 1\", 1, this.jobCoordinatorMetadataManager.getMetrics().getConfigChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Application attempt count should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getApplicationAttemptCount().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesMultipleChanges",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesMultipleChanges() {\r\n    JobCoordinatorMetadata previousMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(NEW_EPOCH_ID, NEW_CONFIG_ID, NEW_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata);\r\n    assertEquals(\"Metadata check should indicate all changes\", ImmutableSet.of(JobMetadataChange.NEW_DEPLOYMENT, JobMetadataChange.JOB_MODEL, JobMetadataChange.CONFIG), metadataChanges);\r\n    assertEquals(\"New deployment should be 1 since Epoch ID changed\", 1, this.jobCoordinatorMetadataManager.getMetrics().getNewDeployment().getValue().intValue());\r\n    assertEquals(\"Job model changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getJobModelChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Config changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getConfigChangedAcrossApplicationAttempt().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testCheckForMetadataChangesNoChanges",
  "sourceCode" : "@Test\r\npublic void testCheckForMetadataChangesNoChanges() {\r\n    JobCoordinatorMetadata previousMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    JobCoordinatorMetadata newMetadata = new JobCoordinatorMetadata(OLD_EPOCH_ID, OLD_CONFIG_ID, OLD_JOB_MODEL_ID);\r\n    Set<JobMetadataChange> metadataChanges = this.jobCoordinatorMetadataManager.checkForMetadataChanges(newMetadata, previousMetadata);\r\n    assertEquals(\"Metadata check should indicate no changes\", ImmutableSet.of(), metadataChanges);\r\n    assertEquals(\"New deployment should be 0 since Epoch ID did not change\", 0, this.jobCoordinatorMetadataManager.getMetrics().getNewDeployment().getValue().intValue());\r\n    assertEquals(\"Job model changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getJobModelChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Config changed across application attempts should be 0\", 0, this.jobCoordinatorMetadataManager.getMetrics().getConfigChangedAcrossApplicationAttempt().getValue().intValue());\r\n    assertEquals(\"Application attempt count should be 1\", 1, this.jobCoordinatorMetadataManager.getMetrics().getApplicationAttemptCount().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testGenerateJobCoordinatorMetadataFailed",
  "sourceCode" : "@Test\r\npublic void testGenerateJobCoordinatorMetadataFailed() {\r\n    doThrow(new RuntimeException(\"Failed to generate epoch id\")).when(jobCoordinatorMetadataManager).fetchEpochIdForJobCoordinator();\r\n    try {\r\n        jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(new JobModel(OLD_CONFIG, containerModelMap), OLD_CONFIG);\r\n        fail(\"Expected generate job coordinator metadata to throw exception\");\r\n    } catch (Exception e) {\r\n        assertTrue(\"Expecting SamzaException to be thrown\", e instanceof SamzaException);\r\n        assertEquals(\"Metadata generation failed count should be 1\", 1, jobCoordinatorMetadataManager.getMetrics().getMetadataGenerationFailedCount().getValue().intValue());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testGenerateJobCoordinatorMetadataForRepeatability",
  "sourceCode" : "@Test\r\npublic void testGenerateJobCoordinatorMetadataForRepeatability() {\r\n    when(jobCoordinatorMetadataManager.getEnvProperty(CONTAINER_ID_PROPERTY)).thenReturn(OLD_CONTAINER_ID);\r\n    JobCoordinatorMetadata expectedMetadata = jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(new JobModel(OLD_CONFIG, containerModelMap), OLD_CONFIG);\r\n    assertEquals(\"Mismatch in epoch identifier.\", OLD_EPOCH_ID, expectedMetadata.getEpochId());\r\n    JobCoordinatorMetadata actualMetadata = jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(new JobModel(OLD_CONFIG, containerModelMap), OLD_CONFIG);\r\n    assertEquals(\"Expected repeatable job coordinator metadata\", expectedMetadata, actualMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testGenerateJobCoordinatorMetadataWithConfigChanges",
  "sourceCode" : "@Test\r\npublic void testGenerateJobCoordinatorMetadataWithConfigChanges() {\r\n    when(jobCoordinatorMetadataManager.getEnvProperty(CONTAINER_ID_PROPERTY)).thenReturn(OLD_CONTAINER_ID);\r\n    JobCoordinatorMetadata expectedMetadata = jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(new JobModel(OLD_CONFIG, containerModelMap), OLD_CONFIG);\r\n    Map<String, String> additionalConfig = new HashMap<>();\r\n    additionalConfig.put(\"yarn.am.high-availability.enabled\", \"true\");\r\n    additionalConfig.putAll(OLD_CONFIG);\r\n    Config modifiedConfig = new MapConfig(additionalConfig);\r\n    JobCoordinatorMetadata actualMetadata = jobCoordinatorMetadataManager.generateJobCoordinatorMetadata(new JobModel(modifiedConfig, containerModelMap), modifiedConfig);\r\n    assertEquals(\"Job coordinator metadata should remain the same\", expectedMetadata, actualMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testReadJobCoordinatorMetadataFailed",
  "sourceCode" : "@Test\r\npublic void testReadJobCoordinatorMetadataFailed() {\r\n    JobCoordinatorMetadata jobCoordinatorMetadata = new JobCoordinatorMetadata(NEW_EPOCH_ID, NEW_CONFIG_ID, NEW_JOB_MODEL_ID);\r\n    Serde<String> mockSerde = spy(new CoordinatorStreamValueSerde(SetJobCoordinatorMetadataMessage.TYPE));\r\n    doThrow(new RuntimeException(\"Failed to read coordinator stream\")).when(mockSerde).fromBytes(any());\r\n    jobCoordinatorMetadataManager = spy(new JobCoordinatorMetadataManager(metadataStore, ClusterType.YARN, new MetricsRegistryMap(), mockSerde));\r\n    jobCoordinatorMetadataManager.writeJobCoordinatorMetadata(jobCoordinatorMetadata);\r\n    JobCoordinatorMetadata actualMetadata = jobCoordinatorMetadataManager.readJobCoordinatorMetadata();\r\n    assertNull(\"Read failed should return null\", actualMetadata);\r\n    assertEquals(\"Metadata read failed count should be 1\", 1, jobCoordinatorMetadataManager.getMetrics().getMetadataReadFailedCount().getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testReadWriteJobCoordinatorMetadata",
  "sourceCode" : "@Test\r\npublic void testReadWriteJobCoordinatorMetadata() {\r\n    JobCoordinatorMetadata jobCoordinatorMetadata = new JobCoordinatorMetadata(NEW_EPOCH_ID, NEW_CONFIG_ID, NEW_JOB_MODEL_ID);\r\n    jobCoordinatorMetadataManager.writeJobCoordinatorMetadata(jobCoordinatorMetadata);\r\n    JobCoordinatorMetadata actualJobCoordinatorMetadata = jobCoordinatorMetadataManager.readJobCoordinatorMetadata();\r\n    assertEquals(\"Mismatch in job coordinator metadata\", jobCoordinatorMetadata, actualJobCoordinatorMetadata);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testWriteNullJobCoordinatorMetadataShouldThrowException",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testWriteNullJobCoordinatorMetadataShouldThrowException() {\r\n    jobCoordinatorMetadataManager.writeJobCoordinatorMetadata(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\metadata\\TestJobCoordinatorMetadataManager.java",
  "methodName" : "testWriteJobCoordinatorMetadataBubblesException",
  "sourceCode" : "@Test\r\npublic void testWriteJobCoordinatorMetadataBubblesException() {\r\n    doThrow(new RuntimeException(\"Failed to write to coordinator stream\")).when(metadataStore).put(anyString(), any());\r\n    try {\r\n        jobCoordinatorMetadataManager.writeJobCoordinatorMetadata(mock(JobCoordinatorMetadata.class));\r\n        fail(\"Expected write job coordinator metadata to throw exception\");\r\n    } catch (Exception e) {\r\n        assertTrue(\"Expecting SamzaException to be thrown\", e instanceof SamzaException);\r\n        assertEquals(\"Metadata write failed count should be 1\", 1, jobCoordinatorMetadataManager.getMetrics().getMetadataWriteFailedCount().getValue().intValue());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModel.java",
  "methodName" : "testMaxChangeLogStreamPartitions",
  "sourceCode" : "@Test\r\npublic void testMaxChangeLogStreamPartitions() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"a\", \"b\"));\r\n    Map<TaskName, TaskModel> tasksForContainer1 = ImmutableMap.of(new TaskName(\"t1\"), new TaskModel(new TaskName(\"t1\"), ImmutableSet.of(), new Partition(0)), new TaskName(\"t2\"), new TaskModel(new TaskName(\"t2\"), ImmutableSet.of(), new Partition(1)));\r\n    Map<TaskName, TaskModel> tasksForContainer2 = ImmutableMap.of(new TaskName(\"t3\"), new TaskModel(new TaskName(\"t3\"), ImmutableSet.of(), new Partition(2)), new TaskName(\"t4\"), new TaskModel(new TaskName(\"t4\"), ImmutableSet.of(), new Partition(3)), new TaskName(\"t5\"), new TaskModel(new TaskName(\"t5\"), ImmutableSet.of(), new Partition(4)));\r\n    ContainerModel containerModel1 = new ContainerModel(\"0\", tasksForContainer1);\r\n    ContainerModel containerModel2 = new ContainerModel(\"1\", tasksForContainer2);\r\n    Map<String, ContainerModel> containers = ImmutableMap.of(\"0\", containerModel1, \"1\", containerModel2);\r\n    JobModel jobModel = new JobModel(config, containers);\r\n    assertEquals(jobModel.getMaxChangeLogStreamPartitions(), 5);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testCompareContainerModels",
  "sourceCode" : "@Test\r\npublic void testCompareContainerModels() {\r\n    final ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    final JobModel first = mock(JobModel.class);\r\n    final JobModel second = mock(JobModel.class);\r\n    final String testProcessor2 = \"testProcessor2\";\r\n    when(first.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel));\r\n    when(second.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel));\r\n    assertTrue(\"Expecting null job models to return true\", JobModelUtil.compareContainerModels(null, null));\r\n    assertTrue(\"Expecting true for job model with same container model\", JobModelUtil.compareContainerModels(first, second));\r\n    when(second.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mock(ContainerModel.class)));\r\n    assertFalse(\"Expecting false for two different job model\", JobModelUtil.compareContainerModels(first, second));\r\n    when(second.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel, testProcessor2, mock(ContainerModel.class)));\r\n    assertFalse(\"Expecting false for two different job model\", JobModelUtil.compareContainerModels(first, second));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testCompareContainerModelForNullProcessor",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testCompareContainerModelForNullProcessor() {\r\n    JobModelUtil.compareContainerModelForProcessor(null, mock(JobModel.class), mock(JobModel.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testCompareContainerModelForBlankProcessor",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testCompareContainerModelForBlankProcessor() {\r\n    JobModelUtil.compareContainerModelForProcessor(\"\", mock(JobModel.class), mock(JobModel.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testCompareContainerModelForProcessor",
  "sourceCode" : "/**\r\n * Tests the following scenarios as part of the same tests to reduce boiler plate and potentially allow\r\n * parallel executions of tests for the class.\r\n *  1. Test null job models for processors\r\n *  2. Test same container models across job models for processors\r\n *  3. Test different container models across job models for processors\r\n *  4. Test absence of container model vs presence across job models for processors\r\n *\r\n * The approach below leans towards performance (parallel execution) as opposed to readability; i.e sharing setup of\r\n * the tests and having multiple tests that test individual scenarios. Additionally, the individual scenarios being\r\n * tested are self explanatory.\r\n */\r\n@Test\r\npublic void testCompareContainerModelForProcessor() {\r\n    final JobModel firstJobModel = mock(JobModel.class);\r\n    final JobModel secondJobModel = mock(JobModel.class);\r\n    final ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    Map<String, ContainerModel> mockContainerModels = mock(Map.class);\r\n    assertTrue(\"Null job models should return true for comparison\", JobModelUtil.compareContainerModelForProcessor(PROCESSOR_ID, null, null));\r\n    when(firstJobModel.getContainers()).thenReturn(mockContainerModels);\r\n    when(secondJobModel.getContainers()).thenReturn(mockContainerModels);\r\n    when(mockContainerModels.get(PROCESSOR_ID)).thenReturn(mockContainerModel);\r\n    assertTrue(\"Expecting both job model to have same container model for the processor\", JobModelUtil.compareContainerModelForProcessor(PROCESSOR_ID, firstJobModel, secondJobModel));\r\n    when(mockContainerModels.get(PROCESSOR_ID)).thenReturn(mockContainerModel).thenReturn(mock(ContainerModel.class));\r\n    assertFalse(\"Expecting container models to be different across job models for the processor\", JobModelUtil.compareContainerModelForProcessor(PROCESSOR_ID, firstJobModel, secondJobModel));\r\n    when(mockContainerModels.get(PROCESSOR_ID)).thenReturn(null).thenReturn(mockContainerModel);\r\n    assertFalse(\"Expecting container models to be different across job models for the processor\", JobModelUtil.compareContainerModelForProcessor(PROCESSOR_ID, firstJobModel, secondJobModel));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testGetTaskNamesForProcessorAbsentInJobModel",
  "sourceCode" : "@Test\r\npublic void testGetTaskNamesForProcessorAbsentInJobModel() {\r\n    JobModel mockJobModel = mock(JobModel.class);\r\n    when(mockJobModel.getContainers()).thenReturn(mock(Map.class));\r\n    Set<TaskName> taskNames = JobModelUtil.getTaskNamesForProcessor(\"testProcessor\", mockJobModel);\r\n    assertTrue(\"TaskNames should be empty\", taskNames.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testGetTaskNamesForProcessorPresentInJobModel",
  "sourceCode" : "@Test\r\npublic void testGetTaskNamesForProcessorPresentInJobModel() {\r\n    TaskName expectedTaskName = new TaskName(\"testTaskName\");\r\n    String processorId = \"testProcessor\";\r\n    JobModel mockJobModel = mock(JobModel.class);\r\n    ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    Map<String, ContainerModel> mockContainers = mock(Map.class);\r\n    when(mockContainers.get(processorId)).thenReturn(mockContainerModel);\r\n    when(mockContainerModel.getTasks()).thenReturn(ImmutableMap.of(expectedTaskName, mock(TaskModel.class)));\r\n    when(mockJobModel.getContainers()).thenReturn(mockContainers);\r\n    Set<TaskName> actualTaskNames = JobModelUtil.getTaskNamesForProcessor(processorId, mockJobModel);\r\n    assertEquals(\"Expecting TaskNames size = 1\", 1, actualTaskNames.size());\r\n    assertTrue(\"Expecting testTaskName to be returned\", actualTaskNames.contains(expectedTaskName));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testGetTaskNamesForProcessorWithNullJobModel",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testGetTaskNamesForProcessorWithNullJobModel() {\r\n    JobModelUtil.getTaskNamesForProcessor(\"processor\", null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testGetTaskNamesForProcessorWithEmptyProcessorId",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetTaskNamesForProcessorWithEmptyProcessorId() {\r\n    JobModelUtil.getTaskNamesForProcessor(\"\", mock(JobModel.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testGetTaskNamesForProcessorWithNullProcessorId",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetTaskNamesForProcessorWithNullProcessorId() {\r\n    JobModelUtil.getTaskNamesForProcessor(null, mock(JobModel.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testTaskToSystemStreamPartitionsWithNullJobModel",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testTaskToSystemStreamPartitionsWithNullJobModel() {\r\n    JobModelUtil.getTaskToSystemStreamPartitions(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\model\\TestJobModelUtil.java",
  "methodName" : "testTaskToSystemStreamPartitions",
  "sourceCode" : "@Test\r\npublic void testTaskToSystemStreamPartitions() {\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"a\", \"b\"));\r\n    Set<SystemStreamPartition> ssps1 = ImmutableSet.of(new SystemStreamPartition(\"system1\", \"stream1_1\", new Partition(0)), new SystemStreamPartition(\"system1\", \"stream1_2\", new Partition(0)));\r\n    Set<SystemStreamPartition> ssps2 = ImmutableSet.of(new SystemStreamPartition(\"system1\", \"stream2_1\", new Partition(1)), new SystemStreamPartition(\"system1\", \"stream2_2\", new Partition(1)));\r\n    Set<SystemStreamPartition> ssps3 = ImmutableSet.of(new SystemStreamPartition(\"system1\", \"stream3_1\", new Partition(2)), new SystemStreamPartition(\"system1\", \"stream3_2\", new Partition(2)));\r\n    Set<SystemStreamPartition> ssps4 = ImmutableSet.of(new SystemStreamPartition(\"system1\", \"stream4_1\", new Partition(3)), new SystemStreamPartition(\"system1\", \"stream4_2\", new Partition(3)));\r\n    // adding this task to both container models\r\n    TaskName task1 = new TaskName(\"task1\");\r\n    TaskName task2 = new TaskName(\"task2\");\r\n    Map<TaskName, TaskModel> tasksForContainer1 = ImmutableMap.of(task1, new TaskModel(task1, ssps1, new Partition(0)), task2, new TaskModel(task2, ssps2, new Partition(1)));\r\n    TaskName task3 = new TaskName(\"task3\");\r\n    Map<TaskName, TaskModel> tasksForContainer2 = ImmutableMap.of(task3, new TaskModel(task3, ssps3, new Partition(2)), task1, new TaskModel(task1, ssps4, new Partition(3)));\r\n    ContainerModel containerModel1 = new ContainerModel(\"0\", tasksForContainer1);\r\n    ContainerModel containerModel2 = new ContainerModel(\"1\", tasksForContainer2);\r\n    Map<String, ContainerModel> containers = ImmutableMap.of(\"0\", containerModel1, \"1\", containerModel2);\r\n    JobModel jobModel = new JobModel(config, containers);\r\n    // test having same task1 in multiple containers\r\n    assertEquals(ssps1.size() + ssps4.size(), JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task1).size());\r\n    assertTrue(JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task1).containsAll(ssps1));\r\n    assertTrue(JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task1).containsAll(ssps4));\r\n    assertEquals(ssps2.size(), JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task2).size());\r\n    assertTrue(JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task2).containsAll(ssps2));\r\n    assertEquals(ssps3.size(), JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task3).size());\r\n    assertTrue(JobModelUtil.getTaskToSystemStreamPartitions(jobModel).get(task3).containsAll(ssps3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\TestShellCommandBuilder.java",
  "methodName" : "testBasicBuild",
  "sourceCode" : "@Test\r\npublic void testBasicBuild() throws MalformedURLException {\r\n    Config config = new MapConfig(ImmutableMap.of(ShellCommandConfig.COMMAND_SHELL_EXECUTE, \"foo\"));\r\n    ShellCommandBuilder shellCommandBuilder = new ShellCommandBuilder();\r\n    shellCommandBuilder.setConfig(config);\r\n    shellCommandBuilder.setId(\"1\");\r\n    shellCommandBuilder.setUrl(new URL(URL_STRING));\r\n    Map<String, String> expectedEnvironment = ImmutableMap.of(ShellCommandConfig.ENV_CONTAINER_ID, \"1\", ShellCommandConfig.ENV_COORDINATOR_URL, URL_STRING, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    // assertions when command path is not set\r\n    assertEquals(\"foo\", shellCommandBuilder.buildCommand());\r\n    assertEquals(expectedEnvironment, shellCommandBuilder.buildEnvironment());\r\n    // assertions when command path is set to empty string\r\n    shellCommandBuilder.setCommandPath(\"\");\r\n    assertEquals(\"foo\", shellCommandBuilder.buildCommand());\r\n    assertEquals(expectedEnvironment, shellCommandBuilder.buildEnvironment());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\TestShellCommandBuilder.java",
  "methodName" : "testBuildEnvironment",
  "sourceCode" : "@Test\r\npublic void testBuildEnvironment() throws MalformedURLException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(ShellCommandConfig.COMMAND_SHELL_EXECUTE, \"foo\").put(ShellCommandConfig.TASK_JVM_OPTS, \"-Xmx4g\").put(ShellCommandConfig.ADDITIONAL_CLASSPATH_DIR, \"/path/to/additional/classpath\").put(ShellCommandConfig.TASK_JAVA_HOME, \"/path/to/java/home\").build());\r\n    ShellCommandBuilder shellCommandBuilder = new ShellCommandBuilder();\r\n    shellCommandBuilder.setConfig(config);\r\n    shellCommandBuilder.setId(\"1\");\r\n    shellCommandBuilder.setUrl(new URL(URL_STRING));\r\n    Map<String, String> expectedEnvironment = new ImmutableMap.Builder<String, String>().put(ShellCommandConfig.ENV_CONTAINER_ID, \"1\").put(ShellCommandConfig.ENV_COORDINATOR_URL, URL_STRING).put(ShellCommandConfig.ENV_JAVA_OPTS, \"-Xmx4g\").put(ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"/path/to/additional/classpath\").put(ShellCommandConfig.ENV_JAVA_HOME, \"/path/to/java/home\").build();\r\n    assertEquals(expectedEnvironment, shellCommandBuilder.buildEnvironment());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\job\\TestShellCommandBuilder.java",
  "methodName" : "testBuildCommandWithCommandPath",
  "sourceCode" : "@Test\r\npublic void testBuildCommandWithCommandPath() throws MalformedURLException {\r\n    Config config = new MapConfig(ImmutableMap.of(ShellCommandConfig.COMMAND_SHELL_EXECUTE, \"foo\"));\r\n    ShellCommandBuilder shellCommandBuilder = new ShellCommandBuilder();\r\n    shellCommandBuilder.setConfig(config);\r\n    shellCommandBuilder.setId(\"1\");\r\n    shellCommandBuilder.setUrl(new URL(URL_STRING));\r\n    shellCommandBuilder.setCommandPath(\"/package/path\");\r\n    assertEquals(\"/package/path/foo\", shellCommandBuilder.buildCommand());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\logging\\TestLoggingContextHolder.java",
  "methodName" : "testGet",
  "sourceCode" : "@Test\r\npublic void testGet() {\r\n    assertNull(this.loggingContextHolder.getConfig());\r\n    this.loggingContextHolder.setConfig(this.config);\r\n    assertEquals(this.config, this.loggingContextHolder.getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\logging\\TestLoggingContextHolder.java",
  "methodName" : "testSetMultiple",
  "sourceCode" : "@Test\r\npublic void testSetMultiple() {\r\n    this.loggingContextHolder.setConfig(this.config);\r\n    Config config0 = mock(Config.class);\r\n    this.loggingContextHolder.setConfig(config0);\r\n    // should still have first config\r\n    assertEquals(this.config, this.loggingContextHolder.getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporter.java",
  "methodName" : "testMetricTypes",
  "sourceCode" : "@Test\r\npublic void testMetricTypes() {\r\n    when(this.readableMetricsRegistry.getGroups()).thenReturn(Collections.singleton(GROUP_NAME));\r\n    Map<String, Metric> metrics = ImmutableMap.of(COUNTER_NAME, this.counter, GAUGE_NAME, this.gauge, TIMER_NAME, this.timer);\r\n    when(this.readableMetricsRegistry.getGroup(GROUP_NAME)).thenReturn(metrics);\r\n    this.loggingMetricsReporter.register(SOURCE_NAME, this.readableMetricsRegistry);\r\n    this.loggingMetricsReporter.start();\r\n    verify(this.loggingMetricsReporter).doLog(\"Metric: source_name-group_name-counter_name, Value: 10\");\r\n    verify(this.loggingMetricsReporter).doLog(\"Metric: source_name-group_name-gauge_name, Value: 20.0\");\r\n    verify(this.loggingMetricsReporter).doLog(\"Metric: source_name-group_name-timer_name, Value: 30.0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporter.java",
  "methodName" : "testMultipleRegister",
  "sourceCode" : "@Test\r\npublic void testMultipleRegister() {\r\n    when(this.readableMetricsRegistry.getGroups()).thenReturn(Collections.singleton(GROUP_NAME));\r\n    when(this.readableMetricsRegistry.getGroup(GROUP_NAME)).thenReturn(ImmutableMap.of(COUNTER_NAME, this.counter));\r\n    ReadableMetricsRegistry otherRegistry = mock(ReadableMetricsRegistry.class);\r\n    String otherGroupName = \"other_group\";\r\n    when(otherRegistry.getGroups()).thenReturn(Collections.singleton(otherGroupName));\r\n    when(otherRegistry.getGroup(otherGroupName)).thenReturn(ImmutableMap.of(GAUGE_NAME, this.gauge));\r\n    this.loggingMetricsReporter.register(SOURCE_NAME, this.readableMetricsRegistry);\r\n    this.loggingMetricsReporter.register(\"other_source\", otherRegistry);\r\n    this.loggingMetricsReporter.start();\r\n    verify(this.loggingMetricsReporter).doLog(\"Metric: source_name-group_name-counter_name, Value: 10\");\r\n    verify(this.loggingMetricsReporter).doLog(\"Metric: other_source-other_group-gauge_name, Value: 20.0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporter.java",
  "methodName" : "testFiltering",
  "sourceCode" : "@Test\r\npublic void testFiltering() {\r\n    Pattern countersOnly = Pattern.compile(\".*counter.*\");\r\n    this.loggingMetricsReporter = spy(new LoggingMetricsReporter(this.scheduledExecutorService, countersOnly, LOGGING_INTERVAL_SECONDS));\r\n    when(this.readableMetricsRegistry.getGroups()).thenReturn(Collections.singleton(GROUP_NAME));\r\n    Map<String, Metric> metrics = ImmutableMap.of(COUNTER_NAME, this.counter, GAUGE_NAME, this.gauge);\r\n    when(this.readableMetricsRegistry.getGroup(GROUP_NAME)).thenReturn(metrics);\r\n    this.loggingMetricsReporter.register(SOURCE_NAME, this.readableMetricsRegistry);\r\n    this.loggingMetricsReporter.start();\r\n    ArgumentCaptor<String> logs = ArgumentCaptor.forClass(String.class);\r\n    verify(this.loggingMetricsReporter).doLog(logs.capture());\r\n    assertEquals(Collections.singletonList(\"Metric: source_name-group_name-counter_name, Value: 10\"), logs.getAllValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporter.java",
  "methodName" : "testNewMetricsAfterRegister",
  "sourceCode" : "@Test\r\npublic void testNewMetricsAfterRegister() {\r\n    when(this.readableMetricsRegistry.getGroups()).thenReturn(Collections.singleton(GROUP_NAME));\r\n    // first round of logging has one metric (counter only), second call has two (counter and gauge)\r\n    when(this.readableMetricsRegistry.getGroup(GROUP_NAME)).thenReturn(ImmutableMap.of(COUNTER_NAME, this.counter)).thenReturn(ImmutableMap.of(COUNTER_NAME, this.counter, GAUGE_NAME, this.gauge));\r\n    // capture the logging task so it can be directly executed by the test\r\n    ArgumentCaptor<Runnable> loggingRunnable = ArgumentCaptor.forClass(Runnable.class);\r\n    when(this.scheduledExecutorService.scheduleAtFixedRate(loggingRunnable.capture(), eq(LOGGING_INTERVAL_SECONDS), eq(LOGGING_INTERVAL_SECONDS), eq(TimeUnit.SECONDS))).thenReturn(null);\r\n    this.loggingMetricsReporter.register(SOURCE_NAME, this.readableMetricsRegistry);\r\n    this.loggingMetricsReporter.start();\r\n    // simulate first scheduled execution of logging task\r\n    loggingRunnable.getValue().run();\r\n    String expectedCounterLog = \"Metric: source_name-group_name-counter_name, Value: 10\";\r\n    // only should get log for counter for the first call\r\n    verify(this.loggingMetricsReporter).doLog(expectedCounterLog);\r\n    String expectedGaugeLog = \"Metric: source_name-group_name-gauge_name, Value: 20.0\";\r\n    verify(this.loggingMetricsReporter, never()).doLog(expectedGaugeLog);\r\n    // simulate second scheduled execution of logging task\r\n    loggingRunnable.getValue().run();\r\n    // should get second log for counter, first log for gauge\r\n    verify(this.loggingMetricsReporter, times(2)).doLog(expectedCounterLog);\r\n    verify(this.loggingMetricsReporter).doLog(expectedGaugeLog);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporterConfig.java",
  "methodName" : "testGetMetricsToLogRegex",
  "sourceCode" : "@Test\r\npublic void testGetMetricsToLogRegex() {\r\n    Map<String, String> configMap = ImmutableMap.of(\"metrics.reporter.reporter_name.log.regex\", \".*metric.*\");\r\n    assertEquals(\".*metric.*\", new LoggingMetricsReporterConfig(new MapConfig(configMap)).getMetricsToLogRegex(REPORTER_NAME));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporterConfig.java",
  "methodName" : "testGetMetricsToLogRegexMissing",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testGetMetricsToLogRegexMissing() {\r\n    new LoggingMetricsReporterConfig(new MapConfig()).getMetricsToLogRegex(REPORTER_NAME);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestLoggingMetricsReporterConfig.java",
  "methodName" : "testGetLoggingIntervalSeconds",
  "sourceCode" : "@Test\r\npublic void testGetLoggingIntervalSeconds() {\r\n    assertEquals(60, new LoggingMetricsReporterConfig(new MapConfig()).getLoggingIntervalSeconds(REPORTER_NAME));\r\n    Map<String, String> configMap = ImmutableMap.of(\"metrics.reporter.reporter_name.logging.interval.seconds\", \"100\");\r\n    assertEquals(100, new LoggingMetricsReporterConfig(new MapConfig(configMap)).getLoggingIntervalSeconds(REPORTER_NAME));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetrics.java",
  "methodName" : "testMetrics",
  "sourceCode" : "@Test\r\npublic void testMetrics() {\r\n    Map<String, Object> group0Metrics = ImmutableMap.of(\"int-value\", 10, \"boolean-value\", true);\r\n    Map<String, Object> group1Metrics = ImmutableMap.of(\"string-value\", \"str\", \"map-value\", ImmutableMap.of(\"a\", \"aa\", \"b\", \"bb\"));\r\n    Map<String, Map<String, Object>> metricsMap = ImmutableMap.of(\"group0\", group0Metrics, \"group1\", group1Metrics);\r\n    Metrics metrics = new Metrics(metricsMap);\r\n    ImmutableMap<String, Object> group0MetricsCopy = ImmutableMap.copyOf(group0Metrics);\r\n    ImmutableMap<String, Object> group1MetricsCopy = ImmutableMap.copyOf(group1Metrics);\r\n    assertEquals(ImmutableMap.of(\"group0\", group0MetricsCopy, \"group1\", group1MetricsCopy), metrics.getAsMap());\r\n    assertEquals(group0MetricsCopy, metrics.get(\"group0\"));\r\n    assertEquals(group1MetricsCopy, metrics.get(\"group1\"));\r\n    assertNull(metrics.get(\"group2\"));\r\n    assertEquals(new Integer(10), metrics.get(\"group0\", \"int-value\"));\r\n    assertEquals(Boolean.TRUE, metrics.get(\"group0\", \"boolean-value\"));\r\n    assertNull(metrics.get(\"group0\", \"key-does-not-exist\"));\r\n    assertEquals(\"str\", metrics.get(\"group1\", \"string-value\"));\r\n    assertEquals(ImmutableMap.of(\"a\", \"aa\", \"b\", \"bb\"), metrics.get(\"group1\", \"map-value\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsHeader.java",
  "methodName" : "testGetAsMap",
  "sourceCode" : "@Test\r\npublic void testGetAsMap() {\r\n    MetricsHeader metricsHeader = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXEC_ENV_CONTAINER_ID, Optional.of(SAMZA_EPOCH_ID), SOURCE, VERSION, SAMZA_VERSION, HOST, TIME, RESET_TIME);\r\n    Map<String, Object> expected = new HashMap<>();\r\n    expected.put(\"job-name\", JOB_NAME);\r\n    expected.put(\"job-id\", JOB_ID);\r\n    expected.put(\"container-name\", CONTAINER_NAME);\r\n    expected.put(\"exec-env-container-id\", EXEC_ENV_CONTAINER_ID);\r\n    expected.put(\"samza-epoch-id\", SAMZA_EPOCH_ID);\r\n    expected.put(\"source\", SOURCE);\r\n    expected.put(\"version\", VERSION);\r\n    expected.put(\"samza-version\", SAMZA_VERSION);\r\n    expected.put(\"host\", HOST);\r\n    expected.put(\"time\", TIME);\r\n    expected.put(\"reset-time\", RESET_TIME);\r\n    assertEquals(expected, metricsHeader.getAsMap());\r\n    // test with empty samza epoch id\r\n    metricsHeader = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXEC_ENV_CONTAINER_ID, Optional.empty(), SOURCE, VERSION, SAMZA_VERSION, HOST, TIME, RESET_TIME);\r\n    expected.remove(\"samza-epoch-id\");\r\n    assertEquals(expected, metricsHeader.getAsMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsHeader.java",
  "methodName" : "testFromMap",
  "sourceCode" : "@Test\r\npublic void testFromMap() {\r\n    Map<String, Object> map = new HashMap<>();\r\n    map.put(\"job-name\", JOB_NAME);\r\n    map.put(\"job-id\", JOB_ID);\r\n    map.put(\"container-name\", CONTAINER_NAME);\r\n    map.put(\"exec-env-container-id\", EXEC_ENV_CONTAINER_ID);\r\n    map.put(\"samza-epoch-id\", SAMZA_EPOCH_ID);\r\n    map.put(\"source\", SOURCE);\r\n    map.put(\"version\", VERSION);\r\n    map.put(\"samza-version\", SAMZA_VERSION);\r\n    map.put(\"host\", HOST);\r\n    map.put(\"time\", TIME);\r\n    map.put(\"reset-time\", RESET_TIME);\r\n    MetricsHeader expected = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXEC_ENV_CONTAINER_ID, Optional.of(SAMZA_EPOCH_ID), SOURCE, VERSION, SAMZA_VERSION, HOST, TIME, RESET_TIME);\r\n    assertEquals(expected, MetricsHeader.fromMap(map));\r\n    // test with missing samza epoch id\r\n    map.remove(\"samza-epoch-id\");\r\n    expected = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, EXEC_ENV_CONTAINER_ID, Optional.empty(), SOURCE, VERSION, SAMZA_VERSION, HOST, TIME, RESET_TIME);\r\n    assertEquals(expected, MetricsHeader.fromMap(map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshot.java",
  "methodName" : "testGetAsMap",
  "sourceCode" : "@Test\r\npublic void testGetAsMap() {\r\n    MetricsSnapshot metricsSnapshot = new MetricsSnapshot(METRICS_HEADER, METRICS);\r\n    Map<String, Object> expected = ImmutableMap.of(\"header\", METRICS_HEADER.getAsMap(), \"metrics\", METRICS.getAsMap());\r\n    assertEquals(expected, metricsSnapshot.getAsMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshot.java",
  "methodName" : "testFromMap",
  "sourceCode" : "@Test\r\npublic void testFromMap() {\r\n    Map<String, Map<String, Object>> map = ImmutableMap.of(\"header\", ImmutableMap.copyOf(METRICS_HEADER.getAsMap()), \"metrics\", ImmutableMap.copyOf((Map<String, Object>) (Map<?, ?>) METRICS.getAsMap()));\r\n    MetricsSnapshot expected = new MetricsSnapshot(METRICS_HEADER, METRICS);\r\n    assertEquals(expected, MetricsSnapshot.fromMap(map));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporter.java",
  "methodName" : "testBlacklistAll",
  "sourceCode" : "@Test\r\npublic void testBlacklistAll() {\r\n    this.metricsSnapshotReporter = getMetricsSnapshotReporter(Optional.of(BLACKLIST_ALL));\r\n    Assert.assertTrue(\"Should ignore all metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.kafka.KafkaSystemProducerMetrics\", \"kafka-flush-ns\"));\r\n    Assert.assertTrue(\"Should ignore all metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.storage.kv.LoggedStoreMetrics\", \"stats-ranges\"));\r\n    Assert.assertTrue(\"Should ignore all metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.SystemProducersMetrics\", \"flushes\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporter.java",
  "methodName" : "testBlacklistNone",
  "sourceCode" : "@Test\r\npublic void testBlacklistNone() {\r\n    this.metricsSnapshotReporter = getMetricsSnapshotReporter(Optional.of(BLACKLIST_NONE));\r\n    Assert.assertFalse(\"Should not ignore any metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.kafka.KafkaSystemProducerMetrics\", \"kafka-flush-ns\"));\r\n    Assert.assertFalse(\"Should not ignore any metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.storage.kv.LoggedStoreMetrics\", \"stats-ranges\"));\r\n    Assert.assertFalse(\"Should not ignore any metrics\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.SystemProducersMetrics\", \"flushes\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporter.java",
  "methodName" : "testBlacklistGroup",
  "sourceCode" : "@Test\r\npublic void testBlacklistGroup() {\r\n    this.metricsSnapshotReporter = getMetricsSnapshotReporter(Optional.of(BLACKLIST_GROUPS));\r\n    Assert.assertTrue(\"Should ignore all metrics from this group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.SystemConsumersMetrics\", \"poll-ns\"));\r\n    Assert.assertTrue(\"Should ignore all metrics from this group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.SystemConsumersMetrics\", \"unprocessed-messages\"));\r\n    Assert.assertTrue(\"Should ignore all metrics from this group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.storage.kv.CachedStoreMetrics\", \"storename-stats-flushes\"));\r\n    Assert.assertFalse(\"Should not ignore any other group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.kafka.KafkaSystemConsumerMetrics\", \"poll-count\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporter.java",
  "methodName" : "testBlacklistAllButTwoGroups",
  "sourceCode" : "@Test\r\npublic void testBlacklistAllButTwoGroups() {\r\n    this.metricsSnapshotReporter = getMetricsSnapshotReporter(Optional.of(BLACKLIST_ALL_BUT_TWO_GROUPS));\r\n    Assert.assertFalse(\"Should not ignore this group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.SystemConsumersMetrics\", \"poll-ns\"));\r\n    Assert.assertFalse(\"Should not ignore this group\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.storage.kv.CachedStoreMetrics\", \"storename-stats-flushes\"));\r\n    Assert.assertTrue(\"Should ignore all metrics from any other groups\", this.metricsSnapshotReporter.shouldIgnore(\"org.apache.samza.system.kafka.KafkaSystemConsumerMetrics\", \"poll-count\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporter.java",
  "methodName" : "testMetricsEmission",
  "sourceCode" : "@Test\r\npublic void testMetricsEmission() {\r\n    // setup\r\n    serializer = null;\r\n    String source = \"testSource\";\r\n    String group = \"someGroup\";\r\n    String metricName = \"someName\";\r\n    MetricsRegistryMap registry = new MetricsRegistryMap();\r\n    metricsSnapshotReporter = getMetricsSnapshotReporter(Optional.empty());\r\n    registry.newGauge(group, metricName, 42);\r\n    metricsSnapshotReporter.register(source, registry);\r\n    ArgumentCaptor<OutgoingMessageEnvelope> outgoingMessageEnvelopeArgumentCaptor = ArgumentCaptor.forClass(OutgoingMessageEnvelope.class);\r\n    // run\r\n    metricsSnapshotReporter.run();\r\n    // assert\r\n    verify(producer, times(1)).send(eq(source), outgoingMessageEnvelopeArgumentCaptor.capture());\r\n    verify(producer, times(1)).flush(eq(source));\r\n    List<OutgoingMessageEnvelope> envelopes = outgoingMessageEnvelopeArgumentCaptor.getAllValues();\r\n    Assert.assertEquals(1, envelopes.size());\r\n    MetricsSnapshot metricsSnapshot = (MetricsSnapshot) envelopes.get(0).getMessage();\r\n    MetricsHeader expectedHeader = new MetricsHeader(JOB_NAME, JOB_ID, CONTAINER_NAME, \"\", Optional.of(\"\"), source, TASK_VERSION, SAMZA_VERSION, HOSTNAME, SEND_TIME, RESET_TIME);\r\n    Assert.assertEquals(expectedHeader, metricsSnapshot.getHeader());\r\n    Map<String, Map<String, Object>> metricMap = metricsSnapshot.getMetrics().getAsMap();\r\n    Assert.assertEquals(1, metricMap.size());\r\n    Assert.assertTrue(metricMap.containsKey(group));\r\n    Assert.assertTrue(metricMap.get(group).containsKey(metricName));\r\n    Assert.assertEquals(42, metricMap.get(group).get(metricName));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporterFactory.java",
  "methodName" : "testGetProducer",
  "sourceCode" : "@Test\r\npublic void testGetProducer() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"metrics.reporter.metrics-reporter.stream\", \"system0.stream0\", \"systems.system0.samza.factory\", MockSystemFactory.class.getName()));\r\n    assertEquals(SYSTEM_PRODUCER, this.factory.getProducer(REPORTER, config, new MetricsRegistryMap()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporterFactory.java",
  "methodName" : "testGetSystemStream",
  "sourceCode" : "@Test\r\npublic void testGetSystemStream() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"metrics.reporter.metrics-reporter.stream\", \"system0.stream0\"));\r\n    assertEquals(new SystemStream(\"system0\", \"stream0\"), this.factory.getSystemStream(REPORTER, config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporterFactory.java",
  "methodName" : "testGetSerdeConfigured",
  "sourceCode" : "@Test\r\npublic void testGetSerdeConfigured() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"metrics.reporter.metrics-reporter.stream\", \"system0.stream0\", \"streams.stream0.samza.system\", \"system0\", \"streams.stream0.samza.msg.serde\", \"snapshot-serde\", \"serializers.registry.snapshot-serde.class\", MockSerdeFactory.class.getName()));\r\n    assertEquals(SERDE, this.factory.getSerde(REPORTER, config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporterFactory.java",
  "methodName" : "testGetSerdeNoSerdeFactory",
  "sourceCode" : "@Test\r\npublic void testGetSerdeNoSerdeFactory() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"metrics.reporter.metrics-reporter.stream\", \"system0.stream0\", \"streams.stream0.samza.system\", \"system0\", \"streams.stream0.samza.msg.serde\", \"snapshot-serde\"));\r\n    assertNull(this.factory.getSerde(REPORTER, config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\reporter\\TestMetricsSnapshotReporterFactory.java",
  "methodName" : "testGetSerdeFallback",
  "sourceCode" : "@Test\r\npublic void testGetSerdeFallback() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"metrics.reporter.metrics-reporter.stream\", \"system0.stream0\", \"streams.stream0.samza.system\", \"system0\"));\r\n    assertTrue(this.factory.getSerde(REPORTER, config) instanceof MetricsSnapshotSerdeV2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\TestJmxMetricsAccessor.java",
  "methodName" : "testGetCounterValues",
  "sourceCode" : "@Test\r\npublic void testGetCounterValues() throws Exception {\r\n    ObjectName counterObject = JmxUtil.getObjectName(SamzaContainerMetrics.class.getName(), \"commit-calls\", \"samza-container-0\");\r\n    objectNames.add(counterObject);\r\n    Long commitCalls = 100L;\r\n    when(conn.getAttribute(counterObject, \"Count\")).thenReturn(commitCalls);\r\n    Map<String, Long> result = jmxMetricsAccessor.getCounterValues(SamzaContainerMetrics.class.getName(), \"commit-calls\");\r\n    assertTrue(result.size() == 1);\r\n    assertTrue(result.get(\"samza-container-0\").equals(commitCalls));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\TestJmxMetricsAccessor.java",
  "methodName" : "testGetGaugeValues",
  "sourceCode" : "@Test\r\npublic void testGetGaugeValues() throws Exception {\r\n    ObjectName gaugeObject = JmxUtil.getObjectName(SamzaContainerMetrics.class.getName(), \"event-loop-utilization\", \"samza-container-1\");\r\n    objectNames.add(gaugeObject);\r\n    Double loopUtil = 0.8;\r\n    when(conn.getAttribute(gaugeObject, \"Value\")).thenReturn(loopUtil);\r\n    Map<String, Double> result = jmxMetricsAccessor.getGaugeValues(SamzaContainerMetrics.class.getName(), \"event-loop-utilization\");\r\n    assertTrue(result.size() == 1);\r\n    assertTrue(result.get(\"samza-container-1\").equals(loopUtil));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\metrics\\TestJmxMetricsAccessor.java",
  "methodName" : "testGetTimerValues",
  "sourceCode" : "@Test\r\npublic void testGetTimerValues() throws Exception {\r\n    ObjectName timerObject = JmxUtil.getObjectName(SamzaContainerMetrics.class.getName(), \"choose-ns\", \"samza-container-2\");\r\n    objectNames.add(timerObject);\r\n    Double time = 42.42;\r\n    when(conn.getAttribute(timerObject, \"AverageTime\")).thenReturn(time);\r\n    Map<String, Double> result = jmxMetricsAccessor.getTimerValues(SamzaContainerMetrics.class.getName(), \"choose-ns\");\r\n    assertTrue(result.size() == 1);\r\n    assertTrue(result.get(\"samza-container-2\").equals(time));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesKeySerde.java",
  "methodName" : "testStringTimeSeriesKey",
  "sourceCode" : "@Test\r\npublic void testStringTimeSeriesKey() {\r\n    TimeSeriesKey<String> storeKey = new TimeSeriesKey<>(\"test\", 1, 23);\r\n    TimeSeriesKeySerde<String> serde = new TimeSeriesKeySerde<>(new StringSerde(\"UTF-8\"));\r\n    byte[] serializedBytes = serde.toBytes(storeKey);\r\n    TimeSeriesKey<String> deserializedTimeSeriesKey = serde.fromBytes(serializedBytes);\r\n    assertEquals(storeKey.getKey(), deserializedTimeSeriesKey.getKey());\r\n    assertEquals(storeKey.getSeqNum(), deserializedTimeSeriesKey.getSeqNum());\r\n    assertEquals(storeKey.getTimestamp(), deserializedTimeSeriesKey.getTimestamp());\r\n    assertEquals(storeKey, deserializedTimeSeriesKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesKeySerde.java",
  "methodName" : "testNullTimeSeriesKey",
  "sourceCode" : "@Test\r\npublic void testNullTimeSeriesKey() {\r\n    TimeSeriesKey<String> storeKey = new TimeSeriesKey<>(null, 1, 23);\r\n    TimeSeriesKeySerde<String> serde = new TimeSeriesKeySerde<>(new StringSerde(\"UTF-8\"));\r\n    byte[] serializedBytes = serde.toBytes(storeKey);\r\n    TimeSeriesKey<String> deserializedTimeSeriesKey = serde.fromBytes(serializedBytes);\r\n    assertEquals(storeKey.getKey(), deserializedTimeSeriesKey.getKey());\r\n    assertEquals(storeKey.getSeqNum(), deserializedTimeSeriesKey.getSeqNum());\r\n    assertEquals(storeKey.getTimestamp(), deserializedTimeSeriesKey.getTimestamp());\r\n    assertEquals(storeKey, deserializedTimeSeriesKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesKeySerde.java",
  "methodName" : "testLongTimeSeriesKey",
  "sourceCode" : "@Test\r\npublic void testLongTimeSeriesKey() {\r\n    TimeSeriesKey<Long> storeKey = new TimeSeriesKey<>(30L, 1, 23);\r\n    TimeSeriesKeySerde<Long> serde = new TimeSeriesKeySerde<>(new LongSerde());\r\n    byte[] serializedBytes = serde.toBytes(storeKey);\r\n    TimeSeriesKey<Long> deserializedTimeSeriesKey = serde.fromBytes(serializedBytes);\r\n    assertEquals(storeKey.getKey(), deserializedTimeSeriesKey.getKey());\r\n    assertEquals(storeKey.getSeqNum(), deserializedTimeSeriesKey.getSeqNum());\r\n    assertEquals(storeKey.getTimestamp(), deserializedTimeSeriesKey.getTimestamp());\r\n    assertEquals(storeKey, deserializedTimeSeriesKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesStoreImpl.java",
  "methodName" : "testGetOnTimestampBoundaries",
  "sourceCode" : "@Test\r\npublic void testGetOnTimestampBoundaries() {\r\n    TimeSeriesStore<String, byte[]> timeSeriesStore = newTimeSeriesStore(new StringSerde(\"UTF-8\"), true);\r\n    // insert an entry with key \"hello\" at timestamps \"1\" and \"2\"\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 1L);\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 2L);\r\n    timeSeriesStore.put(\"hello\", \"world-2\".getBytes(), 2L);\r\n    // read from time-range\r\n    List<TimestampedValue<byte[]>> values = readStore(timeSeriesStore, \"hello\", 0L, 1L);\r\n    Assert.assertEquals(0, values.size());\r\n    // read from time-range [1,2) should return one entry\r\n    values = readStore(timeSeriesStore, \"hello\", 1L, 2L);\r\n    Assert.assertEquals(1, values.size());\r\n    Assert.assertEquals(\"world-1\", new String(values.get(0).getValue()));\r\n    // read from time-range [2,3) should return two entries\r\n    values = readStore(timeSeriesStore, \"hello\", 2L, 3L);\r\n    Assert.assertEquals(2, values.size());\r\n    Assert.assertEquals(\"world-1\", new String(values.get(0).getValue()));\r\n    Assert.assertEquals(2L, values.get(0).getTimestamp());\r\n    // read from time-range [0,3) should return three entries\r\n    values = readStore(timeSeriesStore, \"hello\", 0L, 3L);\r\n    Assert.assertEquals(3, values.size());\r\n    // read from time-range [2,999999) should return two entries\r\n    values = readStore(timeSeriesStore, \"hello\", 2L, 999999L);\r\n    Assert.assertEquals(2, values.size());\r\n    // read from time-range [3,4) should return no entries\r\n    values = readStore(timeSeriesStore, \"hello\", 3L, 4L);\r\n    Assert.assertEquals(0, values.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesStoreImpl.java",
  "methodName" : "testGetWithNonExistentKeys",
  "sourceCode" : "@Test\r\npublic void testGetWithNonExistentKeys() {\r\n    TimeSeriesStore<String, byte[]> timeSeriesStore = newTimeSeriesStore(new StringSerde(\"UTF-8\"), true);\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 1L);\r\n    // read from a non-existent key\r\n    List<TimestampedValue<byte[]>> values = readStore(timeSeriesStore, \"non-existent-key\", 0, Integer.MAX_VALUE);\r\n    Assert.assertEquals(0, values.size());\r\n    // read from an existing key but out of range timestamp\r\n    values = readStore(timeSeriesStore, \"hello\", 2, Integer.MAX_VALUE);\r\n    Assert.assertEquals(0, values.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesStoreImpl.java",
  "methodName" : "testPutWithMultipleEntries",
  "sourceCode" : "@Test\r\npublic void testPutWithMultipleEntries() {\r\n    TimeSeriesStore<String, byte[]> timeSeriesStore = newTimeSeriesStore(new StringSerde(\"UTF-8\"), true);\r\n    // insert 100 entries at timestamps \"1\" and \"2\"\r\n    for (int i = 0; i < 100; i++) {\r\n        timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 1L);\r\n        timeSeriesStore.put(\"hello\", \"world-2\".getBytes(), 2L);\r\n    }\r\n    // read from time-range [0,2) should return 100 entries\r\n    List<TimestampedValue<byte[]>> values = readStore(timeSeriesStore, \"hello\", 0L, 2L);\r\n    Assert.assertEquals(100, values.size());\r\n    values.forEach(timeSeriesValue -> {\r\n        Assert.assertEquals(\"world-1\", new String(timeSeriesValue.getValue()));\r\n    });\r\n    // read from time-range [2,4) should return 100 entries\r\n    values = readStore(timeSeriesStore, \"hello\", 2L, 4L);\r\n    Assert.assertEquals(100, values.size());\r\n    values.forEach(timeSeriesValue -> {\r\n        Assert.assertEquals(\"world-2\", new String(timeSeriesValue.getValue()));\r\n    });\r\n    // read all entries in the store\r\n    values = readStore(timeSeriesStore, \"hello\", 0L, Integer.MAX_VALUE);\r\n    Assert.assertEquals(200, values.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesStoreImpl.java",
  "methodName" : "testGetOnTimestampBoundariesWithOverwriteMode",
  "sourceCode" : "@Test\r\npublic void testGetOnTimestampBoundariesWithOverwriteMode() {\r\n    // instantiate a store in overwrite mode\r\n    TimeSeriesStore<String, byte[]> timeSeriesStore = newTimeSeriesStore(new StringSerde(\"UTF-8\"), false);\r\n    // insert an entry with key \"hello\" at timestamps \"1\" and \"2\"\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 1L);\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 2L);\r\n    timeSeriesStore.put(\"hello\", \"world-2\".getBytes(), 2L);\r\n    // read from time-range\r\n    List<TimestampedValue<byte[]>> values = readStore(timeSeriesStore, \"hello\", 0L, 1L);\r\n    Assert.assertEquals(0, values.size());\r\n    // read from time-range [1,2) should return one entry\r\n    values = readStore(timeSeriesStore, \"hello\", 1L, 2L);\r\n    Assert.assertEquals(1, values.size());\r\n    Assert.assertEquals(\"world-1\", new String(values.get(0).getValue()));\r\n    // read from time-range [2,3) should return the most recent entry\r\n    values = readStore(timeSeriesStore, \"hello\", 2L, 3L);\r\n    Assert.assertEquals(1, values.size());\r\n    Assert.assertEquals(\"world-2\", new String(values.get(0).getValue()));\r\n    Assert.assertEquals(2L, values.get(0).getTimestamp());\r\n    // read from time-range [0,3) should return two entries\r\n    values = readStore(timeSeriesStore, \"hello\", 0L, 3L);\r\n    Assert.assertEquals(2, values.size());\r\n    // read from time-range [2,999999) should return one entry\r\n    values = readStore(timeSeriesStore, \"hello\", 2L, 999999L);\r\n    Assert.assertEquals(1, values.size());\r\n    // read from time-range [3,4) should return no entries\r\n    values = readStore(timeSeriesStore, \"hello\", 3L, 4L);\r\n    Assert.assertEquals(0, values.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimeSeriesStoreImpl.java",
  "methodName" : "testDeletesInOverwriteMode",
  "sourceCode" : "@Test\r\npublic void testDeletesInOverwriteMode() {\r\n    // instantiate a store in overwrite mode\r\n    TimeSeriesStore<String, byte[]> timeSeriesStore = newTimeSeriesStore(new StringSerde(\"UTF-8\"), false);\r\n    // insert an entry with key \"hello\" at timestamps \"1\" and \"2\"\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 1L);\r\n    timeSeriesStore.put(\"hello\", \"world-1\".getBytes(), 2L);\r\n    timeSeriesStore.put(\"hello\", \"world-2\".getBytes(), 2L);\r\n    List<TimestampedValue<byte[]>> values = readStore(timeSeriesStore, \"hello\", 1L, 3L);\r\n    Assert.assertEquals(2, values.size());\r\n    timeSeriesStore.remove(\"hello\", 0L, 3L);\r\n    values = readStore(timeSeriesStore, \"hello\", 1L, 3L);\r\n    Assert.assertEquals(0, values.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimestampedValueSerde.java",
  "methodName" : "testEmptyValueDeserialization",
  "sourceCode" : "@Test\r\npublic void testEmptyValueDeserialization() {\r\n    byte[] bytesWithNoValue = new byte[8];\r\n    ByteBuffer.wrap(bytesWithNoValue).putLong(1234L);\r\n    TimestampedValueSerde<byte[]> timestampedValueSerde = new TimestampedValueSerde<>(new ByteSerde());\r\n    TimestampedValue<byte[]> timestampedValue = timestampedValueSerde.fromBytes(bytesWithNoValue);\r\n    assertEquals(1234L, timestampedValue.getTimestamp());\r\n    assertEquals(0, timestampedValue.getValue().length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\store\\TestTimestampedValueSerde.java",
  "methodName" : "testEmptyValueSerialization",
  "sourceCode" : "@Test\r\npublic void testEmptyValueSerialization() {\r\n    byte[] expectedBytes = new byte[8];\r\n    ByteBuffer.wrap(expectedBytes).putLong(1234L);\r\n    TimestampedValueSerde<Integer> timestampedValueSerde = new TimestampedValueSerde<>(new IntegerSerde());\r\n    TimestampedValue<Integer> timestampedValue = new TimestampedValue<>(null, 1234L);\r\n    assertTrue(Arrays.equals(expectedBytes, timestampedValueSerde.toBytes(timestampedValue)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestAsyncFlatmapOperatorImpl.java",
  "methodName" : "testAsyncFlatMapOperator",
  "sourceCode" : "@Test\r\n@SuppressWarnings(\"unchecked\")\r\npublic void testAsyncFlatMapOperator() {\r\n    AsyncFlatMapOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> mockOp = mock(AsyncFlatMapOperatorSpec.class);\r\n    AsyncFlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> txfmFn = mock(AsyncFlatMapFunction.class);\r\n    when(mockOp.getTransformFn()).thenReturn(txfmFn);\r\n    AsyncFlatmapOperatorImpl<TestMessageEnvelope, TestOutputMessageEnvelope> opImpl = new AsyncFlatmapOperatorImpl<>(mockOp);\r\n    TestMessageEnvelope inMsg = mock(TestMessageEnvelope.class);\r\n    Collection<TestOutputMessageEnvelope> mockOutputs = mock(Collection.class);\r\n    when(txfmFn.apply(inMsg)).thenReturn(CompletableFuture.supplyAsync(() -> mockOutputs));\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    Collection<TestOutputMessageEnvelope> results = opImpl.handleMessage(inMsg, mockCollector, mockCoordinator);\r\n    verify(txfmFn, times(1)).apply(inMsg);\r\n    assertEquals(results, mockOutputs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestAsyncFlatmapOperatorImpl.java",
  "methodName" : "testAsyncFlatMapOperatorClose",
  "sourceCode" : "@Test\r\npublic void testAsyncFlatMapOperatorClose() {\r\n    AsyncFlatMapOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> mockOp = mock(AsyncFlatMapOperatorSpec.class);\r\n    AsyncFlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> txfmFn = mock(AsyncFlatMapFunction.class);\r\n    when(mockOp.getTransformFn()).thenReturn(txfmFn);\r\n    AsyncFlatmapOperatorImpl<TestMessageEnvelope, TestOutputMessageEnvelope> opImpl = new AsyncFlatmapOperatorImpl<>(mockOp);\r\n    // ensure that close is not called yet\r\n    verify(txfmFn, times(0)).close();\r\n    opImpl.handleClose();\r\n    // ensure that close is called once inside handleClose()\r\n    verify(txfmFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestControlMessageSender.java",
  "methodName" : "testSend",
  "sourceCode" : "@Test\r\npublic void testSend() {\r\n    SystemStreamMetadata metadata = mock(SystemStreamMetadata.class);\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = new HashMap<>();\r\n    partitionMetadata.put(new Partition(0), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(1), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(2), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(3), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    when(metadata.getSystemStreamPartitionMetadata()).thenReturn(partitionMetadata);\r\n    StreamMetadataCache metadataCache = mock(StreamMetadataCache.class);\r\n    when(metadataCache.getSystemStreamMetadata(anyObject(), anyBoolean())).thenReturn(metadata);\r\n    SystemStream systemStream = new SystemStream(\"test-system\", \"test-stream\");\r\n    Set<Integer> partitions = new HashSet<>();\r\n    MessageCollector collector = mock(MessageCollector.class);\r\n    doAnswer(invocation -> {\r\n        OutgoingMessageEnvelope envelope = (OutgoingMessageEnvelope) invocation.getArguments()[0];\r\n        partitions.add((Integer) envelope.getPartitionKey());\r\n        assertEquals(envelope.getSystemStream(), systemStream);\r\n        return null;\r\n    }).when(collector).send(any());\r\n    ControlMessageSender sender = new ControlMessageSender(metadataCache);\r\n    WatermarkMessage watermark = new WatermarkMessage(System.currentTimeMillis(), \"task 0\");\r\n    sender.send(watermark, systemStream, collector);\r\n    assertEquals(partitions.size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestControlMessageSender.java",
  "methodName" : "testBroadcast",
  "sourceCode" : "@Test\r\npublic void testBroadcast() {\r\n    SystemStreamMetadata metadata = mock(SystemStreamMetadata.class);\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = new HashMap<>();\r\n    partitionMetadata.put(new Partition(0), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(1), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(2), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    partitionMetadata.put(new Partition(3), mock(SystemStreamMetadata.SystemStreamPartitionMetadata.class));\r\n    when(metadata.getSystemStreamPartitionMetadata()).thenReturn(partitionMetadata);\r\n    StreamMetadataCache metadataCache = mock(StreamMetadataCache.class);\r\n    when(metadataCache.getSystemStreamMetadata(anyObject(), anyBoolean())).thenReturn(metadata);\r\n    SystemStream systemStream = new SystemStream(\"test-system\", \"test-stream\");\r\n    Set<Integer> partitions = new HashSet<>();\r\n    MessageCollector collector = mock(MessageCollector.class);\r\n    doAnswer(invocation -> {\r\n        OutgoingMessageEnvelope envelope = (OutgoingMessageEnvelope) invocation.getArguments()[0];\r\n        partitions.add((Integer) envelope.getPartitionKey());\r\n        assertEquals(envelope.getSystemStream(), systemStream);\r\n        return null;\r\n    }).when(collector).send(any());\r\n    ControlMessageSender sender = new ControlMessageSender(metadataCache);\r\n    WatermarkMessage watermark = new WatermarkMessage(System.currentTimeMillis(), \"task 0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(systemStream, new Partition(0));\r\n    sender.broadcastToOtherPartitions(watermark, ssp, collector);\r\n    assertEquals(partitions.size(), 3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestEndOfStreamStates.java",
  "methodName" : "testUpdate",
  "sourceCode" : "@Test\r\npublic void testUpdate() {\r\n    SystemStream input = new SystemStream(\"system\", \"input\");\r\n    SystemStream intermediate = new SystemStream(\"system\", \"intermediate\");\r\n    Set<SystemStreamPartition> ssps = new HashSet<>();\r\n    SystemStreamPartition inputPartition0 = new SystemStreamPartition(input, new Partition(0));\r\n    SystemStreamPartition intPartition0 = new SystemStreamPartition(intermediate, new Partition(0));\r\n    SystemStreamPartition intPartition1 = new SystemStreamPartition(intermediate, new Partition(1));\r\n    ssps.add(inputPartition0);\r\n    ssps.add(intPartition0);\r\n    ssps.add(intPartition1);\r\n    Map<SystemStream, Integer> producerCounts = new HashMap<>();\r\n    producerCounts.put(intermediate, 2);\r\n    EndOfStreamStates endOfStreamStates = new EndOfStreamStates(ssps, producerCounts);\r\n    assertFalse(endOfStreamStates.isEndOfStream(input));\r\n    assertFalse(endOfStreamStates.isEndOfStream(intermediate));\r\n    assertFalse(endOfStreamStates.allEndOfStream());\r\n    IncomingMessageEnvelope envelope = IncomingMessageEnvelope.buildEndOfStreamEnvelope(inputPartition0);\r\n    endOfStreamStates.update((EndOfStreamMessage) envelope.getMessage(), envelope.getSystemStreamPartition());\r\n    assertTrue(endOfStreamStates.isEndOfStream(input));\r\n    assertFalse(endOfStreamStates.isEndOfStream(intermediate));\r\n    assertFalse(endOfStreamStates.allEndOfStream());\r\n    EndOfStreamMessage eos = new EndOfStreamMessage(\"task 0\");\r\n    endOfStreamStates.update(eos, intPartition0);\r\n    endOfStreamStates.update(eos, intPartition1);\r\n    assertFalse(endOfStreamStates.isEndOfStream(intermediate));\r\n    assertFalse(endOfStreamStates.allEndOfStream());\r\n    eos = new EndOfStreamMessage(\"task 1\");\r\n    endOfStreamStates.update(eos, intPartition0);\r\n    endOfStreamStates.update(eos, intPartition1);\r\n    assertTrue(endOfStreamStates.isEndOfStream(intermediate));\r\n    assertTrue(endOfStreamStates.allEndOfStream());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestFlatmapOperatorImpl.java",
  "methodName" : "testStreamOperator",
  "sourceCode" : "@Test\r\n@SuppressWarnings(\"unchecked\")\r\npublic void testStreamOperator() {\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> mockOp = mock(StreamOperatorSpec.class);\r\n    FlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> txfmFn = mock(FlatMapFunction.class);\r\n    when(mockOp.getTransformFn()).thenReturn(txfmFn);\r\n    FlatmapOperatorImpl<TestMessageEnvelope, TestOutputMessageEnvelope> opImpl = new FlatmapOperatorImpl<>(mockOp);\r\n    TestMessageEnvelope inMsg = mock(TestMessageEnvelope.class);\r\n    Collection<TestOutputMessageEnvelope> mockOutputs = mock(Collection.class);\r\n    when(txfmFn.apply(inMsg)).thenReturn(mockOutputs);\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    Collection<TestOutputMessageEnvelope> results = opImpl.handleMessage(inMsg, mockCollector, mockCoordinator);\r\n    verify(txfmFn, times(1)).apply(inMsg);\r\n    assertEquals(results, mockOutputs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestFlatmapOperatorImpl.java",
  "methodName" : "testStreamOperatorClose",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorClose() {\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> mockOp = mock(StreamOperatorSpec.class);\r\n    FlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> txfmFn = mock(FlatMapFunction.class);\r\n    when(mockOp.getTransformFn()).thenReturn(txfmFn);\r\n    FlatmapOperatorImpl<TestMessageEnvelope, TestOutputMessageEnvelope> opImpl = new FlatmapOperatorImpl<>(mockOp);\r\n    // ensure that close is not called yet\r\n    verify(txfmFn, times(0)).close();\r\n    opImpl.handleClose();\r\n    // ensure that close is called once inside handleClose()\r\n    verify(txfmFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestInputOperatorImpl.java",
  "methodName" : "testWithKeyedInput",
  "sourceCode" : "@Test\r\npublic void testWithKeyedInput() {\r\n    InputOperatorImpl inputOperator = new InputOperatorImpl(new InputOperatorSpec(\"stream-id\", null, null, null, true, \"input-op-id\"));\r\n    IncomingMessageEnvelope ime = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"123\", \"key\", \"msg\");\r\n    Collection<Object> results = inputOperator.handleMessage(ime, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    Object result = results.iterator().next();\r\n    assertEquals(\"key\", ((KV) result).getKey());\r\n    assertEquals(\"msg\", ((KV) result).getValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestInputOperatorImpl.java",
  "methodName" : "testWithUnkeyedInput",
  "sourceCode" : "@Test\r\npublic void testWithUnkeyedInput() {\r\n    InputOperatorImpl inputOperator = new InputOperatorImpl(new InputOperatorSpec(\"stream-id\", null, null, null, false, \"input-op-id\"));\r\n    IncomingMessageEnvelope ime = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"123\", \"key\", \"msg\");\r\n    Collection<Object> results = inputOperator.handleMessage(ime, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    Object result = results.iterator().next();\r\n    assertEquals(\"msg\", result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestInputOperatorImpl.java",
  "methodName" : "testWithInputTransformer",
  "sourceCode" : "@Test\r\npublic void testWithInputTransformer() {\r\n    InputOperatorSpec inputOpSpec = new InputOperatorSpec(\"stream-id\", null, null, IncomingMessageEnvelope::getOffset, true, \"input-op-id\");\r\n    InputOperatorImpl inputOperator = new InputOperatorImpl(inputOpSpec);\r\n    IncomingMessageEnvelope ime = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"123\", \"key\", \"msg\");\r\n    Collection<Object> results = inputOperator.handleMessage(ime, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    Object result = results.iterator().next();\r\n    assertEquals(\"123\", result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestInputOperatorImpl.java",
  "methodName" : "testWithFilteringInputTransformer",
  "sourceCode" : "@Test\r\npublic void testWithFilteringInputTransformer() {\r\n    InputOperatorSpec inputOpSpec = new InputOperatorSpec(\"stream-id\", null, null, (ime) -> null, true, \"input-op-id\");\r\n    InputOperatorImpl inputOperator = new InputOperatorImpl(inputOpSpec);\r\n    IncomingMessageEnvelope ime = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"123\", \"key\", \"msg\");\r\n    Collection<Object> results = inputOperator.handleMessage(ime, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    assertTrue(\"Transformer doesn't return any record. Expected an empty collection\", results.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testComposeFutureWithExecutorWithFrameworkExecutorEnabled",
  "sourceCode" : "@Test\r\npublic void testComposeFutureWithExecutorWithFrameworkExecutorEnabled() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    ExecutorService mockExecutor = mock(ExecutorService.class);\r\n    CompletionStage<Object> mockFuture = mock(CompletionStage.class);\r\n    Function<Object, CompletionStage<Object>> mockFunction = mock(Function.class);\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.operator.framework.executor.enabled\", \"true\"));\r\n    when(this.taskContext.getOperatorExecutor()).thenReturn(mockExecutor);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    opImpl.init(this.internalTaskContext);\r\n    opImpl.composeFutureWithExecutor(mockFuture, mockFunction);\r\n    verify(mockFuture).thenComposeAsync(eq(mockFunction), eq(mockExecutor));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testComposeFutureWithExecutorWithFrameworkExecutorDisabled",
  "sourceCode" : "@Test\r\npublic void testComposeFutureWithExecutorWithFrameworkExecutorDisabled() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    ExecutorService mockExecutor = mock(ExecutorService.class);\r\n    CompletionStage<Object> mockFuture = mock(CompletionStage.class);\r\n    Function<Object, CompletionStage<Object>> mockFunction = mock(Function.class);\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.operator.framework.executor.enabled\", \"false\"));\r\n    when(this.taskContext.getOperatorExecutor()).thenReturn(mockExecutor);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    opImpl.init(this.internalTaskContext);\r\n    opImpl.composeFutureWithExecutor(mockFuture, mockFunction);\r\n    verify(mockFuture).thenCompose(eq(mockFunction));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testAcceptFutureWithExecutorWithFrameworkExecutorDisabled",
  "sourceCode" : "@Test\r\npublic void testAcceptFutureWithExecutorWithFrameworkExecutorDisabled() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    ExecutorService mockExecutor = mock(ExecutorService.class);\r\n    CompletionStage<Object> mockFuture = mock(CompletionStage.class);\r\n    Consumer<Object> mockConsumer = mock(Consumer.class);\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.operator.framework.executor.enabled\", \"false\"));\r\n    when(this.taskContext.getOperatorExecutor()).thenReturn(mockExecutor);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    opImpl.init(this.internalTaskContext);\r\n    opImpl.acceptFutureWithExecutor(mockFuture, mockConsumer);\r\n    verify(mockFuture).thenAccept(eq(mockConsumer));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testAcceptFutureWithExecutorWithFrameworkExecutorEnabled",
  "sourceCode" : "@Test\r\npublic void testAcceptFutureWithExecutorWithFrameworkExecutorEnabled() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    ExecutorService mockExecutor = mock(ExecutorService.class);\r\n    CompletionStage<Object> mockFuture = mock(CompletionStage.class);\r\n    Consumer<Object> mockConsumer = mock(Consumer.class);\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.operator.framework.executor.enabled\", \"true\"));\r\n    when(this.taskContext.getOperatorExecutor()).thenReturn(mockExecutor);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    opImpl.init(this.internalTaskContext);\r\n    opImpl.acceptFutureWithExecutor(mockFuture, mockConsumer);\r\n    verify(mockFuture).thenAcceptAsync(eq(mockConsumer), eq(mockExecutor));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testMultipleInitShouldThrow",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testMultipleInitShouldThrow() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    opImpl.init(this.internalTaskContext);\r\n    opImpl.init(this.internalTaskContext);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testRegisterNextOperatorBeforeInitShouldThrow",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testRegisterNextOperatorBeforeInitShouldThrow() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    opImpl.registerNextOperator(mock(OperatorImpl.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testRegisterOperatorMaintainsInsertionOrder",
  "sourceCode" : "@Test\r\npublic void testRegisterOperatorMaintainsInsertionOrder() {\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mock(Object.class));\r\n    opImpl.init(this.internalTaskContext);\r\n    OperatorImpl<Object, Object> firstOp = mock(OperatorImpl.class);\r\n    OperatorImpl<Object, Object> secondOp = mock(OperatorImpl.class);\r\n    OperatorImpl<Object, Object> thirdOp = mock(OperatorImpl.class);\r\n    opImpl.registerNextOperator(firstOp);\r\n    opImpl.registerNextOperator(secondOp);\r\n    opImpl.registerNextOperator(thirdOp);\r\n    Iterator<OperatorImpl<Object, ?>> iterator = opImpl.registeredOperators.iterator();\r\n    assertTrue(\"Expecting non-empty iterator\", iterator.hasNext());\r\n    assertEquals(\"Expecting first operator to be returned\", firstOp, iterator.next());\r\n    assertTrue(\"Expecting non-empty iterator\", iterator.hasNext());\r\n    assertEquals(\"Expecting second operator to be returned\", secondOp, iterator.next());\r\n    assertTrue(\"Expecting non-empty iterator\", iterator.hasNext());\r\n    assertEquals(\"Expecting third operator to be returned\", thirdOp, iterator.next());\r\n    assertFalse(\"Expecting no more registered operators\", iterator.hasNext());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testOnMessagePropagatesResults",
  "sourceCode" : "@Test\r\npublic void testOnMessagePropagatesResults() {\r\n    Object mockTestOpImplOutput = mock(Object.class);\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mockTestOpImplOutput);\r\n    opImpl.init(this.internalTaskContext);\r\n    // register a couple of operators\r\n    OperatorImpl mockNextOpImpl1 = mock(OperatorImpl.class);\r\n    when(mockNextOpImpl1.getOperatorSpec()).thenReturn(new TestOpSpec());\r\n    when(mockNextOpImpl1.handleMessageAsync(anyObject(), anyObject(), anyObject())).thenReturn(CompletableFuture.completedFuture(Collections.emptyList()));\r\n    mockNextOpImpl1.init(this.internalTaskContext);\r\n    opImpl.registerNextOperator(mockNextOpImpl1);\r\n    OperatorImpl mockNextOpImpl2 = mock(OperatorImpl.class);\r\n    when(mockNextOpImpl2.getOperatorSpec()).thenReturn(new TestOpSpec());\r\n    when(mockNextOpImpl2.handleMessageAsync(anyObject(), anyObject(), anyObject())).thenReturn(CompletableFuture.completedFuture(Collections.emptyList()));\r\n    mockNextOpImpl2.init(this.internalTaskContext);\r\n    opImpl.registerNextOperator(mockNextOpImpl2);\r\n    // send a message to this operator\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    opImpl.onMessage(mock(Object.class), mockCollector, mockCoordinator);\r\n    // verify that it propagates its handleMessage results to next operators\r\n    verify(mockNextOpImpl1, times(1)).handleMessageAsync(mockTestOpImplOutput, mockCollector, mockCoordinator);\r\n    verify(mockNextOpImpl2, times(1)).handleMessageAsync(mockTestOpImplOutput, mockCollector, mockCoordinator);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testOnMessageUpdatesMetrics",
  "sourceCode" : "@Test\r\npublic void testOnMessageUpdatesMetrics() {\r\n    ReadableMetricsRegistry mockMetricsRegistry = mock(ReadableMetricsRegistry.class);\r\n    when(this.context.getContainerContext().getContainerMetricsRegistry()).thenReturn(mockMetricsRegistry);\r\n    Counter mockCounter = mock(Counter.class);\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockMetricsRegistry.newCounter(anyString(), anyString())).thenReturn(mockCounter);\r\n    when(mockMetricsRegistry.newTimer(anyString(), anyString())).thenReturn(mockTimer);\r\n    Object mockTestOpImplOutput = mock(Object.class);\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mockTestOpImplOutput);\r\n    opImpl.init(this.internalTaskContext);\r\n    // send a message to this operator\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    opImpl.onMessage(mock(Object.class), mockCollector, mockCoordinator);\r\n    // verify that it updates message count and timer metrics\r\n    verify(mockCounter, times(1)).inc();\r\n    verify(mockTimer, times(1)).update(anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testOnTimerPropagatesResultsAndTimer",
  "sourceCode" : "@Test\r\npublic void testOnTimerPropagatesResultsAndTimer() {\r\n    Object mockTestOpImplOutput = mock(Object.class);\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mockTestOpImplOutput);\r\n    opImpl.init(this.internalTaskContext);\r\n    // register a couple of operators\r\n    OperatorImpl mockNextOpImpl1 = mock(OperatorImpl.class);\r\n    when(mockNextOpImpl1.getOperatorSpec()).thenReturn(new TestOpSpec());\r\n    when(mockNextOpImpl1.handleMessageAsync(anyObject(), anyObject(), anyObject())).thenReturn(CompletableFuture.completedFuture(Collections.emptyList()));\r\n    mockNextOpImpl1.init(this.internalTaskContext);\r\n    opImpl.registerNextOperator(mockNextOpImpl1);\r\n    OperatorImpl mockNextOpImpl2 = mock(OperatorImpl.class);\r\n    when(mockNextOpImpl2.getOperatorSpec()).thenReturn(new TestOpSpec());\r\n    when(mockNextOpImpl2.handleMessageAsync(anyObject(), anyObject(), anyObject())).thenReturn(CompletableFuture.completedFuture(Collections.emptyList()));\r\n    mockNextOpImpl2.init(this.internalTaskContext);\r\n    opImpl.registerNextOperator(mockNextOpImpl2);\r\n    // send a timer tick to this operator\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    CompletionStage<?> future = opImpl.onTimer(mockCollector, mockCoordinator);\r\n    future.toCompletableFuture().join();\r\n    // verify that it propagates its handleTimer results to next operators\r\n    verify(mockNextOpImpl1, times(1)).handleMessageAsync(mockTestOpImplOutput, mockCollector, mockCoordinator);\r\n    verify(mockNextOpImpl2, times(1)).handleMessageAsync(mockTestOpImplOutput, mockCollector, mockCoordinator);\r\n    // verify that it propagates the timer tick to next operators\r\n    verify(mockNextOpImpl1, times(1)).handleTimer(mockCollector, mockCoordinator);\r\n    verify(mockNextOpImpl2, times(1)).handleTimer(mockCollector, mockCoordinator);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImpl.java",
  "methodName" : "testOnTimerUpdatesMetrics",
  "sourceCode" : "@Test\r\npublic void testOnTimerUpdatesMetrics() {\r\n    ReadableMetricsRegistry mockMetricsRegistry = mock(ReadableMetricsRegistry.class);\r\n    when(this.context.getContainerContext().getContainerMetricsRegistry()).thenReturn(mockMetricsRegistry);\r\n    Counter mockMessageCounter = mock(Counter.class);\r\n    Timer mockTimer = mock(Timer.class);\r\n    when(mockMetricsRegistry.newCounter(anyString(), anyString())).thenReturn(mockMessageCounter);\r\n    when(mockMetricsRegistry.newTimer(anyString(), anyString())).thenReturn(mockTimer);\r\n    Object mockTestOpImplOutput = mock(Object.class);\r\n    OperatorImpl<Object, Object> opImpl = new TestOpImpl(mockTestOpImplOutput);\r\n    opImpl.init(this.internalTaskContext);\r\n    // send a message to this operator\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    opImpl.onTimer(mockCollector, mockCoordinator);\r\n    // verify that it updates metrics\r\n    verify(mockMessageCounter, times(0)).inc();\r\n    verify(mockTimer, times(1)).update(anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testEmptyChain",
  "sourceCode" : "@Test\r\npublic void testEmptyChain() {\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n    }, mock(Config.class));\r\n    OperatorImplGraph opGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), context, mock(Clock.class));\r\n    assertEquals(0, opGraph.getAllInputOperators().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testLinearChain",
  "sourceCode" : "@Test\r\npublic void testLinearChain() {\r\n    String inputStreamId = \"input\";\r\n    String inputSystem = \"input-system\";\r\n    String inputPhysicalName = \"input-stream\";\r\n    String outputStreamId = \"output\";\r\n    String outputSystem = \"output-system\";\r\n    String outputPhysicalName = \"output-stream\";\r\n    String intermediateSystem = \"intermediate-system\";\r\n    HashMap<String, String> configs = new HashMap<>();\r\n    configs.put(JobConfig.JOB_NAME, \"jobName\");\r\n    configs.put(JobConfig.JOB_ID, \"jobId\");\r\n    configs.put(JobConfig.JOB_DEFAULT_SYSTEM, intermediateSystem);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId, inputSystem, inputPhysicalName);\r\n    StreamTestUtils.addStreamConfigs(configs, outputStreamId, outputSystem, outputPhysicalName);\r\n    Config config = new MapConfig(configs);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor = sd.getInputDescriptor(inputStreamId, mock(Serde.class));\r\n        GenericOutputDescriptor outputDescriptor = sd.getOutputDescriptor(outputStreamId, mock(Serde.class));\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(inputDescriptor);\r\n        OutputStream<Object> outputStream = appDesc.getOutputStream(outputDescriptor);\r\n        inputStream.filter(mock(FilterFunction.class)).map(mock(MapFunction.class)).sendTo(outputStream);\r\n    }, config);\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, mock(Clock.class));\r\n    InputOperatorImpl inputOpImpl = opImplGraph.getInputOperator(new SystemStream(inputSystem, inputPhysicalName));\r\n    assertEquals(1, inputOpImpl.registeredOperators.size());\r\n    OperatorImpl filterOpImpl = (FlatmapOperatorImpl) inputOpImpl.registeredOperators.iterator().next();\r\n    assertEquals(1, filterOpImpl.registeredOperators.size());\r\n    assertEquals(OpCode.FILTER, filterOpImpl.getOperatorSpec().getOpCode());\r\n    OperatorImpl mapOpImpl = (FlatmapOperatorImpl) filterOpImpl.registeredOperators.iterator().next();\r\n    assertEquals(1, mapOpImpl.registeredOperators.size());\r\n    assertEquals(OpCode.MAP, mapOpImpl.getOperatorSpec().getOpCode());\r\n    OperatorImpl sendToOpImpl = (OutputOperatorImpl) mapOpImpl.registeredOperators.iterator().next();\r\n    assertEquals(0, sendToOpImpl.registeredOperators.size());\r\n    assertEquals(OpCode.SEND_TO, sendToOpImpl.getOperatorSpec().getOpCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testPartitionByChain",
  "sourceCode" : "@Test\r\npublic void testPartitionByChain() {\r\n    String inputStreamId = \"input\";\r\n    String inputSystem = \"input-system\";\r\n    String inputPhysicalName = \"input-stream\";\r\n    String outputStreamId = \"output\";\r\n    String outputSystem = \"output-system\";\r\n    String outputPhysicalName = \"output-stream\";\r\n    String intermediateStreamId = \"jobName-jobId-partition_by-p1\";\r\n    String intermediateSystem = \"intermediate-system\";\r\n    HashMap<String, String> configs = new HashMap<>();\r\n    configs.put(JobConfig.JOB_NAME, \"jobName\");\r\n    configs.put(JobConfig.JOB_ID, \"jobId\");\r\n    configs.put(JobConfig.JOB_DEFAULT_SYSTEM, intermediateSystem);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId, inputSystem, inputPhysicalName);\r\n    StreamTestUtils.addStreamConfigs(configs, outputStreamId, outputSystem, outputPhysicalName);\r\n    Config config = new MapConfig(configs);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor isd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericSystemDescriptor osd = new GenericSystemDescriptor(outputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor = isd.getInputDescriptor(inputStreamId, mock(Serde.class));\r\n        GenericOutputDescriptor outputDescriptor = osd.getOutputDescriptor(outputStreamId, KVSerde.of(mock(IntegerSerde.class), mock(StringSerde.class)));\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(inputDescriptor);\r\n        OutputStream<KV<Integer, String>> outputStream = appDesc.getOutputStream(outputDescriptor);\r\n        inputStream.partitionBy(Object::hashCode, Object::toString, KVSerde.of(mock(IntegerSerde.class), mock(StringSerde.class)), \"p1\").sendTo(outputStream);\r\n    }, config);\r\n    JobModel jobModel = mock(JobModel.class);\r\n    ContainerModel containerModel = mock(ContainerModel.class);\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(jobModel.getContainers()).thenReturn(Collections.singletonMap(\"0\", containerModel));\r\n    when(containerModel.getTasks()).thenReturn(Collections.singletonMap(new TaskName(\"task 0\"), taskModel));\r\n    when(taskModel.getSystemStreamPartitions()).thenReturn(Collections.emptySet());\r\n    when(((TaskContextImpl) this.context.getTaskContext()).getJobModel()).thenReturn(jobModel);\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, mock(Clock.class));\r\n    InputOperatorImpl inputOpImpl = opImplGraph.getInputOperator(new SystemStream(inputSystem, inputPhysicalName));\r\n    assertEquals(1, inputOpImpl.registeredOperators.size());\r\n    OperatorImpl partitionByOpImpl = (PartitionByOperatorImpl) inputOpImpl.registeredOperators.iterator().next();\r\n    // is terminal but paired with an input operator\r\n    assertEquals(0, partitionByOpImpl.registeredOperators.size());\r\n    assertEquals(OpCode.PARTITION_BY, partitionByOpImpl.getOperatorSpec().getOpCode());\r\n    InputOperatorImpl repartitionedInputOpImpl = opImplGraph.getInputOperator(new SystemStream(intermediateSystem, intermediateStreamId));\r\n    assertEquals(1, repartitionedInputOpImpl.registeredOperators.size());\r\n    OperatorImpl sendToOpImpl = (OutputOperatorImpl) repartitionedInputOpImpl.registeredOperators.iterator().next();\r\n    assertEquals(0, sendToOpImpl.registeredOperators.size());\r\n    assertEquals(OpCode.SEND_TO, sendToOpImpl.getOperatorSpec().getOpCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testBroadcastChain",
  "sourceCode" : "@Test\r\npublic void testBroadcastChain() {\r\n    String inputStreamId = \"input\";\r\n    String inputSystem = \"input-system\";\r\n    String inputPhysicalName = \"input-stream\";\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(JobConfig.JOB_NAME, \"test-job\");\r\n    configMap.put(JobConfig.JOB_ID, \"1\");\r\n    StreamTestUtils.addStreamConfigs(configMap, inputStreamId, inputSystem, inputPhysicalName);\r\n    Config config = new MapConfig(configMap);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor = sd.getInputDescriptor(inputStreamId, mock(Serde.class));\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(inputDescriptor);\r\n        inputStream.filter(mock(FilterFunction.class));\r\n        inputStream.map(mock(MapFunction.class));\r\n    }, config);\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, mock(Clock.class));\r\n    InputOperatorImpl inputOpImpl = opImplGraph.getInputOperator(new SystemStream(inputSystem, inputPhysicalName));\r\n    assertEquals(2, inputOpImpl.registeredOperators.size());\r\n    assertTrue(inputOpImpl.registeredOperators.stream().anyMatch(opImpl -> ((OperatorImpl) opImpl).getOperatorSpec().getOpCode() == OpCode.FILTER));\r\n    assertTrue(inputOpImpl.registeredOperators.stream().anyMatch(opImpl -> ((OperatorImpl) opImpl).getOperatorSpec().getOpCode() == OpCode.MAP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testMergeChain",
  "sourceCode" : "@Test\r\npublic void testMergeChain() {\r\n    String inputStreamId = \"input\";\r\n    String inputSystem = \"input-system\";\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor = sd.getInputDescriptor(inputStreamId, mock(Serde.class));\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(inputDescriptor);\r\n        MessageStream<Object> stream1 = inputStream.filter(mock(FilterFunction.class));\r\n        MessageStream<Object> stream2 = inputStream.map(mock(MapFunction.class));\r\n        stream1.merge(Collections.singleton(stream2)).map(new TestMapFunction<Object, Object>(\"test-map-1\", (Function & Serializable) m -> m));\r\n    }, getConfig());\r\n    TaskName mockTaskName = mock(TaskName.class);\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(taskModel.getTaskName()).thenReturn(mockTaskName);\r\n    when(this.context.getTaskContext().getTaskModel()).thenReturn(taskModel);\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, mock(Clock.class));\r\n    Set<OperatorImpl> opSet = opImplGraph.getAllInputOperators().stream().collect(HashSet::new, (s, op) -> addOperatorRecursively(s, op), HashSet::addAll);\r\n    Object[] mergeOps = opSet.stream().filter(op -> op.getOperatorSpec().getOpCode() == OpCode.MERGE).toArray();\r\n    assertEquals(1, mergeOps.length);\r\n    assertEquals(1, ((OperatorImpl) mergeOps[0]).registeredOperators.size());\r\n    OperatorImpl mapOp = (OperatorImpl) ((OperatorImpl) mergeOps[0]).registeredOperators.iterator().next();\r\n    assertEquals(mapOp.getOperatorSpec().getOpCode(), OpCode.MAP);\r\n    // verify that the DAG after merge is only traversed & initialized once\r\n    assertEquals(TestMapFunction.getInstanceByTaskName(mockTaskName, \"test-map-1\").numInitCalled, 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testJoinChain",
  "sourceCode" : "@Test\r\npublic void testJoinChain() {\r\n    String inputStreamId1 = \"input1\";\r\n    String inputStreamId2 = \"input2\";\r\n    String inputSystem = \"input-system\";\r\n    String inputPhysicalName1 = \"input-stream1\";\r\n    String inputPhysicalName2 = \"input-stream2\";\r\n    HashMap<String, String> configs = new HashMap<>();\r\n    configs.put(JobConfig.JOB_NAME, \"jobName\");\r\n    configs.put(JobConfig.JOB_ID, \"jobId\");\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId1, inputSystem, inputPhysicalName1);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId2, inputSystem, inputPhysicalName2);\r\n    Config config = new MapConfig(configs);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    Integer joinKey = new Integer(1);\r\n    Function<Object, Integer> keyFn = (Function & Serializable) m -> joinKey;\r\n    JoinFunction testJoinFunction = new TestJoinFunction(\"jobName-jobId-join-j1\", (BiFunction & Serializable) (m1, m2) -> KV.of(m1, m2), keyFn, keyFn);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor1 = sd.getInputDescriptor(inputStreamId1, mock(Serde.class));\r\n        GenericInputDescriptor inputDescriptor2 = sd.getInputDescriptor(inputStreamId2, mock(Serde.class));\r\n        MessageStream<Object> inputStream1 = appDesc.getInputStream(inputDescriptor1);\r\n        MessageStream<Object> inputStream2 = appDesc.getInputStream(inputDescriptor2);\r\n        inputStream1.join(inputStream2, testJoinFunction, mock(Serde.class), mock(Serde.class), mock(Serde.class), Duration.ofHours(1), \"j1\");\r\n    }, config);\r\n    TaskName mockTaskName = mock(TaskName.class);\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(taskModel.getTaskName()).thenReturn(mockTaskName);\r\n    when(this.context.getTaskContext().getTaskModel()).thenReturn(taskModel);\r\n    KeyValueStore mockLeftStore = mock(KeyValueStore.class);\r\n    when(this.context.getTaskContext().getStore(eq(\"jobName-jobId-join-j1-L\"))).thenReturn(mockLeftStore);\r\n    KeyValueStore mockRightStore = mock(KeyValueStore.class);\r\n    when(this.context.getTaskContext().getStore(eq(\"jobName-jobId-join-j1-R\"))).thenReturn(mockRightStore);\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, mock(Clock.class));\r\n    // verify that join function is initialized once.\r\n    assertEquals(TestJoinFunction.getInstanceByTaskName(mockTaskName, \"jobName-jobId-join-j1\").numInitCalled, 1);\r\n    InputOperatorImpl inputOpImpl1 = opImplGraph.getInputOperator(new SystemStream(inputSystem, inputPhysicalName1));\r\n    InputOperatorImpl inputOpImpl2 = opImplGraph.getInputOperator(new SystemStream(inputSystem, inputPhysicalName2));\r\n    PartialJoinOperatorImpl leftPartialJoinOpImpl = (PartialJoinOperatorImpl) inputOpImpl1.registeredOperators.iterator().next();\r\n    PartialJoinOperatorImpl rightPartialJoinOpImpl = (PartialJoinOperatorImpl) inputOpImpl2.registeredOperators.iterator().next();\r\n    assertEquals(leftPartialJoinOpImpl.getOperatorSpec(), rightPartialJoinOpImpl.getOperatorSpec());\r\n    assertNotSame(leftPartialJoinOpImpl, rightPartialJoinOpImpl);\r\n    // verify that left partial join operator calls getFirstKey\r\n    Object mockLeftMessage = mock(Object.class);\r\n    long currentTimeMillis = System.currentTimeMillis();\r\n    when(mockLeftStore.get(eq(joinKey))).thenReturn(new TimestampedValue<>(mockLeftMessage, currentTimeMillis));\r\n    IncomingMessageEnvelope leftMessage = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"\", \"\", mockLeftMessage);\r\n    inputOpImpl1.onMessage(leftMessage, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    // verify that right partial join operator calls getSecondKey\r\n    Object mockRightMessage = mock(Object.class);\r\n    when(mockRightStore.get(eq(joinKey))).thenReturn(new TimestampedValue<>(mockRightMessage, currentTimeMillis));\r\n    IncomingMessageEnvelope rightMessage = new IncomingMessageEnvelope(mock(SystemStreamPartition.class), \"\", \"\", mockRightMessage);\r\n    inputOpImpl2.onMessage(rightMessage, mock(MessageCollector.class), mock(TaskCoordinator.class));\r\n    // verify that the join function apply is called with the correct messages on match\r\n    assertEquals(((TestJoinFunction) TestJoinFunction.getInstanceByTaskName(mockTaskName, \"jobName-jobId-join-j1\")).joinResults.size(), 1);\r\n    KV joinResult = (KV) ((TestJoinFunction) TestJoinFunction.getInstanceByTaskName(mockTaskName, \"jobName-jobId-join-j1\")).joinResults.iterator().next();\r\n    assertEquals(joinResult.getKey(), mockLeftMessage);\r\n    assertEquals(joinResult.getValue(), mockRightMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testOperatorGraphInitAndClose",
  "sourceCode" : "@Test\r\npublic void testOperatorGraphInitAndClose() {\r\n    String inputStreamId1 = \"input1\";\r\n    String inputStreamId2 = \"input2\";\r\n    String inputSystem = \"input-system\";\r\n    TaskName mockTaskName = mock(TaskName.class);\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(taskModel.getTaskName()).thenReturn(mockTaskName);\r\n    when(this.context.getTaskContext().getTaskModel()).thenReturn(taskModel);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor1 = sd.getInputDescriptor(inputStreamId1, mock(Serde.class));\r\n        GenericInputDescriptor inputDescriptor2 = sd.getInputDescriptor(inputStreamId2, mock(Serde.class));\r\n        MessageStream<Object> inputStream1 = appDesc.getInputStream(inputDescriptor1);\r\n        MessageStream<Object> inputStream2 = appDesc.getInputStream(inputDescriptor2);\r\n        Function mapFn = (Function & Serializable) m -> m;\r\n        inputStream1.map(new TestMapFunction<Object, Object>(\"1\", mapFn)).map(new TestMapFunction<Object, Object>(\"2\", mapFn));\r\n        inputStream2.map(new TestMapFunction<Object, Object>(\"3\", mapFn)).map(new TestMapFunction<Object, Object>(\"4\", mapFn));\r\n    }, getConfig());\r\n    OperatorImplGraph opImplGraph = new OperatorImplGraph(graphSpec.getOperatorSpecGraph(), this.context, SystemClock.instance());\r\n    List<String> initializedOperators = BaseTestFunction.getInitListByTaskName(mockTaskName);\r\n    // Assert that initialization occurs in topological order.\r\n    assertEquals(initializedOperators.get(0), \"1\");\r\n    assertEquals(initializedOperators.get(1), \"2\");\r\n    assertEquals(initializedOperators.get(2), \"3\");\r\n    assertEquals(initializedOperators.get(3), \"4\");\r\n    // Assert that finalization occurs in reverse topological order.\r\n    opImplGraph.close();\r\n    List<String> closedOperators = BaseTestFunction.getCloseListByTaskName(mockTaskName);\r\n    assertEquals(closedOperators.get(0), \"4\");\r\n    assertEquals(closedOperators.get(1), \"3\");\r\n    assertEquals(closedOperators.get(2), \"2\");\r\n    assertEquals(closedOperators.get(3), \"1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testGetStreamToConsumerTasks",
  "sourceCode" : "@Test\r\npublic void testGetStreamToConsumerTasks() {\r\n    String system = \"test-system\";\r\n    String streamId0 = \"test-stream-0\";\r\n    String streamId1 = \"test-stream-1\";\r\n    HashMap<String, String> configs = new HashMap<>();\r\n    configs.put(JobConfig.JOB_NAME, \"test-app\");\r\n    configs.put(JobConfig.JOB_DEFAULT_SYSTEM, \"test-system\");\r\n    StreamTestUtils.addStreamConfigs(configs, streamId0, system, streamId0);\r\n    StreamTestUtils.addStreamConfigs(configs, streamId1, system, streamId1);\r\n    Config config = new MapConfig(configs);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(system, streamId0, new Partition(0));\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(system, streamId0, new Partition(1));\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(system, streamId1, new Partition(0));\r\n    TaskName task0 = new TaskName(\"Task 0\");\r\n    TaskName task1 = new TaskName(\"Task 1\");\r\n    Set<SystemStreamPartition> ssps = new HashSet<>();\r\n    ssps.add(ssp0);\r\n    ssps.add(ssp2);\r\n    TaskModel tm0 = new TaskModel(task0, ssps, new Partition(0));\r\n    ContainerModel cm0 = new ContainerModel(\"c0\", Collections.singletonMap(task0, tm0));\r\n    TaskModel tm1 = new TaskModel(task1, Collections.singleton(ssp1), new Partition(1));\r\n    ContainerModel cm1 = new ContainerModel(\"c1\", Collections.singletonMap(task1, tm1));\r\n    Map<String, ContainerModel> cms = new HashMap<>();\r\n    cms.put(cm0.getId(), cm0);\r\n    cms.put(cm1.getId(), cm1);\r\n    JobModel jobModel = new JobModel(config, cms);\r\n    Multimap<SystemStream, String> streamToTasks = OperatorImplGraph.getStreamToConsumerTasks(jobModel);\r\n    assertEquals(streamToTasks.get(ssp0.getSystemStream()).size(), 2);\r\n    assertEquals(streamToTasks.get(ssp2.getSystemStream()).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testGetOutputToInputStreams",
  "sourceCode" : "@Test\r\npublic void testGetOutputToInputStreams() {\r\n    String inputStreamId1 = \"input1\";\r\n    String inputStreamId2 = \"input2\";\r\n    String inputStreamId3 = \"input3\";\r\n    String inputSystem = \"input-system\";\r\n    String outputStreamId1 = \"output1\";\r\n    String outputStreamId2 = \"output2\";\r\n    String outputSystem = \"output-system\";\r\n    String intStreamId1 = \"test-app-1-partition_by-p1\";\r\n    String intStreamId2 = \"test-app-1-partition_by-p2\";\r\n    String intSystem = \"test-system\";\r\n    HashMap<String, String> configs = new HashMap<>();\r\n    configs.put(JobConfig.JOB_NAME, \"test-app\");\r\n    configs.put(JobConfig.JOB_DEFAULT_SYSTEM, intSystem);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId1, inputSystem, inputStreamId1);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId2, inputSystem, inputStreamId2);\r\n    StreamTestUtils.addStreamConfigs(configs, inputStreamId3, inputSystem, inputStreamId3);\r\n    StreamTestUtils.addStreamConfigs(configs, outputStreamId1, outputSystem, outputStreamId1);\r\n    StreamTestUtils.addStreamConfigs(configs, outputStreamId2, outputSystem, outputStreamId2);\r\n    Config config = new MapConfig(configs);\r\n    when(this.context.getJobContext().getConfig()).thenReturn(config);\r\n    StreamApplicationDescriptorImpl graphSpec = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        GenericSystemDescriptor isd = new GenericSystemDescriptor(inputSystem, \"mockFactoryClass\");\r\n        GenericInputDescriptor inputDescriptor1 = isd.getInputDescriptor(inputStreamId1, mock(Serde.class));\r\n        GenericInputDescriptor inputDescriptor2 = isd.getInputDescriptor(inputStreamId2, mock(Serde.class));\r\n        GenericInputDescriptor inputDescriptor3 = isd.getInputDescriptor(inputStreamId3, mock(Serde.class));\r\n        GenericSystemDescriptor osd = new GenericSystemDescriptor(outputSystem, \"mockFactoryClass\");\r\n        GenericOutputDescriptor outputDescriptor1 = osd.getOutputDescriptor(outputStreamId1, mock(Serde.class));\r\n        GenericOutputDescriptor outputDescriptor2 = osd.getOutputDescriptor(outputStreamId2, mock(Serde.class));\r\n        MessageStream messageStream1 = appDesc.getInputStream(inputDescriptor1).map(m -> m);\r\n        MessageStream messageStream2 = appDesc.getInputStream(inputDescriptor2).filter(m -> true);\r\n        MessageStream messageStream3 = appDesc.getInputStream(inputDescriptor3).filter(m -> true).partitionBy(m -> \"m\", m -> m, mock(KVSerde.class), \"p1\").map(m -> m);\r\n        OutputStream<Object> outputStream1 = appDesc.getOutputStream(outputDescriptor1);\r\n        OutputStream<Object> outputStream2 = appDesc.getOutputStream(outputDescriptor2);\r\n        messageStream1.join(messageStream2, mock(JoinFunction.class), mock(Serde.class), mock(Serde.class), mock(Serde.class), Duration.ofHours(2), \"j1\").partitionBy(m -> \"m\", m -> m, mock(KVSerde.class), \"p2\").sendTo(outputStream1);\r\n        messageStream3.join(messageStream2, mock(JoinFunction.class), mock(Serde.class), mock(Serde.class), mock(Serde.class), Duration.ofHours(1), \"j2\").sendTo(outputStream2);\r\n    }, config);\r\n    Multimap<SystemStream, SystemStream> outputToInput = OperatorImplGraph.getIntermediateToInputStreamsMap(graphSpec.getOperatorSpecGraph(), new StreamConfig(config));\r\n    Collection<SystemStream> inputs = outputToInput.get(new SystemStream(intSystem, intStreamId2));\r\n    assertEquals(inputs.size(), 2);\r\n    assertTrue(inputs.contains(new SystemStream(inputSystem, inputStreamId1)));\r\n    assertTrue(inputs.contains(new SystemStream(inputSystem, inputStreamId2)));\r\n    inputs = outputToInput.get(new SystemStream(intSystem, intStreamId1));\r\n    assertEquals(inputs.size(), 1);\r\n    assertEquals(inputs.iterator().next(), new SystemStream(inputSystem, inputStreamId3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestOperatorImplGraph.java",
  "methodName" : "testGetProducerTaskCountForIntermediateStreams",
  "sourceCode" : "@Test\r\npublic void testGetProducerTaskCountForIntermediateStreams() {\r\n    String inputStreamId1 = \"input1\";\r\n    String inputStreamId2 = \"input2\";\r\n    String inputStreamId3 = \"input3\";\r\n    String inputSystem1 = \"system1\";\r\n    String inputSystem2 = \"system2\";\r\n    SystemStream input1 = new SystemStream(\"system1\", \"intput1\");\r\n    SystemStream input2 = new SystemStream(\"system2\", \"intput2\");\r\n    SystemStream input3 = new SystemStream(\"system2\", \"intput3\");\r\n    SystemStream int1 = new SystemStream(\"system1\", \"int1\");\r\n    SystemStream int2 = new SystemStream(\"system1\", \"int2\");\r\n    /**\r\n     * the task assignment looks like the following:\r\n     *\r\n     * input1 -----> task0, task1 -----> int1\r\n     *                                    ^\r\n     * input2 ------> task1, task2--------|\r\n     *                                    v\r\n     * input3 ------> task1 -----------> int2\r\n     */\r\n    String task0 = \"Task 0\";\r\n    String task1 = \"Task 1\";\r\n    String task2 = \"Task 2\";\r\n    Multimap<SystemStream, String> streamToConsumerTasks = HashMultimap.create();\r\n    streamToConsumerTasks.put(input1, task0);\r\n    streamToConsumerTasks.put(input1, task1);\r\n    streamToConsumerTasks.put(input2, task1);\r\n    streamToConsumerTasks.put(input2, task2);\r\n    streamToConsumerTasks.put(input3, task1);\r\n    streamToConsumerTasks.put(int1, task0);\r\n    streamToConsumerTasks.put(int1, task1);\r\n    streamToConsumerTasks.put(int2, task0);\r\n    Multimap<SystemStream, SystemStream> intermediateToInputStreams = HashMultimap.create();\r\n    intermediateToInputStreams.put(int1, input1);\r\n    intermediateToInputStreams.put(int1, input2);\r\n    intermediateToInputStreams.put(int2, input2);\r\n    intermediateToInputStreams.put(int2, input3);\r\n    Map<SystemStream, Integer> counts = OperatorImplGraph.getProducerTaskCountForIntermediateStreams(streamToConsumerTasks, intermediateToInputStreams);\r\n    assertTrue(counts.get(int1) == 3);\r\n    assertTrue(counts.get(int2) == 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestSinkOperatorImpl.java",
  "methodName" : "testSinkOperatorSinkFunction",
  "sourceCode" : "@Test\r\npublic void testSinkOperatorSinkFunction() {\r\n    SinkFunction<TestOutputMessageEnvelope> sinkFn = mock(SinkFunction.class);\r\n    SinkOperatorImpl<TestOutputMessageEnvelope> sinkImpl = createSinkOperator(sinkFn);\r\n    TestOutputMessageEnvelope mockMsg = mock(TestOutputMessageEnvelope.class);\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    sinkImpl.handleMessage(mockMsg, mockCollector, mockCoordinator);\r\n    verify(sinkFn, times(1)).apply(mockMsg, mockCollector, mockCoordinator);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestSinkOperatorImpl.java",
  "methodName" : "testSinkOperatorClose",
  "sourceCode" : "@Test\r\npublic void testSinkOperatorClose() {\r\n    TestOutputMessageEnvelope mockMsg = mock(TestOutputMessageEnvelope.class);\r\n    MessageCollector mockCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockCoordinator = mock(TaskCoordinator.class);\r\n    SinkFunction<TestOutputMessageEnvelope> sinkFn = mock(SinkFunction.class);\r\n    SinkOperatorImpl<TestOutputMessageEnvelope> sinkImpl = createSinkOperator(sinkFn);\r\n    sinkImpl.handleMessage(mockMsg, mockCollector, mockCoordinator);\r\n    verify(sinkFn, times(1)).apply(mockMsg, mockCollector, mockCoordinator);\r\n    // ensure that close is not called yet\r\n    verify(sinkFn, times(0)).close();\r\n    sinkImpl.handleClose();\r\n    // ensure that close is called once from handleClose()\r\n    verify(sinkFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestStreamTableJoinOperatorImpl.java",
  "methodName" : "testHandleMessage",
  "sourceCode" : "@Test\r\npublic void testHandleMessage() {\r\n    String tableId = \"t1\";\r\n    StreamTableJoinOperatorSpec mockJoinOpSpec = mock(StreamTableJoinOperatorSpec.class);\r\n    when(mockJoinOpSpec.getTableId()).thenReturn(tableId);\r\n    when(mockJoinOpSpec.getArgs()).thenReturn(new Object[0]);\r\n    when(mockJoinOpSpec.getJoinFn()).thenReturn(new StreamTableJoinFunction<String, KV<String, String>, KV<String, String>, String>() {\r\n\r\n        @Override\r\n        public String apply(KV<String, String> message, KV<String, String> record) {\r\n            if (\"1\".equals(message.getKey())) {\r\n                Assert.assertEquals(\"m1\", message.getValue());\r\n                Assert.assertEquals(\"r1\", record.getValue());\r\n                return \"m1r1\";\r\n            } else if (\"2\".equals(message.getKey())) {\r\n                Assert.assertEquals(\"m2\", message.getValue());\r\n                Assert.assertNull(record);\r\n                return null;\r\n            }\r\n            throw new SamzaException(\"Should never reach here!\");\r\n        }\r\n\r\n        @Override\r\n        public String getMessageKey(KV<String, String> message) {\r\n            return message.getKey();\r\n        }\r\n\r\n        @Override\r\n        public String getRecordKey(KV<String, String> record) {\r\n            return record.getKey();\r\n        }\r\n    });\r\n    ReadWriteUpdateTable table = mock(ReadWriteUpdateTable.class);\r\n    when(table.getAsync(\"1\")).thenReturn(CompletableFuture.completedFuture(\"r1\"));\r\n    when(table.getAsync(\"2\")).thenReturn(CompletableFuture.completedFuture(null));\r\n    Context context = new MockContext();\r\n    when(context.getTaskContext().getUpdatableTable(tableId)).thenReturn(table);\r\n    MessageCollector mockMessageCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockTaskCoordinator = mock(TaskCoordinator.class);\r\n    StreamTableJoinOperatorImpl streamTableJoinOperator = new StreamTableJoinOperatorImpl(mockJoinOpSpec, context);\r\n    // Table has the key\r\n    Collection<TestMessageEnvelope> result;\r\n    result = streamTableJoinOperator.handleMessage(KV.of(\"1\", \"m1\"), mockMessageCollector, mockTaskCoordinator);\r\n    Assert.assertEquals(1, result.size());\r\n    Assert.assertEquals(\"m1r1\", result.iterator().next());\r\n    // Table doesn't have the key\r\n    result = streamTableJoinOperator.handleMessage(KV.of(\"2\", \"m2\"), mockMessageCollector, mockTaskCoordinator);\r\n    Assert.assertEquals(0, result.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestStreamTableJoinOperatorImpl.java",
  "methodName" : "testJoinFunctionIsInvokedOnlyOnce",
  "sourceCode" : "/**\r\n * Ensure join function is not invoked more than once when join function returns null on the first invocation\r\n */\r\n@Test\r\npublic void testJoinFunctionIsInvokedOnlyOnce() {\r\n    final String tableId = \"testTable\";\r\n    final CountDownLatch joinInvokedLatch = new CountDownLatch(1);\r\n    StreamTableJoinOperatorSpec mockJoinOpSpec = mock(StreamTableJoinOperatorSpec.class);\r\n    when(mockJoinOpSpec.getTableId()).thenReturn(tableId);\r\n    when(mockJoinOpSpec.getArgs()).thenReturn(new Object[0]);\r\n    when(mockJoinOpSpec.getJoinFn()).thenReturn(new StreamTableJoinFunction<String, KV<String, String>, KV<String, String>, String>() {\r\n\r\n        @Override\r\n        public String apply(KV<String, String> message, KV<String, String> record) {\r\n            joinInvokedLatch.countDown();\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public String getMessageKey(KV<String, String> message) {\r\n            return message.getKey();\r\n        }\r\n\r\n        @Override\r\n        public String getRecordKey(KV<String, String> record) {\r\n            return record.getKey();\r\n        }\r\n    });\r\n    ReadWriteUpdateTable table = mock(ReadWriteUpdateTable.class);\r\n    when(table.getAsync(\"1\")).thenReturn(CompletableFuture.completedFuture(\"r1\"));\r\n    Context context = new MockContext();\r\n    when(context.getTaskContext().getUpdatableTable(tableId)).thenReturn(table);\r\n    MessageCollector mockMessageCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockTaskCoordinator = mock(TaskCoordinator.class);\r\n    StreamTableJoinOperatorImpl streamTableJoinOperator = new StreamTableJoinOperatorImpl(mockJoinOpSpec, context);\r\n    // Table has the key\r\n    streamTableJoinOperator.handleMessage(KV.of(\"1\", \"m1\"), mockMessageCollector, mockTaskCoordinator);\r\n    assertEquals(\"Join function should only be invoked once\", 0, joinInvokedLatch.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWatermarkStates.java",
  "methodName" : "testUpdate",
  "sourceCode" : "@Test\r\npublic void testUpdate() {\r\n    // advance watermark on input to 5\r\n    WatermarkStates watermarkStates = new WatermarkStates(ssps, producerCounts, new MetricsRegistryMap(), TaskConfig.DEFAULT_TASK_WATERMARK_IDLE_TIMEOUT_MS, TaskConfig.DEFAULT_WATERMARK_QUORUM_SIZE_PERCENTAGE);\r\n    IncomingMessageEnvelope envelope = IncomingMessageEnvelope.buildWatermarkEnvelope(inputPartition0, 5L);\r\n    watermarkStates.update((WatermarkMessage) envelope.getMessage(), envelope.getSystemStreamPartition());\r\n    assertEquals(watermarkStates.getWatermark(input), 5L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // watermark from task 0 on int p0 to 6\r\n    WatermarkMessage watermarkMessage = new WatermarkMessage(6L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), WATERMARK_NOT_EXIST);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // watermark from task 1 on int p0 to 3\r\n    watermarkMessage = new WatermarkMessage(3L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 3L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // watermark from task 0 on int p1 to 10\r\n    watermarkMessage = new WatermarkMessage(10L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition1);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition1), WATERMARK_NOT_EXIST);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // watermark from task 1 on int p1 to 4\r\n    watermarkMessage = new WatermarkMessage(4L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition1);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition1), 4L);\r\n    // verify we got a watermark 3 (min) for int stream\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 3L);\r\n    // advance watermark from task 1 on int p0 to 8\r\n    watermarkMessage = new WatermarkMessage(8L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 6L);\r\n    // verify we got a watermark 4 (min) for int stream\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 4L);\r\n    // advance watermark from task 1 on int p1 to 7\r\n    watermarkMessage = new WatermarkMessage(7L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition1);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition1), 7L);\r\n    // verify we got a watermark 6 (min) for int stream\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 6L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWatermarkStates.java",
  "methodName" : "testIdle",
  "sourceCode" : "@Test\r\npublic void testIdle() {\r\n    MockSystemTime systemTime = new MockSystemTime();\r\n    WatermarkStates watermarkStates = new WatermarkStates(ssps, producerCounts, new MetricsRegistryMap(), TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS, TaskConfig.DEFAULT_WATERMARK_QUORUM_SIZE_PERCENTAGE, systemTime);\r\n    // First watermark\r\n    WatermarkMessage watermarkMessage = new WatermarkMessage(1L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), WATERMARK_NOT_EXIST);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // Advance currentTime to pass the idle timeout\r\n    systemTime.advance(TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS);\r\n    // Watermark is computed based on both task 0 and task 1\r\n    watermarkMessage = new WatermarkMessage(4L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 1L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // Watermark is computed based on \"task 1\" alone since \"task 0\" passes the idle timeout\r\n    watermarkMessage = new WatermarkMessage(5L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 5L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // Advance currentTime without exceeding the timeout\r\n    systemTime.advance(1);\r\n    // Watermark is computed based on \"task 0\" since the currentTime already passes the idle threshold\r\n    watermarkMessage = new WatermarkMessage(6L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition1);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition1), 6L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 5L);\r\n    // Watermark from \"task 1\" is less than current watermark, ignore\r\n    watermarkMessage = new WatermarkMessage(2L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition1);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition1), 6L);\r\n    // verify we got a watermark (min) for int stream\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 5L);\r\n    // Advance currentTime without exceeding the timeout\r\n    systemTime.advance(1);\r\n    // Watermark from \"task 0\" is updated, but less than current watermark\r\n    watermarkMessage = new WatermarkMessage(3L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 5L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 5L);\r\n    // Advance currentTime without exceeding the timeout\r\n    systemTime.advance(1);\r\n    // Watermark is computed this currentTime due to advance in \"task 0\"\r\n    watermarkMessage = new WatermarkMessage(7L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 5L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 5L);\r\n    // Advance currentTime without exceeding the timeout\r\n    systemTime.advance(1);\r\n    // Watermark is computed this currentTime due to advance in \"task 1\"\r\n    watermarkMessage = new WatermarkMessage(10L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 7L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), 6L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWatermarkStates.java",
  "methodName" : "testQuorum",
  "sourceCode" : "@Test\r\npublic void testQuorum() {\r\n    MockSystemTime systemTime = new MockSystemTime();\r\n    WatermarkStates watermarkStates = new WatermarkStates(ssps, producerCounts, new MetricsRegistryMap(), TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS, 1.0, systemTime);\r\n    // First watermark\r\n    WatermarkMessage watermarkMessage = new WatermarkMessage(1L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), WATERMARK_NOT_EXIST);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // Watermark is computed based on both task 0 and task 1\r\n    watermarkMessage = new WatermarkMessage(3L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 1L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    // Advance currentTime to pass the idle timeout\r\n    systemTime.advance(TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS);\r\n    // Watermark is computed based on \"task 1\" alone since \"task 0\" passes the idle timeout\r\n    // Not meeting quorum\r\n    watermarkMessage = new WatermarkMessage(5L, \"task 1\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 1L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n    systemTime.advance(1);\r\n    // Watermark from task 0, now quorum is met.\r\n    watermarkMessage = new WatermarkMessage(3L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 3L);\r\n    assertEquals(watermarkStates.getWatermark(intermediate), WATERMARK_NOT_EXIST);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWatermarkStates.java",
  "methodName" : "testStartup",
  "sourceCode" : "@Test\r\npublic void testStartup() {\r\n    MockSystemTime systemTime = new MockSystemTime();\r\n    WatermarkStates watermarkStates = new WatermarkStates(ssps, producerCounts, new MetricsRegistryMap(), TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS, TaskConfig.DEFAULT_WATERMARK_QUORUM_SIZE_PERCENTAGE, systemTime);\r\n    // Only one active task in the startup\r\n    WatermarkMessage watermarkMessage = new WatermarkMessage(1L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), WATERMARK_NOT_EXIST);\r\n    // Advance currentTime to pass the idle timeout\r\n    systemTime.advance(TEST_TASK_WATERMARK_IDLE_TIMEOUT_MS);\r\n    // Watermark will be soly computed based on task 0\r\n    watermarkMessage = new WatermarkMessage(5L, \"task 0\");\r\n    watermarkStates.update(watermarkMessage, intPartition0);\r\n    assertEquals(watermarkStates.getWatermarkPerSSP(intPartition0), 5L);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testTumblingWindowsDiscardingMode",
  "sourceCode" : "@Test\r\npublic void testTumblingWindowsDiscardingMode() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedTumblingWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofSeconds(1), Triggers.repeat(Triggers.count(2))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    integers.forEach(n -> processSync(task, new IntegerEnvelope(n), messageCollector, taskCoordinator, taskCallback));\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 5);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals((windowPanes.get(1).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals((windowPanes.get(2).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals((windowPanes.get(3).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(4).getKey().getKey(), new Integer(3));\r\n    Assert.assertEquals((windowPanes.get(4).getMessage()).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testNonKeyedTumblingWindowsDiscardingMode",
  "sourceCode" : "@Test\r\npublic void testNonKeyedTumblingWindowsDiscardingMode() throws Exception {\r\n    OperatorSpecGraph sgb = this.getTumblingWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofSeconds(1), Triggers.repeat(Triggers.count(1000))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    Assert.assertEquals(windowPanes.size(), 0);\r\n    integers.forEach(n -> processSync(task, new IntegerEnvelope(n), messageCollector, taskCoordinator, taskCallback));\r\n    Assert.assertEquals(windowPanes.size(), 0);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    Assert.assertEquals(windowPanes.size(), 0);\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 9);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testTumblingAggregatingWindowsDiscardingMode",
  "sourceCode" : "@Test\r\npublic void testTumblingAggregatingWindowsDiscardingMode() throws Exception {\r\n    when(this.context.getTaskContext().getStore(\"jobName-jobId-window-w1\")).thenReturn(new TestInMemoryStore<>(new TimeSeriesKeySerde(new IntegerSerde()), new IntegerSerde()));\r\n    OperatorSpecGraph sgb = this.getAggregateTumblingWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofSeconds(1), Triggers.repeat(Triggers.count(2))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Integer>> windowPanes = new ArrayList<>();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Integer>) envelope.getMessage());\r\n    integers.forEach(n -> processSync(task, new IntegerEnvelope(n), messageCollector, taskCoordinator, taskCallback));\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 5);\r\n    Assert.assertEquals(windowPanes.get(0).getMessage(), new Integer(2));\r\n    Assert.assertEquals(windowPanes.get(1).getMessage(), new Integer(2));\r\n    Assert.assertEquals(windowPanes.get(2).getMessage(), new Integer(2));\r\n    Assert.assertEquals(windowPanes.get(3).getMessage(), new Integer(2));\r\n    Assert.assertEquals(windowPanes.get(4).getMessage(), new Integer(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testTumblingWindowsAccumulatingMode",
  "sourceCode" : "@Test\r\npublic void testTumblingWindowsAccumulatingMode() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedTumblingWindowStreamGraph(AccumulationMode.ACCUMULATING, Duration.ofSeconds(1), Triggers.repeat(Triggers.count(2))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    integers.forEach(n -> processSync(task, new IntegerEnvelope(n), messageCollector, taskCoordinator, taskCallback));\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 7);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals((windowPanes.get(1).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals((windowPanes.get(2).getMessage()).size(), 4);\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals((windowPanes.get(3).getMessage()).size(), 4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testSessionWindowsDiscardingMode",
  "sourceCode" : "@Test\r\npublic void testSessionWindowsDiscardingMode() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedSessionWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofMillis(500)).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getPaneId(), \"1\");\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getKey(), new Integer(1));\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(3), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(3), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 3);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getPaneId(), \"1\");\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getPaneId(), \"1001\");\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getPaneId(), \"1001\");\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    Assert.assertEquals((windowPanes.get(1).getMessage()).size(), 2);\r\n    Assert.assertEquals((windowPanes.get(2).getMessage()).size(), 2);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 4);\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getPaneId(), \"2001\");\r\n    Assert.assertEquals((windowPanes.get(3).getMessage()).size(), 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testSessionWindowsAccumulatingMode",
  "sourceCode" : "@Test\r\npublic void testSessionWindowsAccumulatingMode() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedSessionWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofMillis(500)).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    task.init(this.context);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(2), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getKey(), new Integer(2));\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    Assert.assertEquals((windowPanes.get(1).getMessage()).size(), 4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testCancellationOfOnceTrigger",
  "sourceCode" : "@Test\r\npublic void testCancellationOfOnceTrigger() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedTumblingWindowStreamGraph(AccumulationMode.ACCUMULATING, Duration.ofSeconds(1), Triggers.count(2)).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getPaneId(), \"0\");\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals(windowPanes.get(0).getFiringType(), FiringType.EARLY);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    Assert.assertEquals(windowPanes.get(0).getKey().getPaneId(), \"0\");\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getPaneId(), \"0\");\r\n    Assert.assertEquals(windowPanes.get(1).getFiringType(), FiringType.DEFAULT);\r\n    processSync(task, new IntegerEnvelope(3), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 3);\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getKey(), new Integer(3));\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getPaneId(), \"1000\");\r\n    Assert.assertEquals(windowPanes.get(2).getFiringType(), FiringType.DEFAULT);\r\n    Assert.assertEquals((windowPanes.get(2).getMessage()).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testCancellationOfAnyTrigger",
  "sourceCode" : "@Test\r\npublic void testCancellationOfAnyTrigger() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedTumblingWindowStreamGraph(AccumulationMode.ACCUMULATING, Duration.ofSeconds(1), Triggers.any(Triggers.count(2), Triggers.timeSinceFirstMessage(Duration.ofMillis(500)))).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    //assert that the count trigger fired\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    //advance the timer to enable the triggering of the inner timeSinceFirstMessage trigger\r\n    testClock.advanceTime(Duration.ofMillis(500));\r\n    //assert that the triggering of the count trigger cancelled the inner timeSinceFirstMessage trigger\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    //advance timer by 500 more millis to enable the default trigger\r\n    testClock.advanceTime(Duration.ofMillis(500));\r\n    task.window(messageCollector, taskCoordinator);\r\n    //assert that the default trigger fired\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    Assert.assertEquals(windowPanes.get(1).getFiringType(), FiringType.DEFAULT);\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals(windowPanes.get(1).getKey().getPaneId(), \"0\");\r\n    Assert.assertEquals((windowPanes.get(1).getMessage()).size(), 5);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    //advance timer by 500 millis to enable the inner timeSinceFirstMessage trigger\r\n    testClock.advanceTime(Duration.ofMillis(500));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 3);\r\n    Assert.assertEquals(windowPanes.get(2).getFiringType(), FiringType.EARLY);\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals(windowPanes.get(2).getKey().getPaneId(), \"1000\");\r\n    //advance timer by > 500 millis to enable the default trigger\r\n    testClock.advanceTime(Duration.ofMillis(900));\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 4);\r\n    Assert.assertEquals(windowPanes.get(3).getFiringType(), FiringType.DEFAULT);\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getKey(), new Integer(1));\r\n    Assert.assertEquals(windowPanes.get(3).getKey().getPaneId(), \"1000\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testCancelationOfRepeatingNestedTriggers",
  "sourceCode" : "@Test\r\npublic void testCancelationOfRepeatingNestedTriggers() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedTumblingWindowStreamGraph(AccumulationMode.ACCUMULATING, Duration.ofSeconds(1), Triggers.repeat(Triggers.any(Triggers.count(2), Triggers.timeSinceFirstMessage(Duration.ofMillis(500))))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    //assert that the count trigger fired\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    //advance the timer to enable the potential triggering of the inner timeSinceFirstMessage trigger\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(Duration.ofMillis(500));\r\n    //assert that the triggering of the count trigger cancelled the inner timeSinceFirstMessage trigger\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    Assert.assertEquals(windowPanes.size(), 3);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    //advance timer by 500 more millis to enable the default trigger\r\n    testClock.advanceTime(Duration.ofMillis(500));\r\n    task.window(messageCollector, taskCoordinator);\r\n    //assert that the default trigger fired\r\n    Assert.assertEquals(windowPanes.size(), 4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testEndOfStreamFlushesWithEarlyTriggerFirings",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamFlushesWithEarlyTriggerFirings() throws Exception {\r\n    OperatorSpecGraph sgb = this.getTumblingWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofSeconds(1), Triggers.repeat(Triggers.count(2))).getOperatorSpecGraph();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    TestClock testClock = new TestClock();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    Assert.assertEquals(windowPanes.size(), 0);\r\n    List<Integer> integerList = ImmutableList.of(1, 2, 1, 2, 1);\r\n    integerList.forEach(n -> processSync(task, new IntegerEnvelope(n), messageCollector, taskCoordinator, taskCallback));\r\n    // early triggers should emit (1,2) and (1,2) in the same window.\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    testClock.advanceTime(Duration.ofSeconds(1));\r\n    Assert.assertEquals(windowPanes.size(), 2);\r\n    final IncomingMessageEnvelope endOfStream = IncomingMessageEnvelope.buildEndOfStreamEnvelope(new SystemStreamPartition(\"kafka\", \"integers\", new Partition(0)));\r\n    processSync(task, endOfStream, messageCollector, taskCoordinator, taskCallback);\r\n    // end of stream flushes the last entry (1)\r\n    Assert.assertEquals(windowPanes.size(), 3);\r\n    Assert.assertEquals((windowPanes.get(0).getMessage()).size(), 2);\r\n    verify(taskCoordinator, times(1)).commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    verify(taskCoordinator, times(1)).shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testEndOfStreamFlushesWithDefaultTriggerFirings",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamFlushesWithDefaultTriggerFirings() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedSessionWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofMillis(500)).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    testClock.advanceTime(1000);\r\n    task.window(messageCollector, taskCoordinator);\r\n    Assert.assertEquals(windowPanes.size(), 1);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    final IncomingMessageEnvelope endOfStream = IncomingMessageEnvelope.buildEndOfStreamEnvelope(new SystemStreamPartition(\"kafka\", \"integers\", new Partition(0)));\r\n    processSync(task, endOfStream, messageCollector, taskCoordinator, taskCallback);\r\n    Assert.assertEquals(2, windowPanes.size());\r\n    Assert.assertEquals(2, windowPanes.get(0).getMessage().size());\r\n    verify(taskCoordinator, times(1)).commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    verify(taskCoordinator, times(1)).shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\impl\\TestWindowOperator.java",
  "methodName" : "testEndOfStreamFlushesWithNoTriggerFirings",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamFlushesWithNoTriggerFirings() throws Exception {\r\n    OperatorSpecGraph sgb = this.getKeyedSessionWindowStreamGraph(AccumulationMode.DISCARDING, Duration.ofMillis(500)).getOperatorSpecGraph();\r\n    TestClock testClock = new TestClock();\r\n    List<WindowPane<Integer, Collection<IntegerEnvelope>>> windowPanes = new ArrayList<>();\r\n    StreamOperatorTask task = new StreamOperatorTask(sgb, testClock);\r\n    task.init(this.context);\r\n    MessageCollector messageCollector = envelope -> windowPanes.add((WindowPane<Integer, Collection<IntegerEnvelope>>) envelope.getMessage());\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    processSync(task, new IntegerEnvelope(1), messageCollector, taskCoordinator, taskCallback);\r\n    final IncomingMessageEnvelope endOfStream = IncomingMessageEnvelope.buildEndOfStreamEnvelope(new SystemStreamPartition(\"kafka\", \"integers\", new Partition(0)));\r\n    processSync(task, endOfStream, messageCollector, taskCoordinator, taskCallback);\r\n    Assert.assertEquals(1, windowPanes.size());\r\n    Assert.assertEquals(4, windowPanes.get(0).getMessage().size());\r\n    verify(taskCoordinator, times(1)).commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    verify(taskCoordinator, times(1)).shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithFlatMap",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithFlatMap() {\r\n    FlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> flatMap = m -> {\r\n        List<TestOutputMessageEnvelope> result = new ArrayList<>();\r\n        result.add(new TestOutputMessageEnvelope(m.getKey(), m.getMessage().hashCode()));\r\n        return result;\r\n    };\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createFlatMapOperatorSpec(flatMap, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertNull(streamOperatorSpec.getWatermarkFn());\r\n    assertNull(cloneOperatorSpec.getWatermarkFn());\r\n    assertNull(streamOperatorSpec.getScheduledFn());\r\n    assertNull(cloneOperatorSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithMap",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithMap() {\r\n    MapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> mapFn = m -> new TestOutputMessageEnvelope(m.getKey(), m.getMessage().hashCode());\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(mapFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    MapFunction userFn = (MapFunction) Whitebox.getInternalState(streamOperatorSpec, \"mapFn\");\r\n    assertEquals(userFn, mapFn);\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    MapFunction clonedUserFn = (MapFunction) Whitebox.getInternalState(cloneOperatorSpec, \"mapFn\");\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertTrue(clonedUserFn instanceof MapFunction);\r\n    assertNotEquals(userFn, clonedUserFn);\r\n    assertNull(streamOperatorSpec.getWatermarkFn());\r\n    assertNull(cloneOperatorSpec.getWatermarkFn());\r\n    assertNull(streamOperatorSpec.getScheduledFn());\r\n    assertNull(cloneOperatorSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithFilter",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithFilter() {\r\n    FilterFunction<TestMessageEnvelope> filterFn = m -> m.getKey().equals(\"key1\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestMessageEnvelope> streamOperatorSpec = OperatorSpecs.createFilterOperatorSpec(filterFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    FilterFunction userFn = (FilterFunction) Whitebox.getInternalState(streamOperatorSpec, \"filterFn\");\r\n    assertEquals(userFn, filterFn);\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    FilterFunction clonedUserFn = (FilterFunction) Whitebox.getInternalState(cloneOperatorSpec, \"filterFn\");\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertTrue(clonedUserFn instanceof FilterFunction);\r\n    assertNotEquals(userFn, clonedUserFn);\r\n    assertNull(streamOperatorSpec.getWatermarkFn());\r\n    assertNull(cloneOperatorSpec.getWatermarkFn());\r\n    assertNull(streamOperatorSpec.getScheduledFn());\r\n    assertNull(cloneOperatorSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testInputOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testInputOperatorSpec() {\r\n    Serde<Object> objSerde = new Serde<Object>() {\r\n\r\n        @Override\r\n        public Object fromBytes(byte[] bytes) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public byte[] toBytes(Object object) {\r\n            return new byte[0];\r\n        }\r\n    };\r\n    InputOperatorSpec inputOperatorSpec = new InputOperatorSpec(\"mockStreamId\", new StringSerde(\"UTF-8\"), objSerde, null, true, \"op0\");\r\n    InputOperatorSpec inputOpCopy = (InputOperatorSpec) OperatorSpecTestUtils.copyOpSpec(inputOperatorSpec);\r\n    assertNotEquals(\"Expected deserialized copy of operator spec should not be the same as the original operator spec\", inputOperatorSpec, inputOpCopy);\r\n    assertTrue(inputOperatorSpec.isClone(inputOpCopy));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testOutputOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testOutputOperatorSpec() {\r\n    Serde<Object> objSerde = new Serde<Object>() {\r\n\r\n        @Override\r\n        public Object fromBytes(byte[] bytes) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public byte[] toBytes(Object object) {\r\n            return new byte[0];\r\n        }\r\n    };\r\n    OutputStreamImpl<KV<String, Object>> outputStrmImpl = new OutputStreamImpl<>(\"mockStreamId\", new StringSerde(\"UTF-8\"), objSerde, true);\r\n    OutputOperatorSpec<KV<String, Object>> outputOperatorSpec = new OutputOperatorSpec<>(outputStrmImpl, \"op0\");\r\n    OutputOperatorSpec<KV<String, Object>> outputOpCopy = (OutputOperatorSpec<KV<String, Object>>) OperatorSpecTestUtils.copyOpSpec(outputOperatorSpec);\r\n    assertNotEquals(\"Expected deserialized copy of operator spec should not be the same as the original operator spec\", outputOperatorSpec, outputOpCopy);\r\n    assertTrue(outputOperatorSpec.isClone(outputOpCopy));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testSinkOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testSinkOperatorSpec() {\r\n    SinkFunction<TestMessageEnvelope> sinkFn = (m, c, tc) -> System.out.print(m.toString());\r\n    SinkOperatorSpec<TestMessageEnvelope> sinkOpSpec = new SinkOperatorSpec<>(sinkFn, \"op0\");\r\n    SinkOperatorSpec<TestMessageEnvelope> sinkOpCopy = (SinkOperatorSpec<TestMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(sinkOpSpec);\r\n    assertNotEquals(\"Expected deserialized copy of operator spec should not be the same as the original operator spec\", sinkOpSpec, sinkOpCopy);\r\n    assertTrue(sinkOpSpec.isClone(sinkOpCopy));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testJoinOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testJoinOperatorSpec() {\r\n    InputOperatorSpec leftOpSpec = new InputOperatorSpec(\"test-input-1\", new NoOpSerde<>(), new NoOpSerde<>(), null, false, \"op0\");\r\n    InputOperatorSpec rightOpSpec = new InputOperatorSpec(\"test-input-2\", new NoOpSerde<>(), new NoOpSerde<>(), null, false, \"op1\");\r\n    Serde<Object> objSerde = new Serde<Object>() {\r\n\r\n        @Override\r\n        public Object fromBytes(byte[] bytes) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public byte[] toBytes(Object object) {\r\n            return new byte[0];\r\n        }\r\n    };\r\n    JoinFunction<String, Object, Object, TestOutputMessageEnvelope> joinFn = new TestJoinFunction();\r\n    JoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope> joinOperatorSpec = new JoinOperatorSpec<>(leftOpSpec, rightOpSpec, joinFn, new StringSerde(\"UTF-8\"), objSerde, objSerde, 50000, \"op2\");\r\n    JoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope> joinOpCopy = (JoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(joinOperatorSpec);\r\n    assertNotEquals(\"Expected deserialized copy of operator spec should not be the same as the original operator spec\", joinOperatorSpec, joinOpCopy);\r\n    assertTrue(joinOperatorSpec.isClone(joinOpCopy));\r\n    assertTrue(joinOpCopy.getLeftInputOpSpec().isClone(leftOpSpec));\r\n    assertTrue(joinOpCopy.getRightInputOpSpec().isClone(rightOpSpec));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamTableJoinOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinOperatorSpec() {\r\n    StreamTableJoinFunction<String, Object, Object, TestOutputMessageEnvelope> joinFn = new TestStreamTableJoinFunction();\r\n    String tableId = \"t1\";\r\n    StreamTableJoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope> joinOperatorSpec = new StreamTableJoinOperatorSpec<>(tableId, joinFn, \"join-3\", 1, null, \"3\");\r\n    StreamTableJoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope> joinOpSpecCopy = (StreamTableJoinOperatorSpec<String, Object, Object, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(joinOperatorSpec);\r\n    assertNotEquals(joinOpSpecCopy, joinOperatorSpec);\r\n    assertEquals(joinOpSpecCopy.getOpId(), joinOperatorSpec.getOpId());\r\n    assertEquals(joinOpSpecCopy.getTableId(), joinOperatorSpec.getTableId());\r\n    assertArrayEquals(joinOpSpecCopy.getArgs(), joinOperatorSpec.getArgs());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testSendToTableOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testSendToTableOperatorSpec() {\r\n    String tableId = \"t1\";\r\n    SendToTableOperatorSpec<String, Integer> sendOpSpec = new SendToTableOperatorSpec<>(tableId, \"output-1\");\r\n    SendToTableOperatorSpec<String, Integer> sendToCopy = (SendToTableOperatorSpec<String, Integer>) OperatorSpecTestUtils.copyOpSpec(sendOpSpec);\r\n    assertNotEquals(sendToCopy, sendOpSpec);\r\n    assertEquals(sendToCopy.getOpId(), sendOpSpec.getOpId());\r\n    assertTrue(sendToCopy.getTableId().equals(sendOpSpec.getTableId()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testBroadcastOperatorSpec",
  "sourceCode" : "@Test\r\npublic void testBroadcastOperatorSpec() {\r\n    OutputStreamImpl<TestOutputMessageEnvelope> outputStream = new OutputStreamImpl<>(\"output-0\", new StringSerde(\"UTF-8\"), new JsonSerdeV2<TestOutputMessageEnvelope>(), true);\r\n    BroadcastOperatorSpec<TestOutputMessageEnvelope> broadcastOpSpec = new BroadcastOperatorSpec<>(outputStream, \"broadcast-1\");\r\n    BroadcastOperatorSpec<TestOutputMessageEnvelope> broadcastOpCopy = (BroadcastOperatorSpec<TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(broadcastOpSpec);\r\n    assertNotEquals(broadcastOpCopy, broadcastOpSpec);\r\n    assertEquals(broadcastOpCopy.getOpId(), broadcastOpSpec.getOpId());\r\n    assertTrue(broadcastOpCopy.getOutputStream() != broadcastOpSpec.getOutputStream());\r\n    assertEquals(broadcastOpCopy.getOutputStream().getStreamId(), broadcastOpSpec.getOutputStream().getStreamId());\r\n    assertEquals(broadcastOpCopy.getOutputStream().isKeyed(), broadcastOpSpec.getOutputStream().isKeyed());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testMapStreamOperatorSpecWithWatermark",
  "sourceCode" : "@Test\r\npublic void testMapStreamOperatorSpecWithWatermark() {\r\n    MapWithWatermarkFn testMapFn = new MapWithWatermarkFn();\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(testMapFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    assertEquals(streamOperatorSpec.getWatermarkFn(), testMapFn);\r\n    assertNotNull(cloneOperatorSpec.getWatermarkFn());\r\n    assertNotEquals(cloneOperatorSpec.getTransformFn(), cloneOperatorSpec.getWatermarkFn());\r\n    assertNull(streamOperatorSpec.getScheduledFn());\r\n    assertNull(cloneOperatorSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testMapStreamOperatorSpecWithScheduledFunction",
  "sourceCode" : "@Test\r\npublic void testMapStreamOperatorSpecWithScheduledFunction() {\r\n    MapWithScheduledFn testMapFn = new MapWithScheduledFn();\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(testMapFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    assertNull(streamOperatorSpec.getWatermarkFn());\r\n    assertNull(cloneOperatorSpec.getWatermarkFn());\r\n    assertNotEquals(cloneOperatorSpec.getTransformFn(), cloneOperatorSpec.getWatermarkFn());\r\n    assertEquals(streamOperatorSpec.getScheduledFn(), testMapFn);\r\n    assertNotNull(cloneOperatorSpec.getScheduledFn());\r\n    assertNotEquals(streamOperatorSpec.getScheduledFn(), cloneOperatorSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithMapAndListInClosure",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithMapAndListInClosure() {\r\n    List<Integer> integers = new ArrayList<>(1);\r\n    integers.add(0, 100);\r\n    List<String> keys = new ArrayList<>(1);\r\n    keys.add(0, \"test-1\");\r\n    MapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> mapFn = m -> new TestOutputMessageEnvelope(keys.get(m.getKey().hashCode() % 1), integers.get(m.getMessage().hashCode() % 1));\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(mapFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    MapFunction userFn = (MapFunction) Whitebox.getInternalState(streamOperatorSpec, \"mapFn\");\r\n    assertEquals(userFn, mapFn);\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    MapFunction clonedUserFn = (MapFunction) Whitebox.getInternalState(cloneOperatorSpec, \"mapFn\");\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertTrue(clonedUserFn instanceof MapFunction);\r\n    assertNotEquals(userFn, clonedUserFn);\r\n    // verify changing the values in the original keys and integers list will change the result of the original map function\r\n    TestMessageEnvelope mockImsg = new TestMessageEnvelope(\"input-key-x\", new String(\"value-x\"));\r\n    assertEquals(((MapFunction) userFn).apply(mockImsg), new TestOutputMessageEnvelope(\"test-1\", 100));\r\n    integers.set(0, 200);\r\n    keys.set(0, \"test-2\");\r\n    assertEquals(((MapFunction) userFn).apply(mockImsg), new TestOutputMessageEnvelope(\"test-2\", 200));\r\n    // verify that the cloned map function uses a different copy of lists and still yields the same result\r\n    assertEquals(((MapFunction) clonedUserFn).apply(mockImsg), new TestOutputMessageEnvelope(\"test-1\", 100));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithMapWithFunctionReference",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithMapWithFunctionReference() {\r\n    MapFunction<KV<String, Object>, Object> mapFn = KV::getValue;\r\n    StreamOperatorSpec<KV<String, Object>, Object> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(mapFn, \"op0\");\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    MapFunction userFn = (MapFunction) Whitebox.getInternalState(streamOperatorSpec, \"mapFn\");\r\n    assertEquals(userFn, mapFn);\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    MapFunction clonedUserFn = (MapFunction) Whitebox.getInternalState(cloneOperatorSpec, \"mapFn\");\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertTrue(clonedUserFn instanceof MapFunction);\r\n    assertNotEquals(userFn, clonedUserFn);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestOperatorSpec.java",
  "methodName" : "testStreamOperatorSpecWithMapWithEnum",
  "sourceCode" : "@Test\r\npublic void testStreamOperatorSpecWithMapWithEnum() {\r\n    MapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> mapFn = new MapWithEnum(OperatorSpecTestUtils.TestEnum.One);\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> streamOperatorSpec = OperatorSpecs.createMapOperatorSpec(mapFn, \"op0\");\r\n    assertTrue(streamOperatorSpec instanceof MapOperatorSpec);\r\n    StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope> cloneOperatorSpec = (StreamOperatorSpec<TestMessageEnvelope, TestOutputMessageEnvelope>) OperatorSpecTestUtils.copyOpSpec(streamOperatorSpec);\r\n    assertNotEquals(streamOperatorSpec, cloneOperatorSpec);\r\n    assertTrue(streamOperatorSpec.isClone(cloneOperatorSpec));\r\n    MapFunction userFn = (MapFunction) Whitebox.getInternalState(streamOperatorSpec, \"mapFn\");\r\n    assertEquals(userFn, mapFn);\r\n    assertNotEquals(streamOperatorSpec.getTransformFn(), cloneOperatorSpec.getTransformFn());\r\n    MapFunction clonedUserFn = (MapFunction) Whitebox.getInternalState(cloneOperatorSpec, \"mapFn\");\r\n    assertTrue(cloneOperatorSpec.getTransformFn() instanceof FlatMapFunction);\r\n    assertTrue(clonedUserFn instanceof MapWithEnum);\r\n    assertNotEquals(userFn, clonedUserFn);\r\n    // originally the types should be the same\r\n    assertTrue(((MapWithEnum) userFn).getType() == ((MapWithEnum) clonedUserFn).getType());\r\n    // after changing the type of the cloned user function, the types are different now\r\n    ((MapWithEnum) clonedUserFn).setType(OperatorSpecTestUtils.TestEnum.Two);\r\n    assertTrue(((MapWithEnum) userFn).getType() != ((MapWithEnum) clonedUserFn).getType());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testPartitionBy",
  "sourceCode" : "@Test\r\npublic void testPartitionBy() {\r\n    MapFunction<Object, String> keyFn = m -> m.toString();\r\n    MapFunction<Object, Object> valueFn = m -> m;\r\n    KVSerde<Object, Object> partitionBySerde = KVSerde.of(new NoOpSerde<>(), new NoOpSerde<>());\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(keyFn, valueFn, partitionBySerde, testRepartitionedStreamName);\r\n    }, getConfig());\r\n    assertEquals(2, streamAppDesc.getInputOperators().size());\r\n    Map<String, InputOperatorSpec> inputOpSpecs = streamAppDesc.getInputOperators();\r\n    assertTrue(inputOpSpecs.keySet().contains(String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName)));\r\n    InputOperatorSpec inputOpSpec = inputOpSpecs.get(String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName));\r\n    assertEquals(String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName), inputOpSpec.getStreamId());\r\n    assertTrue(inputOpSpec.getKeySerde() instanceof NoOpSerde);\r\n    assertTrue(inputOpSpec.getValueSerde() instanceof NoOpSerde);\r\n    assertTrue(inputOpSpec.isKeyed());\r\n    assertNull(inputOpSpec.getScheduledFn());\r\n    assertNull(inputOpSpec.getWatermarkFn());\r\n    InputOperatorSpec originInputSpec = inputOpSpecs.get(testInputDescriptor.getStreamId());\r\n    assertTrue(originInputSpec.getRegisteredOperatorSpecs().toArray()[0] instanceof PartitionByOperatorSpec);\r\n    PartitionByOperatorSpec reparOpSpec = (PartitionByOperatorSpec) originInputSpec.getRegisteredOperatorSpecs().toArray()[0];\r\n    assertEquals(reparOpSpec.getOpId(), String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName));\r\n    assertEquals(reparOpSpec.getKeyFunction(), keyFn);\r\n    assertEquals(reparOpSpec.getValueFunction(), valueFn);\r\n    assertEquals(reparOpSpec.getOutputStream().getStreamId(), reparOpSpec.getOpId());\r\n    assertNull(reparOpSpec.getScheduledFn());\r\n    assertNull(reparOpSpec.getWatermarkFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testPartitionByWithNoSerde",
  "sourceCode" : "@Test\r\npublic void testPartitionByWithNoSerde() {\r\n    MapFunction<Object, String> keyFn = m -> m.toString();\r\n    MapFunction<Object, Object> valueFn = m -> m;\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(keyFn, valueFn, mock(KVSerde.class), testRepartitionedStreamName);\r\n    }, getConfig());\r\n    InputOperatorSpec inputOpSpec = streamAppDesc.getInputOperators().get(String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName));\r\n    assertNotNull(inputOpSpec);\r\n    assertNull(inputOpSpec.getKeySerde());\r\n    assertNull(inputOpSpec.getValueSerde());\r\n    assertTrue(inputOpSpec.isKeyed());\r\n    assertNull(inputOpSpec.getScheduledFn());\r\n    assertNull(inputOpSpec.getWatermarkFn());\r\n    InputOperatorSpec originInputSpec = streamAppDesc.getInputOperators().get(testInputDescriptor.getStreamId());\r\n    assertTrue(originInputSpec.getRegisteredOperatorSpecs().toArray()[0] instanceof PartitionByOperatorSpec);\r\n    PartitionByOperatorSpec reparOpSpec = (PartitionByOperatorSpec) originInputSpec.getRegisteredOperatorSpecs().toArray()[0];\r\n    assertEquals(reparOpSpec.getOpId(), String.format(\"%s-%s-partition_by-%s\", testJobName, testJobId, testRepartitionedStreamName));\r\n    assertEquals(reparOpSpec.getKeyFunction(), keyFn);\r\n    assertEquals(reparOpSpec.getValueFunction(), valueFn);\r\n    assertEquals(reparOpSpec.getOutputStream().getStreamId(), reparOpSpec.getOpId());\r\n    assertNull(reparOpSpec.getScheduledFn());\r\n    assertNull(reparOpSpec.getWatermarkFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testCopy",
  "sourceCode" : "@Test\r\npublic void testCopy() {\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(m -> m.toString(), m -> m, mock(KVSerde.class), testRepartitionedStreamName);\r\n    }, getConfig());\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    OperatorSpecGraph clonedGraph = specGraph.clone();\r\n    OperatorSpecTestUtils.assertClonedGraph(specGraph, clonedGraph);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testScheduledFunctionAsKeyFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testScheduledFunctionAsKeyFn() {\r\n    ScheduledMapFn keyFn = new ScheduledMapFn();\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(keyFn, m -> m, mock(KVSerde.class), \"parByKey\");\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testWatermarkFunctionAsKeyFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWatermarkFunctionAsKeyFn() {\r\n    WatermarkMapFn keyFn = new WatermarkMapFn();\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(keyFn, m -> m, mock(KVSerde.class), \"parByKey\");\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testScheduledFunctionAsValueFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testScheduledFunctionAsValueFn() {\r\n    ScheduledMapFn valueFn = new ScheduledMapFn();\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(m -> m.toString(), valueFn, mock(KVSerde.class), \"parByKey\");\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestPartitionByOperatorSpec.java",
  "methodName" : "testWatermarkFunctionAsValueFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testWatermarkFunctionAsValueFn() {\r\n    WatermarkMapFn valueFn = new WatermarkMapFn();\r\n    new StreamApplicationDescriptorImpl(appDesc -> {\r\n        MessageStream<Object> inputStream = appDesc.getInputStream(testInputDescriptor);\r\n        inputStream.partitionBy(m -> m.toString(), valueFn, mock(KVSerde.class), \"parByKey\");\r\n    }, getConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testTriggerIntervalWithNestedTimeTriggers",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalWithNestedTimeTriggers() {\r\n    defaultTrigger = Triggers.timeSinceFirstMessage(Duration.ofMillis(150));\r\n    lateTrigger = Triggers.any(Triggers.count(6), Triggers.timeSinceFirstMessage(Duration.ofMillis(15)));\r\n    earlyTrigger = Triggers.repeat(Triggers.any(Triggers.count(23), Triggers.timeSinceFirstMessage(Duration.ofMillis(15)), Triggers.any(Triggers.any(Triggers.count(6), Triggers.timeSinceFirstMessage(Duration.ofMillis(15)), Triggers.timeSinceFirstMessage(Duration.ofMillis(25)), Triggers.timeSinceLastMessage(Duration.ofMillis(15))))));\r\n    WindowOperatorSpec spec = getWindowOperatorSpec(\"w0\");\r\n    assertEquals(spec.getDefaultTriggerMs(), 5);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testTriggerIntervalWithSingleTimeTrigger",
  "sourceCode" : "@Test\r\npublic void testTriggerIntervalWithSingleTimeTrigger() {\r\n    WindowOperatorSpec spec = getWindowOperatorSpec(\"w0\");\r\n    assertEquals(spec.getDefaultTriggerMs(), 150);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalScheduledFunctionAsInitializer",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalScheduledFunctionAsInitializer() {\r\n    class TimedSupplierFunction implements SupplierFunction<Collection>, ScheduledFunction<Object, Collection> {\r\n\r\n        @Override\r\n        public Collection get() {\r\n            return new ArrayList<>();\r\n        }\r\n\r\n        @Override\r\n        public void schedule(Scheduler<Object> scheduler) {\r\n        }\r\n\r\n        @Override\r\n        public Collection<Collection> onCallback(Object key, long timestamp) {\r\n            return null;\r\n        }\r\n    }\r\n    supplierFunction = new TimedSupplierFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalWatermarkFunctionAsInitializer",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalWatermarkFunctionAsInitializer() {\r\n    class WatermarkSupplierFunction implements SupplierFunction<Collection>, WatermarkFunction<Collection> {\r\n\r\n        @Override\r\n        public Collection get() {\r\n            return new ArrayList<>();\r\n        }\r\n\r\n        @Override\r\n        public Collection<Collection> processWatermark(long watermark) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Long getOutputWatermark() {\r\n            return null;\r\n        }\r\n    }\r\n    supplierFunction = new WatermarkSupplierFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalScheduledFunctionAsKeyFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalScheduledFunctionAsKeyFn() {\r\n    class ScheduledMapFunction implements MapFunction<Object, Object>, ScheduledFunction<Object, Object> {\r\n\r\n        @Override\r\n        public Object apply(Object message) {\r\n            return message.toString();\r\n        }\r\n\r\n        @Override\r\n        public void schedule(Scheduler<Object> scheduler) {\r\n        }\r\n\r\n        @Override\r\n        public Collection<Object> onCallback(Object key, long timestamp) {\r\n            return null;\r\n        }\r\n    }\r\n    keyFn = new ScheduledMapFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalWatermarkFunctionAsKeyFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalWatermarkFunctionAsKeyFn() {\r\n    class WatermarkMapFunction implements MapFunction<Object, Object>, WatermarkFunction<Object> {\r\n\r\n        @Override\r\n        public Object apply(Object message) {\r\n            return message.toString();\r\n        }\r\n\r\n        @Override\r\n        public Collection<Object> processWatermark(long watermark) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Long getOutputWatermark() {\r\n            return null;\r\n        }\r\n    }\r\n    keyFn = new WatermarkMapFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalScheduledFunctionAsEventTimeFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalScheduledFunctionAsEventTimeFn() {\r\n    class ScheduledMapFunction implements MapFunction<Object, Long>, ScheduledFunction<Object, Object> {\r\n\r\n        @Override\r\n        public Long apply(Object message) {\r\n            return 123456L;\r\n        }\r\n\r\n        @Override\r\n        public void schedule(Scheduler<Object> scheduler) {\r\n        }\r\n\r\n        @Override\r\n        public Collection<Object> onCallback(Object key, long timestamp) {\r\n            return null;\r\n        }\r\n    }\r\n    timeFn = new ScheduledMapFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testIllegalWatermarkFunctionAsEventTimeFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testIllegalWatermarkFunctionAsEventTimeFn() {\r\n    class WatermarkMapFunction implements MapFunction<Object, Long>, WatermarkFunction<Object> {\r\n\r\n        @Override\r\n        public Long apply(Object message) {\r\n            return 123456L;\r\n        }\r\n\r\n        @Override\r\n        public Collection<Object> processWatermark(long watermark) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Long getOutputWatermark() {\r\n            return null;\r\n        }\r\n    }\r\n    timeFn = new WatermarkMapFunction();\r\n    getWindowOperatorSpec(\"w0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testScheduledFunctionAsFoldLeftFn",
  "sourceCode" : "@Test\r\npublic void testScheduledFunctionAsFoldLeftFn() {\r\n    class ScheduledFoldLeftFunction implements FoldLeftFunction<Object, Collection>, ScheduledFunction<Object, Collection> {\r\n\r\n        @Override\r\n        public Collection apply(Object message, Collection oldValue) {\r\n            oldValue.add(message);\r\n            return oldValue;\r\n        }\r\n\r\n        @Override\r\n        public void schedule(Scheduler<Object> scheduler) {\r\n        }\r\n\r\n        @Override\r\n        public Collection<Collection> onCallback(Object key, long timestamp) {\r\n            return null;\r\n        }\r\n    }\r\n    foldFn = new ScheduledFoldLeftFunction();\r\n    WindowOperatorSpec<Object, Object, Collection> windowSpec = getWindowOperatorSpec(\"w0\");\r\n    assertEquals(windowSpec.getScheduledFn(), foldFn);\r\n    assertNull(windowSpec.getWatermarkFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testWatermarkFunctionAsFoldLeftFn",
  "sourceCode" : "@Test\r\npublic void testWatermarkFunctionAsFoldLeftFn() {\r\n    class WatermarkFoldLeftFunction implements FoldLeftFunction<Object, Collection>, WatermarkFunction<Object> {\r\n\r\n        @Override\r\n        public Collection<Object> processWatermark(long watermark) {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Long getOutputWatermark() {\r\n            return null;\r\n        }\r\n\r\n        @Override\r\n        public Collection apply(Object message, Collection oldValue) {\r\n            oldValue.add(message);\r\n            return oldValue;\r\n        }\r\n    }\r\n    foldFn = new WatermarkFoldLeftFunction();\r\n    WindowOperatorSpec<Object, Object, Collection> windowSpec = getWindowOperatorSpec(\"w0\");\r\n    assertEquals(windowSpec.getWatermarkFn(), foldFn);\r\n    assertNull(windowSpec.getScheduledFn());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\spec\\TestWindowOperatorSpec.java",
  "methodName" : "testCopy",
  "sourceCode" : "@Test\r\npublic void testCopy() {\r\n    WindowInternal<Object, Object, Collection> window = new WindowInternal<Object, Object, Collection>(defaultTrigger, supplierFunction, foldFn, keyFn, timeFn, WindowType.SESSION, null, mock(Serde.class), mock(Serde.class));\r\n    window.setEarlyTrigger(earlyTrigger);\r\n    WindowOperatorSpec<Object, Object, Collection> spec = new WindowOperatorSpec<>(window, \"w0\");\r\n    WindowOperatorSpec<Object, Object, Collection> copy = (WindowOperatorSpec<Object, Object, Collection>) OperatorSpecTestUtils.copyOpSpec(spec);\r\n    Assert.assertNotEquals(spec, copy);\r\n    Assert.assertTrue(spec.isClone(copy));\r\n    Assert.assertNotEquals(spec.getWindow(), copy.getWindow());\r\n    Assert.assertNotEquals(copy.getWindow().getInitializer(), supplierFunction);\r\n    assertEquals(copy.getWindow().getInitializer().get(), supplierFunction.get());\r\n    Assert.assertNotEquals(copy.getWindow().getFoldLeftFunction(), foldFn);\r\n    Object mockMsg = new Object();\r\n    assertEquals(copy.getWindow().getFoldLeftFunction().apply(mockMsg, new ArrayList<>()), foldFn.apply(mockMsg, new ArrayList<>()));\r\n    Assert.assertNotEquals(copy.getWindow().getKeyExtractor(), keyFn);\r\n    assertEquals(copy.getWindow().getKeyExtractor().apply(mockMsg), keyFn.apply(mockMsg));\r\n    Assert.assertNotEquals(copy.getWindow().getEventTimeExtractor(), timeFn);\r\n    assertEquals(copy.getWindow().getEventTimeExtractor().apply(mockMsg), timeFn.apply(mockMsg));\r\n    assertEquals(copy.getDefaultTriggerMs(), 150);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "join",
  "sourceCode" : "@Test\r\npublic void join() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream with same keys\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    assertEquals(110, outputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinWithSelfThrowsException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void joinWithSelfThrowsException() throws Exception {\r\n    Map<String, String> mapConfig = new HashMap<>();\r\n    mapConfig.put(\"job.name\", \"jobName\");\r\n    mapConfig.put(\"job.id\", \"jobId\");\r\n    StreamTestUtils.addStreamConfigs(mapConfig, \"inStream\", \"insystem\", \"instream\");\r\n    Config config = new MapConfig(mapConfig);\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(appDesc -> {\r\n        IntegerSerde integerSerde = new IntegerSerde();\r\n        KVSerde<Integer, Integer> kvSerde = KVSerde.of(integerSerde, integerSerde);\r\n        GenericSystemDescriptor sd = new GenericSystemDescriptor(\"insystem\", \"mockFactoryClassName\");\r\n        GenericInputDescriptor<KV<Integer, Integer>> inputDescriptor = sd.getInputDescriptor(\"inStream\", kvSerde);\r\n        MessageStream<KV<Integer, Integer>> inStream = appDesc.getInputStream(inputDescriptor);\r\n        inStream.join(inStream, new TestJoinFunction(), integerSerde, kvSerde, kvSerde, JOIN_TTL, \"join\");\r\n    }, config);\r\n    // should throw an exception\r\n    createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinFnInitAndClose",
  "sourceCode" : "@Test\r\npublic void joinFnInitAndClose() throws Exception {\r\n    TestJoinFunction joinFn = new TestJoinFunction();\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(joinFn);\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    MessageCollector messageCollector = mock(MessageCollector.class);\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // close should not be called till now\r\n    sot.close();\r\n    verify(messageCollector, times(0)).send(any(OutgoingMessageEnvelope.class));\r\n    // Make sure the joinFn has been copied instead of directly referred by the task instance\r\n    assertEquals(0, joinFn.getNumInitCalls());\r\n    assertEquals(0, joinFn.getNumCloseCalls());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinReverse",
  "sourceCode" : "@Test\r\npublic void joinReverse() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to second stream\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to first stream with same keys\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    assertEquals(110, outputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinNoMatch",
  "sourceCode" : "@Test\r\npublic void joinNoMatch() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream with different keys\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n + 100, n), messageCollector, taskCoordinator, taskCallback));\r\n    assertTrue(output.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinNoMatchReverse",
  "sourceCode" : "@Test\r\npublic void joinNoMatchReverse() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to second stream\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to first stream with different keys\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n + 100, n), messageCollector, taskCoordinator, taskCallback));\r\n    assertTrue(output.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRetainsLatestMessageForKey",
  "sourceCode" : "@Test\r\npublic void joinRetainsLatestMessageForKey() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to first stream again with same keys but different values\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, 2 * n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream with same key\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    // should use latest messages in the first stream\r\n    assertEquals(165, outputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRetainsLatestMessageForKeyReverse",
  "sourceCode" : "@Test\r\npublic void joinRetainsLatestMessageForKeyReverse() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to second stream\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream again with same keys but different values\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, 2 * n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to first stream with same key\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    // should use latest messages in the second stream\r\n    assertEquals(165, outputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRetainsMatchedMessages",
  "sourceCode" : "@Test\r\npublic void joinRetainsMatchedMessages() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream with same key\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    assertEquals(110, outputSum);\r\n    output.clear();\r\n    // push messages to first stream with same keys once again.\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int newOutputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    // should produce the same output as before\r\n    assertEquals(110, newOutputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRetainsMatchedMessagesReverse",
  "sourceCode" : "@Test\r\npublic void joinRetainsMatchedMessagesReverse() throws Exception {\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(SystemClock.instance(), streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // push messages to second stream with same key\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int outputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    assertEquals(110, outputSum);\r\n    output.clear();\r\n    // push messages to second stream with same keys once again.\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    int newOutputSum = output.stream().reduce(0, (s, m) -> s + m);\r\n    // should produce the same output as before\r\n    assertEquals(110, newOutputSum);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRemovesExpiredMessages",
  "sourceCode" : "@Test\r\npublic void joinRemovesExpiredMessages() throws Exception {\r\n    TestClock testClock = new TestClock();\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(testClock, streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to first stream\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // 1 minute after ttl\r\n    testClock.advanceTime(JOIN_TTL.plus(Duration.ofMinutes(1)));\r\n    // should expire first stream messages\r\n    sot.window(messageCollector, taskCoordinator);\r\n    // push messages to second stream with same key\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    assertTrue(output.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestJoinOperator.java",
  "methodName" : "joinRemovesExpiredMessagesReverse",
  "sourceCode" : "@Test\r\npublic void joinRemovesExpiredMessagesReverse() throws Exception {\r\n    TestClock testClock = new TestClock();\r\n    StreamApplicationDescriptorImpl streamAppDesc = this.getTestJoinStreamGraph(new TestJoinFunction());\r\n    StreamOperatorTask sot = createStreamOperatorTask(testClock, streamAppDesc);\r\n    List<Integer> output = new ArrayList<>();\r\n    MessageCollector messageCollector = envelope -> output.add((Integer) envelope.getMessage());\r\n    // push messages to second stream\r\n    numbers.forEach(n -> processSync(sot, new SecondStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    // 1 minute after ttl\r\n    testClock.advanceTime(JOIN_TTL.plus(Duration.ofMinutes(1)));\r\n    // should expire second stream messages\r\n    sot.window(messageCollector, taskCoordinator);\r\n    // push messages to first stream with same key\r\n    numbers.forEach(n -> processSync(sot, new FirstStreamIME(n, n), messageCollector, taskCoordinator, taskCallback));\r\n    assertTrue(output.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMap",
  "sourceCode" : "@Test\r\npublic void testMap() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    MapFunction<TestMessageEnvelope, TestOutputMessageEnvelope> mockMapFn = mock(MapFunction.class);\r\n    inputStream.map(mockMapFn);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof StreamOperatorSpec);\r\n    FlatMapFunction transformFn = ((StreamOperatorSpec) registeredOpSpec).getTransformFn();\r\n    assertNotNull(transformFn);\r\n    assertEquals(OpCode.MAP, registeredOpSpec.getOpCode());\r\n    TestOutputMessageEnvelope mockOutput = mock(TestOutputMessageEnvelope.class);\r\n    when(mockMapFn.apply(anyObject())).thenReturn(mockOutput);\r\n    assertTrue(transformFn.apply(new Object()).contains(mockOutput));\r\n    when(mockMapFn.apply(anyObject())).thenReturn(null);\r\n    assertTrue(transformFn.apply(null).isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testFlatMap",
  "sourceCode" : "@Test\r\npublic void testFlatMap() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    inputStream.flatMap(mock(FlatMapFunction.class));\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof StreamOperatorSpec);\r\n    assertNotNull(((StreamOperatorSpec) registeredOpSpec).getTransformFn());\r\n    assertEquals(OpCode.FLAT_MAP, registeredOpSpec.getOpCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testFlatMapWithRelaxedTypes",
  "sourceCode" : "@Test\r\npublic void testFlatMapWithRelaxedTypes() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestInputMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    FlatMapFunction flatMapFunction = (FlatMapFunction<TestMessageEnvelope, TestOutputMessageEnvelope>) message -> Collections.emptyList();\r\n    // should compile since TestInputMessageEnvelope extends TestMessageEnvelope\r\n    inputStream.flatMap(flatMapFunction);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof StreamOperatorSpec);\r\n    assertNotNull(((StreamOperatorSpec) registeredOpSpec).getTransformFn());\r\n    assertEquals(OpCode.FLAT_MAP, registeredOpSpec.getOpCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testFilter",
  "sourceCode" : "@Test\r\npublic void testFilter() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    FilterFunction<Object> mockFilterFn = mock(FilterFunction.class);\r\n    inputStream.filter(mockFilterFn);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof StreamOperatorSpec);\r\n    FlatMapFunction transformFn = ((StreamOperatorSpec) registeredOpSpec).getTransformFn();\r\n    assertNotNull(transformFn);\r\n    assertEquals(OpCode.FILTER, registeredOpSpec.getOpCode());\r\n    Object mockInput = new Object();\r\n    when(mockFilterFn.apply(anyObject())).thenReturn(true);\r\n    assertTrue(transformFn.apply(mockInput).contains(mockInput));\r\n    when(mockFilterFn.apply(anyObject())).thenReturn(false);\r\n    assertTrue(transformFn.apply(mockInput).isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testSink",
  "sourceCode" : "@Test\r\npublic void testSink() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    inputStream.sink(mock(SinkFunction.class));\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof SinkOperatorSpec);\r\n    assertNotNull(((SinkOperatorSpec) registeredOpSpec).getSinkFn());\r\n    assertEquals(OpCode.SINK, registeredOpSpec.getOpCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testSendTo",
  "sourceCode" : "@Test\r\npublic void testSendTo() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    OutputStreamImpl<TestMessageEnvelope> mockOutputStreamImpl = mock(OutputStreamImpl.class);\r\n    inputStream.sendTo(mockOutputStreamImpl);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof OutputOperatorSpec);\r\n    assertEquals(OpCode.SEND_TO, registeredOpSpec.getOpCode());\r\n    assertEquals(mockOutputStreamImpl, ((OutputOperatorSpec) registeredOpSpec).getOutputStream());\r\n    // same behavior as above so nothing new to assert. but ensures that this variant compiles.\r\n    MessageStreamImpl<KV<String, TestMessageEnvelope>> keyedInputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    OutputStreamImpl<KV<String, TestMessageEnvelope>> mockKeyedOutputStreamImpl = mock(OutputStreamImpl.class);\r\n    keyedInputStream.sendTo(mockKeyedOutputStreamImpl);\r\n    // can't unit test it, but the following variants should not compile\r\n    //    inputStream.sendTo(mockKeyedOutputStreamImpl);\r\n    //    keyedInputStream.sendTo(mockOutputStreamImpl);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testPartitionBy",
  "sourceCode" : "@Test\r\npublic void testPartitionBy() throws IOException {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    String mockOpName = \"mockName\";\r\n    when(mockGraph.getNextOpId(anyObject(), anyObject())).thenReturn(mockOpName);\r\n    OutputStreamImpl mockOutputStreamImpl = mock(OutputStreamImpl.class);\r\n    KVSerde mockKVSerde = mock(KVSerde.class);\r\n    IntermediateMessageStreamImpl mockIntermediateStream = mock(IntermediateMessageStreamImpl.class);\r\n    when(mockGraph.getIntermediateStream(eq(mockOpName), eq(mockKVSerde), eq(false))).thenReturn(mockIntermediateStream);\r\n    when(mockIntermediateStream.getOutputStream()).thenReturn(mockOutputStreamImpl);\r\n    when(mockIntermediateStream.isKeyed()).thenReturn(true);\r\n    MessageStreamImpl<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    MapFunction mockKeyFunction = mock(MapFunction.class);\r\n    MapFunction mockValueFunction = mock(MapFunction.class);\r\n    inputStream.partitionBy(mockKeyFunction, mockValueFunction, mockKVSerde, \"p1\");\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof PartitionByOperatorSpec);\r\n    assertEquals(OpCode.PARTITION_BY, registeredOpSpec.getOpCode());\r\n    assertEquals(mockOutputStreamImpl, ((PartitionByOperatorSpec) registeredOpSpec).getOutputStream());\r\n    assertEquals(mockKeyFunction, ((PartitionByOperatorSpec) registeredOpSpec).getKeyFunction());\r\n    assertEquals(mockValueFunction, ((PartitionByOperatorSpec) registeredOpSpec).getValueFunction());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testWindowWithRelaxedTypes",
  "sourceCode" : "@Test\r\npublic void testWindowWithRelaxedTypes() throws Exception {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec = mock(OperatorSpec.class);\r\n    MessageStream<TestInputMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec);\r\n    MapFunction<TestMessageEnvelope, String> keyExtractor = m -> m.getKey();\r\n    FoldLeftFunction<TestMessageEnvelope, Integer> aggregator = (m, c) -> c + 1;\r\n    SupplierFunction<Integer> initialValue = () -> 0;\r\n    // should compile since TestMessageEnvelope (input for functions) is base class of TestInputMessageEnvelope (M)\r\n    Window<TestInputMessageEnvelope, String, Integer> window = Windows.keyedTumblingWindow(keyExtractor, Duration.ofHours(1), initialValue, aggregator, null, mock(Serde.class));\r\n    MessageStream<WindowPane<String, Integer>> windowedStream = inputStream.window(window, \"w1\");\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof WindowOperatorSpec);\r\n    assertEquals(OpCode.WINDOW, registeredOpSpec.getOpCode());\r\n    assertEquals(window, ((WindowOperatorSpec) registeredOpSpec).getWindow());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testJoin",
  "sourceCode" : "@Test\r\npublic void testJoin() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec leftInputOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> source1 = new MessageStreamImpl<>(mockGraph, leftInputOpSpec);\r\n    OperatorSpec rightInputOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> source2 = new MessageStreamImpl<>(mockGraph, rightInputOpSpec);\r\n    JoinFunction<String, TestMessageEnvelope, TestMessageEnvelope, TestOutputMessageEnvelope> mockJoinFn = mock(JoinFunction.class);\r\n    Duration joinTtl = Duration.ofMinutes(1);\r\n    source1.join(source2, mockJoinFn, mock(Serde.class), mock(Serde.class), mock(Serde.class), joinTtl, \"j1\");\r\n    ArgumentCaptor<OperatorSpec> leftRegisteredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(leftInputOpSpec).registerNextOperatorSpec(leftRegisteredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> leftRegisteredOpSpec = leftRegisteredOpCaptor.getValue();\r\n    ArgumentCaptor<OperatorSpec> rightRegisteredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(rightInputOpSpec).registerNextOperatorSpec(rightRegisteredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> rightRegisteredOpSpec = rightRegisteredOpCaptor.getValue();\r\n    assertEquals(leftRegisteredOpSpec, rightRegisteredOpSpec);\r\n    assertEquals(OpCode.JOIN, leftRegisteredOpSpec.getOpCode());\r\n    assertTrue(leftRegisteredOpSpec instanceof JoinOperatorSpec);\r\n    assertEquals(mockJoinFn, ((JoinOperatorSpec) leftRegisteredOpSpec).getJoinFn());\r\n    assertEquals(joinTtl.toMillis(), ((JoinOperatorSpec) leftRegisteredOpSpec).getTtlMs());\r\n    assertEquals(leftInputOpSpec, ((JoinOperatorSpec) leftRegisteredOpSpec).getLeftInputOpSpec());\r\n    assertEquals(rightInputOpSpec, ((JoinOperatorSpec) leftRegisteredOpSpec).getRightInputOpSpec());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testSendToTable",
  "sourceCode" : "@Test\r\npublic void testSendToTable() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec inputOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> source = new MessageStreamImpl<>(mockGraph, inputOpSpec);\r\n    TableImpl table = new TableImpl(\"t1\");\r\n    source.sendTo(table);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(inputOpSpec).registerNextOperatorSpec(registeredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec = registeredOpCaptor.getValue();\r\n    assertTrue(registeredOpSpec instanceof SendToTableOperatorSpec);\r\n    SendToTableOperatorSpec sendToTableOperatorSpec = (SendToTableOperatorSpec) registeredOpSpec;\r\n    assertEquals(OpCode.SEND_TO, sendToTableOperatorSpec.getOpCode());\r\n    assertEquals(table.getTableId(), sendToTableOperatorSpec.getTableId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testStreamTableJoin",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoin() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec leftInputOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<KV<String, TestMessageEnvelope>> source1 = new MessageStreamImpl<>(mockGraph, leftInputOpSpec);\r\n    OperatorSpec rightInputOpSpec = mock(OperatorSpec.class);\r\n    MessageStreamImpl<TestMessageEnvelope> source2 = new MessageStreamImpl<>(mockGraph, rightInputOpSpec);\r\n    TableImpl table = new TableImpl(\"t1\");\r\n    source2.sendTo(table);\r\n    StreamTableJoinFunction<String, KV<String, TestMessageEnvelope>, KV<String, TestMessageEnvelope>, TestOutputMessageEnvelope> mockJoinFn = mock(StreamTableJoinFunction.class);\r\n    source1.join(table, mockJoinFn);\r\n    ArgumentCaptor<OperatorSpec> leftRegisteredOpCaptor = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(leftInputOpSpec).registerNextOperatorSpec(leftRegisteredOpCaptor.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> leftRegisteredOpSpec = leftRegisteredOpCaptor.getValue();\r\n    assertTrue(leftRegisteredOpSpec instanceof StreamTableJoinOperatorSpec);\r\n    StreamTableJoinOperatorSpec joinOpSpec = (StreamTableJoinOperatorSpec) leftRegisteredOpSpec;\r\n    assertEquals(OpCode.JOIN, joinOpSpec.getOpCode());\r\n    assertEquals(mockJoinFn, joinOpSpec.getJoinFn());\r\n    assertEquals(table.getTableId(), joinOpSpec.getTableId());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMerge",
  "sourceCode" : "@Test\r\npublic void testMerge() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec mockOpSpec1 = mock(OperatorSpec.class);\r\n    MessageStream<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mockOpSpec1);\r\n    // other streams have the same message type T as input stream message type M\r\n    OperatorSpec mockOpSpec2 = mock(OperatorSpec.class);\r\n    OperatorSpec mockOpSpec3 = mock(OperatorSpec.class);\r\n    Collection<MessageStream<TestMessageEnvelope>> otherStreams1 = ImmutableList.of(new MessageStreamImpl<>(mockGraph, mockOpSpec2), new MessageStreamImpl<>(mockGraph, mockOpSpec3));\r\n    inputStream.merge(otherStreams1);\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor1 = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec1).registerNextOperatorSpec(registeredOpCaptor1.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec1 = registeredOpCaptor1.getValue();\r\n    assertTrue(registeredOpSpec1 instanceof StreamOperatorSpec);\r\n    FlatMapFunction transformFn = ((StreamOperatorSpec) registeredOpSpec1).getTransformFn();\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor2 = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec2).registerNextOperatorSpec(registeredOpCaptor2.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec2 = registeredOpCaptor2.getValue();\r\n    ArgumentCaptor<OperatorSpec> registeredOpCaptor3 = ArgumentCaptor.forClass(OperatorSpec.class);\r\n    verify(mockOpSpec3).registerNextOperatorSpec(registeredOpCaptor3.capture());\r\n    OperatorSpec<?, TestMessageEnvelope> registeredOpSpec3 = registeredOpCaptor3.getValue();\r\n    assertEquals(registeredOpSpec1, registeredOpSpec2);\r\n    assertEquals(registeredOpSpec2, registeredOpSpec3);\r\n    assertEquals(OpCode.MERGE, registeredOpSpec1.getOpCode());\r\n    assertNotNull(transformFn);\r\n    TestMessageEnvelope mockInput = mock(TestMessageEnvelope.class);\r\n    assertTrue(transformFn.apply(mockInput).contains(mockInput));\r\n    assertEquals(1, transformFn.apply(mockInput).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMergeWithRelaxedTypes",
  "sourceCode" : "@Test\r\npublic void testMergeWithRelaxedTypes() {\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    MessageStream<TestMessageEnvelope> inputStream = new MessageStreamImpl<>(mockGraph, mock(OperatorSpec.class));\r\n    // other streams have the same message type T as input stream message type M\r\n    Collection<MessageStream<TestMessageEnvelope>> otherStreams1 = ImmutableList.of(new MessageStreamImpl<>(mockGraph, mock(OperatorSpec.class)), new MessageStreamImpl<>(mockGraph, mock(OperatorSpec.class)));\r\n    // other streams have the same message type T that extends as input stream message type M\r\n    Collection<MessageStream<TestInputMessageEnvelope>> otherStreams2 = ImmutableList.of(new MessageStreamImpl<TestInputMessageEnvelope>(mockGraph, mock(OperatorSpec.class)), new MessageStreamImpl<TestInputMessageEnvelope>(mockGraph, mock(OperatorSpec.class)));\r\n    // other streams have a mix of message types such that T extends input stream message type M\r\n    Collection<MessageStream<TestMessageEnvelope>> otherStreams3 = ImmutableList.of(new MessageStreamImpl<TestMessageEnvelope>(mockGraph, mock(OperatorSpec.class)), // unchecked cast required for the next stream\r\n    (MessageStream) new MessageStreamImpl<TestInputMessageEnvelope>(mockGraph, mock(OperatorSpec.class)));\r\n    // not supported:\r\n    // other streams have a mix of message types such that T extends input stream message type M\r\n    Collection<MessageStream<? extends TestMessageEnvelope>> otherStreams4 = ImmutableList.of(new MessageStreamImpl<TestMessageEnvelope>(mockGraph, mock(OperatorSpec.class)), new MessageStreamImpl<TestInputMessageEnvelope>(mockGraph, mock(OperatorSpec.class)));\r\n    // check if all type combinations compile\r\n    inputStream.merge(otherStreams1);\r\n    inputStream.merge(otherStreams2);\r\n    inputStream.merge(otherStreams3);\r\n    inputStream.merge(otherStreams4);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMergeWithNestedTypes",
  "sourceCode" : "@Test\r\npublic <T> void testMergeWithNestedTypes() {\r\n    class MessageEnvelope<TM> {\r\n    }\r\n    MessageStream<MessageEnvelope<T>> ms1 = mock(MessageStreamImpl.class);\r\n    MessageStream<MessageEnvelope<T>> ms2 = mock(MessageStreamImpl.class);\r\n    MessageStream<MessageEnvelope<T>> ms3 = mock(MessageStreamImpl.class);\r\n    Collection<MessageStream<MessageEnvelope<T>>> otherStreams = ImmutableList.of(ms2, ms3);\r\n    // should compile\r\n    ms1.merge(otherStreams);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMergeAll",
  "sourceCode" : "@Test\r\npublic void testMergeAll() {\r\n    MessageStream<TestMessageEnvelope> input1 = mock(MessageStreamImpl.class);\r\n    MessageStream<TestMessageEnvelope> input2 = mock(MessageStreamImpl.class);\r\n    MessageStream<TestMessageEnvelope> input3 = mock(MessageStreamImpl.class);\r\n    MessageStream.mergeAll(ImmutableList.of(input1, input2, input3));\r\n    ArgumentCaptor<Collection> otherStreamsCaptor = ArgumentCaptor.forClass(Collection.class);\r\n    verify(input1, times(1)).merge(otherStreamsCaptor.capture());\r\n    assertEquals(2, otherStreamsCaptor.getValue().size());\r\n    assertTrue(otherStreamsCaptor.getValue().contains(input2));\r\n    assertTrue(otherStreamsCaptor.getValue().contains(input3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMergeAllWithRelaxedTypes",
  "sourceCode" : "@Test\r\npublic void testMergeAllWithRelaxedTypes() {\r\n    MessageStreamImpl<TestInputMessageEnvelope> input1 = mock(MessageStreamImpl.class);\r\n    MessageStreamImpl<TestMessageEnvelope> input2 = mock(MessageStreamImpl.class);\r\n    Collection<MessageStream<? extends TestMessageEnvelope>> streams = ImmutableList.of(input1, input2);\r\n    // should compile\r\n    MessageStream.mergeAll(streams);\r\n    ArgumentCaptor<Collection> otherStreamsCaptor = ArgumentCaptor.forClass(Collection.class);\r\n    verify(input1, times(1)).merge(otherStreamsCaptor.capture());\r\n    assertEquals(1, otherStreamsCaptor.getValue().size());\r\n    assertTrue(otherStreamsCaptor.getValue().contains(input2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestMessageStreamImpl.java",
  "methodName" : "testMergeAllWithNestedTypes",
  "sourceCode" : "@Test\r\npublic <T> void testMergeAllWithNestedTypes() {\r\n    class MessageEnvelope<TM> {\r\n    }\r\n    MessageStream<MessageEnvelope<T>> input1 = mock(MessageStreamImpl.class);\r\n    MessageStream<MessageEnvelope<T>> input2 = mock(MessageStreamImpl.class);\r\n    MessageStream<MessageEnvelope<T>> input3 = mock(MessageStreamImpl.class);\r\n    // should compile\r\n    MessageStream.mergeAll(ImmutableList.of(input1, input2, input3));\r\n    ArgumentCaptor<Collection> otherStreamsCaptor = ArgumentCaptor.forClass(Collection.class);\r\n    verify(input1, times(1)).merge(otherStreamsCaptor.capture());\r\n    assertEquals(2, otherStreamsCaptor.getValue().size());\r\n    assertTrue(otherStreamsCaptor.getValue().contains(input2));\r\n    assertTrue(otherStreamsCaptor.getValue().contains(input3));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestOperatorSpecGraph.java",
  "methodName" : "testConstructor",
  "sourceCode" : "@Test\r\npublic void testConstructor() {\r\n    OperatorSpecGraph specGraph = new OperatorSpecGraph(mockAppDesc);\r\n    assertEquals(specGraph.getInputOperators(), inputOpSpecMap);\r\n    assertEquals(specGraph.getOutputStreams(), outputStrmMap);\r\n    assertTrue(!specGraph.hasWindowOrJoins());\r\n    assertEquals(specGraph.getAllOperatorSpecs(), this.allOpSpecs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestOperatorSpecGraph.java",
  "methodName" : "testClone",
  "sourceCode" : "@Test\r\npublic void testClone() {\r\n    OperatorSpecGraph operatorSpecGraph = new OperatorSpecGraph(mockAppDesc);\r\n    OperatorSpecGraph clonedSpecGraph = operatorSpecGraph.clone();\r\n    OperatorSpecTestUtils.assertClonedGraph(operatorSpecGraph, clonedSpecGraph);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestOperatorSpecGraph.java",
  "methodName" : "testCloneWithSerializationError",
  "sourceCode" : "@Test(expected = NotSerializableException.class)\r\npublic void testCloneWithSerializationError() throws Throwable {\r\n    OperatorSpec mockFailedOpSpec = PowerMockito.mock(OperatorSpec.class);\r\n    when(mockFailedOpSpec.getOpId()).thenReturn(\"test-failed-op-4\");\r\n    allOpSpecs.add(mockFailedOpSpec);\r\n    inputOpSpecMap.values().stream().findFirst().get().registerNextOperatorSpec(mockFailedOpSpec);\r\n    //failed with serialization error\r\n    try {\r\n        new OperatorSpecGraph(mockAppDesc);\r\n        fail(\"Should have failed with serialization error\");\r\n    } catch (SamzaException se) {\r\n        throw se.getCause();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\operators\\TestOperatorSpecGraph.java",
  "methodName" : "testCloneWithDeserializationError",
  "sourceCode" : "@Test(expected = IOException.class)\r\npublic void testCloneWithDeserializationError() throws Throwable {\r\n    TestDeserializeOperatorSpec testOp = new TestDeserializeOperatorSpec(OperatorSpec.OpCode.MAP, \"test-failed-op-4\");\r\n    this.allOpSpecs.add(testOp);\r\n    inputOpSpecMap.values().stream().findFirst().get().registerNextOperatorSpec(testOp);\r\n    OperatorSpecGraph operatorSpecGraph = new OperatorSpecGraph(mockAppDesc);\r\n    //failed with serialization error\r\n    try {\r\n        operatorSpecGraph.clone();\r\n        fail(\"Should have failed with serialization error\");\r\n    } catch (SamzaException se) {\r\n        throw se.getCause();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testStopByProcessor",
  "sourceCode" : "/**\r\n * Tests stop() method when Container AND JobCoordinator are running\r\n */\r\n@Test\r\npublic void testStopByProcessor() throws InterruptedException {\r\n    JobCoordinator mockJobCoordinator = mock(JobCoordinator.class);\r\n    final CountDownLatch processorListenerStop = new CountDownLatch(1);\r\n    final CountDownLatch processorListenerStart = new CountDownLatch(1);\r\n    TestableStreamProcessor processor = new TestableStreamProcessor(new MapConfig(), new HashMap<>(), mock(StreamTaskFactory.class), new ProcessorLifecycleListener() {\r\n\r\n        @Override\r\n        public void afterStart() {\r\n            processorListenerState.put(ListenerCallback.AFTER_START, true);\r\n            processorListenerStart.countDown();\r\n        }\r\n\r\n        @Override\r\n        public void afterFailure(Throwable t) {\r\n            processorListenerState.put(ListenerCallback.AFTER_FAILURE, true);\r\n        }\r\n\r\n        @Override\r\n        public void afterStop() {\r\n            processorListenerState.put(ListenerCallback.AFTER_STOP, true);\r\n            processorListenerStop.countDown();\r\n        }\r\n\r\n        @Override\r\n        public void beforeStart() {\r\n            processorListenerState.put(ListenerCallback.BEFORE_START, true);\r\n        }\r\n    }, mockJobCoordinator, null);\r\n    final CountDownLatch coordinatorStop = new CountDownLatch(1);\r\n    final Thread jcThread = new Thread(() -> {\r\n        try {\r\n            processor.jobCoordinatorListener.onJobModelExpired();\r\n            processor.jobCoordinatorListener.onNewJobModel(\"1\", getMockJobModel());\r\n            coordinatorStop.await();\r\n            processor.jobCoordinatorListener.onCoordinatorStop();\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n    });\r\n    doAnswer(invocation -> {\r\n        coordinatorStop.countDown();\r\n        return null;\r\n    }).when(mockJobCoordinator).stop();\r\n    doAnswer(invocation -> {\r\n        jcThread.start();\r\n        return null;\r\n    }).when(mockJobCoordinator).start();\r\n    processor.start();\r\n    processorListenerStart.await(10, TimeUnit.SECONDS);\r\n    assertEquals(SamzaContainerStatus.STARTED, processor.getContainerStatus());\r\n    // This block is required for the mockRunloop is actually start.\r\n    // Otherwise, processor.stop gets triggered before mockRunloop begins to block\r\n    processor.runLoopStartForMain.await();\r\n    processor.stop();\r\n    processorListenerStop.await();\r\n    // Assertions on which callbacks are expected to be invoked\r\n    assertTrue(processorListenerState.get(ListenerCallback.BEFORE_START));\r\n    assertTrue(processorListenerState.get(ListenerCallback.AFTER_START));\r\n    assertTrue(processorListenerState.get(ListenerCallback.AFTER_STOP));\r\n    Assert.assertFalse(processorListenerState.get(ListenerCallback.AFTER_FAILURE));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testJobModelExpiredContainerShutdownTimeout",
  "sourceCode" : "/**\r\n * Given that the job model expires, but the container takes too long to stop, a TimeoutException should be propagated\r\n * to the processor lifecycle listener.\r\n */\r\n@Test\r\npublic void testJobModelExpiredContainerShutdownTimeout() throws InterruptedException {\r\n    JobCoordinator mockJobCoordinator = mock(JobCoordinator.class);\r\n    // use this to store the exception passed to afterFailure for the processor lifecycle listener\r\n    AtomicReference<Throwable> afterFailureException = new AtomicReference<>(null);\r\n    TestableStreamProcessor processor = new TestableStreamProcessor(// set a small shutdown timeout so it triggers faster\r\n    new MapConfig(ImmutableMap.of(TaskConfig.TASK_SHUTDOWN_MS, \"1\")), new HashMap<>(), mock(StreamTaskFactory.class), new ProcessorLifecycleListener() {\r\n\r\n        @Override\r\n        public void beforeStart() {\r\n        }\r\n\r\n        @Override\r\n        public void afterStart() {\r\n        }\r\n\r\n        @Override\r\n        public void afterFailure(Throwable t) {\r\n            afterFailureException.set(t);\r\n        }\r\n\r\n        @Override\r\n        public void afterStop() {\r\n        }\r\n    }, mockJobCoordinator, null, // take an extra second to shut down so that task shutdown timeout gets reached\r\n    Duration.of(1, ChronoUnit.SECONDS));\r\n    Thread jcThread = new Thread(() -> {\r\n        // gets processor into rebalance mode so onNewJobModel creates a new container\r\n        processor.jobCoordinatorListener.onJobModelExpired();\r\n        processor.jobCoordinatorListener.onNewJobModel(\"1\", getMockJobModel());\r\n        try {\r\n            // wait for the run loop to be ready before triggering rebalance\r\n            processor.runLoopStartForMain.await();\r\n        } catch (InterruptedException e) {\r\n            e.printStackTrace();\r\n        }\r\n        processor.jobCoordinatorListener.onJobModelExpired();\r\n    });\r\n    doAnswer(invocation -> {\r\n        jcThread.start();\r\n        return null;\r\n    }).when(mockJobCoordinator).start();\r\n    // ensure that the coordinator stop occurred before checking the exception being thrown\r\n    CountDownLatch coordinatorStop = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        processor.jobCoordinatorListener.onCoordinatorStop();\r\n        coordinatorStop.countDown();\r\n        return null;\r\n    }).when(mockJobCoordinator).stop();\r\n    processor.start();\r\n    // make sure the job model expired callback completed\r\n    assertTrue(\"Job coordinator stop not called\", coordinatorStop.await(10, TimeUnit.SECONDS));\r\n    assertNotNull(afterFailureException.get());\r\n    assertTrue(afterFailureException.get() instanceof TimeoutException);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testContainerFailureCorrectlyStopsProcessor",
  "sourceCode" : "/**\r\n * Tests that a failure in container correctly stops a running JobCoordinator and propagates the exception\r\n * through the StreamProcessor\r\n *\r\n * Assertions:\r\n * - JobCoordinator has been stopped from the JobCoordinatorListener callback\r\n * - ProcessorLifecycleListener#afterStop(Throwable) has been invoked w/ non-null Throwable\r\n */\r\n@Test\r\npublic void testContainerFailureCorrectlyStopsProcessor() throws InterruptedException {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    Throwable expectedThrowable = new SamzaException(\"Failure in Container!\");\r\n    AtomicReference<Throwable> actualThrowable = new AtomicReference<>();\r\n    final CountDownLatch runLoopStartedLatch = new CountDownLatch(1);\r\n    RunLoop failingRunLoop = mock(RunLoop.class);\r\n    doAnswer(invocation -> {\r\n        try {\r\n            runLoopStartedLatch.countDown();\r\n            throw expectedThrowable;\r\n        } catch (InterruptedException ie) {\r\n            ie.printStackTrace();\r\n        }\r\n        return null;\r\n    }).when(failingRunLoop).run();\r\n    SamzaContainer mockContainer = StreamProcessorTestUtils.getDummyContainer(failingRunLoop, mock(StreamTask.class));\r\n    final CountDownLatch processorListenerFailed = new CountDownLatch(1);\r\n    TestableStreamProcessor processor = new TestableStreamProcessor(new MapConfig(), new HashMap<>(), mock(StreamTaskFactory.class), new ProcessorLifecycleListener() {\r\n\r\n        @Override\r\n        public void beforeStart() {\r\n            processorListenerState.put(ListenerCallback.BEFORE_START, true);\r\n        }\r\n\r\n        @Override\r\n        public void afterStart() {\r\n            processorListenerState.put(ListenerCallback.AFTER_START, true);\r\n        }\r\n\r\n        @Override\r\n        public void afterStop() {\r\n            processorListenerState.put(ListenerCallback.AFTER_STOP, true);\r\n        }\r\n\r\n        @Override\r\n        public void afterFailure(Throwable t) {\r\n            processorListenerState.put(ListenerCallback.AFTER_FAILURE, true);\r\n            actualThrowable.getAndSet(t);\r\n            processorListenerFailed.countDown();\r\n        }\r\n    }, mockJobCoordinator, mockContainer);\r\n    final CountDownLatch coordinatorStop = new CountDownLatch(1);\r\n    doAnswer(invocation -> {\r\n        coordinatorStop.countDown();\r\n        return null;\r\n    }).when(mockJobCoordinator).stop();\r\n    doAnswer(invocation -> {\r\n        new Thread(() -> {\r\n            try {\r\n                processor.jobCoordinatorListener.onJobModelExpired();\r\n                processor.jobCoordinatorListener.onNewJobModel(\"1\", getMockJobModel());\r\n                coordinatorStop.await();\r\n                processor.jobCoordinatorListener.onCoordinatorStop();\r\n            } catch (InterruptedException e) {\r\n                e.printStackTrace();\r\n            }\r\n        }).start();\r\n        return null;\r\n    }).when(mockJobCoordinator).start();\r\n    processor.start();\r\n    // This block is required for the mockRunloop is actually started.\r\n    // Otherwise, processor.stop gets triggered before mockRunloop begins to block\r\n    runLoopStartedLatch.await();\r\n    assertTrue(\"Container failed and processor listener failed was not invoked within timeout!\", processorListenerFailed.await(30, TimeUnit.SECONDS));\r\n    assertEquals(expectedThrowable, actualThrowable.get());\r\n    assertTrue(processorListenerState.get(ListenerCallback.BEFORE_START));\r\n    assertTrue(processorListenerState.get(ListenerCallback.AFTER_START));\r\n    Assert.assertFalse(processorListenerState.get(ListenerCallback.AFTER_STOP));\r\n    assertTrue(processorListenerState.get(ListenerCallback.AFTER_FAILURE));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testStartOperationShouldBeIdempotent",
  "sourceCode" : "@Test\r\npublic void testStartOperationShouldBeIdempotent() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    Mockito.doNothing().when(mockJobCoordinator).start();\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    StreamProcessor streamProcessor = Mockito.spy(new StreamProcessor(\"TestProcessorId\", new MapConfig(), new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class)));\r\n    assertEquals(State.NEW, streamProcessor.getState());\r\n    streamProcessor.start();\r\n    assertEquals(State.STARTED, streamProcessor.getState());\r\n    streamProcessor.start();\r\n    assertEquals(State.STARTED, streamProcessor.getState());\r\n    Mockito.verify(mockJobCoordinator, Mockito.times(1)).start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testOnJobModelExpiredShouldMakeCorrectStateTransitions",
  "sourceCode" : "@Test\r\npublic void testOnJobModelExpiredShouldMakeCorrectStateTransitions() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    SamzaContainer mockSamzaContainer = Mockito.mock(SamzaContainer.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class));\r\n    /**\r\n     * Without a SamzaContainer running in StreamProcessor and current StreamProcessor state is STARTED,\r\n     * onJobModelExpired should move the state to IN_REBALANCE.\r\n     */\r\n    streamProcessor.start();\r\n    assertEquals(State.STARTED, streamProcessor.getState());\r\n    streamProcessor.jobCoordinatorListener.onJobModelExpired();\r\n    assertEquals(State.IN_REBALANCE, streamProcessor.getState());\r\n    /**\r\n     * When there's initialized SamzaContainer in StreamProcessor and the container shutdown\r\n     * fails in onJobModelExpired. onJobModelExpired should move StreamProcessor to STOPPING\r\n     * state and should shutdown JobCoordinator.\r\n     */\r\n    Mockito.doNothing().when(mockJobCoordinator).start();\r\n    Mockito.doNothing().when(mockJobCoordinator).stop();\r\n    Mockito.doNothing().when(mockSamzaContainer).shutdown();\r\n    Mockito.when(mockSamzaContainer.hasStopped()).thenReturn(false);\r\n    Mockito.when(mockSamzaContainer.getStatus()).thenReturn(SamzaContainerStatus.STARTED).thenReturn(SamzaContainerStatus.STOPPED);\r\n    streamProcessor.container = mockSamzaContainer;\r\n    streamProcessor.state = State.STARTED;\r\n    streamProcessor.jobCoordinatorListener.onJobModelExpired();\r\n    assertEquals(State.STOPPING, streamProcessor.getState());\r\n    Mockito.verify(mockSamzaContainer, Mockito.times(1)).shutdown();\r\n    Mockito.verify(mockJobCoordinator, Mockito.times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testJobModelExpiredDuringAnExistingRebalance",
  "sourceCode" : "@Test\r\npublic void testJobModelExpiredDuringAnExistingRebalance() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    ExecutorService mockExecutorService = Mockito.mock(ExecutorService.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class));\r\n    runJobModelExpireDuringRebalance(streamProcessor, mockExecutorService, false);\r\n    assertEquals(State.IN_REBALANCE, streamProcessor.state);\r\n    assertNotEquals(mockExecutorService, streamProcessor.containerExecutorService);\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).shutdownNow();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testJobModelExpiredDuringAnExistingRebalanceWithContainerInterruptFailed",
  "sourceCode" : "@Test\r\npublic void testJobModelExpiredDuringAnExistingRebalanceWithContainerInterruptFailed() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    ExecutorService mockExecutorService = Mockito.mock(ExecutorService.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class));\r\n    runJobModelExpireDuringRebalance(streamProcessor, mockExecutorService, true);\r\n    assertEquals(State.STOPPING, streamProcessor.state);\r\n    assertEquals(mockExecutorService, streamProcessor.containerExecutorService);\r\n    Mockito.verify(mockExecutorService, Mockito.times(1)).shutdownNow();\r\n    Mockito.verify(mockJobCoordinator, Mockito.times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testOnNewJobModelShouldResultInValidStateTransitions",
  "sourceCode" : "@Test\r\npublic void testOnNewJobModelShouldResultInValidStateTransitions() throws Exception {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    SamzaContainer mockSamzaContainer = Mockito.mock(SamzaContainer.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new TestableStreamProcessor(config, new HashMap<>(), null, lifecycleListener, mockJobCoordinator, mockSamzaContainer);\r\n    streamProcessor.state = State.IN_REBALANCE;\r\n    Mockito.doNothing().when(mockSamzaContainer).run();\r\n    streamProcessor.jobCoordinatorListener.onNewJobModel(\"TestProcessorId\", new JobModel(new MapConfig(), new HashMap<>()));\r\n    Mockito.verify(mockSamzaContainer, Mockito.times(1)).setContainerListener(any());\r\n    Mockito.verify(mockSamzaContainer, Mockito.atMost(1)).run();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testStopShouldBeIdempotent",
  "sourceCode" : "@Test\r\npublic void testStopShouldBeIdempotent() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    SamzaContainer mockSamzaContainer = Mockito.mock(SamzaContainer.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = PowerMockito.spy(new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class)));\r\n    Mockito.doNothing().when(mockJobCoordinator).stop();\r\n    Mockito.doNothing().when(mockSamzaContainer).shutdown();\r\n    Mockito.when(mockSamzaContainer.hasStopped()).thenReturn(false);\r\n    Mockito.when(mockSamzaContainer.getStatus()).thenReturn(SamzaContainerStatus.STARTED).thenReturn(SamzaContainerStatus.STOPPED);\r\n    streamProcessor.state = State.RUNNING;\r\n    streamProcessor.stop();\r\n    assertEquals(State.STOPPING, streamProcessor.state);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testCoordinatorFailureShouldStopTheStreamProcessor",
  "sourceCode" : "@Test\r\npublic void testCoordinatorFailureShouldStopTheStreamProcessor() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    SamzaContainer mockSamzaContainer = Mockito.mock(SamzaContainer.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class));\r\n    Exception failureException = new Exception(\"dummy exception\");\r\n    streamProcessor.container = mockSamzaContainer;\r\n    streamProcessor.state = State.RUNNING;\r\n    streamProcessor.jobCoordinatorListener.onCoordinatorFailure(failureException);\r\n    Mockito.doNothing().when(mockSamzaContainer).shutdown();\r\n    Mockito.when(mockSamzaContainer.hasStopped()).thenReturn(false);\r\n    assertEquals(State.STOPPED, streamProcessor.state);\r\n    Mockito.verify(lifecycleListener).afterFailure(failureException);\r\n    Mockito.verify(mockSamzaContainer).shutdown();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testCoordinatorStopShouldStopTheStreamProcessor",
  "sourceCode" : "@Test\r\npublic void testCoordinatorStopShouldStopTheStreamProcessor() {\r\n    JobCoordinator mockJobCoordinator = Mockito.mock(JobCoordinator.class);\r\n    ProcessorLifecycleListener lifecycleListener = Mockito.mock(ProcessorLifecycleListener.class);\r\n    MapConfig config = new MapConfig(ImmutableMap.of(\"task.shutdown.ms\", \"0\"));\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", config, new HashMap<>(), null, Optional.empty(), Optional.empty(), Optional.empty(), sp -> lifecycleListener, mockJobCoordinator, Mockito.mock(MetadataStore.class));\r\n    streamProcessor.state = State.RUNNING;\r\n    streamProcessor.jobCoordinatorListener.onCoordinatorStop();\r\n    assertEquals(State.STOPPED, streamProcessor.state);\r\n    Mockito.verify(lifecycleListener).afterStop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\processor\\TestStreamProcessor.java",
  "methodName" : "testStreamProcessorWithStreamProcessorListenerFactory",
  "sourceCode" : "@Test\r\npublic void testStreamProcessorWithStreamProcessorListenerFactory() {\r\n    AtomicReference<MockStreamProcessorLifecycleListener> mockListener = new AtomicReference<>();\r\n    StreamProcessor streamProcessor = new StreamProcessor(\"TestProcessorId\", mock(Config.class), new HashMap<>(), mock(TaskFactory.class), Optional.empty(), Optional.empty(), Optional.empty(), sp -> mockListener.updateAndGet(old -> new MockStreamProcessorLifecycleListener(sp)), mock(JobCoordinator.class), Mockito.mock(MetadataStore.class));\r\n    assertEquals(streamProcessor, mockListener.get().processor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestApplicationRunnerMain.java",
  "methodName" : "TestRunOperation",
  "sourceCode" : "@Test\r\npublic void TestRunOperation() {\r\n    assertEquals(0, TestApplicationRunnerInvocationCounts.runCount);\r\n    ApplicationRunnerMain.main(new String[] { \"-config\", \"job.config.loader.factory=org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\", \"-config\", \"job.config.loader.properties.path=\" + getClass().getResource(\"/test.properties\").getPath(), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName()), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_RUNNER_CLASS, TestApplicationRunnerInvocationCounts.class.getName()) });\r\n    assertEquals(1, TestApplicationRunnerInvocationCounts.runCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestApplicationRunnerMain.java",
  "methodName" : "TestKillOperation",
  "sourceCode" : "@Test\r\npublic void TestKillOperation() {\r\n    assertEquals(0, TestApplicationRunnerInvocationCounts.killCount);\r\n    ApplicationRunnerMain.main(new String[] { \"-config\", \"job.config.loader.factory=org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\", \"-config\", \"job.config.loader.properties.path=\" + getClass().getResource(\"/test.properties\").getPath(), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName()), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_RUNNER_CLASS, TestApplicationRunnerInvocationCounts.class.getName()), \"--operation=kill\" });\r\n    assertEquals(1, TestApplicationRunnerInvocationCounts.killCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestApplicationRunnerMain.java",
  "methodName" : "TestStatusOperation",
  "sourceCode" : "@Test\r\npublic void TestStatusOperation() {\r\n    assertEquals(0, TestApplicationRunnerInvocationCounts.statusCount);\r\n    ApplicationRunnerMain.main(new String[] { \"-config\", \"job.config.loader.factory=org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\", \"-config\", \"job.config.loader.properties.path=\" + getClass().getResource(\"/test.properties\").getPath(), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName()), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_RUNNER_CLASS, TestApplicationRunnerInvocationCounts.class.getName()), \"--operation=status\" });\r\n    assertEquals(1, TestApplicationRunnerInvocationCounts.statusCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestApplicationRunnerMain.java",
  "methodName" : "TestLoadConfig",
  "sourceCode" : "@Test\r\npublic void TestLoadConfig() {\r\n    ApplicationRunnerMain.ApplicationRunnerCommandLine cmdLine = new ApplicationRunnerMain.ApplicationRunnerCommandLine();\r\n    OptionSet options = cmdLine.parser().parse(\"-config\", \"job.config.loader.factory=org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\", \"-config\", \"job.config.loader.properties.path=\" + getClass().getResource(\"/test.properties\").getPath(), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName()), \"-config\", String.format(\"%s=%s\", ApplicationConfig.APP_RUNNER_CLASS, TestApplicationRunnerInvocationCounts.class.getName()));\r\n    Config actual = cmdLine.loadConfig(options);\r\n    Config expected = new MapConfig(ImmutableMap.of(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\", ConfigLoaderFactory.CONFIG_LOADER_PROPERTIES_PREFIX + \"path\", getClass().getResource(\"/test.properties\").getPath(), ApplicationConfig.APP_CLASS, MockStreamApplication.class.getName(), ApplicationConfig.APP_RUNNER_CLASS, TestApplicationRunnerInvocationCounts.class.getName()));\r\n    assertEquals(expected, actual);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestApplicationRunners.java",
  "methodName" : "testGetAppRunner",
  "sourceCode" : "@Test\r\npublic void testGetAppRunner() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(ApplicationConfig.APP_RUNNER_CLASS, MockApplicationRunner.class.getName());\r\n    Config config = new MapConfig(configMap);\r\n    StreamApplication app = mock(StreamApplication.class);\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(app, config);\r\n    assertTrue(appRunner instanceof MockApplicationRunner);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestClusterBasedProcessorLifecycleListener.java",
  "methodName" : "testLifecycleListenerBeforeStart",
  "sourceCode" : "@Test\r\npublic void testLifecycleListenerBeforeStart() {\r\n    clusterBasedProcessorLifecycleListener.beforeStart();\r\n    Mockito.verify(clusterBasedProcessorLifecycleListener).addJVMShutdownHook(any(Thread.class));\r\n    Mockito.verify(processorLifecycleListener).beforeStart();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestClusterBasedProcessorLifecycleListener.java",
  "methodName" : "testLifecycleListenerAfterStart",
  "sourceCode" : "@Test\r\npublic void testLifecycleListenerAfterStart() {\r\n    clusterBasedProcessorLifecycleListener.afterStart();\r\n    Mockito.verify(processorLifecycleListener).afterStart();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestClusterBasedProcessorLifecycleListener.java",
  "methodName" : "testLifecycleListenerAfterStop",
  "sourceCode" : "@Test\r\npublic void testLifecycleListenerAfterStop() {\r\n    clusterBasedProcessorLifecycleListener.afterStop();\r\n    Mockito.verify(processorLifecycleListener).afterStop();\r\n    Mockito.verify(clusterBasedProcessorLifecycleListener).removeJVMShutdownHook(any(Thread.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestClusterBasedProcessorLifecycleListener.java",
  "methodName" : "testLifecycleListenerAfterFailure",
  "sourceCode" : "@Test\r\npublic void testLifecycleListenerAfterFailure() {\r\n    SamzaException e = new SamzaException(\"Should call afterFailure\");\r\n    clusterBasedProcessorLifecycleListener.afterFailure(e);\r\n    Mockito.verify(processorLifecycleListener).afterFailure(e);\r\n    Mockito.verify(clusterBasedProcessorLifecycleListener).removeJVMShutdownHook(any(Thread.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestClusterBasedProcessorLifecycleListener.java",
  "methodName" : "testShutdownHookInvokesShutdownHookCallback",
  "sourceCode" : "@Test\r\npublic void testShutdownHookInvokesShutdownHookCallback() {\r\n    doAnswer(invocation -> {\r\n        // Simulate call to container.shutdown()\r\n        clusterBasedProcessorLifecycleListener.afterStop();\r\n        return null;\r\n    }).when(mockShutdownHookCallback).run();\r\n    // call beforeStart to setup shutdownHook\r\n    clusterBasedProcessorLifecycleListener.beforeStart();\r\n    // Simulating shutdown hook invocation by JVM.\r\n    // The shutdownHookThread should return immediately and shutdown\r\n    // cleanly if mockShutdownHookCallback.run() is invoked.\r\n    clusterBasedProcessorLifecycleListener.getShutdownHookThread().run();\r\n    Mockito.verify(mockShutdownHookCallback).run();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestContainerLaunchUtil.java",
  "methodName" : "testRunWithException",
  "sourceCode" : "@Test\r\npublic void testRunWithException() throws Exception {\r\n    final CountDownLatch completionLatch = new CountDownLatch(1);\r\n    PowerMockito.mockStatic(ContainerLaunchUtil.class);\r\n    PowerMockito.doReturn(mock(CoordinatorStreamStore.class)).when(ContainerLaunchUtil.class, \"buildCoordinatorStreamStore\", eq(CONFIG), any());\r\n    PowerMockito.doAnswer(invocation -> {\r\n        completionLatch.countDown();\r\n        return null;\r\n    }).when(ContainerLaunchUtil.class, \"exitProcess\", eq(1));\r\n    PowerMockito.doCallRealMethod().when(ContainerLaunchUtil.class, \"run\", eq(APP_DESC), eq(JOB_NAME), eq(JOB_ID), eq(CONTAINER_ID), any(), any(), eq(JOB_MODEL), eq(CONFIG), any());\r\n    int exitCode = ContainerLaunchUtil.run(APP_DESC, JOB_NAME, JOB_ID, CONTAINER_ID, Optional.empty(), Optional.empty(), JOB_MODEL, CONFIG, Optional.empty());\r\n    assertEquals(1, exitCode);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestContainerLaunchUtil.java",
  "methodName" : "testRunSuccessfully",
  "sourceCode" : "@Test\r\npublic void testRunSuccessfully() throws Exception {\r\n    int exitCode = 0;\r\n    final CountDownLatch completionLatch = new CountDownLatch(1);\r\n    PowerMockito.mockStatic(ContainerLaunchUtil.class);\r\n    PowerMockito.doReturn(mock(CoordinatorStreamStore.class)).when(ContainerLaunchUtil.class, \"buildCoordinatorStreamStore\", eq(CONFIG), any());\r\n    PowerMockito.doAnswer(invocation -> {\r\n        completionLatch.countDown();\r\n        return null;\r\n    }).when(ContainerLaunchUtil.class, \"exitProcess\", eq(exitCode));\r\n    PowerMockito.doReturn(exitCode).when(ContainerLaunchUtil.class, \"run\", eq(APP_DESC), eq(JOB_NAME), eq(JOB_ID), eq(CONTAINER_ID), any(), any(), eq(JOB_MODEL), eq(CONFIG), any());\r\n    PowerMockito.doCallRealMethod().when(ContainerLaunchUtil.class, \"run\", eq(APP_DESC), eq(JOB_NAME), eq(JOB_ID), eq(CONTAINER_ID), any(), any(), eq(JOB_MODEL));\r\n    ContainerLaunchUtil.run(APP_DESC, JOB_NAME, JOB_ID, CONTAINER_ID, Optional.empty(), Optional.empty(), JOB_MODEL);\r\n    assertTrue(completionLatch.await(1, TimeUnit.SECONDS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunStreamTask",
  "sourceCode" : "@Test\r\npublic void testRunStreamTask() throws Exception {\r\n    final Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    cfgs.put(ApplicationConfig.APP_NAME, \"test-app\");\r\n    cfgs.put(ApplicationConfig.APP_ID, \"test-appId\");\r\n    config = new MapConfig(cfgs);\r\n    mockApp = new LegacyTaskApplication(IdentityStreamTask.class.getName());\r\n    prepareTest();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    CoordinatorStreamStore metadataStore = mock(CoordinatorStreamStore.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        listener.afterStop();\r\n        return null;\r\n    }).when(sp).start();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), any(CoordinatorStreamStore.class));\r\n    doReturn(metadataStore).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    doReturn(ApplicationStatus.SuccessfulFinish).when(runner).status();\r\n    runner.run(externalContext);\r\n    verify(metadataStore).init();\r\n    verify(metadataStore).close();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, runner.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunStreamTaskWithoutExternalContext",
  "sourceCode" : "@Test\r\npublic void testRunStreamTaskWithoutExternalContext() throws Exception {\r\n    final Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    cfgs.put(ApplicationConfig.APP_NAME, \"test-app\");\r\n    cfgs.put(ApplicationConfig.APP_ID, \"test-appId\");\r\n    config = new MapConfig(cfgs);\r\n    mockApp = new LegacyTaskApplication(IdentityStreamTask.class.getName());\r\n    prepareTest();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    CoordinatorStreamStore metadataStore = mock(CoordinatorStreamStore.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        listener.afterStop();\r\n        return null;\r\n    }).when(sp).start();\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.empty()), any(CoordinatorStreamStore.class));\r\n    doReturn(metadataStore).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    doReturn(ApplicationStatus.SuccessfulFinish).when(runner).status();\r\n    runner.run();\r\n    verify(metadataStore).init();\r\n    verify(metadataStore).close();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, runner.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunComplete",
  "sourceCode" : "@Test\r\npublic void testRunComplete() throws Exception {\r\n    Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    config = new MapConfig(cfgs);\r\n    ProcessorLifecycleListenerFactory mockFactory = (pContext, cfg) -> mock(ProcessorLifecycleListener.class);\r\n    mockApp = (StreamApplication) appDesc -> appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    prepareTest();\r\n    // return the jobConfigs from the planner\r\n    doReturn(Collections.singletonList(new JobConfig(new MapConfig(config)))).when(localPlanner).prepareJobs();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    CoordinatorStreamStore coordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        listener.afterStop();\r\n        return null;\r\n    }).when(sp).start();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), any(CoordinatorStreamStore.class));\r\n    doReturn(coordinatorStreamStore).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    runner.run(externalContext);\r\n    runner.waitForFinish();\r\n    verify(coordinatorStreamStore).init();\r\n    verify(coordinatorStreamStore).close();\r\n    assertEquals(runner.status(), ApplicationStatus.SuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunCompleteWithouCoordinatorStreamStore",
  "sourceCode" : "@Test\r\npublic void testRunCompleteWithouCoordinatorStreamStore() throws Exception {\r\n    Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    config = new MapConfig(cfgs);\r\n    ProcessorLifecycleListenerFactory mockFactory = (pContext, cfg) -> mock(ProcessorLifecycleListener.class);\r\n    mockApp = (StreamApplication) appDesc -> appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    prepareTest();\r\n    // return the jobConfigs from the planner\r\n    doReturn(Collections.singletonList(new JobConfig(new MapConfig(config)))).when(localPlanner).prepareJobs();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        listener.afterStop();\r\n        return null;\r\n    }).when(sp).start();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), eq(null));\r\n    doReturn(null).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    runner.run(externalContext);\r\n    runner.waitForFinish();\r\n    assertEquals(runner.status(), ApplicationStatus.SuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunFailure",
  "sourceCode" : "@Test\r\npublic void testRunFailure() throws Exception {\r\n    Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.PROCESSOR_ID, \"0\");\r\n    config = new MapConfig(cfgs);\r\n    ProcessorLifecycleListenerFactory mockFactory = (pContext, cfg) -> mock(ProcessorLifecycleListener.class);\r\n    mockApp = (StreamApplication) appDesc -> appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    prepareTest();\r\n    // return the jobConfigs from the planner\r\n    doReturn(Collections.singletonList(new JobConfig(new MapConfig(config)))).when(localPlanner).prepareJobs();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    CoordinatorStreamStore coordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        throw new Exception(\"test failure\");\r\n    }).when(sp).start();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), any(CoordinatorStreamStore.class));\r\n    doReturn(coordinatorStreamStore).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    try {\r\n        runner.run(externalContext);\r\n        runner.waitForFinish();\r\n    } catch (Throwable th) {\r\n        assertNotNull(th);\r\n    }\r\n    verify(coordinatorStreamStore).init();\r\n    verify(coordinatorStreamStore, never()).close();\r\n    assertEquals(runner.status(), ApplicationStatus.UnsuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testKill",
  "sourceCode" : "@Test\r\npublic void testKill() throws Exception {\r\n    Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    config = new MapConfig(cfgs);\r\n    ProcessorLifecycleListenerFactory mockFactory = (pContext, cfg) -> mock(ProcessorLifecycleListener.class);\r\n    mockApp = (StreamApplication) appDesc -> appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    prepareTest();\r\n    // return the jobConfigs from the planner\r\n    doReturn(Collections.singletonList(new JobConfig(new MapConfig(config)))).when(localPlanner).prepareJobs();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    CoordinatorStreamStore coordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        return null;\r\n    }).when(sp).start();\r\n    doAnswer(new Answer() {\r\n\r\n        private int count = 0;\r\n\r\n        @Override\r\n        public Object answer(InvocationOnMock invocation) throws Throwable {\r\n            if (++count == 1) {\r\n                ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n                listener.afterStop();\r\n                return null;\r\n            }\r\n            return null;\r\n        }\r\n    }).when(sp).stop();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), any(CoordinatorStreamStore.class));\r\n    doReturn(coordinatorStreamStore).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    runner.run(externalContext);\r\n    runner.kill();\r\n    verify(coordinatorStreamStore).init();\r\n    verify(coordinatorStreamStore, atLeastOnce()).close();\r\n    assertEquals(runner.status(), ApplicationStatus.SuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testKillWithoutCoordinatorStream",
  "sourceCode" : "@Test\r\npublic void testKillWithoutCoordinatorStream() throws Exception {\r\n    Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    config = new MapConfig(cfgs);\r\n    ProcessorLifecycleListenerFactory mockFactory = (pContext, cfg) -> mock(ProcessorLifecycleListener.class);\r\n    mockApp = (StreamApplication) appDesc -> appDesc.withProcessorLifecycleListenerFactory(mockFactory);\r\n    prepareTest();\r\n    // return the jobConfigs from the planner\r\n    doReturn(Collections.singletonList(new JobConfig(new MapConfig(config)))).when(localPlanner).prepareJobs();\r\n    StreamProcessor sp = mock(StreamProcessor.class);\r\n    ArgumentCaptor<StreamProcessor.StreamProcessorLifecycleListenerFactory> captor = ArgumentCaptor.forClass(StreamProcessor.StreamProcessorLifecycleListenerFactory.class);\r\n    doAnswer(i -> {\r\n        ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n        listener.afterStart();\r\n        return null;\r\n    }).when(sp).start();\r\n    doAnswer(new Answer() {\r\n\r\n        private int count = 0;\r\n\r\n        @Override\r\n        public Object answer(InvocationOnMock invocation) throws Throwable {\r\n            if (++count == 1) {\r\n                ProcessorLifecycleListener listener = captor.getValue().createInstance(sp);\r\n                listener.afterStop();\r\n                return null;\r\n            }\r\n            return null;\r\n        }\r\n    }).when(sp).stop();\r\n    ExternalContext externalContext = mock(ExternalContext.class);\r\n    doReturn(sp).when(runner).createStreamProcessor(anyObject(), anyObject(), captor.capture(), eq(Optional.of(externalContext)), any(CoordinatorStreamStore.class));\r\n    doReturn(null).when(runner).createCoordinatorStreamStore(any(Config.class));\r\n    runner.run(externalContext);\r\n    runner.kill();\r\n    assertEquals(runner.status(), ApplicationStatus.SuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testWaitForFinishReturnsBeforeTimeout",
  "sourceCode" : "@Test\r\npublic void testWaitForFinishReturnsBeforeTimeout() {\r\n    long timeoutInMs = 1000;\r\n    runner.getShutdownLatch().countDown();\r\n    boolean finished = runner.waitForFinish(Duration.ofMillis(timeoutInMs));\r\n    assertTrue(\"Application did not finish before the timeout.\", finished);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testWaitForFinishTimesout",
  "sourceCode" : "@Test\r\npublic void testWaitForFinishTimesout() {\r\n    long timeoutInMs = 100;\r\n    boolean finished = runner.waitForFinish(Duration.ofMillis(timeoutInMs));\r\n    assertFalse(\"Application finished before the timeout.\", finished);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testCreateProcessorIdShouldReturnProcessorIdDefinedInConfiguration",
  "sourceCode" : "@Test\r\npublic void testCreateProcessorIdShouldReturnProcessorIdDefinedInConfiguration() {\r\n    String processorId = \"testProcessorId\";\r\n    MapConfig configMap = new MapConfig(ImmutableMap.of(ApplicationConfig.PROCESSOR_ID, processorId));\r\n    String actualProcessorId = LocalApplicationRunner.createProcessorId(new ApplicationConfig(configMap));\r\n    assertEquals(processorId, actualProcessorId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testCreateProcessorIdShouldInvokeProcessorIdGeneratorDefinedInConfiguration",
  "sourceCode" : "@Test\r\npublic void testCreateProcessorIdShouldInvokeProcessorIdGeneratorDefinedInConfiguration() {\r\n    String processorId = \"testProcessorId\";\r\n    MapConfig configMap = new MapConfig(ImmutableMap.of(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, MockProcessorIdGenerator.class.getCanonicalName()));\r\n    String actualProcessorId = LocalApplicationRunner.createProcessorId(new ApplicationConfig(configMap));\r\n    assertEquals(processorId, actualProcessorId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testCreateProcessorIdShouldThrowExceptionWhenProcessorIdAndGeneratorAreNotDefined",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testCreateProcessorIdShouldThrowExceptionWhenProcessorIdAndGeneratorAreNotDefined() {\r\n    ApplicationConfig mockConfig = Mockito.mock(ApplicationConfig.class);\r\n    Mockito.when(mockConfig.getProcessorId()).thenReturn(null);\r\n    LocalApplicationRunner.createProcessorId(mockConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunIdForBatch",
  "sourceCode" : "/**\r\n * For app.mode=BATCH ensure that the run.id generation utils --\r\n * DistributedLock, ClusterMembership and MetadataStore are created.\r\n * Also ensure that metadataStore.put is invoked (to write the run.id)\r\n */\r\n@Test\r\npublic void testRunIdForBatch() throws Exception {\r\n    final Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_MODE, \"BATCH\");\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    cfgs.put(JobConfig.JOB_NAME, \"test-task-job\");\r\n    cfgs.put(JobConfig.JOB_ID, \"jobId\");\r\n    config = new MapConfig(cfgs);\r\n    mockApp = new LegacyTaskApplication(IdentityStreamTask.class.getName());\r\n    prepareTestForRunId();\r\n    runner.run();\r\n    verify(coordinationUtils, Mockito.times(1)).getLock(CoordinationConstants.RUNID_LOCK_ID);\r\n    verify(clusterMembership, Mockito.times(1)).getNumberOfProcessors();\r\n    verify(metadataStore, Mockito.times(1)).put(eq(CoordinationConstants.RUNID_STORE_KEY), any(byte[].class));\r\n    verify(metadataStore, Mockito.times(1)).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testRunIdForStream",
  "sourceCode" : "/**\r\n * For app.mode=STREAM ensure that the run.id generation utils --\r\n * DistributedLock, ClusterMembership and MetadataStore are NOT created.\r\n * Also ensure that metadataStore.put is NOT invoked\r\n */\r\n@Test\r\npublic void testRunIdForStream() throws Exception {\r\n    final Map<String, String> cfgs = new HashMap<>();\r\n    cfgs.put(ApplicationConfig.APP_MODE, \"STREAM\");\r\n    cfgs.put(ApplicationConfig.APP_PROCESSOR_ID_GENERATOR_CLASS, UUIDGenerator.class.getName());\r\n    cfgs.put(JobConfig.JOB_NAME, \"test-task-job\");\r\n    cfgs.put(JobConfig.JOB_ID, \"jobId\");\r\n    config = new MapConfig(cfgs);\r\n    mockApp = new LegacyTaskApplication(IdentityStreamTask.class.getName());\r\n    prepareTestForRunId();\r\n    runner.run();\r\n    verify(coordinationUtils, Mockito.times(0)).getLock(CoordinationConstants.RUNID_LOCK_ID);\r\n    verify(coordinationUtils, Mockito.times(0)).getClusterMembership();\r\n    verify(clusterMembership, Mockito.times(0)).getNumberOfProcessors();\r\n    verify(metadataStore, Mockito.times(0)).put(eq(CoordinationConstants.RUNID_STORE_KEY), any(byte[].class));\r\n    verify(metadataStore, Mockito.times(0)).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testGetCoordinatorStreamStoreFactoryWithoutJobCoordinatorSystem",
  "sourceCode" : "/**\r\n * Default metadata store factory should be null if no job coordinator system defined and the default\r\n * ZkJobCoordinator is used.\r\n */\r\n@Test\r\npublic void testGetCoordinatorStreamStoreFactoryWithoutJobCoordinatorSystem() {\r\n    Optional<MetadataStoreFactory> metadataStoreFactory = LocalApplicationRunner.getDefaultCoordinatorStreamStoreFactory(new MapConfig());\r\n    assertFalse(metadataStoreFactory.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testGetCoordinatorStreamStoreFactoryWithJobCoordinatorSystem",
  "sourceCode" : "/**\r\n * Default metadata store factory should not be null if job coordinator system defined and the default\r\n * ZkJobCoordinator is used.\r\n */\r\n@Test\r\npublic void testGetCoordinatorStreamStoreFactoryWithJobCoordinatorSystem() {\r\n    Optional<MetadataStoreFactory> metadataStoreFactory = LocalApplicationRunner.getDefaultCoordinatorStreamStoreFactory(new MapConfig(ImmutableMap.of(JobConfig.JOB_COORDINATOR_SYSTEM, \"test-system\")));\r\n    assertTrue(metadataStoreFactory.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testGetCoordinatorStreamStoreFactoryWithDefaultSystem",
  "sourceCode" : "/**\r\n * Default metadata store factory should not be null if default system defined and the default\r\n * ZkJobCoordinator is used.\r\n */\r\n@Test\r\npublic void testGetCoordinatorStreamStoreFactoryWithDefaultSystem() {\r\n    Optional<MetadataStoreFactory> metadataStoreFactory = LocalApplicationRunner.getDefaultCoordinatorStreamStoreFactory(new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, \"test-system\")));\r\n    assertTrue(metadataStoreFactory.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testGetCoordinatorStreamStoreFactoryWithNonZkJobCoordinator",
  "sourceCode" : "/**\r\n * Default metadata store factory be null if job coordinator system or default system defined and a non ZkJobCoordinator\r\n * job coordinator is used.\r\n */\r\n@Test\r\npublic void testGetCoordinatorStreamStoreFactoryWithNonZkJobCoordinator() {\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(JobConfig.JOB_DEFAULT_SYSTEM, \"test-system\", JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, PassthroughJobCoordinatorFactory.class.getName()));\r\n    Optional<MetadataStoreFactory> metadataStoreFactory = LocalApplicationRunner.getDefaultCoordinatorStreamStoreFactory(mapConfig);\r\n    assertFalse(metadataStoreFactory.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testCreateCoordinatorStreamWithCoordinatorFactory",
  "sourceCode" : "/**\r\n * Underlying coordinator stream should be created if using CoordinatorStreamMetadataStoreFactory\r\n */\r\n@Test\r\npublic void testCreateCoordinatorStreamWithCoordinatorFactory() throws Exception {\r\n    CoordinatorStreamStore coordinatorStreamStore = mock(CoordinatorStreamStore.class);\r\n    CoordinatorStreamMetadataStoreFactory coordinatorStreamMetadataStoreFactory = mock(CoordinatorStreamMetadataStoreFactory.class);\r\n    doReturn(coordinatorStreamStore).when(coordinatorStreamMetadataStoreFactory).getMetadataStore(anyString(), any(Config.class), any(MetricsRegistry.class));\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    PowerMockito.whenNew(SystemAdmins.class).withAnyArguments().thenReturn(systemAdmins);\r\n    LocalApplicationRunner localApplicationRunner = spy(new LocalApplicationRunner(mockApp, config, coordinatorStreamMetadataStoreFactory));\r\n    // create store only if successful in creating the underlying coordinator stream\r\n    doReturn(true).when(localApplicationRunner).createUnderlyingCoordinatorStream(eq(config));\r\n    assertEquals(coordinatorStreamStore, localApplicationRunner.createCoordinatorStreamStore(config));\r\n    verify(localApplicationRunner).createUnderlyingCoordinatorStream(eq(config));\r\n    // do not create store if creating the underlying coordinator stream fails\r\n    doReturn(false).when(localApplicationRunner).createUnderlyingCoordinatorStream(eq(config));\r\n    assertNull(localApplicationRunner.createCoordinatorStreamStore(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestLocalApplicationRunner.java",
  "methodName" : "testCreateCoordinatorStreamWithoutCoordinatorFactory",
  "sourceCode" : "/**\r\n * Underlying coordinator stream should not be created if not using CoordinatorStreamMetadataStoreFactory\r\n */\r\n@Test\r\npublic void testCreateCoordinatorStreamWithoutCoordinatorFactory() throws Exception {\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    PowerMockito.whenNew(SystemAdmins.class).withAnyArguments().thenReturn(systemAdmins);\r\n    LocalApplicationRunner localApplicationRunner = spy(new LocalApplicationRunner(mockApp, config, new InMemoryMetadataStoreFactory()));\r\n    doReturn(false).when(localApplicationRunner).createUnderlyingCoordinatorStream(eq(config));\r\n    MetadataStore coordinatorStreamStore = localApplicationRunner.createCoordinatorStreamStore(config);\r\n    assertTrue(coordinatorStreamStore instanceof InMemoryMetadataStore);\r\n    // creating underlying coordinator stream should not be called for other coordinator stream metadata store types.\r\n    verify(localApplicationRunner, never()).createUnderlyingCoordinatorStream(eq(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testWaitForFinishReturnsBeforeTimeout",
  "sourceCode" : "@Test\r\npublic void testWaitForFinishReturnsBeforeTimeout() {\r\n    doReturn(ApplicationStatus.SuccessfulFinish).when(runner).getApplicationStatus(any(JobConfig.class));\r\n    boolean finished = runner.waitForFinish(Duration.ofMillis(5000));\r\n    assertTrue(\"Application did not finish before the timeout.\", finished);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testWaitForFinishTimesout",
  "sourceCode" : "@Test\r\npublic void testWaitForFinishTimesout() {\r\n    doReturn(ApplicationStatus.Running).when(runner).getApplicationStatus(any(JobConfig.class));\r\n    boolean finished = runner.waitForFinish(Duration.ofMillis(1000));\r\n    assertFalse(\"Application finished before the timeout.\", finished);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testRunWithConfigLoaderFactoryPresent",
  "sourceCode" : "@Test\r\npublic void testRunWithConfigLoaderFactoryPresent() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(ApplicationConfig.APP_NAME, \"test-app\");\r\n    config.put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\");\r\n    config.put(JobConfig.STREAM_JOB_FACTORY_CLASS, MockStreamJobFactory.class.getName());\r\n    runner = new RemoteApplicationRunner(null, new MapConfig(config));\r\n    runner.run(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testGetStatus",
  "sourceCode" : "@Test\r\npublic void testGetStatus() {\r\n    Map<String, String> m = new HashMap<>();\r\n    m.put(JobConfig.JOB_NAME, \"jobName\");\r\n    m.put(JobConfig.STREAM_JOB_FACTORY_CLASS, MockStreamJobFactory.class.getName());\r\n    m.put(JobConfig.JOB_ID, \"newJob\");\r\n    StreamApplication userApp = appDesc -> {\r\n    };\r\n    runner = spy(new RemoteApplicationRunner(userApp, new MapConfig(m)));\r\n    Assert.assertEquals(ApplicationStatus.New, runner.getApplicationStatus(new JobConfig(new MapConfig(m))));\r\n    m.put(JobConfig.JOB_ID, \"runningJob\");\r\n    runner = spy(new RemoteApplicationRunner(userApp, new MapConfig(m)));\r\n    Assert.assertEquals(ApplicationStatus.Running, runner.getApplicationStatus(new JobConfig(new MapConfig(m))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testLocalRunWithConfigLoaderFactoryPresent",
  "sourceCode" : "@Test\r\npublic void testLocalRunWithConfigLoaderFactoryPresent() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(ApplicationConfig.APP_NAME, \"test-app\");\r\n    config.put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\");\r\n    config.put(JobConfig.STREAM_JOB_FACTORY_CLASS, ProcessJobFactory.class.getName());\r\n    try {\r\n        runner.run(null);\r\n        Assert.fail(\"Should have went to the planning phase\");\r\n    } catch (SamzaException e) {\r\n        Assert.assertFalse(e.getMessage().contains(\"No jobs to run.\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\runtime\\TestRemoteApplicationRunner.java",
  "methodName" : "testLocalGetStatus",
  "sourceCode" : "//TODO: SAMZA-2738: Return real status for local jobs after avoiding recreating the Job in runner.status()\r\n@Ignore\r\n@Test\r\npublic void testLocalGetStatus() {\r\n    Map<String, String> m = new HashMap<>();\r\n    m.put(JobConfig.JOB_NAME, \"jobName\");\r\n    m.put(JobConfig.STREAM_JOB_FACTORY_CLASS, ProcessJobFactory.class.getName());\r\n    m.put(JobConfig.JOB_ID, \"newJob\");\r\n    StreamApplication userApp = appDesc -> {\r\n    };\r\n    runner = spy(new RemoteApplicationRunner(userApp, new MapConfig(m)));\r\n    Assert.assertEquals(ApplicationStatus.New, runner.getApplicationStatus(new JobConfig(new MapConfig(m))));\r\n    m.put(JobConfig.JOB_ID, \"runningJob\");\r\n    runner = spy(new RemoteApplicationRunner(userApp, new MapConfig(m)));\r\n    Assert.assertEquals(ApplicationStatus.Running, runner.getApplicationStatus(new JobConfig(new MapConfig(m))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestCallbackSchedulerImpl.java",
  "methodName" : "testScheduleCallback",
  "sourceCode" : "/**\r\n * scheduleCallback should delegate to the inner scheduler\r\n */\r\n@Test\r\npublic void testScheduleCallback() {\r\n    @SuppressWarnings(\"unchecked\")\r\n    ScheduledCallback<String> stringCallback = mock(ScheduledCallback.class);\r\n    scheduler.scheduleCallback(\"string_key\", 123, stringCallback);\r\n    verify(epochTimeScheduler).setTimer(\"string_key\", 123, stringCallback);\r\n    // check some other type of key\r\n    @SuppressWarnings(\"unchecked\")\r\n    ScheduledCallback<Integer> intCallback = mock(ScheduledCallback.class);\r\n    scheduler.scheduleCallback(777, 456, intCallback);\r\n    verify(epochTimeScheduler).setTimer(777, 456, intCallback);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestCallbackSchedulerImpl.java",
  "methodName" : "testDeleteCallback",
  "sourceCode" : "/**\r\n * deleteCallback should delegate to the inner scheduler\r\n */\r\n@Test\r\npublic void testDeleteCallback() {\r\n    scheduler.deleteCallback(\"string_key\");\r\n    verify(epochTimeScheduler).deleteTimer(\"string_key\");\r\n    // check some other type of key\r\n    scheduler.deleteCallback(777);\r\n    verify(epochTimeScheduler).deleteTimer(777);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testDuplicateTimerWithCancelableCallback",
  "sourceCode" : "@Test\r\n@SuppressWarnings(\"unchecked\")\r\npublic void testDuplicateTimerWithCancelableCallback() {\r\n    final String timerKey = \"timer-1\";\r\n    ScheduledFuture mockScheduledFuture1 = mock(ScheduledFuture.class);\r\n    ScheduledFuture mockScheduledFuture2 = mock(ScheduledFuture.class);\r\n    ScheduledExecutorService executor = mock(ScheduledExecutorService.class);\r\n    when(mockScheduledFuture1.cancel(anyBoolean())).thenReturn(true);\r\n    when(executor.schedule((Runnable) anyObject(), anyLong(), anyObject())).thenReturn(mockScheduledFuture1).thenAnswer(invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        Runnable runnable = (Runnable) args[0];\r\n        runnable.run();\r\n        return mockScheduledFuture2;\r\n    });\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(executor);\r\n    long timestamp = System.currentTimeMillis() + 10000;\r\n    ScheduledCallback<String> expectedScheduledCallback = mock(ScheduledCallback.class);\r\n    scheduler.setTimer(timerKey, timestamp, mock(ScheduledCallback.class));\r\n    scheduler.setTimer(timerKey, timestamp, expectedScheduledCallback);\r\n    // verify the interactions with the scheduled future and the scheduler\r\n    verify(executor, times(2)).schedule((Runnable) anyObject(), anyLong(), anyObject());\r\n    verify(mockScheduledFuture1, times(1)).cancel(anyBoolean());\r\n    // verify the ready timer and its callback contents to ensure the second invocation callback overwrites the\r\n    // first callback\r\n    Set<Map.Entry<EpochTimeScheduler.TimerKey<?>, ScheduledCallback>> readyTimers = scheduler.removeReadyTimers().entrySet();\r\n    assertEquals(\"Only one timer should be ready to be fired\", readyTimers.size(), 1);\r\n    Map.Entry<EpochTimeScheduler.TimerKey<?>, ScheduledCallback> timerEntry = readyTimers.iterator().next();\r\n    assertEquals(\"Expected the scheduled callback from the second invocation\", timerEntry.getValue(), expectedScheduledCallback);\r\n    assertEquals(\"Expected timer-1 as the key for ready timer\", timerEntry.getKey().getKey(), timerKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testDuplicateTimerWithUnsuccessfulCancellation",
  "sourceCode" : "@Test\r\n@SuppressWarnings(\"unchecked\")\r\npublic void testDuplicateTimerWithUnsuccessfulCancellation() {\r\n    final String timerKey = \"timer-1\";\r\n    ScheduledFuture mockScheduledFuture1 = mock(ScheduledFuture.class);\r\n    ScheduledExecutorService executor = mock(ScheduledExecutorService.class);\r\n    when(mockScheduledFuture1.cancel(anyBoolean())).thenReturn(false);\r\n    when(mockScheduledFuture1.isDone()).thenReturn(false);\r\n    when(executor.schedule((Runnable) anyObject(), anyLong(), anyObject())).thenReturn(mockScheduledFuture1);\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(executor);\r\n    long timestamp = System.currentTimeMillis() + 10000;\r\n    scheduler.setTimer(timerKey, timestamp, mock(ScheduledCallback.class));\r\n    scheduler.setTimer(timerKey, timestamp, mock(ScheduledCallback.class));\r\n    // verify the interactions with the scheduled future and the scheduler\r\n    verify(executor, times(1)).schedule((Runnable) anyObject(), anyLong(), anyObject());\r\n    verify(mockScheduledFuture1, times(1)).cancel(anyBoolean());\r\n    verify(mockScheduledFuture1, times(1)).isDone();\r\n    Map<Object, ScheduledFuture> scheduledFutures = scheduler.getScheduledFutures();\r\n    assertTrue(\"Expected the timer to be in the queue\", scheduledFutures.containsKey(timerKey));\r\n    assertEquals(\"Expected the scheduled callback from the first invocation\", scheduledFutures.get(timerKey), mockScheduledFuture1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testDuplicateTimerWithFinishedCallbacks",
  "sourceCode" : "@Test\r\npublic void testDuplicateTimerWithFinishedCallbacks() {\r\n    final String timerKey = \"timer-1\";\r\n    ScheduledFuture mockScheduledFuture1 = mock(ScheduledFuture.class);\r\n    ScheduledFuture mockScheduledFuture2 = mock(ScheduledFuture.class);\r\n    ScheduledExecutorService executor = mock(ScheduledExecutorService.class);\r\n    when(mockScheduledFuture1.cancel(anyBoolean())).thenReturn(false);\r\n    when(mockScheduledFuture1.isDone()).thenReturn(true);\r\n    when(executor.schedule((Runnable) anyObject(), anyLong(), anyObject())).thenReturn(mockScheduledFuture1).thenAnswer(invocation -> {\r\n        Object[] args = invocation.getArguments();\r\n        Runnable runnable = (Runnable) args[0];\r\n        runnable.run();\r\n        return mockScheduledFuture2;\r\n    });\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(executor);\r\n    long timestamp = System.currentTimeMillis() + 10000;\r\n    ScheduledCallback<String> expectedScheduledCallback = mock(ScheduledCallback.class);\r\n    scheduler.setTimer(timerKey, timestamp, mock(ScheduledCallback.class));\r\n    scheduler.setTimer(timerKey, timestamp, expectedScheduledCallback);\r\n    // verify the interactions with the scheduled future and the scheduler\r\n    verify(executor, times(2)).schedule((Runnable) anyObject(), anyLong(), anyObject());\r\n    verify(mockScheduledFuture1, times(1)).cancel(anyBoolean());\r\n    verify(mockScheduledFuture1, times(1)).isDone();\r\n    // verify the ready timer and its callback contents to ensure the second invocation callback overwrites the\r\n    // first callback\r\n    Set<Map.Entry<EpochTimeScheduler.TimerKey<?>, ScheduledCallback>> readyTimers = scheduler.removeReadyTimers().entrySet();\r\n    assertEquals(\"Only one timer should be ready to be fired\", readyTimers.size(), 1);\r\n    Map.Entry<EpochTimeScheduler.TimerKey<?>, ScheduledCallback> timerEntry = readyTimers.iterator().next();\r\n    assertEquals(\"Expected the scheduled callback from the second invocation\", timerEntry.getValue(), expectedScheduledCallback);\r\n    assertEquals(\"Expected timer-1 as the key for ready timer\", timerEntry.getKey().getKey(), timerKey);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testSingleTimer",
  "sourceCode" : "@Test\r\npublic void testSingleTimer() {\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(createExecutorService());\r\n    List<String> results = new ArrayList<>();\r\n    scheduler.setTimer(\"single-timer\", 1, (key, collector, coordinator) -> {\r\n        results.add(key);\r\n    });\r\n    fireTimers(scheduler);\r\n    assertTrue(results.size() == 1);\r\n    assertEquals(results.get(0), \"single-timer\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testMultipleTimers",
  "sourceCode" : "@Test\r\npublic void testMultipleTimers() {\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(createExecutorService());\r\n    List<String> results = new ArrayList<>();\r\n    scheduler.setTimer(\"multiple-timer-3\", 3, (key, collector, coordinator) -> {\r\n        results.add(key + \":3\");\r\n    });\r\n    scheduler.setTimer(\"multiple-timer-2\", 2, (key, collector, coordinator) -> {\r\n        results.add(key + \":2\");\r\n    });\r\n    scheduler.setTimer(\"multiple-timer-1\", 1, (key, collector, coordinator) -> {\r\n        results.add(key + \":1\");\r\n    });\r\n    fireTimers(scheduler);\r\n    assertTrue(results.size() == 3);\r\n    assertEquals(results.get(0), \"multiple-timer-1:1\");\r\n    assertEquals(results.get(1), \"multiple-timer-2:2\");\r\n    assertEquals(results.get(2), \"multiple-timer-3:3\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testMultipleKeys",
  "sourceCode" : "@Test\r\npublic void testMultipleKeys() {\r\n    Object key1 = new Object();\r\n    Object key2 = new Object();\r\n    List<String> results = new ArrayList<>();\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(createExecutorService());\r\n    scheduler.setTimer(key1, 2, (key, collector, coordinator) -> {\r\n        assertEquals(key, key1);\r\n        results.add(\"key1:2\");\r\n    });\r\n    scheduler.setTimer(key2, 1, (key, collector, coordinator) -> {\r\n        assertEquals(key, key2);\r\n        results.add(\"key2:1\");\r\n    });\r\n    fireTimers(scheduler);\r\n    assertTrue(results.size() == 2);\r\n    assertEquals(results.get(0), \"key2:1\");\r\n    assertEquals(results.get(1), \"key1:2\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testMultipleKeyTypes",
  "sourceCode" : "@Test\r\npublic void testMultipleKeyTypes() {\r\n    String key1 = \"key\";\r\n    Long key2 = Long.MAX_VALUE;\r\n    List<String> results = new ArrayList<>();\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(createExecutorService());\r\n    scheduler.setTimer(key1, 1, (key, collector, coordinator) -> {\r\n        assertEquals(key, key1);\r\n        results.add(\"key:1\");\r\n    });\r\n    scheduler.setTimer(key2, 2, (key, collector, coordinator) -> {\r\n        assertEquals(key.longValue(), Long.MAX_VALUE);\r\n        results.add(Long.MAX_VALUE + \":2\");\r\n    });\r\n    fireTimers(scheduler);\r\n    assertTrue(results.size() == 2);\r\n    assertEquals(results.get(0), key1 + \":1\");\r\n    assertEquals(results.get(1), key2 + \":2\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testRemoveTimer",
  "sourceCode" : "@Test\r\npublic void testRemoveTimer() {\r\n    ScheduledExecutorService service = mock(ScheduledExecutorService.class);\r\n    ScheduledFuture future = mock(ScheduledFuture.class);\r\n    when(future.cancel(anyBoolean())).thenReturn(true);\r\n    when(service.schedule((Runnable) anyObject(), anyLong(), anyObject())).thenAnswer(invocation -> future);\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(service);\r\n    List<String> results = new ArrayList<>();\r\n    scheduler.setTimer(\"timer\", 1, (key, collector, coordinator) -> {\r\n        results.add(key);\r\n    });\r\n    scheduler.deleteTimer(\"timer\");\r\n    fireTimers(scheduler);\r\n    assertTrue(results.isEmpty());\r\n    verify(future, times(1)).cancel(anyBoolean());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\scheduler\\TestEpochTimeScheduler.java",
  "methodName" : "testTimerListener",
  "sourceCode" : "@Test\r\npublic void testTimerListener() {\r\n    EpochTimeScheduler scheduler = EpochTimeScheduler.create(createExecutorService());\r\n    List<String> results = new ArrayList<>();\r\n    scheduler.registerListener(() -> {\r\n        results.add(\"timer-listener\");\r\n    });\r\n    scheduler.setTimer(\"timer-listener\", 1, (key, collector, coordinator) -> {\r\n    });\r\n    fireTimers(scheduler);\r\n    assertTrue(results.size() == 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestIntermediateMessageSerde.java",
  "methodName" : "testUserMessageSerde",
  "sourceCode" : "@Test\r\npublic void testUserMessageSerde() {\r\n    IntermediateMessageSerde imserde = new IntermediateMessageSerde(new ObjectSerde());\r\n    String msg = \"this is a test message\";\r\n    TestUserMessage userMessage = new TestUserMessage(msg, 0, System.currentTimeMillis());\r\n    byte[] bytes = imserde.toBytes(userMessage);\r\n    TestUserMessage de = (TestUserMessage) imserde.fromBytes(bytes);\r\n    assertEquals(MessageType.of(de), MessageType.USER_MESSAGE);\r\n    assertEquals(de.getMessage(), msg);\r\n    assertEquals(de.getOffset(), 0);\r\n    assertTrue(de.getTimestamp() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestIntermediateMessageSerde.java",
  "methodName" : "testWatermarkMessageSerde",
  "sourceCode" : "@Test\r\npublic void testWatermarkMessageSerde() {\r\n    IntermediateMessageSerde imserde = new IntermediateMessageSerde(new ObjectSerde());\r\n    String taskName = \"task-1\";\r\n    WatermarkMessage watermark = new WatermarkMessage(System.currentTimeMillis(), taskName);\r\n    byte[] bytes = imserde.toBytes(watermark);\r\n    WatermarkMessage de = (WatermarkMessage) imserde.fromBytes(bytes);\r\n    assertEquals(MessageType.of(de), MessageType.WATERMARK);\r\n    assertEquals(de.getTaskName(), taskName);\r\n    assertTrue(de.getTimestamp() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestIntermediateMessageSerde.java",
  "methodName" : "testEndOfStreamMessageSerde",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamMessageSerde() {\r\n    IntermediateMessageSerde imserde = new IntermediateMessageSerde(new ObjectSerde());\r\n    String streamId = \"test-stream\";\r\n    String taskName = \"task-1\";\r\n    EndOfStreamMessage eos = new EndOfStreamMessage(taskName);\r\n    byte[] bytes = imserde.toBytes(eos);\r\n    EndOfStreamMessage de = (EndOfStreamMessage) imserde.fromBytes(bytes);\r\n    assertEquals(MessageType.of(de), MessageType.END_OF_STREAM);\r\n    assertEquals(de.getTaskName(), taskName);\r\n    assertEquals(de.getVersion(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestIntermediateMessageSerde.java",
  "methodName" : "testUserMessageSerdeException",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testUserMessageSerdeException() {\r\n    Serde<?> mockUserMessageSerde = mock(Serde.class);\r\n    when(mockUserMessageSerde.fromBytes(anyObject())).then(new Answer<Object>() {\r\n\r\n        @Override\r\n        public Object answer(InvocationOnMock invocation) throws Throwable {\r\n            byte[] bytes = invocation.getArgumentAt(0, byte[].class);\r\n            if (Arrays.equals(bytes, new byte[] { 1, 2 })) {\r\n                throw new IllegalArgumentException(\"User message serde failed to deserialize this message.\");\r\n            } else {\r\n                // Intermediate message serde shouldn't try to deserialize user message with wrong bytes\r\n                Assert.fail();\r\n                return null;\r\n            }\r\n        }\r\n    });\r\n    IntermediateMessageSerde imserde = new IntermediateMessageSerde(mockUserMessageSerde);\r\n    byte[] bytes = new byte[] { 0, 1, 2 };\r\n    imserde.fromBytes(bytes);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestMetricsSnapshotSerdeV2.java",
  "methodName" : "testSerializeThenDeserialize",
  "sourceCode" : "@Test\r\npublic void testSerializeThenDeserialize() {\r\n    SamzaException samzaException = new SamzaException(\"this is a samza exception\", new RuntimeException(\"cause\"));\r\n    MetricsSnapshot metricsSnapshot = metricsSnapshot(samzaException, true);\r\n    MetricsSnapshotSerdeV2 metricsSnapshotSerde = new MetricsSnapshotSerdeV2();\r\n    byte[] serializedBytes = metricsSnapshotSerde.toBytes(metricsSnapshot);\r\n    MetricsSnapshot deserializedMetricsSnapshot = metricsSnapshotSerde.fromBytes(serializedBytes);\r\n    assertEquals(metricsSnapshot, deserializedMetricsSnapshot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestMetricsSnapshotSerdeV2.java",
  "methodName" : "testSerializeThenDeserializeEmptySamzaEpochIdInHeader",
  "sourceCode" : "@Test\r\npublic void testSerializeThenDeserializeEmptySamzaEpochIdInHeader() {\r\n    SamzaException samzaException = new SamzaException(\"this is a samza exception\", new RuntimeException(\"cause\"));\r\n    MetricsSnapshot metricsSnapshot = metricsSnapshot(samzaException, false);\r\n    MetricsSnapshotSerdeV2 metricsSnapshotSerde = new MetricsSnapshotSerdeV2();\r\n    byte[] serializedBytes = metricsSnapshotSerde.toBytes(metricsSnapshot);\r\n    MetricsSnapshot deserializedMetricsSnapshot = metricsSnapshotSerde.fromBytes(serializedBytes);\r\n    assertEquals(metricsSnapshot, deserializedMetricsSnapshot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestMetricsSnapshotSerdeV2.java",
  "methodName" : "testDeserializeRaw",
  "sourceCode" : "/**\r\n * Helps for verifying compatibility when schemas evolve.\r\n *\r\n * Maps have non-deterministic ordering when serialized, so it is difficult to check exact serialized results. It\r\n * isn't really necessary to check the serialized results anyways. We just need to make sure serialized data can be\r\n * read by old and new systems.\r\n */\r\n@Test\r\npublic void testDeserializeRaw() {\r\n    SamzaException samzaException = new SamzaException(\"this is a samza exception\", new RuntimeException(\"cause\"));\r\n    MetricsSnapshot metricsSnapshot = metricsSnapshot(samzaException, true);\r\n    MetricsSnapshotSerdeV2 metricsSnapshotSerde = new MetricsSnapshotSerdeV2();\r\n    assertEquals(metricsSnapshot, metricsSnapshotSerde.fromBytes(expectedSeralizedSnapshot(samzaException, true, false).getBytes(StandardCharsets.UTF_8)));\r\n    assertEquals(metricsSnapshot, metricsSnapshotSerde.fromBytes(expectedSeralizedSnapshot(samzaException, true, true).getBytes(StandardCharsets.UTF_8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\serializers\\TestMetricsSnapshotSerdeV2.java",
  "methodName" : "testDeserializeRawEmptySamzaEpochIdInHeader",
  "sourceCode" : "/**\r\n * Helps for verifying compatibility when schemas evolve.\r\n */\r\n@Test\r\npublic void testDeserializeRawEmptySamzaEpochIdInHeader() {\r\n    SamzaException samzaException = new SamzaException(\"this is a samza exception\", new RuntimeException(\"cause\"));\r\n    MetricsSnapshot metricsSnapshot = metricsSnapshot(samzaException, false);\r\n    MetricsSnapshotSerdeV2 metricsSnapshotSerde = new MetricsSnapshotSerdeV2();\r\n    assertEquals(metricsSnapshot, metricsSnapshotSerde.fromBytes(expectedSeralizedSnapshot(samzaException, false, false).getBytes(StandardCharsets.UTF_8)));\r\n    assertEquals(metricsSnapshot, metricsSnapshotSerde.fromBytes(expectedSeralizedSnapshot(samzaException, false, true).getBytes(StandardCharsets.UTF_8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testSerializeJobModel",
  "sourceCode" : "@Test\r\npublic void testSerializeJobModel() throws IOException {\r\n    String serializedString = this.samzaObjectMapper.writeValueAsString(this.jobModel);\r\n    // use a plain ObjectMapper to read JSON to make comparison easier\r\n    ObjectNode serializedAsJson = (ObjectNode) new ObjectMapper().readTree(serializedString);\r\n    ObjectNode expectedJson = buildJobModelJson();\r\n    /*\r\n     * Jackson serializes all get* methods even if they aren't regular getters. We only care about certain fields now\r\n     * since those are the only ones that get deserialized.\r\n     */\r\n    assertEquals(expectedJson.get(\"config\"), serializedAsJson.get(\"config\"));\r\n    assertEquals(expectedJson.get(\"containers\"), serializedAsJson.get(\"containers\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testSerializeTaskModel",
  "sourceCode" : "@Test\r\npublic void testSerializeTaskModel() throws IOException {\r\n    TaskModel taskModel = new TaskModel(new TaskName(\"Standby Partition 0\"), new HashSet<>(), new Partition(0), TaskMode.Standby);\r\n    String serializedString = this.samzaObjectMapper.writeValueAsString(taskModel);\r\n    TaskModel deserializedTaskModel = this.samzaObjectMapper.readValue(serializedString, TaskModel.class);\r\n    assertEquals(taskModel, deserializedTaskModel);\r\n    String sampleSerializedString = \"{\\\"task-name\\\":\\\"Partition 0\\\",\\\"system-stream-partitions\\\":[],\\\"changelog-partition\\\":0}\";\r\n    deserializedTaskModel = this.samzaObjectMapper.readValue(sampleSerializedString, TaskModel.class);\r\n    taskModel = new TaskModel(new TaskName(\"Partition 0\"), new HashSet<>(), new Partition(0), TaskMode.Active);\r\n    assertEquals(taskModel, deserializedTaskModel);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeJobModel",
  "sourceCode" : "@Test\r\npublic void testDeserializeJobModel() throws IOException {\r\n    ObjectNode asJson = buildJobModelJson();\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(asJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeWithIgnoredFields",
  "sourceCode" : "/**\r\n * Deserialization should not fail if there are fields which are ignored.\r\n */\r\n@Test\r\npublic void testDeserializeWithIgnoredFields() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    // JobModel ignores all unknown fields\r\n    jobModelJson.put(\"unknown_job_model_key\", \"unknown_job_model_value\");\r\n    ObjectNode taskPartitionMappings = new ObjectMapper().createObjectNode();\r\n    taskPartitionMappings.put(\"1\", (Integer) null);\r\n    // old key that used to be serialized\r\n    jobModelJson.put(\"task-partition-mappings\", taskPartitionMappings);\r\n    ObjectNode allContainerLocality = new ObjectMapper().createObjectNode();\r\n    allContainerLocality.put(\"1\", (Integer) null);\r\n    // currently gets serialized since there is a getAllContainerLocality\r\n    jobModelJson.put(\"all-container-locality\", allContainerLocality);\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(jobModelJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeContainerIdAndProcessorId",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON with a processor-id and a container-id, deserialization should properly ignore\r\n * the container-id.\r\n */\r\n@Test\r\npublic void testDeserializeContainerIdAndProcessorId() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode containerModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\");\r\n    containerModelJson.put(\"container-id\", 123);\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(jobModelJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeUnknownContainerModelField",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON with an unknown field, deserialization should properly ignore it.\r\n */\r\n@Test\r\npublic void testDeserializeUnknownContainerModelField() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode containerModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\");\r\n    containerModelJson.put(\"unknown_container_model_key\", \"unknown_container_model_value\");\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(jobModelJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeContainerModelOnlyContainerId",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON without a processor-id but with a container-id, deserialization should use the\r\n * container-id to calculate the processor-id.\r\n */\r\n@Test\r\npublic void testDeserializeContainerModelOnlyContainerId() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode containerModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\");\r\n    containerModelJson.remove(\"processor-id\");\r\n    containerModelJson.put(\"container-id\", 1);\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(jobModelJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeUnknownTaskModelField",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON with an unknown field, deserialization should properly ignore it.\r\n */\r\n@Test\r\npublic void testDeserializeUnknownTaskModelField() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode taskModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\").get(\"tasks\").get(\"test\");\r\n    taskModelJson.put(\"unknown_task_model_key\", \"unknown_task_model_value\");\r\n    assertEquals(this.jobModel, deserializeFromObjectNode(jobModelJson));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeContainerModelMissingProcessorIdAndContainerId",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON with neither a processor-id nor a container-id, deserialization should fail.\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testDeserializeContainerModelMissingProcessorIdAndContainerId() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode containerModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\");\r\n    containerModelJson.remove(\"processor-id\");\r\n    deserializeFromObjectNode(jobModelJson);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeContainerModelIdFieldOnly",
  "sourceCode" : "/**\r\n * Given a {@link ContainerModel} JSON with only an \"id\" field, deserialization should fail.\r\n * This verifies that even though {@link ContainerModel} has a getId method, the \"id\" field is not used, since\r\n * \"processor-id\" is the field that is supposed to be used.\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testDeserializeContainerModelIdFieldOnly() throws IOException {\r\n    ObjectNode jobModelJson = buildJobModelJson();\r\n    ObjectNode containerModelJson = (ObjectNode) jobModelJson.get(\"containers\").get(\"1\");\r\n    containerModelJson.remove(\"processor-id\");\r\n    containerModelJson.put(\"id\", 1);\r\n    deserializeFromObjectNode(jobModelJson);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testSerializeSystemStreamPartition",
  "sourceCode" : "@Test\r\npublic void testSerializeSystemStreamPartition() throws IOException {\r\n    // case 1: keyBucket not explicitly mentioned\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1));\r\n    String serializedString = this.samzaObjectMapper.writeValueAsString(ssp);\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    ObjectNode sspJson = objectMapper.createObjectNode();\r\n    sspJson.put(\"system\", \"foo\");\r\n    sspJson.put(\"stream\", \"bar\");\r\n    sspJson.put(\"partition\", 1);\r\n    // use a plain ObjectMapper to read JSON to make comparison easier\r\n    ObjectNode serializedAsJson = (ObjectNode) new ObjectMapper().readTree(serializedString);\r\n    ObjectNode expectedJson = sspJson;\r\n    assertEquals(expectedJson.get(\"system\"), serializedAsJson.get(\"system\"));\r\n    assertEquals(expectedJson.get(\"stream\"), serializedAsJson.get(\"stream\"));\r\n    assertEquals(expectedJson.get(\"partition\"), serializedAsJson.get(\"partition\"));\r\n    //Case 2: with non-null keyBucket\r\n    ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1), 1);\r\n    serializedString = this.samzaObjectMapper.writeValueAsString(ssp);\r\n    sspJson = objectMapper.createObjectNode();\r\n    sspJson.put(\"system\", \"foo\");\r\n    sspJson.put(\"stream\", \"bar\");\r\n    sspJson.put(\"partition\", 1);\r\n    sspJson.put(\"keyBucket\", 1);\r\n    // use a plain ObjectMapper to read JSON to make comparison easier\r\n    serializedAsJson = (ObjectNode) new ObjectMapper().readTree(serializedString);\r\n    expectedJson = sspJson;\r\n    assertEquals(expectedJson.get(\"system\"), serializedAsJson.get(\"system\"));\r\n    assertEquals(expectedJson.get(\"stream\"), serializedAsJson.get(\"stream\"));\r\n    assertEquals(expectedJson.get(\"partition\"), serializedAsJson.get(\"partition\"));\r\n    assertEquals(expectedJson.get(\"keyBucket\"), serializedAsJson.get(\"keyBucket\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testDeserializeSystemStreamPartition",
  "sourceCode" : "@Test\r\npublic void testDeserializeSystemStreamPartition() throws IOException {\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    // case 1: keyBucket not explicitly mentioned\r\n    ObjectNode sspJson = objectMapper.createObjectNode();\r\n    sspJson.put(\"system\", \"foo\");\r\n    sspJson.put(\"stream\", \"bar\");\r\n    sspJson.put(\"partition\", 1);\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1));\r\n    String jsonString = new ObjectMapper().writeValueAsString(sspJson);\r\n    SystemStreamPartition deserSSP = this.samzaObjectMapper.readValue(jsonString, SystemStreamPartition.class);\r\n    assertEquals(ssp, deserSSP);\r\n    // case 2: explicitly set key bucket\r\n    sspJson = objectMapper.createObjectNode();\r\n    sspJson.put(\"system\", \"foo\");\r\n    sspJson.put(\"stream\", \"bar\");\r\n    sspJson.put(\"partition\", 1);\r\n    sspJson.put(\"keyBucket\", 1);\r\n    ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1), 1);\r\n    jsonString = new ObjectMapper().writeValueAsString(sspJson);\r\n    deserSSP = this.samzaObjectMapper.readValue(jsonString, SystemStreamPartition.class);\r\n    assertEquals(ssp, deserSSP);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testSerdeSystemStreamPartitionKey",
  "sourceCode" : "@Test\r\npublic void testSerdeSystemStreamPartitionKey() throws IOException {\r\n    // get object mapper which has old deserialization logic for SSP as Key\r\n    ObjectMapper oldDeserForSSPObjectMapper = getOldDeserForSSpKeyObjectMapper();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1));\r\n    String offset = \"100\";\r\n    String sspmapString = this.samzaObjectMapper.writeValueAsString(ImmutableMap.of(ssp, offset));\r\n    TypeReference<HashMap<SystemStreamPartition, String>> typeRef = new TypeReference<HashMap<SystemStreamPartition, String>>() {\r\n    };\r\n    // case 1: deserialize with the new logic for SSP key deser\r\n    Map<SystemStreamPartition, String> deserSSPMap = this.samzaObjectMapper.readValue(sspmapString, typeRef);\r\n    SystemStreamPartition deserSSP = deserSSPMap.keySet().stream().findAny().get();\r\n    String deserOffset = deserSSPMap.values().stream().findFirst().get();\r\n    assertEquals(ssp.getSystem(), deserSSP.getSystem());\r\n    assertEquals(ssp.getStream(), deserSSP.getStream());\r\n    assertEquals(ssp.getPartition(), deserSSP.getPartition());\r\n    assertEquals(ssp.getKeyBucket(), deserSSP.getKeyBucket());\r\n    assertEquals(offset, deserOffset);\r\n    // case 2: deserialize with the OLD logic for SSP key deser\r\n    deserSSPMap = oldDeserForSSPObjectMapper.readValue(sspmapString, typeRef);\r\n    deserSSP = deserSSPMap.keySet().stream().findAny().get();\r\n    deserOffset = deserSSPMap.values().stream().findFirst().get();\r\n    assertEquals(ssp.getSystem(), deserSSP.getSystem());\r\n    assertEquals(ssp.getStream(), deserSSP.getStream());\r\n    assertEquals(ssp.getPartition(), deserSSP.getPartition());\r\n    assertEquals(ssp.getKeyBucket(), deserSSP.getKeyBucket());\r\n    assertEquals(offset, deserOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\model\\TestSamzaObjectMapper.java",
  "methodName" : "testSerDePreElasticSystemStreamPartition",
  "sourceCode" : "@Test\r\npublic void testSerDePreElasticSystemStreamPartition() throws IOException {\r\n    ObjectMapper preElasticObjectMapper = getPreEleasticObjectMapper();\r\n    ObjectMapper elasticObjectMapper = SamzaObjectMapper.getObjectMapper();\r\n    //Scenario 1: ssp serialized with preElasticMapper and deserialized by new Mapper with elasticity\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1));\r\n    String serializedString = preElasticObjectMapper.writeValueAsString(ssp);\r\n    ObjectMapper objectMapper = new ObjectMapper();\r\n    ObjectNode serializedSSPAsJson = objectMapper.createObjectNode();\r\n    serializedSSPAsJson.put(\"system\", \"foo\");\r\n    serializedSSPAsJson.put(\"stream\", \"bar\");\r\n    serializedSSPAsJson.put(\"partition\", 1);\r\n    JsonNode deserSSPAsJson = elasticObjectMapper.readTree(serializedString);\r\n    assertEquals(serializedSSPAsJson.get(\"system\"), deserSSPAsJson.get(\"system\"));\r\n    assertEquals(serializedSSPAsJson.get(\"stream\"), deserSSPAsJson.get(\"stream\"));\r\n    assertEquals(serializedSSPAsJson.get(\"partition\"), deserSSPAsJson.get(\"partition\"));\r\n    assertEquals(serializedSSPAsJson.get(\"keyBucket\"), deserSSPAsJson.get(\"-1\"));\r\n    //Scenario 1: ssp serialized with new elasticMapper and deserialized by old preElastic Mapper\r\n    SystemStreamPartition sspWithKeyBucket = new SystemStreamPartition(\"foo\", \"bar\", new Partition(1), 1);\r\n    serializedString = elasticObjectMapper.writeValueAsString(sspWithKeyBucket);\r\n    serializedSSPAsJson = objectMapper.createObjectNode();\r\n    serializedSSPAsJson.put(\"system\", \"foo\");\r\n    serializedSSPAsJson.put(\"stream\", \"bar\");\r\n    serializedSSPAsJson.put(\"partition\", 1);\r\n    serializedSSPAsJson.put(\"keyBucket\", 1);\r\n    deserSSPAsJson = preElasticObjectMapper.readTree(serializedString);\r\n    assertEquals(serializedSSPAsJson.get(\"system\"), deserSSPAsJson.get(\"system\"));\r\n    assertEquals(serializedSSPAsJson.get(\"stream\"), deserSSPAsJson.get(\"stream\"));\r\n    assertEquals(serializedSSPAsJson.get(\"partition\"), deserSSPAsJson.get(\"partition\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestCheckpointV2Serde.java",
  "methodName" : "testCheckpointV2Serde",
  "sourceCode" : "@Test\r\npublic void testCheckpointV2Serde() {\r\n    CheckpointV2Serde serde = new CheckpointV2Serde();\r\n    Map<SystemStreamPartition, String> offsets = new HashMap<>();\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(777));\r\n    offsets.put(systemStreamPartition, \"1\");\r\n    // State Checkpoint marker\r\n    Map<String, Map<String, String>> factoryStateCheckpointMarkersMap = new HashMap<>();\r\n    Map<String, String> stateCheckpointMarkersMap = new HashMap<>();\r\n    stateCheckpointMarkersMap.put(\"store1\", \"marker1\");\r\n    stateCheckpointMarkersMap.put(\"store2\", \"marker2\");\r\n    Map<String, String> stateCheckpointMarkersMap2 = new HashMap<>();\r\n    stateCheckpointMarkersMap2.put(\"store1\", \"marker3\");\r\n    stateCheckpointMarkersMap2.put(\"store2\", \"marker4\");\r\n    factoryStateCheckpointMarkersMap.put(\"factory1\", stateCheckpointMarkersMap);\r\n    factoryStateCheckpointMarkersMap.put(\"factory2\", stateCheckpointMarkersMap2);\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    CheckpointV2 checkpoint = new CheckpointV2(checkpointId, offsets, factoryStateCheckpointMarkersMap);\r\n    CheckpointV2 deserializedCheckpoint = serde.fromBytes(serde.toBytes(checkpoint));\r\n    // Validate input checkpoints\r\n    assertEquals(checkpointId, deserializedCheckpoint.getCheckpointId());\r\n    assertEquals(\"1\", deserializedCheckpoint.getOffsets().get(systemStreamPartition));\r\n    assertEquals(1, deserializedCheckpoint.getOffsets().size());\r\n    // Validate state checkpoints\r\n    assertEquals(2, deserializedCheckpoint.getStateCheckpointMarkers().size());\r\n    assertTrue(deserializedCheckpoint.getStateCheckpointMarkers().containsKey(\"factory1\"));\r\n    assertEquals(stateCheckpointMarkersMap, deserializedCheckpoint.getStateCheckpointMarkers().get(\"factory1\"));\r\n    assertTrue(deserializedCheckpoint.getStateCheckpointMarkers().containsKey(\"factory2\"));\r\n    assertEquals(stateCheckpointMarkersMap2, deserializedCheckpoint.getStateCheckpointMarkers().get(\"factory2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestCheckpointV2Serde.java",
  "methodName" : "testCheckpointV2SerdeStatelessJob",
  "sourceCode" : "@Test\r\npublic void testCheckpointV2SerdeStatelessJob() {\r\n    CheckpointV2Serde serde = new CheckpointV2Serde();\r\n    Map<SystemStreamPartition, String> offsets = new HashMap<>();\r\n    SystemStreamPartition systemStreamPartition = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(777));\r\n    offsets.put(systemStreamPartition, \"1\");\r\n    // State Checkpoint marker\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    CheckpointV2 checkpoint = new CheckpointV2(checkpointId, offsets, new HashMap<>());\r\n    CheckpointV2 deserializedCheckpoint = serde.fromBytes(serde.toBytes(checkpoint));\r\n    // Validate input checkpoints\r\n    assertEquals(checkpointId, deserializedCheckpoint.getCheckpointId());\r\n    assertEquals(\"1\", deserializedCheckpoint.getOffsets().get(systemStreamPartition));\r\n    assertEquals(1, deserializedCheckpoint.getOffsets().size());\r\n    // No state checkpoints, but a map is still created\r\n    assertEquals(0, deserializedCheckpoint.getStateCheckpointMarkers().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestMetricsSnapshotSerde.java",
  "methodName" : "testSerializeThenDeserialize",
  "sourceCode" : "@Test\r\npublic void testSerializeThenDeserialize() {\r\n    MetricsSnapshot snapshot = metricsSnapshot(true);\r\n    MetricsSnapshotSerde serde = new MetricsSnapshotSerde();\r\n    byte[] bytes = serde.toBytes(snapshot);\r\n    assertEquals(snapshot, serde.fromBytes(bytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestMetricsSnapshotSerde.java",
  "methodName" : "testSerializeThenDeserializeEmptySamzaEpochIdInHeader",
  "sourceCode" : "@Test\r\npublic void testSerializeThenDeserializeEmptySamzaEpochIdInHeader() {\r\n    MetricsSnapshot snapshot = metricsSnapshot(true);\r\n    MetricsSnapshotSerde serde = new MetricsSnapshotSerde();\r\n    byte[] bytes = serde.toBytes(snapshot);\r\n    assertEquals(snapshot, serde.fromBytes(bytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestMetricsSnapshotSerde.java",
  "methodName" : "testDeserializeRaw",
  "sourceCode" : "/**\r\n * Helps for verifying compatibility when schemas evolve.\r\n *\r\n * Maps have non-deterministic ordering when serialized, so it is difficult to check exact serialized results. It\r\n * isn't really necessary to check the serialized results anyways. We just need to make sure serialized data can be\r\n * read by old and new systems.\r\n */\r\n@Test\r\npublic void testDeserializeRaw() {\r\n    MetricsSnapshot snapshot = metricsSnapshot(true);\r\n    MetricsSnapshotSerde serde = new MetricsSnapshotSerde();\r\n    assertEquals(snapshot, serde.fromBytes(expectedSeralizedSnapshot(true, false).getBytes(StandardCharsets.UTF_8)));\r\n    assertEquals(snapshot, serde.fromBytes(expectedSeralizedSnapshot(true, true).getBytes(StandardCharsets.UTF_8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\serializers\\TestMetricsSnapshotSerde.java",
  "methodName" : "testDeserializeRawEmptySamzaEpochIdInHeader",
  "sourceCode" : "/**\r\n * Helps for verifying compatibility when schemas evolve.\r\n */\r\n@Test\r\npublic void testDeserializeRawEmptySamzaEpochIdInHeader() {\r\n    MetricsSnapshot snapshot = metricsSnapshot(false);\r\n    MetricsSnapshotSerde serde = new MetricsSnapshotSerde();\r\n    assertEquals(snapshot, serde.fromBytes(expectedSeralizedSnapshot(false, false).getBytes(StandardCharsets.UTF_8)));\r\n    assertEquals(snapshot, serde.fromBytes(expectedSeralizedSnapshot(false, true).getBytes(StandardCharsets.UTF_8)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testDefaultMetadataStore",
  "sourceCode" : "@Test\r\npublic void testDefaultMetadataStore() {\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    Assert.assertNotNull(startpointManager);\r\n    Assert.assertEquals(NamespaceAwareCoordinatorStreamStore.class, startpointManager.getReadWriteStore().getClass());\r\n    Assert.assertEquals(NamespaceAwareCoordinatorStreamStore.class, startpointManager.getFanOutStore().getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testStaleStartpoints",
  "sourceCode" : "@Test\r\npublic void testStaleStartpoints() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"mockSystem\", \"mockStream\", new Partition(2));\r\n    TaskName taskName = new TaskName(\"MockTask\");\r\n    long staleTimestamp = Instant.now().toEpochMilli() - StartpointManager.DEFAULT_EXPIRATION_DURATION.toMillis() - 2;\r\n    StartpointTimestamp startpoint = new StartpointTimestamp(staleTimestamp, staleTimestamp);\r\n    startpointManager.writeStartpoint(ssp, startpoint);\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp).isPresent());\r\n    startpointManager.writeStartpoint(ssp, taskName, startpoint);\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp, taskName).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testNoLongerUsableAfterStop",
  "sourceCode" : "@Test\r\npublic void testNoLongerUsableAfterStop() throws IOException {\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    startpointManager.start();\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"mockSystem\", \"mockStream\", new Partition(2));\r\n    TaskName taskName = new TaskName(\"MockTask\");\r\n    Startpoint startpoint = new StartpointOldest();\r\n    startpointManager.stop();\r\n    try {\r\n        startpointManager.writeStartpoint(ssp, startpoint);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.writeStartpoint(ssp, taskName, startpoint);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.readStartpoint(ssp);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.readStartpoint(ssp, taskName);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.deleteStartpoint(ssp);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.deleteStartpoint(ssp, taskName);\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.fanOut(new HashMap<>());\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.getFanOutForTask(new TaskName(\"t0\"));\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.removeFanOutForTask(new TaskName(\"t0\"));\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n    try {\r\n        startpointManager.removeFanOutForTaskSSPs(new TaskName(\"t0\"), ImmutableSet.of(ssp));\r\n        Assert.fail(\"Expected precondition exception.\");\r\n    } catch (IllegalStateException ex) {\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testBasics",
  "sourceCode" : "@Test\r\npublic void testBasics() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"mockSystem\", \"mockStream\", new Partition(2));\r\n    TaskName taskName = new TaskName(\"MockTask\");\r\n    StartpointTimestamp startpoint1 = new StartpointTimestamp(111111111L);\r\n    StartpointTimestamp startpoint2 = new StartpointTimestamp(222222222L);\r\n    StartpointSpecific startpoint3 = new StartpointSpecific(\"1\");\r\n    StartpointSpecific startpoint4 = new StartpointSpecific(\"2\");\r\n    // Test createdTimestamp field is not null by default\r\n    Assert.assertNotNull(startpoint1.getCreationTimestamp());\r\n    Assert.assertNotNull(startpoint2.getCreationTimestamp());\r\n    Assert.assertNotNull(startpoint3.getCreationTimestamp());\r\n    Assert.assertNotNull(startpoint4.getCreationTimestamp());\r\n    // Test reads on non-existent keys\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp).isPresent());\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp, taskName).isPresent());\r\n    // Test writes\r\n    Startpoint startpointFromStore;\r\n    startpointManager.writeStartpoint(ssp, startpoint1);\r\n    startpointManager.writeStartpoint(ssp, taskName, startpoint2);\r\n    startpointFromStore = startpointManager.readStartpoint(ssp).get();\r\n    Assert.assertEquals(StartpointTimestamp.class, startpointFromStore.getClass());\r\n    Assert.assertEquals(startpoint1.getTimestampOffset(), ((StartpointTimestamp) startpointFromStore).getTimestampOffset());\r\n    Assert.assertTrue(startpointFromStore.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    startpointFromStore = startpointManager.readStartpoint(ssp, taskName).get();\r\n    Assert.assertEquals(StartpointTimestamp.class, startpointFromStore.getClass());\r\n    Assert.assertEquals(startpoint2.getTimestampOffset(), ((StartpointTimestamp) startpointFromStore).getTimestampOffset());\r\n    Assert.assertTrue(startpointFromStore.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    // Test overwrites\r\n    startpointManager.writeStartpoint(ssp, startpoint3);\r\n    startpointManager.writeStartpoint(ssp, taskName, startpoint4);\r\n    startpointFromStore = startpointManager.readStartpoint(ssp).get();\r\n    Assert.assertEquals(StartpointSpecific.class, startpointFromStore.getClass());\r\n    Assert.assertEquals(startpoint3.getSpecificOffset(), ((StartpointSpecific) startpointFromStore).getSpecificOffset());\r\n    Assert.assertTrue(startpointFromStore.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    startpointFromStore = startpointManager.readStartpoint(ssp, taskName).get();\r\n    Assert.assertEquals(StartpointSpecific.class, startpointFromStore.getClass());\r\n    Assert.assertEquals(startpoint4.getSpecificOffset(), ((StartpointSpecific) startpointFromStore).getSpecificOffset());\r\n    Assert.assertTrue(startpointFromStore.getCreationTimestamp() <= Instant.now().toEpochMilli());\r\n    // Test deletes on SSP keys does not affect SSP+TaskName keys\r\n    startpointManager.deleteStartpoint(ssp);\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp).isPresent());\r\n    Assert.assertTrue(startpointManager.readStartpoint(ssp, taskName).isPresent());\r\n    // Test deletes on SSP+TaskName keys does not affect SSP keys\r\n    startpointManager.writeStartpoint(ssp, startpoint3);\r\n    startpointManager.deleteStartpoint(ssp, taskName);\r\n    Assert.assertFalse(startpointManager.readStartpoint(ssp, taskName).isPresent());\r\n    Assert.assertTrue(startpointManager.readStartpoint(ssp).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testFanOutBasic",
  "sourceCode" : "@Test\r\npublic void testFanOutBasic() throws IOException {\r\n    SystemStreamPartition sspBroadcast = new SystemStreamPartition(\"mockSystem1\", \"mockStream1\", new Partition(2));\r\n    SystemStreamPartition sspSingle = new SystemStreamPartition(\"mockSystem2\", \"mockStream2\", new Partition(3));\r\n    TaskName taskWithNonBroadcast = new TaskName(\"t1\");\r\n    List<TaskName> tasks = ImmutableList.of(new TaskName(\"t0\"), taskWithNonBroadcast, new TaskName(\"t2\"), new TaskName(\"t3\"), new TaskName(\"t4\"), new TaskName(\"t5\"));\r\n    Map<TaskName, Set<SystemStreamPartition>> taskToSSPs = tasks.stream().collect(Collectors.toMap(task -> task, task -> task.equals(taskWithNonBroadcast) ? ImmutableSet.of(sspBroadcast, sspSingle) : ImmutableSet.of(sspBroadcast)));\r\n    StartpointSpecific startpoint42 = new StartpointSpecific(\"42\");\r\n    startpointManager.writeStartpoint(sspBroadcast, startpoint42);\r\n    startpointManager.writeStartpoint(sspSingle, startpoint42);\r\n    // startpoint42 should remap with key sspBroadcast to all tasks + sspBroadcast\r\n    Map<TaskName, Map<SystemStreamPartition, Startpoint>> tasksFannedOutTo = startpointManager.fanOut(taskToSSPs);\r\n    Assert.assertEquals(tasks.size(), tasksFannedOutTo.size());\r\n    Assert.assertTrue(tasksFannedOutTo.keySet().containsAll(tasks));\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspBroadcast).isPresent());\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspSingle).isPresent());\r\n    for (TaskName taskName : tasks) {\r\n        Map<SystemStreamPartition, Startpoint> fanOutForTask = startpointManager.getFanOutForTask(taskName);\r\n        if (taskName.equals(taskWithNonBroadcast)) {\r\n            // Non-broadcast startpoint should be fanned out to only one task\r\n            Assert.assertEquals(\"Should have broadcast and non-broadcast SSP\", 2, fanOutForTask.size());\r\n        } else {\r\n            Assert.assertEquals(\"Should only have broadcast SSP\", 1, fanOutForTask.size());\r\n        }\r\n        // Broadcast SSP should be on every task\r\n        Startpoint startpointFromStore = fanOutForTask.get(sspBroadcast);\r\n        Assert.assertEquals(StartpointSpecific.class, startpointFromStore.getClass());\r\n        Assert.assertEquals(startpoint42.getSpecificOffset(), ((StartpointSpecific) startpointFromStore).getSpecificOffset());\r\n        // startpoint mapped only to task \"t1\" for Non-broadcast SSP\r\n        startpointFromStore = fanOutForTask.get(sspSingle);\r\n        if (taskName.equals(taskWithNonBroadcast)) {\r\n            Assert.assertEquals(StartpointSpecific.class, startpointFromStore.getClass());\r\n            Assert.assertEquals(startpoint42.getSpecificOffset(), ((StartpointSpecific) startpointFromStore).getSpecificOffset());\r\n        } else {\r\n            Assert.assertNull(\"Should not have non-broadcast SSP\", startpointFromStore);\r\n        }\r\n        startpointManager.removeFanOutForTask(taskName);\r\n        Assert.assertTrue(startpointManager.getFanOutForTask(taskName).isEmpty());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testFanOutWithStartpointResolutions",
  "sourceCode" : "@Test\r\npublic void testFanOutWithStartpointResolutions() throws IOException {\r\n    SystemStreamPartition sspBroadcast = new SystemStreamPartition(\"mockSystem1\", \"mockStream1\", new Partition(2));\r\n    SystemStreamPartition sspSingle = new SystemStreamPartition(\"mockSystem2\", \"mockStream2\", new Partition(3));\r\n    List<TaskName> tasks = ImmutableList.of(new TaskName(\"t0\"), new TaskName(\"t1\"), new TaskName(\"t2\"), new TaskName(\"t3\"), new TaskName(\"t4\"));\r\n    TaskName taskWithNonBroadcast = tasks.get(1);\r\n    TaskName taskBroadcastInPast = tasks.get(2);\r\n    TaskName taskBroadcastInFuture = tasks.get(3);\r\n    Map<TaskName, Set<SystemStreamPartition>> taskToSSPs = tasks.stream().collect(Collectors.toMap(task -> task, task -> task.equals(taskWithNonBroadcast) ? ImmutableSet.of(sspBroadcast, sspSingle) : ImmutableSet.of(sspBroadcast)));\r\n    Instant now = Instant.now();\r\n    StartpointMock startpointPast = new StartpointMock(now.minusMillis(10000L).toEpochMilli());\r\n    StartpointMock startpointPresent = new StartpointMock(now.toEpochMilli());\r\n    StartpointMock startpointFuture = new StartpointMock(now.plusMillis(10000L).toEpochMilli());\r\n    startpointManager.getObjectMapper().registerSubtypes(StartpointMock.class);\r\n    startpointManager.writeStartpoint(sspSingle, startpointPast);\r\n    startpointManager.writeStartpoint(sspSingle, startpointPresent);\r\n    startpointManager.writeStartpoint(sspBroadcast, startpointPresent);\r\n    startpointManager.writeStartpoint(sspBroadcast, taskBroadcastInPast, startpointPast);\r\n    startpointManager.writeStartpoint(sspBroadcast, taskBroadcastInFuture, startpointFuture);\r\n    Map<TaskName, Map<SystemStreamPartition, Startpoint>> fannedOut = startpointManager.fanOut(taskToSSPs);\r\n    Assert.assertEquals(tasks.size(), fannedOut.size());\r\n    Assert.assertTrue(fannedOut.keySet().containsAll(tasks));\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspBroadcast).isPresent());\r\n    for (TaskName taskName : fannedOut.keySet()) {\r\n        Assert.assertFalse(\"Should be deleted after fan out for task: \" + taskName.getTaskName(), startpointManager.readStartpoint(sspBroadcast, taskName).isPresent());\r\n    }\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspSingle).isPresent());\r\n    for (TaskName taskName : tasks) {\r\n        Map<SystemStreamPartition, Startpoint> fanOutForTask = startpointManager.getFanOutForTask(taskName);\r\n        if (taskName.equals(taskWithNonBroadcast)) {\r\n            Assert.assertEquals(startpointPresent, fanOutForTask.get(sspSingle));\r\n            Assert.assertEquals(startpointPresent, fanOutForTask.get(sspBroadcast));\r\n        } else if (taskName.equals(taskBroadcastInPast)) {\r\n            Assert.assertNull(fanOutForTask.get(sspSingle));\r\n            Assert.assertEquals(startpointPresent, fanOutForTask.get(sspBroadcast));\r\n        } else if (taskName.equals(taskBroadcastInFuture)) {\r\n            Assert.assertNull(fanOutForTask.get(sspSingle));\r\n            Assert.assertEquals(startpointFuture, fanOutForTask.get(sspBroadcast));\r\n        } else {\r\n            Assert.assertNull(fanOutForTask.get(sspSingle));\r\n            Assert.assertEquals(startpointPresent, fanOutForTask.get(sspBroadcast));\r\n        }\r\n        startpointManager.removeFanOutForTask(taskName);\r\n        Assert.assertTrue(startpointManager.getFanOutForTask(taskName).isEmpty());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testRemoveAllFanOuts",
  "sourceCode" : "@Test\r\npublic void testRemoveAllFanOuts() throws IOException {\r\n    SystemStreamPartition sspBroadcast = new SystemStreamPartition(\"mockSystem1\", \"mockStream1\", new Partition(2));\r\n    SystemStreamPartition sspSingle = new SystemStreamPartition(\"mockSystem2\", \"mockStream2\", new Partition(3));\r\n    TaskName taskWithNonBroadcast = new TaskName(\"t1\");\r\n    List<TaskName> tasks = ImmutableList.of(new TaskName(\"t0\"), taskWithNonBroadcast, new TaskName(\"t2\"), new TaskName(\"t3\"), new TaskName(\"t4\"), new TaskName(\"t5\"));\r\n    Map<TaskName, Set<SystemStreamPartition>> taskToSSPs = tasks.stream().collect(Collectors.toMap(task -> task, task -> task.equals(taskWithNonBroadcast) ? ImmutableSet.of(sspBroadcast, sspSingle) : ImmutableSet.of(sspBroadcast)));\r\n    StartpointSpecific startpoint42 = new StartpointSpecific(\"42\");\r\n    startpointManager.writeStartpoint(sspBroadcast, startpoint42);\r\n    startpointManager.writeStartpoint(sspSingle, startpoint42);\r\n    // startpoint42 should remap with key sspBroadcast to all tasks + sspBroadcast\r\n    Map<TaskName, Map<SystemStreamPartition, Startpoint>> tasksFannedOutTo = startpointManager.fanOut(taskToSSPs);\r\n    Assert.assertEquals(tasks.size(), tasksFannedOutTo.size());\r\n    Assert.assertTrue(tasksFannedOutTo.keySet().containsAll(tasks));\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspBroadcast).isPresent());\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspSingle).isPresent());\r\n    startpointManager.removeAllFanOuts();\r\n    // Write back to ensure removing all fan outs doesn't remove all startpoints\r\n    startpointManager.writeStartpoint(sspBroadcast, startpoint42);\r\n    startpointManager.writeStartpoint(sspSingle, startpoint42);\r\n    Assert.assertEquals(0, startpointManager.getFanOutStore().all().size());\r\n    Assert.assertTrue(\"Should not be deleted after remove all fan outs\", startpointManager.readStartpoint(sspBroadcast).isPresent());\r\n    Assert.assertTrue(\"Should not be deleted after remove all fan outs\", startpointManager.readStartpoint(sspSingle).isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testRemoveFanOutForTaskSSPs",
  "sourceCode" : "@Test\r\npublic void testRemoveFanOutForTaskSSPs() throws Exception {\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(\"mockSystem\", \"mockStream\", new Partition(0));\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"mockSystem\", \"mockStream\", new Partition(1));\r\n    TaskName taskName = new TaskName(\"mockTask\");\r\n    Map<TaskName, Set<SystemStreamPartition>> taskToSSPs = ImmutableMap.of(taskName, ImmutableSet.of(ssp0, ssp1));\r\n    StartpointSpecific startpoint42 = new StartpointSpecific(\"42\");\r\n    startpointManager.writeStartpoint(ssp0, startpoint42);\r\n    startpointManager.writeStartpoint(ssp1, startpoint42);\r\n    Map<TaskName, Map<SystemStreamPartition, Startpoint>> tasksFannedOutTo = startpointManager.fanOut(taskToSSPs);\r\n    Assert.assertEquals(ImmutableSet.of(taskName), tasksFannedOutTo.keySet());\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(ssp0).isPresent());\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(ssp1).isPresent());\r\n    // no action takes if not specify any system stream partition\r\n    startpointManager.removeFanOutForTaskSSPs(taskName, ImmutableSet.of());\r\n    Assert.assertEquals(ImmutableMap.of(ssp0, startpoint42, ssp1, startpoint42), startpointManager.getFanOutForTask(taskName));\r\n    // partially removal: remove the fanned out startpoint for the specified system stream partition only\r\n    startpointManager.removeFanOutForTaskSSPs(taskName, ImmutableSet.of(ssp0));\r\n    Assert.assertEquals(ImmutableMap.of(ssp1, startpoint42), startpointManager.getFanOutForTask(taskName));\r\n    // remove the whole task's startpoints if all the task's partitions' are removed\r\n    startpointManager.removeFanOutForTaskSSPs(taskName, ImmutableSet.of(ssp1));\r\n    Assert.assertEquals(ImmutableMap.of(), startpointManager.getFanOutForTask(taskName));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointManager.java",
  "methodName" : "testDeleteAllStartpoints",
  "sourceCode" : "@Test\r\npublic void testDeleteAllStartpoints() throws IOException {\r\n    SystemStreamPartition sspBroadcast = new SystemStreamPartition(\"mockSystem1\", \"mockStream1\", new Partition(2));\r\n    SystemStreamPartition sspSingle = new SystemStreamPartition(\"mockSystem2\", \"mockStream2\", new Partition(3));\r\n    TaskName taskWithNonBroadcast = new TaskName(\"t1\");\r\n    List<TaskName> tasks = ImmutableList.of(new TaskName(\"t0\"), taskWithNonBroadcast, new TaskName(\"t2\"), new TaskName(\"t3\"), new TaskName(\"t4\"), new TaskName(\"t5\"));\r\n    Map<TaskName, Set<SystemStreamPartition>> taskToSSPs = tasks.stream().collect(Collectors.toMap(task -> task, task -> task.equals(taskWithNonBroadcast) ? ImmutableSet.of(sspBroadcast, sspSingle) : ImmutableSet.of(sspBroadcast)));\r\n    StartpointSpecific startpoint42 = new StartpointSpecific(\"42\");\r\n    startpointManager.writeStartpoint(sspBroadcast, startpoint42);\r\n    startpointManager.writeStartpoint(sspSingle, startpoint42);\r\n    // startpoint42 should remap with key sspBroadcast to all tasks + sspBroadcast\r\n    Map<TaskName, Map<SystemStreamPartition, Startpoint>> tasksFannedOutTo = startpointManager.fanOut(taskToSSPs);\r\n    Assert.assertEquals(tasks.size(), tasksFannedOutTo.size());\r\n    Assert.assertTrue(tasksFannedOutTo.keySet().containsAll(tasks));\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspBroadcast).isPresent());\r\n    Assert.assertFalse(\"Should be deleted after fan out\", startpointManager.readStartpoint(sspSingle).isPresent());\r\n    // Re-populate startpoints after fan out\r\n    startpointManager.writeStartpoint(sspBroadcast, startpoint42);\r\n    startpointManager.writeStartpoint(sspSingle, startpoint42);\r\n    Assert.assertEquals(2, startpointManager.getReadWriteStore().all().size());\r\n    startpointManager.deleteAllStartpoints();\r\n    Assert.assertEquals(0, startpointManager.getReadWriteStore().all().size());\r\n    // Fan outs should be untouched\r\n    Assert.assertEquals(tasks.size(), startpointManager.getFanOutStore().all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointObjectMapper.java",
  "methodName" : "testStartpointSpecificSerde",
  "sourceCode" : "@Test\r\npublic void testStartpointSpecificSerde() throws IOException {\r\n    StartpointSpecific startpointSpecific = new StartpointSpecific(\"42\");\r\n    Startpoint startpointFromSerde = MAPPER.readValue(MAPPER.writeValueAsBytes(startpointSpecific), Startpoint.class);\r\n    Assert.assertEquals(startpointSpecific.getClass(), startpointFromSerde.getClass());\r\n    Assert.assertEquals(startpointSpecific.getCreationTimestamp(), startpointFromSerde.getCreationTimestamp());\r\n    Assert.assertEquals(startpointSpecific.getSpecificOffset(), ((StartpointSpecific) startpointFromSerde).getSpecificOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointObjectMapper.java",
  "methodName" : "testStartpointTimestampSerde",
  "sourceCode" : "@Test\r\npublic void testStartpointTimestampSerde() throws IOException {\r\n    StartpointTimestamp startpointTimestamp = new StartpointTimestamp(123456L);\r\n    Startpoint startpointFromSerde = MAPPER.readValue(MAPPER.writeValueAsBytes(startpointTimestamp), Startpoint.class);\r\n    Assert.assertEquals(startpointTimestamp.getClass(), startpointFromSerde.getClass());\r\n    Assert.assertEquals(startpointTimestamp.getCreationTimestamp(), startpointFromSerde.getCreationTimestamp());\r\n    Assert.assertEquals(startpointTimestamp.getTimestampOffset(), ((StartpointTimestamp) startpointFromSerde).getTimestampOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointObjectMapper.java",
  "methodName" : "testStartpointEarliestSerde",
  "sourceCode" : "@Test\r\npublic void testStartpointEarliestSerde() throws IOException {\r\n    StartpointOldest startpointOldest = new StartpointOldest();\r\n    Startpoint startpointFromSerde = MAPPER.readValue(MAPPER.writeValueAsBytes(startpointOldest), Startpoint.class);\r\n    Assert.assertEquals(startpointOldest.getClass(), startpointFromSerde.getClass());\r\n    Assert.assertEquals(startpointOldest.getCreationTimestamp(), startpointFromSerde.getCreationTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointObjectMapper.java",
  "methodName" : "testStartpointLatestSerde",
  "sourceCode" : "@Test\r\npublic void testStartpointLatestSerde() throws IOException {\r\n    StartpointUpcoming startpointUpcoming = new StartpointUpcoming();\r\n    Startpoint startpointFromSerde = MAPPER.readValue(MAPPER.writeValueAsBytes(startpointUpcoming), Startpoint.class);\r\n    Assert.assertEquals(startpointUpcoming.getClass(), startpointFromSerde.getClass());\r\n    Assert.assertEquals(startpointUpcoming.getCreationTimestamp(), startpointFromSerde.getCreationTimestamp());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\startpoint\\TestStartpointObjectMapper.java",
  "methodName" : "testFanOutSerde",
  "sourceCode" : "@Test\r\npublic void testFanOutSerde() throws IOException {\r\n    StartpointFanOutPerTask startpointFanOutPerTask = new StartpointFanOutPerTask(Instant.now().minusSeconds(60));\r\n    startpointFanOutPerTask.getFanOuts().put(new SystemStreamPartition(\"system1\", \"stream1\", new Partition(1)), new StartpointUpcoming());\r\n    startpointFanOutPerTask.getFanOuts().put(new SystemStreamPartition(\"system2\", \"stream2\", new Partition(2)), new StartpointOldest());\r\n    String serialized = MAPPER.writeValueAsString(startpointFanOutPerTask);\r\n    StartpointFanOutPerTask startpointFanOutPerTaskFromSerde = MAPPER.readValue(serialized, StartpointFanOutPerTask.class);\r\n    Assert.assertEquals(startpointFanOutPerTask, startpointFanOutPerTaskFromSerde);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\index\\TestDirIndex.java",
  "methodName" : "testDirIndexValidationForFileAddedAndRemovedChecksBlobsAndMetadataToo",
  "sourceCode" : "@Test\r\npublic void testDirIndexValidationForFileAddedAndRemovedChecksBlobsAndMetadataToo() {\r\n    FileBlob filePresentBlob = new FileBlob(\"filePresentBlobId\", 0);\r\n    FileMetadata filePresentMetadata = new FileMetadata(1234L, 2345L, 12345, \"owner\", \"group\", \"permission\");\r\n    FileIndex filePresent = new FileIndex(\"mutableFile\", Collections.singletonList(filePresentBlob), filePresentMetadata, 789L);\r\n    // same name and metadata, different blobs\r\n    FileBlob fileRemovedBlob = new FileBlob(\"fileRemovedBlobId\", 0);\r\n    FileIndex fileRemoved = new FileIndex(\"mutableFile\", Collections.singletonList(fileRemovedBlob), filePresentMetadata, 789L);\r\n    new DirIndex(\"dir\", Collections.singletonList(filePresent), Collections.singletonList(fileRemoved), Collections.emptyList(), Collections.emptyList());\r\n    // but if same name and blobs, different metadata, should fail due to blob duplication validation\r\n    try {\r\n        FileMetadata otherFileMetadata = new FileMetadata(541345L, 624L, 2125L, \"owner\", \"group\", \"permission\");\r\n        FileIndex otherFileRemoved = new FileIndex(\"mutableFile\", Collections.singletonList(filePresentBlob), otherFileMetadata, 789L);\r\n        new DirIndex(\"dir\", Collections.singletonList(filePresent), Collections.singletonList(otherFileRemoved), Collections.emptyList(), Collections.emptyList());\r\n        Assert.fail(\"Should have failed validation if same blob present in file added and removed, even if same name\");\r\n    } catch (IllegalStateException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\index\\TestDirIndex.java",
  "methodName" : "testDirIndexValidationFailsIfSameBlobAddedAndRemoved",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testDirIndexValidationFailsIfSameBlobAddedAndRemoved() {\r\n    FileBlob filePresentBlob = new FileBlob(\"filePresentBlobId\", 0);\r\n    FileMetadata filePresentMetadata = new FileMetadata(1234L, 2345L, 12345, \"owner\", \"group\", \"permission\");\r\n    FileIndex filePresent = new FileIndex(\"filePresent\", Collections.singletonList(filePresentBlob), filePresentMetadata, 789L);\r\n    // different name and metadata, same blob id\r\n    FileBlob fileRemovedBlob = new FileBlob(\"filePresentBlobId\", 0);\r\n    FileMetadata fileRemovedMetadata = new FileMetadata(324L, 625L, 4253L, \"owner\", \"group\", \"permission\");\r\n    FileIndex fileRemoved = new FileIndex(\"fileRemoved\", Collections.singletonList(fileRemovedBlob), fileRemovedMetadata, 1234L);\r\n    // should fail since same blob id is present in file present and removed\r\n    new DirIndex(\"dir\", Collections.singletonList(filePresent), Collections.singletonList(fileRemoved), Collections.emptyList(), Collections.emptyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\serde\\TestSnapshotIndexSerde.java",
  "methodName" : "testSnapshotIndexSerde",
  "sourceCode" : "@Test\r\npublic void testSnapshotIndexSerde() throws IOException {\r\n    // create local and remote snapshots\r\n    String local = \"[a, b, c/1, d/1/2]\";\r\n    String remote = \"[a, b, z, c/1/2, e/1]\";\r\n    Path localSnapshot = BlobStoreTestUtil.createLocalDir(local);\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(CheckpointId.create(), \"job\", \"123\", \"task\", \"store\");\r\n    SnapshotIndex testRemoteSnapshot = new SnapshotIndex(System.currentTimeMillis(), snapshotMetadata, dirIndex, Optional.empty());\r\n    SnapshotIndexSerde snapshotIndexSerde = new SnapshotIndexSerde();\r\n    byte[] serialized = snapshotIndexSerde.toBytes(testRemoteSnapshot);\r\n    SnapshotIndex deserialized = snapshotIndexSerde.fromBytes(serialized);\r\n    Assert.assertNotNull(deserialized);\r\n    Assert.assertEquals(deserialized, testRemoteSnapshot);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testInitWithInvalidCheckpoint",
  "sourceCode" : "@Test\r\npublic void testInitWithInvalidCheckpoint() {\r\n    // init called with null checkpoint storeStorageEngineMap\r\n    blobStoreBackupManager.init(null);\r\n    // verify delete snapshot index blob called from init 0 times because prevSnapshotMap returned from init is empty\r\n    // in case of null checkpoint.\r\n    verify(blobStoreUtil, times(0)).deleteSnapshotIndexBlob(anyString(), any(Metadata.class));\r\n    when(blobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), anyBoolean())).thenCallRealMethod();\r\n    // init called with Checkpoint V1 -> unsupported\r\n    Checkpoint checkpoint = new CheckpointV1(new HashMap<>());\r\n    try {\r\n        blobStoreBackupManager.init(checkpoint);\r\n    } catch (SamzaException exception) {\r\n        Assert.fail(\"Checkpoint V1 is expected to only log warning.\");\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testUploadWithNoPreviousCheckpoints",
  "sourceCode" : "@Test\r\npublic void testUploadWithNoPreviousCheckpoints() throws IOException {\r\n    // Track directory for post cleanup\r\n    List<String> checkpointDirsToClean = new ArrayList<>();\r\n    // Setup: init local/remote snapshots and back manager with no previous checkpoints\r\n    indexBlobIdAndLocalRemoteSnapshotsPair = setupRemoteAndLocalSnapshots(false);\r\n    Checkpoint checkpoint = new CheckpointV2(checkpointId, new HashMap<>(), ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), new HashMap<>()));\r\n    blobStoreBackupManager.init(checkpoint);\r\n    // mock: set task store dir to return corresponding test local store and create checkpoint dir\r\n    ArgumentCaptor<String> stringCaptor = ArgumentCaptor.forClass(String.class);\r\n    when(storageManagerUtil.getTaskStoreDir(any(File.class), stringCaptor.capture(), any(TaskName.class), any(TaskMode.class))).then((Answer<File>) invocation -> {\r\n        String storeName = invocation.getArgumentAt(1, String.class);\r\n        String snapshotIndexBlobId = testStoreNameAndSCMMap.get(storeName);\r\n        String storeDir = indexBlobIdAndLocalRemoteSnapshotsPair.get(snapshotIndexBlobId).getLeft();\r\n        try {\r\n            BlobStoreTestUtil.createTestCheckpointDirectory(storeDir, checkpointId.serialize());\r\n            checkpointDirsToClean.add(storeDir + \"-\" + checkpointId.serialize());\r\n        } catch (IOException e) {\r\n            Assert.fail(\"Couldn't create checkpoint directory. Test failed.\");\r\n        }\r\n        return new File(storeDir);\r\n    });\r\n    ArgumentCaptor<File> storeDirCaptor = ArgumentCaptor.forClass(File.class);\r\n    when(storageManagerUtil.getStoreCheckpointDir(storeDirCaptor.capture(), eq(checkpointId))).thenAnswer(new Answer<String>() {\r\n\r\n        @Override\r\n        public String answer(InvocationOnMock invocation) throws Throwable {\r\n            File storeDir = invocation.getArgumentAt(0, File.class);\r\n            return storeDir.getAbsolutePath() + \"-\" + checkpointId.serialize();\r\n        }\r\n    });\r\n    SortedSet<DirDiff> actualDirDiffs = new TreeSet<>(Comparator.comparing(DirDiff::getDirName));\r\n    // mock: mock putDir and capture DirDiff\r\n    ArgumentCaptor<DirDiff> dirDiffCaptor = ArgumentCaptor.forClass(DirDiff.class);\r\n    ArgumentCaptor<SnapshotMetadata> snapshotMetadataCaptor = ArgumentCaptor.forClass(SnapshotMetadata.class);\r\n    when(blobStoreUtil.putDir(dirDiffCaptor.capture(), snapshotMetadataCaptor.capture())).then((Answer<CompletableFuture<DirIndex>>) invocation -> {\r\n        DirDiff dirDiff = invocation.getArgumentAt(0, DirDiff.class);\r\n        SnapshotMetadata snapshotMetadata = invocation.getArgumentAt(1, SnapshotMetadata.class);\r\n        actualDirDiffs.add(dirDiff);\r\n        SnapshotIndex snapshotIndex = testBlobStore.get(testStoreNameAndSCMMap.get(snapshotMetadata.getStoreName()));\r\n        return CompletableFuture.completedFuture(snapshotIndex.getDirIndex());\r\n    });\r\n    SortedSet<SnapshotIndex> expectedSnapshotIndexesUploaded = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(Pair::getRight).collect(Collectors.toCollection(() -> new TreeSet<>(Comparator.comparing(SnapshotIndex::getCreationTimeMillis))));\r\n    String expectedPreviousSnapshotIndexBlobId = \"empty\";\r\n    // mock: mock putSnapshotIndex and capture previous snapshot index\r\n    SortedSet<SnapshotIndex> actualSnapshotIndexesUploaded = new TreeSet<>(Comparator.comparing(SnapshotIndex::getCreationTimeMillis));\r\n    final String[] actualPreviousSnapshotIndexBlobId = { \"empty\" };\r\n    ArgumentCaptor<SnapshotIndex> snapshotIndexCaptor = ArgumentCaptor.forClass(SnapshotIndex.class);\r\n    when(blobStoreUtil.putSnapshotIndex(snapshotIndexCaptor.capture())).then((Answer<CompletableFuture<String>>) invocation -> {\r\n        SnapshotIndex snapshotIndex = invocation.getArgumentAt(0, SnapshotIndex.class);\r\n        actualSnapshotIndexesUploaded.add(snapshotIndex);\r\n        if (!snapshotIndex.getPrevSnapshotIndexBlobId().equals(Optional.empty())) {\r\n            actualPreviousSnapshotIndexBlobId[0] = \"not-empty\";\r\n        }\r\n        return CompletableFuture.completedFuture(\"random-blob-id\");\r\n    });\r\n    // execute\r\n    blobStoreBackupManager.upload(checkpointId, testStoreNameAndSCMMap);\r\n    // setup expected dir diffs after execute: needs checkpoint dirs created in upload()\r\n    TreeSet<DirDiff> expectedDirDiffs = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(localRemoteSnapshotPair -> {\r\n        File localCheckpointDir = new File(localRemoteSnapshotPair.getLeft() + \"-\" + checkpointId.serialize());\r\n        DirIndex dirIndex = new DirIndex(localCheckpointDir.getName(), Collections.emptyList(), Collections.emptyList(), Collections.emptyList(), Collections.emptyList());\r\n        return DirDiffUtil.getDirDiff(localCheckpointDir, dirIndex, DirDiffUtil.areSameFile(false, true));\r\n    }).collect(Collectors.toCollection(() -> new TreeSet<>(Comparator.comparing(DirDiff::getDirName))));\r\n    // assert - asset all DirDiff are put to blob store\r\n    Assert.assertEquals(actualDirDiffs, expectedDirDiffs);\r\n    // assert - assert no previous snapshot indexes were found\r\n    Assert.assertEquals(actualPreviousSnapshotIndexBlobId[0], expectedPreviousSnapshotIndexBlobId);\r\n    // assert - assert all snapshot indexes are uploaded\r\n    Assert.assertEquals(actualSnapshotIndexesUploaded, expectedSnapshotIndexesUploaded);\r\n    // cleanup\r\n    checkpointDirsToClean.forEach(path -> {\r\n        try {\r\n            if (Files.exists(Paths.get(path)) && Files.isDirectory(Paths.get(path))) {\r\n                PathUtils.deleteDirectory(Paths.get(path));\r\n            }\r\n        } catch (IOException exception) {\r\n            Assert.fail(\"Failed to cleanup temporary checkpoint dirs.\");\r\n        }\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testUploadWithPreviousCheckpoints",
  "sourceCode" : "@Test\r\npublic void testUploadWithPreviousCheckpoints() throws IOException {\r\n    // Track directory for post cleanup\r\n    List<String> checkpointDirsToClean = new ArrayList<>();\r\n    // Setup: init back manager with previous checkpoints\r\n    //indexBlobIdAndLocalRemoteSnapshotsPair = setupRemoteAndLocalSnapshots(true);\r\n    Map<String, String> previousCheckpoints = // map store name, previous snapshot index blob id\r\n    indexBlobIdAndLocalRemoteSnapshotsPair.entrySet().stream().collect(Collectors.toMap(e -> e.getValue().getLeft(), e -> e.getValue().getRight().getPrevSnapshotIndexBlobId().get()));\r\n    Checkpoint checkpoint = new CheckpointV2(checkpointId, new HashMap<>(), ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), previousCheckpoints));\r\n    when(blobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), anyBoolean())).thenCallRealMethod();\r\n    when(blobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), anyBoolean())).thenCallRealMethod();\r\n    blobStoreBackupManager.init(checkpoint);\r\n    // mock: set task store dir to return corresponding test local store and create checkpoint dir\r\n    ArgumentCaptor<String> stringCaptor = ArgumentCaptor.forClass(String.class);\r\n    when(storageManagerUtil.getTaskStoreDir(any(File.class), stringCaptor.capture(), any(TaskName.class), any(TaskMode.class))).then((Answer<File>) invocation -> {\r\n        String storeName = invocation.getArgumentAt(1, String.class);\r\n        String snapshotIndexBlobId = testStoreNameAndSCMMap.get(storeName);\r\n        String storeDir = indexBlobIdAndLocalRemoteSnapshotsPair.get(snapshotIndexBlobId).getLeft();\r\n        try {\r\n            BlobStoreTestUtil.createTestCheckpointDirectory(storeDir, checkpointId.serialize());\r\n            checkpointDirsToClean.add(storeDir + \"-\" + checkpointId.serialize());\r\n        } catch (IOException e) {\r\n            Assert.fail(\"Couldn't create checkpoint directory. Test failed.\");\r\n        }\r\n        return new File(storeDir);\r\n    });\r\n    ArgumentCaptor<File> storeDirCaptor = ArgumentCaptor.forClass(File.class);\r\n    when(storageManagerUtil.getStoreCheckpointDir(storeDirCaptor.capture(), eq(checkpointId))).thenAnswer(new Answer<String>() {\r\n\r\n        @Override\r\n        public String answer(InvocationOnMock invocation) throws Throwable {\r\n            File storeDir = invocation.getArgumentAt(0, File.class);\r\n            return storeDir.getAbsolutePath() + \"-\" + checkpointId.serialize();\r\n        }\r\n    });\r\n    // mock: mock putDir and capture DirDiff\r\n    SortedSet<DirDiff> actualDirDiffs = new TreeSet<>(Comparator.comparing(DirDiff::getDirName));\r\n    ArgumentCaptor<DirDiff> dirDiffCaptor = ArgumentCaptor.forClass(DirDiff.class);\r\n    ArgumentCaptor<SnapshotMetadata> snapshotMetadataCaptor = ArgumentCaptor.forClass(SnapshotMetadata.class);\r\n    when(blobStoreUtil.putDir(dirDiffCaptor.capture(), snapshotMetadataCaptor.capture())).then((Answer<CompletableFuture<DirIndex>>) invocation -> {\r\n        DirDiff dirDiff = invocation.getArgumentAt(0, DirDiff.class);\r\n        SnapshotMetadata snapshotMetadata = invocation.getArgumentAt(1, SnapshotMetadata.class);\r\n        actualDirDiffs.add(dirDiff);\r\n        SnapshotIndex snapshotIndex = testBlobStore.get(testStoreNameAndSCMMap.get(snapshotMetadata.getStoreName()));\r\n        return CompletableFuture.completedFuture(snapshotIndex.getDirIndex());\r\n    });\r\n    // mock: mock putSnapshotIndex and capture previous snapshot index\r\n    SortedSet<SnapshotIndex> expectedSnapshotIndexesUploaded = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(Pair::getRight).collect(Collectors.toCollection(() -> new TreeSet<>(Comparator.comparing(SnapshotIndex::getCreationTimeMillis))));\r\n    SortedSet<SnapshotIndex> actualSnapshotIndexesUploaded = new TreeSet<>(Comparator.comparing(SnapshotIndex::getCreationTimeMillis));\r\n    SortedSet<String> actualPreviousSnapshotIndexBlobIds = new TreeSet<>();\r\n    SortedSet<String> expectedPreviousSnapshotIndexBlobIds = new TreeSet<>(previousCheckpoints.values());\r\n    ArgumentCaptor<SnapshotIndex> snapshotIndexCaptor = ArgumentCaptor.forClass(SnapshotIndex.class);\r\n    when(blobStoreUtil.putSnapshotIndex(snapshotIndexCaptor.capture())).then((Answer<CompletableFuture<String>>) invocation -> {\r\n        SnapshotIndex snapshotIndex = invocation.getArgumentAt(0, SnapshotIndex.class);\r\n        actualSnapshotIndexesUploaded.add(snapshotIndex);\r\n        if (snapshotIndex.getPrevSnapshotIndexBlobId().isPresent()) {\r\n            actualPreviousSnapshotIndexBlobIds.add(snapshotIndex.getPrevSnapshotIndexBlobId().get());\r\n        }\r\n        return CompletableFuture.completedFuture(\"random-blob-id\");\r\n    });\r\n    // execute\r\n    blobStoreBackupManager.upload(checkpointId, ImmutableMap.of());\r\n    TreeSet<DirDiff> expectedDirDiffs = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(localRemoteSnapshotPair -> DirDiffUtil.getDirDiff(new File(localRemoteSnapshotPair.getLeft() + \"-\" + checkpointId.serialize()), localRemoteSnapshotPair.getRight().getDirIndex(), DirDiffUtil.areSameFile(false, true))).collect(Collectors.toCollection(() -> new TreeSet<>(Comparator.comparing(DirDiff::getDirName))));\r\n    // assert - asset all DirDiff are put to blob store\r\n    Assert.assertEquals(actualDirDiffs, expectedDirDiffs);\r\n    // assert - assert no previous snapshot indexes were found\r\n    Assert.assertEquals(actualPreviousSnapshotIndexBlobIds, expectedPreviousSnapshotIndexBlobIds);\r\n    // assert - assert all snapshot indexes are uploaded\r\n    Assert.assertEquals(actualSnapshotIndexesUploaded, expectedSnapshotIndexesUploaded);\r\n    // cleanup\r\n    checkpointDirsToClean.forEach(path -> {\r\n        try {\r\n            if (Files.exists(Paths.get(path)) && Files.isDirectory(Paths.get(path))) {\r\n                PathUtils.deleteDirectory(Paths.get(path));\r\n            }\r\n        } catch (IOException exception) {\r\n            Assert.fail(\"Failed to cleanup temporary checkpoint dirs.\");\r\n        }\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testCleanupRemovesTTLForAllIndexBlobs",
  "sourceCode" : "@Test\r\npublic void testCleanupRemovesTTLForAllIndexBlobs() {\r\n    SortedSet<String> actualRemoveTTLsResult = new TreeSet<>(testStoreNameAndSCMMap.values());\r\n    SortedSet<String> expectedRemoveTTLsResult = new TreeSet<>();\r\n    // mock\r\n    ArgumentCaptor<String> captor = ArgumentCaptor.forClass(String.class);\r\n    when(blobStoreUtil.removeTTL(captor.capture(), any(SnapshotIndex.class), any(Metadata.class))).then((Answer<CompletionStage<Void>>) invocation -> {\r\n        String blobId = invocation.getArgumentAt(0, String.class);\r\n        expectedRemoveTTLsResult.add(blobId);\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    // stub out non-tested methods\r\n    when(blobStoreUtil.cleanUpDir(any(DirIndex.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(blobStoreUtil.deleteSnapshotIndexBlob(any(String.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    // execute\r\n    blobStoreBackupManager.cleanUp(checkpointId, testStoreNameAndSCMMap);\r\n    // Assert\r\n    Assert.assertEquals(actualRemoveTTLsResult, expectedRemoveTTLsResult);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testCleanupCleansUpRemoteSnapshot",
  "sourceCode" : "@Test\r\npublic void testCleanupCleansUpRemoteSnapshot() throws Exception {\r\n    SortedSet<DirIndex> actualCleanedupDirs = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(remoteLocalPair -> remoteLocalPair.getRight().getDirIndex()).collect(Collectors.toCollection(() -> new TreeSet<>(Comparator.comparing(DirIndex::getDirName))));\r\n    SortedSet<DirIndex> expectedCleanupDirs = new TreeSet<>(Comparator.comparing(DirIndex::getDirName));\r\n    // mock\r\n    ArgumentCaptor<DirIndex> captor = ArgumentCaptor.forClass(DirIndex.class);\r\n    when(blobStoreUtil.cleanUpDir(captor.capture(), any(Metadata.class))).then((Answer<CompletionStage<Void>>) invocation -> {\r\n        DirIndex dirIndex = invocation.getArgumentAt(0, DirIndex.class);\r\n        expectedCleanupDirs.add(dirIndex);\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    // stub out non-tested methods\r\n    when(blobStoreUtil.removeTTL(anyString(), any(SnapshotIndex.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(blobStoreUtil.deleteSnapshotIndexBlob(any(String.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    // execute\r\n    blobStoreBackupManager.cleanUp(checkpointId, testStoreNameAndSCMMap);\r\n    // Assert\r\n    Assert.assertEquals(actualCleanedupDirs, expectedCleanupDirs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testCleanupRemovesOldSnapshots",
  "sourceCode" : "@Test\r\npublic void testCleanupRemovesOldSnapshots() throws Exception {\r\n    TreeSet<String> expectedOldSnapshotsRemoved = indexBlobIdAndLocalRemoteSnapshotsPair.values().stream().map(remoteLocalPair -> {\r\n        Optional<String> prevSnapshotIndexBlobId = remoteLocalPair.getRight().getPrevSnapshotIndexBlobId();\r\n        return prevSnapshotIndexBlobId.orElse(null);\r\n    }).collect(Collectors.toCollection(TreeSet::new));\r\n    SortedSet<String> actualOldSnapshotsRemoved = new TreeSet<>();\r\n    // mock\r\n    ArgumentCaptor<String> captor = ArgumentCaptor.forClass(String.class);\r\n    when(blobStoreUtil.deleteSnapshotIndexBlob(captor.capture(), any(Metadata.class))).then((Answer<CompletionStage<Void>>) invocation -> {\r\n        String prevIndexBlobId = invocation.getArgumentAt(0, String.class);\r\n        actualOldSnapshotsRemoved.add(prevIndexBlobId);\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    // stub out non-tested methods\r\n    when(blobStoreUtil.removeTTL(anyString(), any(SnapshotIndex.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(blobStoreUtil.cleanUpDir(any(DirIndex.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    // execute\r\n    blobStoreBackupManager.cleanUp(checkpointId, testStoreNameAndSCMMap);\r\n    // Assert\r\n    Assert.assertEquals(actualOldSnapshotsRemoved, expectedOldSnapshotsRemoved);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreBackupManager.java",
  "methodName" : "testCleanupIgnoresStoresNotConfiguredWithBlobStoreStateBackend",
  "sourceCode" : "@Test\r\npublic void testCleanupIgnoresStoresNotConfiguredWithBlobStoreStateBackend() throws Exception {\r\n    // TODO HIGH shesharm Complete test\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testDeleteUnusedStoresRemovesStoresDeletedFromConfig",
  "sourceCode" : "@Test\r\npublic void testDeleteUnusedStoresRemovesStoresDeletedFromConfig() {\r\n    String jobName = \"testJobName\";\r\n    String jobId = \"testJobId\";\r\n    String taskName = \"taskName\";\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    BlobStoreConfig blobStoreConfig = mock(BlobStoreConfig.class);\r\n    SnapshotIndex mockSnapshotIndex = mock(SnapshotIndex.class);\r\n    String blobId = \"blobId\";\r\n    Map<String, Pair<String, SnapshotIndex>> initialStoreSnapshotIndexes = ImmutableMap.of(\"oldStoreName\", Pair.of(blobId, mockSnapshotIndex));\r\n    when(storageConfig.getStoresWithBackupFactory(eq(BlobStoreStateBackendFactory.class.getName()))).thenReturn(ImmutableList.of(\"newStoreName\"));\r\n    when(storageConfig.getStoresWithRestoreFactory(eq(BlobStoreStateBackendFactory.class.getName()))).thenReturn(ImmutableList.of(\"newStoreName\"));\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    when(mockSnapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    BlobStoreUtil blobStoreUtil = mock(BlobStoreUtil.class);\r\n    when(blobStoreUtil.cleanSnapshotIndex(anyString(), any(SnapshotIndex.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(null));\r\n    BlobStoreRestoreManager.deleteUnusedStoresFromBlobStore(jobName, jobId, taskName, storageConfig, blobStoreConfig, initialStoreSnapshotIndexes, blobStoreUtil, EXECUTOR);\r\n    verify(blobStoreUtil, times(1)).cleanSnapshotIndex(eq(blobId), any(SnapshotIndex.class), any(Metadata.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testShouldRestoreIfNoCheckpointDir",
  "sourceCode" : "@Test\r\npublic void testShouldRestoreIfNoCheckpointDir() throws IOException {\r\n    String taskName = \"taskName\";\r\n    String storeName = \"storeName\";\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    Path storeCheckpointDir = Paths.get(\"/tmp/non-existent-checkpoint-dir\");\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    when(storageConfig.cleanLoggedStoreDirsOnStart(anyString())).thenReturn(false);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    boolean shouldRestore = BlobStoreRestoreManager.shouldRestore(taskName, storeName, dirIndex, storeCheckpointDir, storageConfig, dirDiffUtil);\r\n    verifyZeroInteractions(dirDiffUtil);\r\n    assertTrue(shouldRestore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testShouldRestoreIfCleanStateOnRestartEnabled",
  "sourceCode" : "@Test\r\npublic void testShouldRestoreIfCleanStateOnRestartEnabled() throws IOException {\r\n    String taskName = \"taskName\";\r\n    String storeName = \"storeName\";\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    // must exist\r\n    Path storeCheckpointDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    // clean on restart\r\n    when(storageConfig.cleanLoggedStoreDirsOnStart(anyString())).thenReturn(true);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    boolean shouldRestore = BlobStoreRestoreManager.shouldRestore(taskName, storeName, dirIndex, storeCheckpointDir, storageConfig, dirDiffUtil);\r\n    verifyZeroInteractions(dirDiffUtil);\r\n    // should not restore, should retain checkpoint dir instead\r\n    assertTrue(shouldRestore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testShouldRestoreIfCheckpointDirNotIdenticalToRemoteSnapshot",
  "sourceCode" : "@Test\r\npublic void testShouldRestoreIfCheckpointDirNotIdenticalToRemoteSnapshot() throws IOException {\r\n    String taskName = \"taskName\";\r\n    String storeName = \"storeName\";\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    // must exist\r\n    Path storeCheckpointDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    when(storageConfig.cleanLoggedStoreDirsOnStart(anyString())).thenReturn(false);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    when(dirDiffUtil.areSameDir(anySet(), anyBoolean(), anyBoolean())).thenReturn((arg1, arg2) -> false);\r\n    boolean shouldRestore = BlobStoreRestoreManager.shouldRestore(taskName, storeName, dirIndex, storeCheckpointDir, storageConfig, dirDiffUtil);\r\n    assertTrue(shouldRestore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testShouldNotRestoreIfPreviousCheckpointDirIdenticalToRemoteSnapshot",
  "sourceCode" : "@Test\r\npublic void testShouldNotRestoreIfPreviousCheckpointDirIdenticalToRemoteSnapshot() throws IOException {\r\n    String taskName = \"taskName\";\r\n    String storeName = \"storeName\";\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    // must exist\r\n    Path storeCheckpointDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    when(storageConfig.cleanLoggedStoreDirsOnStart(anyString())).thenReturn(false);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    // are same dir\r\n    when(dirDiffUtil.areSameDir(anySet(), anyBoolean(), anyBoolean())).thenReturn((arg1, arg2) -> true);\r\n    boolean shouldRestore = BlobStoreRestoreManager.shouldRestore(taskName, storeName, dirIndex, storeCheckpointDir, storageConfig, dirDiffUtil);\r\n    verify(dirDiffUtil, times(1)).areSameDir(anySet(), anyBoolean(), anyBoolean());\r\n    // should not restore, should retain checkpoint dir instead\r\n    assertFalse(shouldRestore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testRestoreDeletesStoreDir",
  "sourceCode" : "@Test\r\npublic void testRestoreDeletesStoreDir() throws IOException {\r\n    String jobName = \"testJobName\";\r\n    String jobId = \"testJobId\";\r\n    TaskName taskName = mock(TaskName.class);\r\n    BlobStoreRestoreManagerMetrics metrics = new BlobStoreRestoreManagerMetrics(new MetricsRegistryMap());\r\n    metrics.initStoreMetrics(ImmutableList.of(\"storeName\"));\r\n    Set<String> storesToRestore = ImmutableSet.of(\"storeName\");\r\n    SnapshotIndex snapshotIndex = mock(SnapshotIndex.class);\r\n    Map<String, Pair<String, SnapshotIndex>> prevStoreSnapshotIndexes = ImmutableMap.of(\"storeName\", Pair.of(\"blobId\", snapshotIndex));\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(\"[a]\");\r\n    when(snapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    when(snapshotIndex.getSnapshotMetadata()).thenReturn(new SnapshotMetadata(CheckpointId.create(), \"jobName\", \"jobId\", \"taskName\", \"storeName\"));\r\n    Path loggedBaseDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    // create store dir to be deleted during restore\r\n    Path storeDir = Files.createTempDirectory(loggedBaseDir, \"storeDir\");\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), any(CheckpointId.class))).thenReturn(Paths.get(storeDir.toString(), \"checkpointId\").toString());\r\n    when(storageManagerUtil.getTaskStoreDir(eq(loggedBaseDir.toFile()), eq(\"storeName\"), eq(taskName), eq(TaskMode.Active))).thenReturn(storeDir.toFile());\r\n    BlobStoreUtil blobStoreUtil = mock(BlobStoreUtil.class);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    // return immediately without restoring.\r\n    when(blobStoreUtil.restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean())).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(dirDiffUtil.areSameDir(anySet(), anyBoolean(), anyBoolean())).thenReturn((arg1, arg2) -> true);\r\n    BlobStoreRestoreManager.restoreStores(jobName, jobId, taskName, storesToRestore, prevStoreSnapshotIndexes, loggedBaseDir.toFile(), storageConfig, metrics, storageManagerUtil, blobStoreUtil, dirDiffUtil, EXECUTOR, false, true);\r\n    // verify that the store directory restore was called and skipped (i.e. shouldRestore == true)\r\n    verify(blobStoreUtil, times(1)).restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean());\r\n    // verify that the store directory was deleted prior to restore\r\n    // (should still not exist at the end since restore is no-op)\r\n    assertFalse(storeDir.toFile().exists());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testRestoreDeletesCheckpointDirsIfRestoring",
  "sourceCode" : "@Test\r\npublic void testRestoreDeletesCheckpointDirsIfRestoring() throws IOException {\r\n    String jobName = \"testJobName\";\r\n    String jobId = \"testJobId\";\r\n    TaskName taskName = mock(TaskName.class);\r\n    BlobStoreRestoreManagerMetrics metrics = new BlobStoreRestoreManagerMetrics(new MetricsRegistryMap());\r\n    metrics.initStoreMetrics(ImmutableList.of(\"storeName\"));\r\n    Set<String> storesToRestore = ImmutableSet.of(\"storeName\");\r\n    SnapshotIndex snapshotIndex = mock(SnapshotIndex.class);\r\n    Map<String, Pair<String, SnapshotIndex>> prevStoreSnapshotIndexes = ImmutableMap.of(\"storeName\", Pair.of(\"blobId\", snapshotIndex));\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(\"[a]\");\r\n    when(snapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    when(snapshotIndex.getSnapshotMetadata()).thenReturn(new SnapshotMetadata(checkpointId, \"jobName\", \"jobId\", \"taskName\", \"storeName\"));\r\n    Path loggedBaseDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    // create store dir to be deleted during restore\r\n    Path storeDir = Files.createTempDirectory(loggedBaseDir, \"storeDir\");\r\n    Path storeCheckpointDir1 = Files.createTempDirectory(loggedBaseDir, \"storeDir-\" + checkpointId);\r\n    CheckpointId olderCheckpoint = CheckpointId.create();\r\n    Path storeCheckpointDir2 = Files.createTempDirectory(loggedBaseDir, \"storeDir-\" + olderCheckpoint);\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    when(storageManagerUtil.getTaskStoreDir(eq(loggedBaseDir.toFile()), eq(\"storeName\"), eq(taskName), eq(TaskMode.Active))).thenReturn(storeDir.toFile());\r\n    when(storageManagerUtil.getStoreCheckpointDir(eq(storeDir.toFile()), eq(checkpointId))).thenReturn(Paths.get(storeDir.toString(), checkpointId.toString()).toString());\r\n    when(storageManagerUtil.getTaskStoreCheckpointDirs(any(File.class), anyString(), any(TaskName.class), any(TaskMode.class))).thenReturn(ImmutableList.of(storeCheckpointDir1.toFile(), storeCheckpointDir2.toFile()));\r\n    BlobStoreUtil blobStoreUtil = mock(BlobStoreUtil.class);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    when(dirDiffUtil.areSameDir(anySet(), anyBoolean(), anyBoolean())).thenReturn((arg1, arg2) -> true);\r\n    // return immediately without restoring.\r\n    when(blobStoreUtil.restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean())).thenReturn(CompletableFuture.completedFuture(null));\r\n    BlobStoreRestoreManager.restoreStores(jobName, jobId, taskName, storesToRestore, prevStoreSnapshotIndexes, loggedBaseDir.toFile(), storageConfig, metrics, storageManagerUtil, blobStoreUtil, dirDiffUtil, EXECUTOR, false, true);\r\n    // verify that the store directory restore was called and skipped (i.e. shouldRestore == true)\r\n    verify(blobStoreUtil, times(1)).restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean());\r\n    // verify that the checkpoint directories were deleted prior to restore (should not exist at the end)\r\n    assertFalse(storeCheckpointDir1.toFile().exists());\r\n    assertFalse(storeCheckpointDir2.toFile().exists());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testRestoreRetainsCheckpointDirsIfValid",
  "sourceCode" : "@Test\r\npublic void testRestoreRetainsCheckpointDirsIfValid() throws IOException {\r\n    String jobName = \"testJobName\";\r\n    String jobId = \"testJobId\";\r\n    TaskName taskName = mock(TaskName.class);\r\n    BlobStoreRestoreManagerMetrics metrics = new BlobStoreRestoreManagerMetrics(new MetricsRegistryMap());\r\n    metrics.initStoreMetrics(ImmutableList.of(\"storeName\"));\r\n    Set<String> storesToRestore = ImmutableSet.of(\"storeName\");\r\n    SnapshotIndex snapshotIndex = mock(SnapshotIndex.class);\r\n    Map<String, Pair<String, SnapshotIndex>> prevStoreSnapshotIndexes = ImmutableMap.of(\"storeName\", Pair.of(\"blobId\", snapshotIndex));\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(\"[a]\");\r\n    when(snapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    when(snapshotIndex.getSnapshotMetadata()).thenReturn(new SnapshotMetadata(checkpointId, \"jobName\", \"jobId\", \"taskName\", \"storeName\"));\r\n    Path loggedBaseDir = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    // create store dir to be deleted during restore\r\n    Path storeDir = Files.createTempDirectory(loggedBaseDir, \"storeDir-\");\r\n    // create checkpoint dir so that shouldRestore = false (areSameDir == true later)\r\n    Path storeCheckpointDir = Files.createTempDirectory(loggedBaseDir, \"storeDir-\" + checkpointId + \"-\");\r\n    // create a dummy file to verify after dir rename.\r\n    Path tempFile = Files.createTempFile(storeCheckpointDir, \"tempFile-\", null);\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    when(storageManagerUtil.getTaskStoreDir(eq(loggedBaseDir.toFile()), eq(\"storeName\"), eq(taskName), eq(TaskMode.Active))).thenReturn(storeDir.toFile());\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), eq(checkpointId))).thenReturn(storeCheckpointDir.toString());\r\n    when(storageManagerUtil.getTaskStoreCheckpointDirs(any(File.class), anyString(), any(TaskName.class), any(TaskMode.class))).thenReturn(ImmutableList.of(storeCheckpointDir.toFile()));\r\n    BlobStoreUtil blobStoreUtil = mock(BlobStoreUtil.class);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    // ensures shouldRestore is not called\r\n    when(dirDiffUtil.areSameDir(anySet(), anyBoolean(), anyBoolean())).thenReturn((arg1, arg2) -> true);\r\n    // return immediately without restoring.\r\n    when(blobStoreUtil.restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean())).thenReturn(CompletableFuture.completedFuture(null));\r\n    BlobStoreRestoreManager.restoreStores(jobName, jobId, taskName, storesToRestore, prevStoreSnapshotIndexes, loggedBaseDir.toFile(), storageConfig, metrics, storageManagerUtil, blobStoreUtil, dirDiffUtil, EXECUTOR, false, true);\r\n    // verify that the store directory restore was not called (should have restored from checkpoint dir)\r\n    verify(blobStoreUtil, times(0)).restoreDir(eq(storeDir.toFile()), eq(dirIndex), any(Metadata.class), anyBoolean());\r\n    // verify that the checkpoint dir was renamed to store dir\r\n    assertFalse(storeCheckpointDir.toFile().exists());\r\n    assertTrue(storeDir.toFile().exists());\r\n    assertTrue(Files.exists(Paths.get(storeDir.toString(), tempFile.getFileName().toString())));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\TestBlobStoreRestoreManager.java",
  "methodName" : "testRestoreSkipsStoresWithMissingCheckpointSCM",
  "sourceCode" : "@Test\r\npublic void testRestoreSkipsStoresWithMissingCheckpointSCM() {\r\n    // store renamed from oldStoreName to newStoreName. No SCM for newStoreName in previous checkpoint.\r\n    String jobName = \"testJobName\";\r\n    String jobId = \"testJobId\";\r\n    TaskName taskName = mock(TaskName.class);\r\n    BlobStoreRestoreManagerMetrics metrics = new BlobStoreRestoreManagerMetrics(new MetricsRegistryMap());\r\n    metrics.initStoreMetrics(ImmutableList.of(\"newStoreName\"));\r\n    // new store in config\r\n    Set<String> storesToRestore = ImmutableSet.of(\"newStoreName\");\r\n    SnapshotIndex snapshotIndex = mock(SnapshotIndex.class);\r\n    Map<String, Pair<String, SnapshotIndex>> prevStoreSnapshotIndexes = mock(Map.class);\r\n    when(prevStoreSnapshotIndexes.containsKey(\"newStoreName\")).thenReturn(false);\r\n    DirIndex dirIndex = mock(DirIndex.class);\r\n    when(snapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    when(snapshotIndex.getSnapshotMetadata()).thenReturn(new SnapshotMetadata(checkpointId, \"jobName\", \"jobId\", \"taskName\", \"storeName\"));\r\n    Path loggedBaseDir = mock(Path.class);\r\n    // create store dir to be deleted during restore\r\n    StorageConfig storageConfig = mock(StorageConfig.class);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    BlobStoreUtil blobStoreUtil = mock(BlobStoreUtil.class);\r\n    DirDiffUtil dirDiffUtil = mock(DirDiffUtil.class);\r\n    BlobStoreRestoreManager.restoreStores(jobName, jobId, taskName, storesToRestore, prevStoreSnapshotIndexes, loggedBaseDir.toFile(), storageConfig, metrics, storageManagerUtil, blobStoreUtil, dirDiffUtil, EXECUTOR, false, true);\r\n    // verify that we checked the previously checkpointed SCMs.\r\n    verify(prevStoreSnapshotIndexes, times(1)).containsKey(eq(\"newStoreName\"));\r\n    // verify that the store directory restore was never called\r\n    verify(blobStoreUtil, times(0)).restoreDir(any(File.class), any(DirIndex.class), any(Metadata.class), anyBoolean());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testPutDir",
  "sourceCode" : "@Test\r\npublic // TODO HIGH shesharm test with empty (0 byte) files\r\nvoid testPutDir() throws IOException, InterruptedException, ExecutionException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    String local = \"[a, c, z/1, y/2, p/m/3, q/n/4]\";\r\n    String remote = \"[a, b, z/1, x/2, p/m/3, p/m/5, r/o/6]\";\r\n    String expectedAdded = \"[c, y/2, q/n/4]\";\r\n    String expectedRetained = \"[a, z/1, p/m/3]\";\r\n    String expectedRemoved = \"[b, x/2, r/o/6, p/m/5]\";\r\n    SortedSet<String> expectedAddedFiles = BlobStoreTestUtil.getExpected(expectedAdded);\r\n    SortedSet<String> expectedRetainedFiles = BlobStoreTestUtil.getExpected(expectedRetained);\r\n    SortedSet<String> expectedPresentFiles = new TreeSet<>(expectedAddedFiles);\r\n    expectedPresentFiles.addAll(expectedRetainedFiles);\r\n    SortedSet<String> expectedRemovedFiles = BlobStoreTestUtil.getExpected(expectedRemoved);\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    SortedSet<String> allUploaded = new TreeSet<>();\r\n    // Set up mocks\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<String>>) invocation -> {\r\n        Metadata metadata = invocation.getArgumentAt(1, Metadata.class);\r\n        String path = metadata.getPayloadPath();\r\n        allUploaded.add(path.substring(localSnapshotDir.toAbsolutePath().toString().length() + 1));\r\n        return CompletableFuture.completedFuture(path);\r\n    });\r\n    // Execute\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    DirIndex dirIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndex = dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    SortedSet<String> allPresent = new TreeSet<>();\r\n    SortedSet<String> allRemoved = new TreeSet<>();\r\n    BlobStoreTestUtil.getAllPresentInIndex(\"\", dirIndex, allPresent);\r\n    BlobStoreTestUtil.getAllRemovedInIndex(\"\", dirIndex, allRemoved);\r\n    // Assert\r\n    assertEquals(expectedAddedFiles, allUploaded);\r\n    assertEquals(expectedPresentFiles, allPresent);\r\n    assertEquals(expectedRemovedFiles, allRemoved);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testPutDirFailsIfAnyFileUploadFails",
  "sourceCode" : "@Test\r\npublic void testPutDirFailsIfAnyFileUploadFails() throws IOException, TimeoutException, InterruptedException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    String local = \"[a, b]\";\r\n    String remote = \"[]\";\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    // Set up mocks\r\n    SamzaException exception = new SamzaException(\"Error uploading file\");\r\n    CompletableFuture<String> failedFuture = new CompletableFuture<>();\r\n    failedFuture.completeExceptionally(exception);\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<String>>) invocation -> {\r\n        Metadata metadata = invocation.getArgumentAt(1, Metadata.class);\r\n        String path = metadata.getPayloadPath();\r\n        if (path.endsWith(\"a\")) {\r\n            return CompletableFuture.completedFuture(\"aBlobId\");\r\n        } else {\r\n            return failedFuture;\r\n        }\r\n    });\r\n    // Execute\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (ExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        // Assert that the result future fails and that the cause is propagated correctly\r\n        assertEquals(exception, cause);\r\n        return;\r\n    }\r\n    fail(\"DirIndex future should have been completed with an exception\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testPutDirFailsIfAnySubDirFileUploadFails",
  "sourceCode" : "@Test\r\npublic void testPutDirFailsIfAnySubDirFileUploadFails() throws IOException, TimeoutException, InterruptedException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    String local = \"[a/1, b/2]\";\r\n    String remote = \"[]\";\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    // Set up mocks\r\n    SamzaException exception = new SamzaException(\"Error uploading file\");\r\n    CompletableFuture<String> failedFuture = new CompletableFuture<>();\r\n    failedFuture.completeExceptionally(exception);\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<String>>) invocation -> {\r\n        Metadata metadata = invocation.getArgumentAt(1, Metadata.class);\r\n        String path = metadata.getPayloadPath();\r\n        if (path.endsWith(\"1\")) {\r\n            return CompletableFuture.completedFuture(\"a1BlobId\");\r\n        } else {\r\n            return failedFuture;\r\n        }\r\n    });\r\n    // Execute\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (ExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        // Assert that the result future fails and that the cause is propagated correctly\r\n        assertEquals(exception, cause);\r\n        return;\r\n    }\r\n    fail(\"DirIndex future should have been completed with an exception\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testCleanup",
  "sourceCode" : "@Test\r\npublic void testCleanup() throws IOException, ExecutionException, InterruptedException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    // Using unique file names since test util uses only the file name (leaf node)\r\n    // as the mock blob id, not the full file path.\r\n    String local = \"[a, c, z/1, y/2, p/m/3, q/n/4]\";\r\n    String remote = \"[a, b, z/1, x/5, p/m/3, r/o/6]\";\r\n    String expectedRemoved = \"[b, 5, 6]\";\r\n    // keep only the last character (the file name).\r\n    SortedSet<String> expectedRemovedFiles = BlobStoreTestUtil.getExpected(expectedRemoved);\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(\"blobId\"));\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    DirIndex dirIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndex = dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    // Set up mocks\r\n    SortedSet<String> allDeleted = new TreeSet<>();\r\n    when(blobStoreManager.delete(anyString(), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<Void>>) invocation -> {\r\n        String blobId = invocation.getArgumentAt(0, String.class);\r\n        allDeleted.add(blobId);\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    // Execute\r\n    CompletionStage<Void> cleanUpFuture = blobStoreUtil.cleanUpDir(dirIndex, metadata);\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        cleanUpFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    // Assert\r\n    assertEquals(expectedRemovedFiles, allDeleted);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testCleanUpFailsIfAnyFileDeleteFails",
  "sourceCode" : "@Test\r\npublic void testCleanUpFailsIfAnyFileDeleteFails() throws IOException, TimeoutException, InterruptedException, ExecutionException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    // Using unique file names since test util uses only the file name (leaf node)\r\n    // as the mock blob id, not the full file path.\r\n    String local = \"[a, b]\";\r\n    String remote = \"[c, d]\";\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(\"blobId\"));\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    DirIndex dirIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndex = dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    // Set up mocks\r\n    SamzaException exception = new SamzaException(\"Error deleting file\");\r\n    CompletableFuture<Void> failedFuture = new CompletableFuture<>();\r\n    failedFuture.completeExceptionally(exception);\r\n    when(blobStoreManager.delete(anyString(), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<Void>>) invocation -> {\r\n        String blobId = invocation.getArgumentAt(0, String.class);\r\n        if (blobId.equals(\"c\")) {\r\n            return CompletableFuture.completedFuture(null);\r\n        } else {\r\n            return failedFuture;\r\n        }\r\n    });\r\n    // Execute\r\n    CompletionStage<Void> cleanUpFuture = blobStoreUtil.cleanUpDir(dirIndex, metadata);\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        cleanUpFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (ExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        // Assert that the result future fails and that the cause is propagated correctly\r\n        assertEquals(exception, cause);\r\n        return;\r\n    }\r\n    fail(\"Clean up future should have been completed with an exception\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testCleanUpFailsIfAnySubDirFileDeleteFails",
  "sourceCode" : "@Test\r\npublic void testCleanUpFailsIfAnySubDirFileDeleteFails() throws IOException, TimeoutException, InterruptedException, ExecutionException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    // Using unique file names since test util uses only the file name (leaf node)\r\n    // as the mock blob id, not the full file path.\r\n    String local = \"[a/1, b/2]\";\r\n    String remote = \"[c/3, d/4]\";\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenReturn(CompletableFuture.completedFuture(\"blobId\"));\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    DirIndex dirIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndex = dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    // Set up mocks\r\n    SamzaException exception = new SamzaException(\"Error deleting file\");\r\n    CompletableFuture<Void> failedFuture = new CompletableFuture<>();\r\n    failedFuture.completeExceptionally(exception);\r\n    when(blobStoreManager.delete(anyString(), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<Void>>) invocation -> {\r\n        String blobId = invocation.getArgumentAt(0, String.class);\r\n        if (blobId.equals(\"3\")) {\r\n            return CompletableFuture.completedFuture(null);\r\n        } else {\r\n            return failedFuture;\r\n        }\r\n    });\r\n    // Execute\r\n    CompletionStage<Void> cleanUpFuture = blobStoreUtil.cleanUpDir(dirIndex, metadata);\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        cleanUpFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (ExecutionException e) {\r\n        Throwable cause = e.getCause();\r\n        // Assert that the result future fails and that the cause is propagated correctly\r\n        assertEquals(exception, cause);\r\n        return;\r\n    }\r\n    fail(\"Clean up future should have been completed with an exception\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRemoveTTL",
  "sourceCode" : "@Test\r\npublic void testRemoveTTL() throws IOException, ExecutionException, InterruptedException {\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    // File, dir and recursive dir added, retained and removed in local\r\n    // Using unique file names since test setup returns it as the blob id\r\n    String local = \"[a, c, z/1, y/2, p/m/3, q/n/4]\";\r\n    String remote = \"[a, b, z/1, x/5, p/m/3, r/o/6]\";\r\n    String expectedAdded = \"[c, y/2, q/n/4]\";\r\n    String expectedRetained = \"[a, z/1, p/m/3]\";\r\n    SortedSet<String> expectedAddedFiles = BlobStoreTestUtil.getExpected(expectedAdded);\r\n    SortedSet<String> expectedRetainedFiles = BlobStoreTestUtil.getExpected(expectedRetained);\r\n    SortedSet<String> expectedPresentFiles = new TreeSet<>(expectedAddedFiles);\r\n    expectedPresentFiles.addAll(expectedRetainedFiles);\r\n    // Set up environment\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    when(blobStoreManager.put(any(InputStream.class), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<String>>) invocation -> {\r\n        Metadata metadata = invocation.getArgumentAt(1, Metadata.class);\r\n        String path = metadata.getPayloadPath();\r\n        String fileName = path.substring(path.length() - 1);\r\n        return CompletableFuture.completedFuture(fileName);\r\n    });\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    CompletionStage<DirIndex> dirIndexFuture = blobStoreUtil.putDir(dirDiff, snapshotMetadata);\r\n    DirIndex dirIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putDir is broken.\r\n        dirIndex = dirIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putDir should be already complete.\");\r\n    }\r\n    SnapshotIndex mockSnapshotIndex = mock(SnapshotIndex.class);\r\n    when(mockSnapshotIndex.getSnapshotMetadata()).thenReturn(snapshotMetadata);\r\n    when(mockSnapshotIndex.getDirIndex()).thenReturn(dirIndex);\r\n    SortedSet<String> allTTLRemoved = new TreeSet<>();\r\n    when(blobStoreManager.removeTTL(anyString(), any(Metadata.class))).thenAnswer((Answer<CompletableFuture<String>>) invocation -> {\r\n        String blobId = invocation.getArgumentAt(0, String.class);\r\n        allTTLRemoved.add(blobId);\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    // Execute\r\n    blobStoreUtil.removeTTL(\"snapshotIndexBlobId\", mockSnapshotIndex, metadata);\r\n    // Assert\r\n    SortedSet<String> expectedBlobIds = new TreeSet<>();\r\n    // test uses unique file name (last char) as the blob ID.\r\n    expectedPresentFiles.forEach(f -> expectedBlobIds.add(f.substring(f.length() - 1)));\r\n    expectedBlobIds.add(\"snapshotIndexBlobId\");\r\n    assertEquals(expectedBlobIds, allTTLRemoved);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testPutFileChecksumAndMetadata",
  "sourceCode" : "@Test\r\npublic void testPutFileChecksumAndMetadata() throws IOException, ExecutionException, InterruptedException {\r\n    // Setup\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    Path path = Files.createTempFile(\"samza-testPutFileChecksum-\", \".tmp\");\r\n    FileUtil fileUtil = new FileUtil();\r\n    fileUtil.writeToTextFile(path.toFile(), RandomStringUtils.random(1000), false);\r\n    long expectedChecksum = FileUtils.checksumCRC32(path.toFile());\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    ArgumentCaptor<Metadata> argumentCaptor = ArgumentCaptor.forClass(Metadata.class);\r\n    when(blobStoreManager.put(any(InputStream.class), argumentCaptor.capture())).thenAnswer((Answer<CompletionStage<String>>) invocation -> {\r\n        InputStream inputStream = invocation.getArgumentAt(0, InputStream.class);\r\n        IOUtils.copy(inputStream, NullOutputStream.NULL_OUTPUT_STREAM);\r\n        return CompletableFuture.completedFuture(\"blobId\");\r\n    });\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    CompletionStage<FileIndex> fileIndexFuture = blobStoreUtil.putFile(path.toFile(), snapshotMetadata);\r\n    FileIndex fileIndex = null;\r\n    try {\r\n        // should be already complete. if not, future composition in putFile is broken.\r\n        fileIndex = fileIndexFuture.toCompletableFuture().get(0, TimeUnit.MILLISECONDS);\r\n    } catch (TimeoutException e) {\r\n        fail(\"Future returned from putFile should be already complete.\");\r\n    }\r\n    // Assert\r\n    Metadata metadata = (Metadata) argumentCaptor.getValue();\r\n    assertEquals(path.toAbsolutePath().toString(), metadata.getPayloadPath());\r\n    assertEquals(path.toFile().length(), Long.valueOf(metadata.getPayloadSize()).longValue());\r\n    assertEquals(expectedChecksum, fileIndex.getChecksum());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testAreSameFile",
  "sourceCode" : "@Test\r\npublic void testAreSameFile() throws IOException {\r\n    FileUtil fileUtil = new FileUtil();\r\n    // 1. test with sst file with same attributes\r\n    Path sstFile = Files.createTempFile(\"samza-testAreSameFiles-\", \".sst\");\r\n    PosixFileAttributes sstFileAttribs = Files.readAttributes(sstFile, PosixFileAttributes.class);\r\n    FileMetadata sstFileMetadata = new FileMetadata(sstFileAttribs.creationTime().toMillis(), sstFileAttribs.lastModifiedTime().toMillis(), sstFileAttribs.size(), sstFileAttribs.owner().toString(), sstFileAttribs.group().toString(), PosixFilePermissions.toString(sstFileAttribs.permissions()));\r\n    // checksum should be ignored for sst file. Set any dummy value\r\n    FileIndex sstFileIndex = new FileIndex(sstFile.getFileName().toString(), Collections.emptyList(), sstFileMetadata, 0L);\r\n    assertTrue(DirDiffUtil.areSameFile(false, true).test(sstFile.toFile(), sstFileIndex));\r\n    // 2. test with sst file with different timestamps\r\n    // Update last modified time\r\n    Files.setLastModifiedTime(sstFile, FileTime.fromMillis(System.currentTimeMillis() + 1000L));\r\n    assertTrue(DirDiffUtil.areSameFile(false, true).test(sstFile.toFile(), sstFileIndex));\r\n    // 3. test with non-sst files with same metadata and content\r\n    Path tmpFile = Files.createTempFile(\"samza-testAreSameFiles-\", \".tmp\");\r\n    fileUtil.writeToTextFile(tmpFile.toFile(), RandomStringUtils.random(1000), false);\r\n    PosixFileAttributes tmpFileAttribs = Files.readAttributes(tmpFile, PosixFileAttributes.class);\r\n    FileMetadata tmpFileMetadata = new FileMetadata(tmpFileAttribs.creationTime().toMillis(), tmpFileAttribs.lastModifiedTime().toMillis(), tmpFileAttribs.size(), tmpFileAttribs.owner().toString(), tmpFileAttribs.group().toString(), PosixFilePermissions.toString(tmpFileAttribs.permissions()));\r\n    FileIndex tmpFileIndex = new FileIndex(tmpFile.getFileName().toString(), Collections.emptyList(), tmpFileMetadata, FileUtils.checksumCRC32(tmpFile.toFile()));\r\n    assertTrue(DirDiffUtil.areSameFile(false, true).test(tmpFile.toFile(), tmpFileIndex));\r\n    // 4. test with non-sst files with different attributes\r\n    // change lastModifiedTime of local file\r\n    FileTime prevLastModified = tmpFileAttribs.lastModifiedTime();\r\n    Files.setLastModifiedTime(tmpFile, FileTime.fromMillis(System.currentTimeMillis() + 1000L));\r\n    assertTrue(DirDiffUtil.areSameFile(false, true).test(tmpFile.toFile(), tmpFileIndex));\r\n    // change content/checksum of local file\r\n    // reset attributes to match with remote file\r\n    Files.setLastModifiedTime(tmpFile, prevLastModified);\r\n    //new content\r\n    fileUtil.writeToTextFile(tmpFile.toFile(), RandomStringUtils.random(1000), false);\r\n    assertFalse(DirDiffUtil.areSameFile(false, true).test(tmpFile.toFile(), tmpFileIndex));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirRestoresMultiPartFilesCorrectly",
  "sourceCode" : "@Test\r\npublic void testRestoreDirRestoresMultiPartFilesCorrectly() throws IOException {\r\n    Path restoreDirBasePath = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    // remote file == 26 blobs, blob ids from a to z, blob contents from a to z, offsets 0 to 25.\r\n    DirIndex mockDirIndex = mock(DirIndex.class);\r\n    when(mockDirIndex.getDirName()).thenReturn(DirIndex.ROOT_DIR_NAME);\r\n    FileIndex mockFileIndex = mock(FileIndex.class);\r\n    when(mockFileIndex.getFileName()).thenReturn(\"1.sst\");\r\n    // setup mock file attributes. create a temp file to get current user/group/permissions so that they\r\n    // match with restored files.\r\n    File tmpFile = Paths.get(restoreDirBasePath.toString(), \"tempfile-\" + new Random().nextInt()).toFile();\r\n    tmpFile.createNewFile();\r\n    PosixFileAttributes attrs = Files.readAttributes(tmpFile.toPath(), PosixFileAttributes.class);\r\n    FileMetadata fileMetadata = new // ctime mtime does not matter. size == 26\r\n    FileMetadata(// ctime mtime does not matter. size == 26\r\n    1234L, // ctime mtime does not matter. size == 26\r\n    1243L, // ctime mtime does not matter. size == 26\r\n    26, attrs.owner().getName(), attrs.group().getName(), PosixFilePermissions.toString(attrs.permissions()));\r\n    when(mockFileIndex.getFileMetadata()).thenReturn(fileMetadata);\r\n    // delete so that it doesn't show up in restored dir contents.\r\n    Files.delete(tmpFile.toPath());\r\n    List<FileBlob> mockFileBlobs = new ArrayList<>();\r\n    StringBuilder fileContents = new StringBuilder();\r\n    for (int i = 0; i < 26; i++) {\r\n        FileBlob mockFileBlob = mock(FileBlob.class);\r\n        char c = (char) ('a' + i);\r\n        // blob contents == blobId\r\n        fileContents.append(c);\r\n        when(mockFileBlob.getBlobId()).thenReturn(String.valueOf(c));\r\n        when(mockFileBlob.getOffset()).thenReturn(i);\r\n        mockFileBlobs.add(mockFileBlob);\r\n    }\r\n    when(mockFileIndex.getBlobs()).thenReturn(mockFileBlobs);\r\n    CRC32 checksum = new CRC32();\r\n    checksum.update(fileContents.toString().getBytes());\r\n    when(mockFileIndex.getChecksum()).thenReturn(checksum.getValue());\r\n    when(mockDirIndex.getFilesPresent()).thenReturn(ImmutableList.of(mockFileIndex));\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenAnswer((Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(blobId.getBytes());\r\n        ((FileOutputStream) outputStream).getFD().sync();\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mockBlobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    blobStoreUtil.restoreDir(restoreDirBasePath.toFile(), mockDirIndex, metadata, false).join();\r\n    assertTrue(new DirDiffUtil().areSameDir(Collections.emptySet(), false, true).test(restoreDirBasePath.toFile(), mockDirIndex));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirRetriesFileRestoreOnRetriableExceptions",
  "sourceCode" : "@Test\r\npublic void testRestoreDirRetriesFileRestoreOnRetriableExceptions() throws IOException {\r\n    Path restoreDirBasePath = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    DirIndex mockDirIndex = mock(DirIndex.class);\r\n    when(mockDirIndex.getDirName()).thenReturn(DirIndex.ROOT_DIR_NAME);\r\n    FileIndex mockFileIndex = mock(FileIndex.class);\r\n    when(mockFileIndex.getFileName()).thenReturn(\"1.sst\");\r\n    // setup mock file attributes. create a temp file to get current user/group/permissions so that they\r\n    // match with restored files.\r\n    File tmpFile = Paths.get(restoreDirBasePath.toString(), \"tempfile-\" + new Random().nextInt()).toFile();\r\n    tmpFile.createNewFile();\r\n    byte[] fileContents = \"fileContents\".getBytes();\r\n    PosixFileAttributes attrs = Files.readAttributes(tmpFile.toPath(), PosixFileAttributes.class);\r\n    FileMetadata fileMetadata = new // ctime mtime does not matter. size == 26\r\n    FileMetadata(// ctime mtime does not matter. size == 26\r\n    1234L, // ctime mtime does not matter. size == 26\r\n    1243L, // ctime mtime does not matter. size == 26\r\n    fileContents.length, attrs.owner().getName(), attrs.group().getName(), PosixFilePermissions.toString(attrs.permissions()));\r\n    when(mockFileIndex.getFileMetadata()).thenReturn(fileMetadata);\r\n    // delete so that it doesn't show up in restored dir contents.\r\n    Files.delete(tmpFile.toPath());\r\n    List<FileBlob> mockFileBlobs = new ArrayList<>();\r\n    FileBlob mockFileBlob = mock(FileBlob.class);\r\n    when(mockFileBlob.getBlobId()).thenReturn(\"fileBlobId\");\r\n    when(mockFileBlob.getOffset()).thenReturn(0);\r\n    mockFileBlobs.add(mockFileBlob);\r\n    when(mockFileIndex.getBlobs()).thenReturn(mockFileBlobs);\r\n    CRC32 checksum = new CRC32();\r\n    checksum.update(fileContents);\r\n    when(mockFileIndex.getChecksum()).thenReturn(checksum.getValue());\r\n    when(mockDirIndex.getFilesPresent()).thenReturn(ImmutableList.of(mockFileIndex));\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenAnswer(// first try, retriable error\r\n    (Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(\"bad-data\".getBytes());\r\n        ((FileOutputStream) outputStream).getFD().sync();\r\n        return FutureUtil.failedFuture(new RetriableException());\r\n    }).// 2nd try\r\n    thenAnswer(// 2nd try\r\n    (Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(fileContents);\r\n        ((FileOutputStream) outputStream).getFD().sync();\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mockBlobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    blobStoreUtil.restoreDir(restoreDirBasePath.toFile(), mockDirIndex, metadata, false).join();\r\n    assertTrue(new DirDiffUtil().areSameDir(Collections.emptySet(), false, true).test(restoreDirBasePath.toFile(), mockDirIndex));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirFailsRestoreOnNonRetriableExceptions",
  "sourceCode" : "@Test\r\npublic void testRestoreDirFailsRestoreOnNonRetriableExceptions() throws IOException {\r\n    Path restoreDirBasePath = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    DirIndex mockDirIndex = mock(DirIndex.class);\r\n    when(mockDirIndex.getDirName()).thenReturn(DirIndex.ROOT_DIR_NAME);\r\n    FileIndex mockFileIndex = mock(FileIndex.class);\r\n    when(mockFileIndex.getFileName()).thenReturn(\"1.sst\");\r\n    // setup mock file attributes. create a temp file to get current user/group/permissions so that they\r\n    // match with restored files.\r\n    File tmpFile = Paths.get(restoreDirBasePath.toString(), \"tempfile-\" + new Random().nextInt()).toFile();\r\n    tmpFile.createNewFile();\r\n    byte[] fileContents = \"fileContents\".getBytes();\r\n    PosixFileAttributes attrs = Files.readAttributes(tmpFile.toPath(), PosixFileAttributes.class);\r\n    FileMetadata fileMetadata = new // ctime mtime does not matter. size == 26\r\n    FileMetadata(// ctime mtime does not matter. size == 26\r\n    1234L, // ctime mtime does not matter. size == 26\r\n    1243L, // ctime mtime does not matter. size == 26\r\n    fileContents.length, attrs.owner().getName(), attrs.group().getName(), PosixFilePermissions.toString(attrs.permissions()));\r\n    when(mockFileIndex.getFileMetadata()).thenReturn(fileMetadata);\r\n    // delete so that it doesn't show up in restored dir contents.\r\n    Files.delete(tmpFile.toPath());\r\n    List<FileBlob> mockFileBlobs = new ArrayList<>();\r\n    FileBlob mockFileBlob = mock(FileBlob.class);\r\n    when(mockFileBlob.getBlobId()).thenReturn(\"fileBlobId\");\r\n    when(mockFileBlob.getOffset()).thenReturn(0);\r\n    mockFileBlobs.add(mockFileBlob);\r\n    when(mockFileIndex.getBlobs()).thenReturn(mockFileBlobs);\r\n    CRC32 checksum = new CRC32();\r\n    checksum.update(fileContents);\r\n    when(mockFileIndex.getChecksum()).thenReturn(checksum.getValue());\r\n    when(mockDirIndex.getFilesPresent()).thenReturn(ImmutableList.of(mockFileIndex));\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenReturn(// non retriable error\r\n    FutureUtil.failedFuture(new IllegalArgumentException())).thenAnswer((Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(fileContents);\r\n        ((FileOutputStream) outputStream).getFD().sync();\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mockBlobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    try {\r\n        blobStoreUtil.restoreDir(restoreDirBasePath.toFile(), mockDirIndex, metadata, false).join();\r\n        fail(\"Should have failed on non-retriable errors during file restore\");\r\n    } catch (CompletionException e) {\r\n        assertTrue(e.getCause() instanceof IllegalArgumentException);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreIgnoresDifferentFileOwnersOnConfig",
  "sourceCode" : "@Test\r\npublic void testRestoreIgnoresDifferentFileOwnersOnConfig() throws IOException {\r\n    Path restoreDirBasePath = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    // remote file == 26 blobs, blob ids from a to z, blob contents from a to z, offsets 0 to 25.\r\n    DirIndex mockDirIndex = mock(DirIndex.class);\r\n    when(mockDirIndex.getDirName()).thenReturn(DirIndex.ROOT_DIR_NAME);\r\n    FileIndex mockFileIndex = mock(FileIndex.class);\r\n    when(mockFileIndex.getFileName()).thenReturn(\"1.sst\");\r\n    // setup mock file attributes. create a temp file to get current user/group/permissions so that they\r\n    // match with restored files.\r\n    File tmpFile = Paths.get(restoreDirBasePath.toString(), \"tempfile-\" + new Random().nextInt()).toFile();\r\n    tmpFile.createNewFile();\r\n    PosixFileAttributes attrs = Files.readAttributes(tmpFile.toPath(), PosixFileAttributes.class);\r\n    // create remote file with different owner than local file\r\n    FileMetadata fileMetadata = new // ctime mtime does not matter. size == 26\r\n    FileMetadata(// ctime mtime does not matter. size == 26\r\n    1234L, // ctime mtime does not matter. size == 26\r\n    1243L, // ctime mtime does not matter. size == 26\r\n    26, attrs.owner().getName() + \"_different\", attrs.group().getName(), PosixFilePermissions.toString(attrs.permissions()));\r\n    when(mockFileIndex.getFileMetadata()).thenReturn(fileMetadata);\r\n    // delete so that it doesn't show up in restored dir contents.\r\n    Files.delete(tmpFile.toPath());\r\n    List<FileBlob> mockFileBlobs = new ArrayList<>();\r\n    StringBuilder fileContents = new StringBuilder();\r\n    for (int i = 0; i < 26; i++) {\r\n        FileBlob mockFileBlob = mock(FileBlob.class);\r\n        char c = (char) ('a' + i);\r\n        // blob contents == blobId\r\n        fileContents.append(c);\r\n        when(mockFileBlob.getBlobId()).thenReturn(String.valueOf(c));\r\n        when(mockFileBlob.getOffset()).thenReturn(i);\r\n        mockFileBlobs.add(mockFileBlob);\r\n    }\r\n    when(mockFileIndex.getBlobs()).thenReturn(mockFileBlobs);\r\n    CRC32 checksum = new CRC32();\r\n    checksum.update(fileContents.toString().getBytes());\r\n    when(mockFileIndex.getChecksum()).thenReturn(checksum.getValue());\r\n    when(mockDirIndex.getFilesPresent()).thenReturn(ImmutableList.of(mockFileIndex));\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenAnswer((Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(blobId.getBytes());\r\n        ((FileOutputStream) outputStream).getFD().sync();\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    BlobStoreConfig config = mock(BlobStoreConfig.class);\r\n    when(config.shouldCompareFileOwnersOnRestore()).thenReturn(false);\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mockBlobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    blobStoreUtil.restoreDir(restoreDirBasePath.toFile(), mockDirIndex, metadata, false).join();\r\n    assertTrue(new DirDiffUtil().areSameDir(Collections.emptySet(), false, config.shouldCompareFileOwnersOnRestore()).test(restoreDirBasePath.toFile(), mockDirIndex));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirRecreatesEmptyFilesAndDirs",
  "sourceCode" : "@Test\r\n// TODO remove\r\n@Ignore\r\npublic void testRestoreDirRecreatesEmptyFilesAndDirs() throws IOException {\r\n    String prevSnapshotFiles = \"[a, b, z/1, y/1, p/m/1, q/n/1]\";\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(prevSnapshotFiles);\r\n    String localSnapshotFiles = \"[a, b, z/1, y/1, p/m/1, q/n/1]\";\r\n    Path localSnapshot = BlobStoreTestUtil.createLocalDir(localSnapshotFiles);\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenAnswer((Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(blobId.getBytes());\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    boolean result = new DirDiffUtil().areSameDir(new TreeSet<>(), false, true).test(localSnapshot.toFile(), dirIndex);\r\n    assertFalse(result);\r\n    //ToDo complete\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirVerifiesFileChecksums",
  "sourceCode" : "@Test\r\npublic void testRestoreDirVerifiesFileChecksums() {\r\n    // ToDo shesharma restore dir only restores SST files. Since other metadata files are in ignore list,\r\n    // no checksum matching would be done? Check later.\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testRestoreDirCreatesCorrectDirectoryStructure",
  "sourceCode" : "@Test\r\npublic void testRestoreDirCreatesCorrectDirectoryStructure() throws IOException {\r\n    String prevSnapshotFiles = \"[a, b, z/1, y/1, p/m/1, q/n/1]\";\r\n    DirIndex dirIndex = BlobStoreTestUtil.createDirIndex(prevSnapshotFiles);\r\n    BlobStoreManager mockBlobStoreManager = mock(BlobStoreManager.class);\r\n    when(mockBlobStoreManager.get(anyString(), any(OutputStream.class), any(Metadata.class), any(Boolean.class))).thenAnswer((Answer<CompletionStage<Void>>) invocationOnMock -> {\r\n        String blobId = invocationOnMock.getArgumentAt(0, String.class);\r\n        OutputStream outputStream = invocationOnMock.getArgumentAt(1, OutputStream.class);\r\n        outputStream.write(blobId.getBytes());\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    Path restoreDirBasePath = Files.createTempDirectory(BlobStoreTestUtil.TEMP_DIR_PREFIX);\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mockBlobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    blobStoreUtil.restoreDir(restoreDirBasePath.toFile(), dirIndex, metadata, false).join();\r\n    assertTrue(new DirDiffUtil().areSameDir(Collections.emptySet(), false, true).test(restoreDirBasePath.toFile(), dirIndex));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIReturnsEmptyMapForNullCheckpoint",
  "sourceCode" : "/**\r\n * Tests related to {@link BlobStoreUtil#getStoreSnapshotIndexes}\r\n */\r\n@Test\r\npublic void testGetSSIReturnsEmptyMapForNullCheckpoint() {\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mock(BlobStoreManager.class), MoreExecutors.newDirectExecutorService(), blobStoreConfig, null, null);\r\n    Map<String, Pair<String, SnapshotIndex>> snapshotIndexes = blobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", null, new HashSet<>(), false);\r\n    assertTrue(snapshotIndexes.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIReturnsEmptyMapIfNoEntryForBlobStoreBackendFactory",
  "sourceCode" : "@Test\r\npublic void testGetSSIReturnsEmptyMapIfNoEntryForBlobStoreBackendFactory() {\r\n    CheckpointV2 mockCheckpoint = mock(CheckpointV2.class);\r\n    when(mockCheckpoint.getVersion()).thenReturn((short) 2);\r\n    when(mockCheckpoint.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"com.OtherStateBackendFactory\", ImmutableMap.of(\"storeName\", \"otherSCM\")));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mock(BlobStoreManager.class), MoreExecutors.newDirectExecutorService(), blobStoreConfig, null, null);\r\n    Map<String, Pair<String, SnapshotIndex>> snapshotIndexes = blobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", mockCheckpoint, new HashSet<>(), false);\r\n    assertTrue(snapshotIndexes.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIReturnsEmptyMapIfNoStoreForBlobStoreBackendFactory",
  "sourceCode" : "@Test\r\npublic void testGetSSIReturnsEmptyMapIfNoStoreForBlobStoreBackendFactory() {\r\n    CheckpointV2 mockCheckpoint = mock(CheckpointV2.class);\r\n    when(mockCheckpoint.getVersion()).thenReturn((short) 2);\r\n    when(mockCheckpoint.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of()));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(mock(BlobStoreManager.class), MoreExecutors.newDirectExecutorService(), blobStoreConfig, null, null);\r\n    Map<String, Pair<String, SnapshotIndex>> snapshotIndexes = blobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", mockCheckpoint, new HashSet<>(), false);\r\n    assertTrue(snapshotIndexes.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIThrowsExceptionOnSyncBlobStoreErrors",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetSSIThrowsExceptionOnSyncBlobStoreErrors() {\r\n    Checkpoint checkpoint = createCheckpointV2(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of(\"storeName\", \"snapshotIndexBlobId\"));\r\n    Set<String> storesToBackupOrRestore = new HashSet<>();\r\n    storesToBackupOrRestore.add(\"storeName\");\r\n    BlobStoreUtil mockBlobStoreUtil = mock(BlobStoreUtil.class);\r\n    when(mockBlobStoreUtil.getSnapshotIndex(anyString(), any(Metadata.class), anyBoolean())).thenThrow(new RuntimeException());\r\n    when(mockBlobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), any(Boolean.class))).thenCallRealMethod();\r\n    mockBlobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", checkpoint, storesToBackupOrRestore, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testSerdeException",
  "sourceCode" : "@Test\r\npublic void testSerdeException() throws ExecutionException, InterruptedException {\r\n    final String blobId = \"foo\";\r\n    final BlobStoreManager testBlobStoreManager = new DeserTestBlobStoreManager();\r\n    final BlobStoreUtil util = new BlobStoreUtil(testBlobStoreManager, Executors.newSingleThreadExecutor(), blobStoreConfig, null, null);\r\n    final CompletableFuture<SnapshotIndex> future = util.getSnapshotIndex(blobId, mock(Metadata.class), true).handle((snapshotIndex, throwable) -> {\r\n        if (throwable != null) {\r\n            Assert.assertEquals(throwable.getMessage(), String.format(\"Unable to get SnapshotIndex blob. The blob ID is : %s\", blobId));\r\n            Assert.assertEquals(throwable.getCause().getMessage(), \"org.apache.samza.SamzaException: Exception in deserializing SnapshotIndex bytes foobar\");\r\n        }\r\n        return snapshotIndex;\r\n    });\r\n    future.get();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIThrowsExceptionIfAnyNonIgnoredAsyncBlobStoreErrors",
  "sourceCode" : "@Test\r\npublic void testGetSSIThrowsExceptionIfAnyNonIgnoredAsyncBlobStoreErrors() {\r\n    String store = \"storeName1\";\r\n    String otherStore = \"storeName2\";\r\n    Set<String> storesToBackupOrRestore = new HashSet<>();\r\n    storesToBackupOrRestore.add(store);\r\n    storesToBackupOrRestore.add(otherStore);\r\n    Checkpoint checkpoint = createCheckpointV2(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of(store, \"snapshotIndexBlobId1\", otherStore, \"snapshotIndexBlobId2\"));\r\n    SnapshotIndex store1SnapshotIndex = mock(SnapshotIndex.class);\r\n    BlobStoreUtil mockBlobStoreUtil = mock(BlobStoreUtil.class);\r\n    when(mockBlobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), anyBoolean())).thenCallRealMethod();\r\n    RuntimeException nonIgnoredException = new RuntimeException();\r\n    CompletableFuture<SnapshotIndex> failedFuture = FutureUtil.failedFuture(nonIgnoredException);\r\n    when(mockBlobStoreUtil.getSnapshotIndex(eq(\"snapshotIndexBlobId1\"), any(Metadata.class), anyBoolean())).thenReturn(// should fail even if some errors are ignored\r\n    FutureUtil.failedFuture(new DeletedException()));\r\n    when(mockBlobStoreUtil.getSnapshotIndex(eq(\"snapshotIndexBlobId2\"), any(Metadata.class), anyBoolean())).thenReturn(failedFuture);\r\n    try {\r\n        mockBlobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", checkpoint, storesToBackupOrRestore, false);\r\n        fail(\"Should have thrown an exception\");\r\n    } catch (Exception e) {\r\n        Throwable cause = FutureUtil.unwrapExceptions(CompletionException.class, FutureUtil.unwrapExceptions(SamzaException.class, e));\r\n        assertEquals(nonIgnoredException, cause);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetSSIReturnsCorrectSCMSnapshotIndexPair",
  "sourceCode" : "@Test\r\npublic void testGetSSIReturnsCorrectSCMSnapshotIndexPair() {\r\n    String storeName = \"storeName\";\r\n    String otherStoreName = \"otherStoreName\";\r\n    Set<String> storesToBackupOrRestore = ImmutableSet.of(storeName, otherStoreName);\r\n    String storeSnapshotIndexBlobId = \"snapshotIndexBlobId\";\r\n    String otherStoreSnapshotIndexBlobId = \"otherSnapshotIndexBlobId\";\r\n    SnapshotIndex mockStoreSnapshotIndex = mock(SnapshotIndex.class);\r\n    SnapshotIndex mockOtherStooreSnapshotIndex = mock(SnapshotIndex.class);\r\n    CheckpointV2 checkpoint = createCheckpointV2(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of(storeName, storeSnapshotIndexBlobId, otherStoreName, otherStoreSnapshotIndexBlobId));\r\n    BlobStoreUtil mockBlobStoreUtil = mock(BlobStoreUtil.class);\r\n    when(mockBlobStoreUtil.getSnapshotIndex(eq(storeSnapshotIndexBlobId), any(Metadata.class), any(Boolean.class))).thenReturn(CompletableFuture.completedFuture(mockStoreSnapshotIndex));\r\n    when(mockBlobStoreUtil.getSnapshotIndex(eq(otherStoreSnapshotIndexBlobId), any(Metadata.class), any(Boolean.class))).thenReturn(CompletableFuture.completedFuture(mockOtherStooreSnapshotIndex));\r\n    when(mockBlobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), any(Boolean.class))).thenCallRealMethod();\r\n    Map<String, Pair<String, SnapshotIndex>> snapshotIndexes = mockBlobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", checkpoint, storesToBackupOrRestore, false);\r\n    assertEquals(storeSnapshotIndexBlobId, snapshotIndexes.get(storeName).getKey());\r\n    assertEquals(mockStoreSnapshotIndex, snapshotIndexes.get(storeName).getValue());\r\n    assertEquals(otherStoreSnapshotIndexBlobId, snapshotIndexes.get(otherStoreName).getKey());\r\n    assertEquals(mockOtherStooreSnapshotIndex, snapshotIndexes.get(otherStoreName).getValue());\r\n    verify(mockBlobStoreUtil, times(2)).getSnapshotIndex(anyString(), any(Metadata.class), any(Boolean.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testGetCheckpointIndexIgnoresStoresNotInStoresToBackupRestoreSet",
  "sourceCode" : "@Test\r\npublic void testGetCheckpointIndexIgnoresStoresNotInStoresToBackupRestoreSet() {\r\n    String store = \"storeName1\";\r\n    String anotherStore = \"storeName2\";\r\n    String oneMoreStore = \"storeName3\";\r\n    SnapshotIndex mockStoreSnapshotIndex = mock(SnapshotIndex.class);\r\n    Set<String> storesToBackupOrRestore = ImmutableSet.of(store, anotherStore);\r\n    CheckpointV2 checkpoint = createCheckpointV2(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of(store, \"1\", anotherStore, \"2\", oneMoreStore, \"3\"));\r\n    BlobStoreUtil mockBlobStoreUtil = mock(BlobStoreUtil.class);\r\n    when(mockBlobStoreUtil.getSnapshotIndex(any(String.class), any(Metadata.class), anyBoolean())).thenReturn(CompletableFuture.completedFuture(mockStoreSnapshotIndex));\r\n    when(mockBlobStoreUtil.getStoreSnapshotIndexes(anyString(), anyString(), anyString(), any(Checkpoint.class), anySetOf(String.class), anyBoolean())).thenCallRealMethod();\r\n    Map<String, Pair<String, SnapshotIndex>> snapshotIndexes = mockBlobStoreUtil.getStoreSnapshotIndexes(\"testJobName\", \"testJobId\", \"taskName\", checkpoint, storesToBackupOrRestore, false);\r\n    verify(mockBlobStoreUtil, times(storesToBackupOrRestore.size())).getSnapshotIndex(anyString(), any(Metadata.class), anyBoolean());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestBlobStoreUtil.java",
  "methodName" : "testPutFileRetriedMoreThanThreeTimes",
  "sourceCode" : "/**\r\n * This test verifies that a retriable exception is retried more than 3 times (default retry is limited to 3 attempts)\r\n */\r\n@Test\r\npublic void testPutFileRetriedMoreThanThreeTimes() throws Exception {\r\n    SnapshotMetadata snapshotMetadata = new SnapshotMetadata(checkpointId, jobName, jobId, taskName, storeName);\r\n    Path path = Files.createTempFile(\"samza-testPutFileChecksum-\", \".tmp\");\r\n    FileUtil fileUtil = new FileUtil();\r\n    fileUtil.writeToTextFile(path.toFile(), RandomStringUtils.random(1000), false);\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    ArgumentCaptor<Metadata> argumentCaptor = ArgumentCaptor.forClass(Metadata.class);\r\n    when(blobStoreManager.put(any(InputStream.class), argumentCaptor.capture())).// first try, retriable error\r\n    thenAnswer(// first try, retriable error\r\n    (Answer<CompletionStage<String>>) invocationOnMock -> {\r\n        return FutureUtil.failedFuture(new RetriableException());\r\n    }).// second try, retriable error\r\n    thenAnswer(// second try, retriable error\r\n    (Answer<CompletionStage<String>>) invocationOnMock -> {\r\n        return FutureUtil.failedFuture(new RetriableException());\r\n    }).// third try, retriable error\r\n    thenAnswer(// third try, retriable error\r\n    (Answer<CompletionStage<String>>) invocationOnMock -> {\r\n        return FutureUtil.failedFuture(new RetriableException());\r\n    }).thenAnswer((Answer<CompletionStage<String>>) invocation -> CompletableFuture.completedFuture(\"blobId\"));\r\n    BlobStoreUtil blobStoreUtil = new BlobStoreUtil(blobStoreManager, EXECUTOR, blobStoreConfig, null, null);\r\n    blobStoreUtil.putFile(path.toFile(), snapshotMetadata).join();\r\n    // Verify put operation is retried 4 times\r\n    verify(blobStoreManager, times(4)).put(any(InputStream.class), any(Metadata.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtil.java",
  "methodName" : "testGetDirDiff",
  "sourceCode" : "@Test\r\npublic void testGetDirDiff() throws IOException {\r\n    // Setup\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(this.local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(this.remote);\r\n    // Execute\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> localFile.getName().equals(remoteFile.getFileName()));\r\n    SortedSet<String> allAdded = new TreeSet<>();\r\n    SortedSet<String> allRemoved = new TreeSet<>();\r\n    SortedSet<String> allRetained = new TreeSet<>();\r\n    BlobStoreTestUtil.getAllAddedInDiff(basePath, dirDiff, allAdded);\r\n    BlobStoreTestUtil.getAllRemovedInDiff(\"\", dirDiff, allRemoved);\r\n    BlobStoreTestUtil.getAllRetainedInDiff(\"\", dirDiff, allRetained);\r\n    // Assert\r\n    SortedSet<String> expectedAddedFiles = BlobStoreTestUtil.getExpected(this.expectedAdded);\r\n    SortedSet<String> expectedRetainedFiles = BlobStoreTestUtil.getExpected(this.expectedRetained);\r\n    SortedSet<String> expectedRemovedFiles = BlobStoreTestUtil.getExpected(this.expectedRemoved);\r\n    assertEquals(expectedAddedFiles, allAdded);\r\n    assertEquals(expectedRetainedFiles, allRetained);\r\n    assertEquals(expectedRemovedFiles, allRemoved);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_SameFile",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_SameFile() {\r\n    Assert.assertTrue(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentFile",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentFile() {\r\n    remoteFile = new FileIndex(localFile.getName() + \"_other\", new ArrayList<>(), remoteFileMetadata, localChecksum + 1);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentCrc",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentCrc() {\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum + 1);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentSize",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentSize() {\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength + 1, localFileAttrs.owner().getName(), localFileAttrs.group().getName(), PosixFilePermissions.toString(localFileAttrs.permissions()));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentOwner",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentOwner() {\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName() + \"_different\", localFileAttrs.group().getName(), PosixFilePermissions.toString(localFileAttrs.permissions()));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_IgnoreDifferentOwner",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_IgnoreDifferentOwner() {\r\n    areSameFile = DirDiffUtil.areSameFile(false, false);\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName() + \"_different\", localFileAttrs.group().getName(), PosixFilePermissions.toString(localFileAttrs.permissions()));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertTrue(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentGroup",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentGroup() {\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName(), localFileAttrs.group().getName() + \"_different\", PosixFilePermissions.toString(localFileAttrs.permissions()));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentOwnerRead",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentOwnerRead() {\r\n    Set<PosixFilePermission> remoteFilePermissions = localFileAttrs.permissions();\r\n    remoteFilePermissions.remove(PosixFilePermission.OWNER_READ);\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName(), localFileAttrs.group().getName(), PosixFilePermissions.toString(remoteFilePermissions));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentOwnerWrite",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentOwnerWrite() {\r\n    Set<PosixFilePermission> remoteFilePermissions = localFileAttrs.permissions();\r\n    remoteFilePermissions.remove(PosixFilePermission.OWNER_WRITE);\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName(), localFileAttrs.group().getName(), PosixFilePermissions.toString(remoteFilePermissions));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_DifferentOwnerExecute",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_DifferentOwnerExecute() {\r\n    Set<PosixFilePermission> remoteFilePermissions = localFileAttrs.permissions();\r\n    remoteFilePermissions.add(PosixFilePermission.OWNER_EXECUTE);\r\n    remoteFileMetadata = new FileMetadata(0, 0, localContentLength, localFileAttrs.owner().getName(), localFileAttrs.group().getName(), PosixFilePermissions.toString(remoteFilePermissions));\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_SmallFile_DifferentCrc",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_SmallFile_DifferentCrc() {\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum + 1);\r\n    Assert.assertFalse(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_LargeFile_DifferentCrc",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_LargeFile_DifferentCrc() throws Exception {\r\n    createFile(LARGE_FILE);\r\n    remoteFile = new FileIndex(localFile.getName(), new ArrayList<>(), remoteFileMetadata, localChecksum + 1);\r\n    Assert.assertTrue(areSameFile.test(localFile, remoteFile));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilAreSameFile.java",
  "methodName" : "testAreSameFile_Cache",
  "sourceCode" : "@Test\r\npublic void testAreSameFile_Cache() throws Exception {\r\n    createFile(LARGE_FILE);\r\n    for (int i = 0; i < 5; i++) {\r\n        BiPredicate<File, FileIndex> areSameFile = DirDiffUtil.areSameFile(false, true);\r\n        for (int j = 0; j < 20; j++) {\r\n            localFile = mock(File.class);\r\n            when(localFile.getName()).thenReturn(\"name\");\r\n            Path path = mock(Path.class);\r\n            when(localFile.toPath()).thenReturn(path);\r\n            FileSystem fileSystem = mock(FileSystem.class);\r\n            when(path.getFileSystem()).thenReturn(fileSystem);\r\n            FileSystemProvider fileSystemProvider = mock(FileSystemProvider.class);\r\n            PosixFileAttributes localFileAttributes = mock(PosixFileAttributes.class);\r\n            when(localFileAttributes.size()).thenReturn(Long.valueOf(LARGE_FILE));\r\n            Map<String, Object> filePropMap = ImmutableMap.of(\"gid\", j % 4, \"uid\", j % 2);\r\n            when(fileSystemProvider.readAttributes(Mockito.any(Path.class), Mockito.any(String.class), Mockito.anyVararg())).thenReturn(filePropMap);\r\n            when(fileSystemProvider.readAttributes(Mockito.any(Path.class), (Class<BasicFileAttributes>) Mockito.any())).thenReturn(localFileAttributes);\r\n            UserPrincipal userPrincipal = mock(UserPrincipal.class);\r\n            when(userPrincipal.getName()).thenReturn(\"owner\");\r\n            GroupPrincipal groupPrincipal = mock(GroupPrincipal.class);\r\n            when(groupPrincipal.getName()).thenReturn(\"group\");\r\n            when(localFileAttributes.owner()).thenReturn(userPrincipal);\r\n            when(localFileAttributes.group()).thenReturn(groupPrincipal);\r\n            Set<PosixFilePermission> permissions = ImmutableSet.of();\r\n            when(localFileAttributes.permissions()).thenReturn(permissions);\r\n            when(fileSystem.provider()).thenReturn(fileSystemProvider);\r\n            remoteFile = mock(FileIndex.class);\r\n            when(remoteFile.getFileName()).thenReturn(\"name\");\r\n            FileMetadata remoteFileMetadata = mock(FileMetadata.class);\r\n            when(remoteFileMetadata.getSize()).thenReturn(Long.valueOf(LARGE_FILE));\r\n            when(remoteFileMetadata.getGroup()).thenReturn(\"group\");\r\n            when(remoteFileMetadata.getOwner()).thenReturn(\"owner\");\r\n            when(remoteFileMetadata.getPermissions()).thenReturn(\"---------\");\r\n            when(remoteFile.getFileMetadata()).thenReturn(remoteFileMetadata);\r\n            Assert.assertTrue(areSameFile.test(localFile, remoteFile));\r\n            Mockito.verify(localFileAttributes, times(j > 3 ? 0 : 1)).group();\r\n            Mockito.verify(localFileAttributes, times(j > 1 ? 0 : 1)).owner();\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\blobstore\\util\\TestDirDiffUtilMisc.java",
  "methodName" : "testGetDirDiffWhenIsSameFileReturnsFalseForSameFileName",
  "sourceCode" : "/**\r\n * Test the case when a file has been modified locally. I.e., when isSameFile returns false for a local file with\r\n * the same name as the remote file. The file should be marked for both deletion and addition.\r\n */\r\n@Test\r\npublic void testGetDirDiffWhenIsSameFileReturnsFalseForSameFileName() throws IOException {\r\n    String local = \"[a]\";\r\n    String remote = \"[a]\";\r\n    String expectedAdded = \"[a]\";\r\n    String expectedRetained = \"[]\";\r\n    String expectedRemoved = \"[a]\";\r\n    Path localSnapshotDir = BlobStoreTestUtil.createLocalDir(local);\r\n    String basePath = localSnapshotDir.toAbsolutePath().toString();\r\n    DirIndex remoteSnapshotDir = BlobStoreTestUtil.createDirIndex(remote);\r\n    // Execute\r\n    DirDiff dirDiff = DirDiffUtil.getDirDiff(localSnapshotDir.toFile(), remoteSnapshotDir, (localFile, remoteFile) -> false);\r\n    SortedSet<String> allAdded = new TreeSet<>();\r\n    SortedSet<String> allRemoved = new TreeSet<>();\r\n    SortedSet<String> allRetained = new TreeSet<>();\r\n    BlobStoreTestUtil.getAllAddedInDiff(basePath, dirDiff, allAdded);\r\n    BlobStoreTestUtil.getAllRemovedInDiff(\"\", dirDiff, allRemoved);\r\n    BlobStoreTestUtil.getAllRetainedInDiff(\"\", dirDiff, allRetained);\r\n    // Assert\r\n    SortedSet<String> expectedAddedFiles = BlobStoreTestUtil.getExpected(expectedAdded);\r\n    SortedSet<String> expectedRetainedFiles = BlobStoreTestUtil.getExpected(expectedRetained);\r\n    SortedSet<String> expectedRemovedFiles = BlobStoreTestUtil.getExpected(expectedRemoved);\r\n    assertEquals(expectedAddedFiles, allAdded);\r\n    assertEquals(expectedRetainedFiles, allRetained);\r\n    assertEquals(expectedRemovedFiles, allRemoved);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestChangelogStreamManager.java",
  "methodName" : "createChangelogStreams",
  "sourceCode" : "@Test\r\npublic void createChangelogStreams() {\r\n    Map<String, String> map = new ImmutableMap.Builder<String, String>().put(\"stores.store0.factory\", \"factory.class\").put(\"stores.store0.changelog\", SYSTEM + \".\" + STREAM).put(\"stores.store0.accesslog.enabled\", \"true\").put(String.format(\"systems.%s.samza.factory\", SYSTEM), MockSystemAdminFactory.class.getName()).build();\r\n    Config config = new MapConfig(map);\r\n    ChangelogStreamManager.createChangelogStreams(config, MAX_CHANGELOG_STREAM_PARTITIONS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestKafkaChangelogStateBackendFactory.java",
  "methodName" : "testGetChangelogSSP",
  "sourceCode" : "@Test\r\npublic void testGetChangelogSSP() {\r\n    KafkaChangelogStateBackendFactory factory = new KafkaChangelogStateBackendFactory();\r\n    TaskName taskName0 = new TaskName(\"task0\");\r\n    TaskName taskName1 = new TaskName(\"task1\");\r\n    TaskModel taskModel0 = new TaskModel(taskName0, ImmutableSet.of(new SystemStreamPartition(\"input\", \"stream\", new Partition(0))), new Partition(10));\r\n    TaskModel taskModel1 = new TaskModel(taskName1, ImmutableSet.of(new SystemStreamPartition(\"input\", \"stream\", new Partition(1))), new Partition(11));\r\n    ContainerModel containerModel = new ContainerModel(\"processorId\", ImmutableMap.of(taskName0, taskModel0, taskName1, taskModel1));\r\n    Map<String, SystemStream> changeLogSystemStreams = ImmutableMap.of(\"store0\", new SystemStream(\"changelogSystem0\", \"store0-changelog\"), \"store1\", new SystemStream(\"changelogSystem1\", \"store1-changelog\"));\r\n    Set<SystemStreamPartition> expected = ImmutableSet.of(new SystemStreamPartition(\"changelogSystem0\", \"store0-changelog\", new Partition(10)), new SystemStreamPartition(\"changelogSystem1\", \"store1-changelog\", new Partition(10)), new SystemStreamPartition(\"changelogSystem0\", \"store0-changelog\", new Partition(11)), new SystemStreamPartition(\"changelogSystem1\", \"store1-changelog\", new Partition(11)));\r\n    Assert.assertEquals(expected, factory.getChangelogSSPForContainer(changeLogSystemStreams, new ContainerContextImpl(containerModel, null, null)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestKafkaChangelogStateBackendFactory.java",
  "methodName" : "testGetChangelogSSPsForContainerNoChangelogs",
  "sourceCode" : "@Test\r\npublic void testGetChangelogSSPsForContainerNoChangelogs() {\r\n    KafkaChangelogStateBackendFactory factory = new KafkaChangelogStateBackendFactory();\r\n    TaskName taskName0 = new TaskName(\"task0\");\r\n    TaskName taskName1 = new TaskName(\"task1\");\r\n    TaskModel taskModel0 = new TaskModel(taskName0, ImmutableSet.of(new SystemStreamPartition(\"input\", \"stream\", new Partition(0))), new Partition(10));\r\n    TaskModel taskModel1 = new TaskModel(taskName1, ImmutableSet.of(new SystemStreamPartition(\"input\", \"stream\", new Partition(1))), new Partition(11));\r\n    ContainerModel containerModel = new ContainerModel(\"processorId\", ImmutableMap.of(taskName0, taskModel0, taskName1, taskModel1));\r\n    Assert.assertEquals(Collections.emptySet(), factory.getChangelogSSPForContainer(Collections.emptyMap(), new ContainerContextImpl(containerModel, null, null)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestStorageRecovery.java",
  "methodName" : "testStorageEngineReceivedAllValues",
  "sourceCode" : "@Test\r\npublic void testStorageEngineReceivedAllValues() {\r\n    MockCoordinatorStreamSystemFactory.enableMockConsumerCache();\r\n    StorageRecovery storageRecovery = new StorageRecovery(config, path);\r\n    storageRecovery.run();\r\n    // because the stream has two partitions\r\n    assertEquals(2, MockStorageEngine.incomingMessageEnvelopes.size());\r\n    assertEquals(msg, MockStorageEngine.incomingMessageEnvelopes.get(0));\r\n    assertEquals(msg, MockStorageEngine.incomingMessageEnvelopes.get(1));\r\n    // correct path is passed to the store engine\r\n    String expectedStoreDir = String.format(\"%s/state/%s/Partition_\", path, STORE_NAME);\r\n    String actualStoreDir = MockStorageEngine.storeDir.toString();\r\n    assertEquals(expectedStoreDir, actualStoreDir.substring(0, actualStoreDir.length() - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputHandler.java",
  "methodName" : "testGetStartingOffsetsWhenStreamMetadataIsNull",
  "sourceCode" : "/**\r\n * This test is for cases, when calls to systemAdmin (e.g., KafkaSystemAdmin's) get-stream-metadata method return null.\r\n */\r\n@Test\r\npublic void testGetStartingOffsetsWhenStreamMetadataIsNull() {\r\n    final String taskName = \"test-get-starting-offset-task\";\r\n    Set<SystemStreamPartition> ssps = IntStream.range(1, 2).mapToObj(idx -> new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(idx))).collect(Collectors.toSet());\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = ssps.stream().collect(Collectors.toMap(SystemStreamPartition::getPartition, x -> new SystemStreamMetadata.SystemStreamPartitionMetadata(null, \"1\", \"2\")));\r\n    TaskSideInputHandler handler = new MockTaskSideInputHandlerBuilder(taskName, TaskMode.Active).addStreamMetadata(Collections.singletonMap(new SystemStream(TEST_SYSTEM, TEST_STREAM), new SystemStreamMetadata(TEST_STREAM, partitionMetadata))).addStore(TEST_STORE, ssps).build();\r\n    handler.init();\r\n    ssps.forEach(ssp -> {\r\n        String startingOffset = handler.getStartingOffset(new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, ssp.getPartition()));\r\n        Assert.assertNull(\"Starting offset should be null\", startingOffset);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputHandler.java",
  "methodName" : "testGetStartingOffsets",
  "sourceCode" : "@Test\r\npublic void testGetStartingOffsets() {\r\n    final String storeName = \"test-get-starting-offset-store\";\r\n    final String taskName = \"test-get-starting-offset-task\";\r\n    Set<SystemStreamPartition> ssps = IntStream.range(1, 6).mapToObj(idx -> new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(idx))).collect(Collectors.toSet());\r\n    TaskSideInputHandler handler = new MockTaskSideInputHandlerBuilder(taskName, TaskMode.Active).addStore(storeName, ssps).build();\r\n    // set up file and oldest offsets. for even partitions, fileOffsets will be larger; for odd partitions oldestOffsets will be larger\r\n    Map<SystemStreamPartition, String> fileOffsets = ssps.stream().collect(Collectors.toMap(Function.identity(), ssp -> {\r\n        int partitionId = ssp.getPartition().getPartitionId();\r\n        int offset = partitionId % 2 == 0 ? partitionId + 10 : partitionId;\r\n        return String.valueOf(offset);\r\n    }));\r\n    Map<SystemStreamPartition, String> oldestOffsets = ssps.stream().collect(Collectors.toMap(Function.identity(), ssp -> {\r\n        int partitionId = ssp.getPartition().getPartitionId();\r\n        int offset = partitionId % 2 == 0 ? partitionId : partitionId + 10;\r\n        return String.valueOf(offset);\r\n    }));\r\n    doCallRealMethod().when(handler).getStartingOffsets(fileOffsets, oldestOffsets);\r\n    Map<SystemStreamPartition, String> startingOffsets = handler.getStartingOffsets(fileOffsets, oldestOffsets);\r\n    assertTrue(\"Failed to get starting offsets for all ssps\", startingOffsets.size() == 5);\r\n    startingOffsets.forEach((ssp, offset) -> {\r\n        int partitionId = ssp.getPartition().getPartitionId();\r\n        String expectedOffset = partitionId % 2 == 0 ? // 1 + fileOffset\r\n        getOffsetAfter(String.valueOf(ssp.getPartition().getPartitionId() + 10)) : // oldestOffset\r\n        String.valueOf(ssp.getPartition().getPartitionId() + 10);\r\n        assertEquals(\"Larger of fileOffsets and oldestOffsets should always be chosen\", expectedOffset, offset);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testInit",
  "sourceCode" : "@Test\r\npublic void testInit() {\r\n    final String storeName = \"test-init-store\";\r\n    final String taskName = \"test-init-task\";\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, LOGGED_STORE_DIR).addLoggedStore(storeName, ImmutableSet.of()).build();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    File storeDir = testSideInputStorageManager.getStoreLocation(storeName);\r\n    assertTrue(\"Store directory: \" + storeDir.getPath() + \" is missing.\", storeDir.exists());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    final String storeName = \"test-flush-store\";\r\n    final String taskName = \"test-flush-task\";\r\n    final SystemStreamPartition ssp = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    final String offset = \"123\";\r\n    final ImmutableMap<SystemStreamPartition, String> processedOffsets = ImmutableMap.of(ssp, offset);\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, LOGGED_STORE_DIR).addLoggedStore(storeName, ImmutableSet.of(ssp)).build();\r\n    Map<String, StorageEngine> stores = new HashMap<>();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    testSideInputStorageManager.flush(processedOffsets);\r\n    for (StorageEngine storageEngine : stores.values()) {\r\n        verify(storageEngine).flush();\r\n    }\r\n    verify(testSideInputStorageManager).writeFileOffsets(eq(processedOffsets));\r\n    File storeDir = testSideInputStorageManager.getStoreLocation(storeName);\r\n    assertTrue(\"Store directory: \" + storeDir.getPath() + \" is missing.\", storeDir.exists());\r\n    Map<SystemStreamPartition, String> fileOffsets = testSideInputStorageManager.getFileOffsets();\r\n    assertTrue(\"Failed to get offset for ssp: \" + ssp.toString() + \" from file.\", fileOffsets.containsKey(ssp));\r\n    assertEquals(\"Mismatch between last processed offset and file offset.\", fileOffsets.get(ssp), offset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testStop",
  "sourceCode" : "@Test\r\npublic void testStop() {\r\n    final String storeName = \"test-stop-store\";\r\n    final String taskName = \"test-stop-task\";\r\n    final SystemStreamPartition ssp = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    final String offset = \"123\";\r\n    final ImmutableMap<SystemStreamPartition, String> processedOffsets = ImmutableMap.of(ssp, offset);\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, NON_LOGGED_STORE_DIR).addInMemoryStore(storeName, ImmutableSet.of()).build();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    testSideInputStorageManager.stop(processedOffsets);\r\n    verify(testSideInputStorageManager.getStore(storeName)).stop();\r\n    verify(testSideInputStorageManager).writeFileOffsets(eq(processedOffsets));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testWriteOffsetFilesForNonPersistedStore",
  "sourceCode" : "@Test\r\npublic void testWriteOffsetFilesForNonPersistedStore() {\r\n    final String storeName = \"test-write-offset-non-persisted-store\";\r\n    final String taskName = \"test-write-offset-for-non-persisted-task\";\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, NON_LOGGED_STORE_DIR).addInMemoryStore(storeName, ImmutableSet.of()).build();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    // should be no-op\r\n    testSideInputStorageManager.writeFileOffsets(Collections.emptyMap());\r\n    File storeDir = testSideInputStorageManager.getStoreLocation(storeName);\r\n    assertFalse(\"Store directory: \" + storeDir.getPath() + \" should not be created for non-persisted store\", storeDir.exists());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testWriteOffsetFilesForPersistedStore",
  "sourceCode" : "@Test\r\npublic void testWriteOffsetFilesForPersistedStore() {\r\n    final String storeName = \"test-write-offset-persisted-store\";\r\n    final String storeName2 = \"test-write-offset-persisted-store-2\";\r\n    final String taskName = \"test-write-offset-for-persisted-task\";\r\n    final String offset = \"123\";\r\n    final SystemStreamPartition ssp = new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(0));\r\n    final SystemStreamPartition ssp2 = new SystemStreamPartition(\"test-system2\", \"test-stream2\", new Partition(0));\r\n    Map<SystemStreamPartition, String> processedOffsets = ImmutableMap.of(ssp, offset, ssp2, offset);\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, LOGGED_STORE_DIR).addLoggedStore(storeName, ImmutableSet.of(ssp)).addLoggedStore(storeName2, ImmutableSet.of(ssp2)).build();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    testSideInputStorageManager.writeFileOffsets(processedOffsets);\r\n    File storeDir = testSideInputStorageManager.getStoreLocation(storeName);\r\n    assertTrue(\"Store directory: \" + storeDir.getPath() + \" is missing.\", storeDir.exists());\r\n    Map<SystemStreamPartition, String> fileOffsets = testSideInputStorageManager.getFileOffsets();\r\n    assertTrue(\"Failed to get offset for ssp: \" + ssp.toString() + \" from file.\", fileOffsets.containsKey(ssp));\r\n    assertEquals(\"Mismatch between last processed offset and file offset.\", fileOffsets.get(ssp), offset);\r\n    assertTrue(\"Failed to get offset for ssp: \" + ssp2.toString() + \" from file.\", fileOffsets.containsKey(ssp2));\r\n    assertEquals(\"Mismatch between last processed offset and file offset.\", fileOffsets.get(ssp2), offset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskSideInputStorageManager.java",
  "methodName" : "testGetFileOffsets",
  "sourceCode" : "@Test\r\npublic void testGetFileOffsets() {\r\n    final String storeName = \"test-get-file-offsets-store\";\r\n    final String taskName = \"test-get-file-offsets-task\";\r\n    final String offset = \"123\";\r\n    Set<SystemStreamPartition> ssps = IntStream.range(1, 6).mapToObj(idx -> new SystemStreamPartition(\"test-system\", \"test-stream\", new Partition(idx))).collect(Collectors.toSet());\r\n    TaskSideInputStorageManager testSideInputStorageManager = new MockTaskSideInputStorageManagerBuilder(taskName, LOGGED_STORE_DIR).addLoggedStore(storeName, ssps).build();\r\n    initializeSideInputStorageManager(testSideInputStorageManager);\r\n    Map<SystemStreamPartition, String> processedOffsets = ssps.stream().collect(Collectors.toMap(Function.identity(), ssp -> offset));\r\n    testSideInputStorageManager.writeFileOffsets(processedOffsets);\r\n    Map<SystemStreamPartition, String> fileOffsets = testSideInputStorageManager.getFileOffsets();\r\n    ssps.forEach(ssp -> {\r\n        assertTrue(\"Failed to get offset for ssp: \" + ssp.toString() + \" from file.\", fileOffsets.containsKey(ssp));\r\n        assertEquals(\"Mismatch between last processed offset and file offset.\", fileOffsets.get(ssp), offset);\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testCommitManagerStart",
  "sourceCode" : "@Test\r\npublic void testCommitManagerStart() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Checkpoint checkpoint = mock(Checkpoint.class);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), null, null);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    cm.init(checkpoint);\r\n    verify(taskBackupManager1).init(eq(checkpoint));\r\n    verify(taskBackupManager2).init(eq(checkpoint));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testCommitManagerStartNullCheckpointManager",
  "sourceCode" : "@Test\r\npublic void testCommitManagerStartNullCheckpointManager() {\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    TaskName task = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(task, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), null, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), null, null);\r\n    cm.init(null);\r\n    verify(taskBackupManager1).init(eq(null));\r\n    verify(taskBackupManager2).init(eq(null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testSnapshotAndCommitAllFactories",
  "sourceCode" : "@Test\r\npublic void testSnapshotAndCommitAllFactories() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Checkpoint checkpoint = mock(Checkpoint.class);\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), null, metrics);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    cm.init(checkpoint);\r\n    verify(taskBackupManager1).init(eq(checkpoint));\r\n    verify(taskBackupManager2).init(eq(checkpoint));\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    Map<String, String> factory1Checkpoints = ImmutableMap.of(\"store1\", \"system;stream;1\", \"store2\", \"system;stream;2\");\r\n    Map<String, String> factory2Checkpoints = ImmutableMap.of(\"store1\", \"blobId1\", \"store2\", \"blobId2\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(Collections.emptyMap());\r\n    when(taskBackupManager1.snapshot(newCheckpointId)).thenReturn(factory1Checkpoints);\r\n    when(taskBackupManager2.snapshot(newCheckpointId)).thenReturn(factory2Checkpoints);\r\n    when(taskBackupManager1.upload(newCheckpointId, factory1Checkpoints)).thenReturn(CompletableFuture.completedFuture(factory1Checkpoints));\r\n    when(taskBackupManager2.upload(newCheckpointId, factory2Checkpoints)).thenReturn(CompletableFuture.completedFuture(factory2Checkpoints));\r\n    Map<String, Map<String, String>> snapshotSCMs = cm.snapshot(newCheckpointId);\r\n    cm.upload(newCheckpointId, snapshotSCMs);\r\n    // Test flow for snapshot\r\n    verify(taskBackupManager1).snapshot(newCheckpointId);\r\n    verify(taskBackupManager2).snapshot(newCheckpointId);\r\n    // Test flow for upload\r\n    verify(taskBackupManager1).upload(newCheckpointId, factory1Checkpoints);\r\n    verify(taskBackupManager2).upload(newCheckpointId, factory2Checkpoints);\r\n    verify(checkpointTimer).update(anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testFlushAndCheckpointOnSnapshot",
  "sourceCode" : "@Test\r\npublic void testFlushAndCheckpointOnSnapshot() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Checkpoint checkpoint = mock(Checkpoint.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    StorageEngine mockPStore = mock(StorageEngine.class);\r\n    StoreProperties pStoreProps = mock(StoreProperties.class);\r\n    when(mockPStore.getStoreProperties()).thenReturn(pStoreProps);\r\n    when(pStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(pStoreProps.isDurableStore()).thenReturn(false);\r\n    StorageEngine mockLIStore = mock(StorageEngine.class);\r\n    StoreProperties liStoreProps = mock(StoreProperties.class);\r\n    when(mockLIStore.getStoreProperties()).thenReturn(liStoreProps);\r\n    when(liStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(liStoreProps.isDurableStore()).thenReturn(true);\r\n    StorageEngine mockIStore = mock(StorageEngine.class);\r\n    StoreProperties iStoreProps = mock(StoreProperties.class);\r\n    when(mockIStore.getStoreProperties()).thenReturn(iStoreProps);\r\n    when(iStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(iStoreProps.isDurableStore()).thenReturn(false);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    Map<String, StorageEngine> storageEngines = ImmutableMap.of(\"storeLP\", mockLPStore, \"storeP\", mockPStore, \"storeLI\", mockLIStore, \"storeI\", mockIStore);\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), null, metrics);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    cm.init(checkpoint);\r\n    verify(taskBackupManager1).init(eq(checkpoint));\r\n    verify(taskBackupManager2).init(eq(checkpoint));\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    Map<String, String> factory1Checkpoints = ImmutableMap.of(\"store1\", \"system;stream;1\", \"store2\", \"system;stream;2\");\r\n    Map<String, String> factory2Checkpoints = ImmutableMap.of(\"store1\", \"blobId1\", \"store2\", \"blobId2\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(storageEngines);\r\n    when(taskBackupManager1.snapshot(newCheckpointId)).thenReturn(factory1Checkpoints);\r\n    when(taskBackupManager1.upload(newCheckpointId, factory1Checkpoints)).thenReturn(CompletableFuture.completedFuture(factory1Checkpoints));\r\n    when(taskBackupManager2.snapshot(newCheckpointId)).thenReturn(factory2Checkpoints);\r\n    when(taskBackupManager2.upload(newCheckpointId, factory2Checkpoints)).thenReturn(CompletableFuture.completedFuture(factory2Checkpoints));\r\n    when(mockLIStore.checkpoint(newCheckpointId)).thenReturn(Optional.empty());\r\n    cm.init(checkpoint);\r\n    cm.snapshot(newCheckpointId);\r\n    // Assert stores where flushed\r\n    verify(mockIStore).flush();\r\n    verify(mockPStore).flush();\r\n    verify(mockLIStore).flush();\r\n    verify(mockLPStore).flush();\r\n    // only logged and persisted stores are checkpointed\r\n    verify(mockLPStore).checkpoint(newCheckpointId);\r\n    // ensure that checkpoint is never called for non-logged persistent stores since they're\r\n    // always cleared on restart.\r\n    verify(mockPStore, never()).checkpoint(any());\r\n    // ensure that checkpoint is never called for non-persistent stores\r\n    verify(mockIStore, never()).checkpoint(any());\r\n    verify(mockLIStore, never()).checkpoint(any());\r\n    verify(checkpointTimer).update(anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testSnapshotFailsIfErrorCreatingCheckpoint",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testSnapshotFailsIfErrorCreatingCheckpoint() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    when(mockLPStore.checkpoint(any())).thenThrow(new IllegalStateException());\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    Map<String, StorageEngine> storageEngines = ImmutableMap.of(\"storeLP\", mockLPStore);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), null, metrics);\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(storageEngines);\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    cm.init(null);\r\n    cm.snapshot(newCheckpointId);\r\n    // Assert stores where flushed\r\n    verify(mockLPStore).flush();\r\n    // only logged and persisted stores are checkpointed\r\n    verify(mockLPStore).checkpoint(newCheckpointId);\r\n    verify(taskBackupManager1, never()).snapshot(any());\r\n    verify(taskBackupManager2, never()).snapshot(any());\r\n    verify(taskBackupManager1, never()).upload(any(), any());\r\n    verify(taskBackupManager2, never()).upload(any(), any());\r\n    fail(\"Should have thrown an exception when the storageEngine#checkpoint did not succeed\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testCleanupAllBackupManagers",
  "sourceCode" : "@Test\r\npublic void testCleanupAllBackupManagers() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Checkpoint checkpoint = mock(Checkpoint.class);\r\n    File durableStoreDir = mock(File.class);\r\n    when(durableStoreDir.listFiles()).thenReturn(new File[0]);\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), durableStoreDir, metrics);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(Collections.emptyMap());\r\n    when(taskBackupManager1.cleanUp(any(), any())).thenReturn(CompletableFuture.<Void>completedFuture(null));\r\n    when(taskBackupManager2.cleanUp(any(), any())).thenReturn(CompletableFuture.<Void>completedFuture(null));\r\n    Map<String, String> factory1Checkpoints = ImmutableMap.of(\"store1\", \"system;stream;1\", \"store2\", \"system;stream;2\");\r\n    Map<String, String> factory2Checkpoints = ImmutableMap.of(\"store1\", \"blobId1\", \"store2\", \"blobId2\");\r\n    Map<String, Map<String, String>> factoryCheckpointsMap = ImmutableMap.of(\"factory1\", factory1Checkpoints, \"factory2\", factory2Checkpoints);\r\n    when(taskBackupManager1.cleanUp(any(), any())).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(taskBackupManager2.cleanUp(any(), any())).thenReturn(CompletableFuture.completedFuture(null));\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    cm.cleanUp(newCheckpointId, factoryCheckpointsMap).join();\r\n    verify(taskBackupManager1).cleanUp(newCheckpointId, factory1Checkpoints);\r\n    verify(taskBackupManager2).cleanUp(newCheckpointId, factory2Checkpoints);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testCleanupFailsIfBackupManagerNotInitiated",
  "sourceCode" : "@Test\r\npublic void testCleanupFailsIfBackupManagerNotInitiated() {\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Checkpoint checkpoint = mock(Checkpoint.class);\r\n    File durableStoreDir = mock(File.class);\r\n    when(durableStoreDir.listFiles()).thenReturn(new File[0]);\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(Collections.emptyMap());\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), new StorageManagerUtil(), durableStoreDir, metrics);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    Map<String, Map<String, String>> factoryCheckpointsMap = ImmutableMap.of(// factory 3 should be ignored\r\n    \"factory3\", // factory 3 should be ignored\r\n    Collections.emptyMap());\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    cm.cleanUp(newCheckpointId, factoryCheckpointsMap);\r\n    // should not fail the commit because the job should ignore any factories checkpoints not initialized\r\n    // in case the user is in a migration phase from on state backend to another\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testPersistToFileSystemCheckpointV1AndV2Checkpoint",
  "sourceCode" : "@Test\r\npublic void testPersistToFileSystemCheckpointV1AndV2Checkpoint() throws IOException {\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    StorageEngine mockPStore = mock(StorageEngine.class);\r\n    StoreProperties pStoreProps = mock(StoreProperties.class);\r\n    when(mockPStore.getStoreProperties()).thenReturn(pStoreProps);\r\n    when(pStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(pStoreProps.isDurableStore()).thenReturn(false);\r\n    StorageEngine mockLIStore = mock(StorageEngine.class);\r\n    StoreProperties liStoreProps = mock(StoreProperties.class);\r\n    when(mockLIStore.getStoreProperties()).thenReturn(liStoreProps);\r\n    when(liStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(liStoreProps.isDurableStore()).thenReturn(true);\r\n    StorageEngine mockIStore = mock(StorageEngine.class);\r\n    StoreProperties iStoreProps = mock(StoreProperties.class);\r\n    when(mockIStore.getStoreProperties()).thenReturn(iStoreProps);\r\n    when(iStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(iStoreProps.isDurableStore()).thenReturn(false);\r\n    Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore, \"persistentStore\", mockPStore, \"loggedInMemStore\", mockLIStore, \"inMemStore\", mockIStore);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStream changelogSystemStream = new SystemStream(\"changelogSystem\", \"changelogStream\");\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", changelogSystemStream, \"loggedInMemStore\", new SystemStream(\"system\", \"stream\"));\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File durableStoreDir = new File(\"durableStorePath\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(durableStoreDir), any(), any(), any())).thenReturn(durableStoreDir);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, changelogPartition, null, null, ForkJoinPool.commonPool(), storageManagerUtil, durableStoreDir, metrics));\r\n    doNothing().when(commitManager).writeChangelogOffsetFile(any(), any(), any(), any());\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), any(CheckpointId.class))).thenAnswer((Answer<String>) invocation -> {\r\n        File file = invocation.getArgumentAt(0, File.class);\r\n        CheckpointId checkpointId = invocation.getArgumentAt(1, CheckpointId.class);\r\n        return file + \"-\" + checkpointId;\r\n    });\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    String newestOffset = \"1\";\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(newCheckpointId, newestOffset);\r\n    Map<SystemStreamPartition, String> offsetsJava = ImmutableMap.of(changelogSSP, kafkaChangelogSSPOffset.toString());\r\n    commitManager.init(null);\r\n    // invoke persist to file system for v2 checkpoint\r\n    commitManager.writeCheckpointToStoreDirectories(new CheckpointV1(offsetsJava));\r\n    verify(commitManager).writeChangelogOffsetFiles(offsetsJava);\r\n    // evoked twice, for OFFSET-V1 and OFFSET-V2\r\n    verify(commitManager).writeChangelogOffsetFile(eq(\"loggedPersistentStore\"), eq(changelogSSP), eq(newestOffset), eq(durableStoreDir));\r\n    File checkpointFile = Paths.get(storageManagerUtil.getStoreCheckpointDir(durableStoreDir, kafkaChangelogSSPOffset.getCheckpointId())).toFile();\r\n    verify(commitManager).writeChangelogOffsetFile(eq(\"loggedPersistentStore\"), eq(changelogSSP), eq(newestOffset), eq(checkpointFile));\r\n    Map<String, String> storeSCM = ImmutableMap.of(\"loggedPersistentStore\", \"system;loggedPersistentStoreStream;1\", \"persistentStore\", \"system;persistentStoreStream;1\", \"loggedInMemStore\", \"system;loggedInMemStoreStream;1\", \"inMemStore\", \"system;inMemStoreStream;1\");\r\n    CheckpointV2 checkpoint = new CheckpointV2(newCheckpointId, Collections.emptyMap(), Collections.singletonMap(\"factory\", storeSCM));\r\n    // invoke persist to file system for v2 checkpoint\r\n    commitManager.writeCheckpointToStoreDirectories(checkpoint);\r\n    // Validate only durable and persisted stores are persisted\r\n    // This should be evoked twice, for checkpointV1 and checkpointV2\r\n    verify(storageManagerUtil, times(2)).getTaskStoreDir(eq(durableStoreDir), eq(\"loggedPersistentStore\"), eq(taskName), any());\r\n    File checkpointPath = Paths.get(storageManagerUtil.getStoreCheckpointDir(durableStoreDir, newCheckpointId)).toFile();\r\n    verify(storageManagerUtil).writeCheckpointV2File(eq(checkpointPath), eq(checkpoint));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testPersistToFileSystemCheckpointV2Only",
  "sourceCode" : "@Test\r\npublic void testPersistToFileSystemCheckpointV2Only() throws IOException {\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    StorageEngine mockPStore = mock(StorageEngine.class);\r\n    StoreProperties pStoreProps = mock(StoreProperties.class);\r\n    when(mockPStore.getStoreProperties()).thenReturn(pStoreProps);\r\n    when(pStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(pStoreProps.isDurableStore()).thenReturn(false);\r\n    StorageEngine mockLIStore = mock(StorageEngine.class);\r\n    StoreProperties liStoreProps = mock(StoreProperties.class);\r\n    when(mockLIStore.getStoreProperties()).thenReturn(liStoreProps);\r\n    when(liStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(liStoreProps.isDurableStore()).thenReturn(true);\r\n    StorageEngine mockIStore = mock(StorageEngine.class);\r\n    StoreProperties iStoreProps = mock(StoreProperties.class);\r\n    when(mockIStore.getStoreProperties()).thenReturn(iStoreProps);\r\n    when(iStoreProps.isPersistedToDisk()).thenReturn(false);\r\n    when(iStoreProps.isDurableStore()).thenReturn(false);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore, \"persistentStore\", mockPStore, \"loggedInMemStore\", mockLIStore, \"inMemStore\", mockIStore);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStream changelogSystemStream = new SystemStream(\"changelogSystem\", \"changelogStream\");\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", changelogSystemStream, \"loggedInMemStore\", new SystemStream(\"system\", \"stream\"));\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File durableStoreDir = new File(\"durableStorePath\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(durableStoreDir), eq(\"loggedPersistentStore\"), any(), any())).thenReturn(durableStoreDir);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, changelogPartition, null, null, ForkJoinPool.commonPool(), storageManagerUtil, durableStoreDir, metrics));\r\n    doNothing().when(commitManager).writeChangelogOffsetFile(any(), any(), any(), any());\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), any(CheckpointId.class))).thenAnswer((Answer<String>) invocation -> {\r\n        File file = invocation.getArgumentAt(0, File.class);\r\n        CheckpointId checkpointId = invocation.getArgumentAt(1, CheckpointId.class);\r\n        return file + \"-\" + checkpointId;\r\n    });\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    java.util.Map<String, String> storeSCM = ImmutableMap.of(\"loggedPersistentStore\", \"system;loggedPersistentStoreStream;1\", \"persistentStore\", \"system;persistentStoreStream;1\", \"loggedInMemStore\", \"system;loggedInMemStoreStream;1\", \"inMemStore\", \"system;inMemStoreStream;1\");\r\n    CheckpointV2 checkpoint = new CheckpointV2(newCheckpointId, Collections.emptyMap(), Collections.singletonMap(\"factory\", storeSCM));\r\n    commitManager.init(null);\r\n    // invoke persist to file system\r\n    commitManager.writeCheckpointToStoreDirectories(checkpoint);\r\n    // Validate only durable and persisted stores are persisted\r\n    verify(storageManagerUtil).getTaskStoreDir(eq(durableStoreDir), eq(\"loggedPersistentStore\"), eq(taskName), any());\r\n    File checkpointPath = Paths.get(storageManagerUtil.getStoreCheckpointDir(durableStoreDir, newCheckpointId)).toFile();\r\n    verify(storageManagerUtil).writeCheckpointV2File(eq(checkpointPath), eq(checkpoint));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testWriteChangelogOffsetFilesV1",
  "sourceCode" : "@Test\r\npublic void testWriteChangelogOffsetFilesV1() throws IOException {\r\n    Map<String, Map<SystemStreamPartition, String>> mockFileSystem = new HashMap<>();\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStream changelogSystemStream = new SystemStream(\"changelogSystem\", \"changelogStream\");\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", changelogSystemStream);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File tmpTestPath = new File(\"store-checkpoint-test\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(tmpTestPath), eq(\"loggedPersistentStore\"), any(), any())).thenReturn(tmpTestPath);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, changelogPartition, null, null, ForkJoinPool.commonPool(), storageManagerUtil, tmpTestPath, metrics));\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), any(CheckpointId.class))).thenAnswer((Answer<String>) invocation -> {\r\n        File file = invocation.getArgumentAt(0, File.class);\r\n        CheckpointId checkpointId = invocation.getArgumentAt(1, CheckpointId.class);\r\n        return file + \"-\" + checkpointId;\r\n    });\r\n    doAnswer(invocation -> {\r\n        String fileDir = invocation.getArgumentAt(3, File.class).getName();\r\n        SystemStreamPartition ssp = invocation.getArgumentAt(1, SystemStreamPartition.class);\r\n        String offset = invocation.getArgumentAt(2, String.class);\r\n        if (mockFileSystem.containsKey(fileDir)) {\r\n            mockFileSystem.get(fileDir).put(ssp, offset);\r\n        } else {\r\n            Map<SystemStreamPartition, String> sspOffsets = new HashMap<>();\r\n            sspOffsets.put(ssp, offset);\r\n            mockFileSystem.put(fileDir, sspOffsets);\r\n        }\r\n        return null;\r\n    }).when(commitManager).writeChangelogOffsetFile(any(), any(), any(), any());\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    String newestOffset = \"1\";\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(newCheckpointId, newestOffset);\r\n    java.util.Map<SystemStreamPartition, String> offsetsJava = ImmutableMap.of(changelogSSP, kafkaChangelogSSPOffset.toString());\r\n    commitManager.init(null);\r\n    // invoke persist to file system for v2 checkpoint\r\n    commitManager.writeCheckpointToStoreDirectories(new CheckpointV1(offsetsJava));\r\n    assertEquals(2, mockFileSystem.size());\r\n    // check if v2 offsets are written correctly\r\n    String v2FilePath = storageManagerUtil.getStoreCheckpointDir(tmpTestPath, newCheckpointId);\r\n    assertTrue(mockFileSystem.containsKey(v2FilePath));\r\n    assertTrue(mockFileSystem.get(v2FilePath).containsKey(changelogSSP));\r\n    assertEquals(1, mockFileSystem.get(v2FilePath).size());\r\n    assertEquals(newestOffset, mockFileSystem.get(v2FilePath).get(changelogSSP));\r\n    // check if v1 offsets are written correctly\r\n    String v1FilePath = tmpTestPath.getPath();\r\n    assertTrue(mockFileSystem.containsKey(v1FilePath));\r\n    assertTrue(mockFileSystem.get(v1FilePath).containsKey(changelogSSP));\r\n    assertEquals(1, mockFileSystem.get(v1FilePath).size());\r\n    assertEquals(newestOffset, mockFileSystem.get(v1FilePath).get(changelogSSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testWriteChangelogOffsetFilesV2andV1",
  "sourceCode" : "@Test\r\npublic void testWriteChangelogOffsetFilesV2andV1() throws IOException {\r\n    Map<String, Map<SystemStreamPartition, String>> mockFileSystem = new HashMap<>();\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    Map<String, CheckpointV2> mockCheckpointFileSystem = new HashMap<>();\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStream changelogSystemStream = new SystemStream(\"changelogSystem\", \"changelogStream\");\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", changelogSystemStream);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File tmpTestPath = new File(\"store-checkpoint-test\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(tmpTestPath), eq(\"loggedPersistentStore\"), any(), any())).thenReturn(tmpTestPath);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, changelogPartition, null, null, ForkJoinPool.commonPool(), storageManagerUtil, tmpTestPath, metrics));\r\n    doAnswer(invocation -> {\r\n        String fileDir = invocation.getArgumentAt(3, File.class).getName();\r\n        SystemStreamPartition ssp = invocation.getArgumentAt(1, SystemStreamPartition.class);\r\n        String offset = invocation.getArgumentAt(2, String.class);\r\n        if (mockFileSystem.containsKey(fileDir)) {\r\n            mockFileSystem.get(fileDir).put(ssp, offset);\r\n        } else {\r\n            Map<SystemStreamPartition, String> sspOffsets = new HashMap<>();\r\n            sspOffsets.put(ssp, offset);\r\n            mockFileSystem.put(fileDir, sspOffsets);\r\n        }\r\n        return null;\r\n    }).when(commitManager).writeChangelogOffsetFile(any(), any(), any(), any());\r\n    doAnswer(invocation -> {\r\n        String storeDir = invocation.getArgumentAt(0, File.class).getName();\r\n        CheckpointV2 checkpointV2 = invocation.getArgumentAt(1, CheckpointV2.class);\r\n        mockCheckpointFileSystem.put(storeDir, checkpointV2);\r\n        return null;\r\n    }).when(storageManagerUtil).writeCheckpointV2File(any(), any());\r\n    when(storageManagerUtil.getStoreCheckpointDir(any(File.class), any(CheckpointId.class))).thenAnswer((Answer<String>) invocation -> {\r\n        File file = invocation.getArgumentAt(0, File.class);\r\n        CheckpointId checkpointId = invocation.getArgumentAt(1, CheckpointId.class);\r\n        return file + \"-\" + checkpointId;\r\n    });\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    String newestOffset = \"1\";\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(newCheckpointId, newestOffset);\r\n    java.util.Map<SystemStreamPartition, String> offsetsJava = ImmutableMap.of(changelogSSP, kafkaChangelogSSPOffset.toString());\r\n    commitManager.init(null);\r\n    // invoke persist to file system for v1 checkpoint\r\n    commitManager.writeCheckpointToStoreDirectories(new CheckpointV1(offsetsJava));\r\n    assertEquals(2, mockFileSystem.size());\r\n    // check if v2 offsets are written correctly\r\n    String v2FilePath = storageManagerUtil.getStoreCheckpointDir(tmpTestPath, newCheckpointId);\r\n    assertTrue(mockFileSystem.containsKey(v2FilePath));\r\n    assertTrue(mockFileSystem.get(v2FilePath).containsKey(changelogSSP));\r\n    assertEquals(1, mockFileSystem.get(v2FilePath).size());\r\n    assertEquals(newestOffset, mockFileSystem.get(v2FilePath).get(changelogSSP));\r\n    // check if v1 offsets are written correctly\r\n    String v1FilePath = tmpTestPath.getPath();\r\n    assertTrue(mockFileSystem.containsKey(v1FilePath));\r\n    assertTrue(mockFileSystem.get(v1FilePath).containsKey(changelogSSP));\r\n    assertEquals(1, mockFileSystem.get(v1FilePath).size());\r\n    assertEquals(newestOffset, mockFileSystem.get(v1FilePath).get(changelogSSP));\r\n    java.util.Map<String, String> storeSCM = ImmutableMap.of(\"loggedPersistentStore\", \"system;loggedPersistentStoreStream;1\", \"persistentStore\", \"system;persistentStoreStream;1\", \"loggedInMemStore\", \"system;loggedInMemStoreStream;1\", \"inMemStore\", \"system;inMemStoreStream;1\");\r\n    CheckpointV2 checkpoint = new CheckpointV2(newCheckpointId, Collections.emptyMap(), Collections.singletonMap(\"factory\", storeSCM));\r\n    // invoke persist to file system with checkpoint v2\r\n    commitManager.writeCheckpointToStoreDirectories(checkpoint);\r\n    assertTrue(mockCheckpointFileSystem.containsKey(v2FilePath));\r\n    assertEquals(checkpoint, mockCheckpointFileSystem.get(v2FilePath));\r\n    assertTrue(mockCheckpointFileSystem.containsKey(v1FilePath));\r\n    assertEquals(checkpoint, mockCheckpointFileSystem.get(v1FilePath));\r\n    assertEquals(2, mockCheckpointFileSystem.size());\r\n    CheckpointV2 updatedCheckpoint = new CheckpointV2(newCheckpointId, ImmutableMap.of(new SystemStreamPartition(\"inputSystem\", \"inputStream\", changelogPartition), \"5\"), Collections.singletonMap(\"factory\", storeSCM));\r\n    commitManager.writeCheckpointToStoreDirectories(updatedCheckpoint);\r\n    assertEquals(updatedCheckpoint, mockCheckpointFileSystem.get(v2FilePath));\r\n    assertEquals(updatedCheckpoint, mockCheckpointFileSystem.get(v1FilePath));\r\n    assertEquals(2, mockCheckpointFileSystem.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testWriteChangelogOffsetFilesWithEmptyChangelogTopic",
  "sourceCode" : "@Test\r\npublic void testWriteChangelogOffsetFilesWithEmptyChangelogTopic() throws IOException {\r\n    Map<String, Map<SystemStreamPartition, String>> mockFileSystem = new HashMap<>();\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStream changelogSystemStream = new SystemStream(\"changelogSystem\", \"changelogStream\");\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", changelogSystemStream);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File tmpTestPath = new File(\"store-checkpoint-test\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(tmpTestPath), any(), any(), any())).thenReturn(tmpTestPath);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, changelogPartition, null, null, ForkJoinPool.commonPool(), storageManagerUtil, tmpTestPath, metrics));\r\n    doAnswer(invocation -> {\r\n        String storeName = invocation.getArgumentAt(0, String.class);\r\n        String fileDir = invocation.getArgumentAt(3, File.class).getName();\r\n        String mockKey = storeName + fileDir;\r\n        SystemStreamPartition ssp = invocation.getArgumentAt(1, SystemStreamPartition.class);\r\n        String offset = invocation.getArgumentAt(2, String.class);\r\n        if (mockFileSystem.containsKey(mockKey)) {\r\n            mockFileSystem.get(mockKey).put(ssp, offset);\r\n        } else {\r\n            Map<SystemStreamPartition, String> sspOffsets = new HashMap<>();\r\n            sspOffsets.put(ssp, offset);\r\n            mockFileSystem.put(mockKey, sspOffsets);\r\n        }\r\n        return null;\r\n    }).when(commitManager).writeChangelogOffsetFile(any(), any(), any(), any());\r\n    CheckpointId newCheckpointId = CheckpointId.create();\r\n    String newestOffset = null;\r\n    KafkaChangelogSSPOffset kafkaChangelogSSPOffset = new KafkaChangelogSSPOffset(newCheckpointId, newestOffset);\r\n    java.util.Map<SystemStreamPartition, String> offsetsJava = ImmutableMap.of(changelogSSP, kafkaChangelogSSPOffset.toString());\r\n    commitManager.init(null);\r\n    // invoke persist to file system for v2 checkpoint\r\n    commitManager.writeCheckpointToStoreDirectories(new CheckpointV1(offsetsJava));\r\n    assertTrue(mockFileSystem.isEmpty());\r\n    // verify that delete was called on current store dir offset file\r\n    verify(storageManagerUtil, times(1)).deleteOffsetFile(eq(tmpTestPath));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testThrowOnWriteCheckpointDirIfUnsuccessful",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testThrowOnWriteCheckpointDirIfUnsuccessful() {\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    StorageEngine mockLPStore = mock(StorageEngine.class);\r\n    StoreProperties lpStoreProps = mock(StoreProperties.class);\r\n    when(mockLPStore.getStoreProperties()).thenReturn(lpStoreProps);\r\n    when(lpStoreProps.isPersistedToDisk()).thenReturn(true);\r\n    when(lpStoreProps.isDurableStore()).thenReturn(true);\r\n    Path mockPath = mock(Path.class);\r\n    when(mockLPStore.checkpoint(any())).thenReturn(Optional.of(mockPath));\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"loggedPersistentStore\", mockLPStore);\r\n    java.util.Map<String, SystemStream> storeChangelogsStreams = ImmutableMap.of(\"loggedPersistentStore\", mock(SystemStream.class));\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    File tmpTestPath = new File(\"store-checkpoint-test\");\r\n    when(storageManagerUtil.getTaskStoreDir(eq(tmpTestPath), eq(\"loggedPersistentStore\"), any(), any())).thenReturn(tmpTestPath);\r\n    TaskName taskName = new TaskName(\"task\");\r\n    TaskStorageCommitManager commitManager = spy(new TaskStorageCommitManager(taskName, Collections.emptyMap(), containerStorageManager, storeChangelogsStreams, mock(Partition.class), null, null, ForkJoinPool.commonPool(), storageManagerUtil, tmpTestPath, metrics));\r\n    java.util.Map<String, String> storeSCM = ImmutableMap.of(\"loggedPersistentStore\", \"system;loggedPersistentStoreStream;1\", \"persistentStore\", \"system;persistentStoreStream;1\", \"loggedInMemStore\", \"system;loggedInMemStoreStream;1\", \"inMemStore\", \"system;inMemStoreStream;1\");\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(taskStores);\r\n    CheckpointV2 checkpoint = new CheckpointV2(CheckpointId.create(), Collections.emptyMap(), Collections.singletonMap(\"factory\", storeSCM));\r\n    doThrow(IOException.class).when(storageManagerUtil).writeCheckpointV2File(eq(tmpTestPath), eq(checkpoint));\r\n    commitManager.init(null);\r\n    // Should throw samza exception since writeCheckpointV2 failed\r\n    commitManager.writeCheckpointToStoreDirectories(checkpoint);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTaskStorageCommitManager.java",
  "methodName" : "testRemoveOldCheckpointsWhenBaseDirContainsRegularFiles",
  "sourceCode" : "@Test\r\npublic void testRemoveOldCheckpointsWhenBaseDirContainsRegularFiles() {\r\n    ContainerStorageManager containerStorageManager = mock(ContainerStorageManager.class);\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    TaskBackupManager taskBackupManager1 = mock(TaskBackupManager.class);\r\n    TaskBackupManager taskBackupManager2 = mock(TaskBackupManager.class);\r\n    File durableStoreDir = mock(File.class);\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    StorageManagerUtil storageManagerUtil = mock(StorageManagerUtil.class);\r\n    TaskName taskName = new TaskName(\"task1\");\r\n    Map<String, TaskBackupManager> backupManagers = ImmutableMap.of(\"factory1\", taskBackupManager1, \"factory2\", taskBackupManager2);\r\n    when(containerStorageManager.getAllStores(taskName)).thenReturn(Collections.emptyMap());\r\n    TaskStorageCommitManager cm = new TaskStorageCommitManager(taskName, backupManagers, containerStorageManager, Collections.emptyMap(), new Partition(1), checkpointManager, new MapConfig(), ForkJoinPool.commonPool(), storageManagerUtil, durableStoreDir, metrics);\r\n    File mockStoreDir = mock(File.class);\r\n    String mockStoreDirName = \"notDirectory\";\r\n    when(durableStoreDir.listFiles()).thenReturn(new File[] { mockStoreDir });\r\n    when(mockStoreDir.getName()).thenReturn(mockStoreDirName);\r\n    when(storageManagerUtil.getTaskStoreDir(eq(durableStoreDir), eq(mockStoreDirName), eq(taskName), eq(TaskMode.Active))).thenReturn(mockStoreDir);\r\n    // null here can happen if listFiles is called on a non-directory\r\n    when(mockStoreDir.listFiles(any(FileFilter.class))).thenReturn(null);\r\n    cm.cleanUp(CheckpointId.create(), new HashMap<>()).join();\r\n    verify(durableStoreDir).listFiles();\r\n    verify(mockStoreDir).listFiles(any(FileFilter.class));\r\n    verify(storageManagerUtil).getTaskStoreDir(eq(durableStoreDir), eq(mockStoreDirName), eq(taskName), eq(TaskMode.Active));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetCurrentChangelogOffsets",
  "sourceCode" : "@Test\r\npublic void testGetCurrentChangelogOffsets() {\r\n    // test gets metadata for all and only task store changelog SSPs\r\n    // test all changelogs have same partition\r\n    // test does not change returned ssp metadata\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    when(mockTaskModel.getTaskName()).thenReturn(new TaskName(\"Partition 0\"));\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    String store2Name = \"store2\";\r\n    String changelog2SystemName = \"system2\";\r\n    String changelog2StreamName = \"store2Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStream changelog2SystemStream = new SystemStream(changelog2SystemName, changelog2StreamName);\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream, store2Name, changelog2SystemStream);\r\n    SSPMetadataCache mockSSPMetadataCache = mock(SSPMetadataCache.class);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"1\", \"2\");\r\n    when(mockSSPMetadataCache.getMetadata(eq(changelog1SSP))).thenReturn(changelog1SSPMetadata);\r\n    SystemStreamPartition changelog2SSP = new SystemStreamPartition(changelog2SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog2SSPMetadata = new SystemStreamPartitionMetadata(\"1\", \"2\", \"3\");\r\n    when(mockSSPMetadataCache.getMetadata(eq(changelog2SSP))).thenReturn(changelog2SSPMetadata);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> currentChangelogOffsets = TransactionalStateTaskRestoreManager.getCurrentChangelogOffsets(mockTaskModel, mockStoreChangelogs, mockSSPMetadataCache);\r\n    verify(mockSSPMetadataCache, times(1)).getMetadata(changelog1SSP);\r\n    verify(mockSSPMetadataCache, times(1)).getMetadata(changelog2SSP);\r\n    verifyNoMoreInteractions(mockSSPMetadataCache);\r\n    assertEquals(2, currentChangelogOffsets.size());\r\n    assertEquals(changelog1SSPMetadata, currentChangelogOffsets.get(changelog1SSP));\r\n    assertEquals(changelog2SSPMetadata, currentChangelogOffsets.get(changelog2SSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForNonLoggedPersistentStore_AlwaysClearStore",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForNonLoggedPersistentStore_AlwaysClearStore() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(false);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of();\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of();\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of();\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockNonLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, null, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    assertEquals(1, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    // ensure that there is nothing to retain or restore.\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    assertEquals(0, storeActions.storesToRestore.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testStoreDeletedWhenCleanDirsFlagSet",
  "sourceCode" : "@Test\r\npublic void testStoreDeletedWhenCleanDirsFlagSet() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    when(mockTaskModel.getTaskMode()).thenReturn(TaskMode.Active);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    // set the clean.on.container.start config set on the store\r\n    Config mockConfig = new MapConfig(Collections.singletonMap(\"stores.store1.clean.on.container.start\", \"true\"));\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    File dummyCurrentDir = new File(\"currentDir\");\r\n    File dummyCheckpointDir = new File(\"checkpointDir1\");\r\n    when(mockStorageManagerUtil.getTaskStoreDir(mockLoggedStoreBaseDir, store1Name, taskName, TaskMode.Active)).thenReturn(dummyCurrentDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(mockLoggedStoreBaseDir, store1Name, taskName, TaskMode.Active)).thenReturn(ImmutableList.of(dummyCheckpointDir));\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that current and checkpoint directories are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.size());\r\n    assertTrue(storeActions.storeDirsToDelete.containsValue(dummyCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.containsValue(dummyCurrentDir));\r\n    // ensure that we restore from the oldest changelog offset to checkpointed changelog offset\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_RestoreToCheckpointedOffset",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_RestoreToCheckpointedOffset() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent (in memory) store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to delete or retain\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    // ensure that we restore from the oldest changelog offset to checkpointed changelog offset\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetNewerThanNewest",
  "sourceCode" : "/**\r\n * This can happen if the changelog topic was manually deleted and recreated, and the checkpointed/local changelog\r\n * offset is not valid anymore.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetNewerThanNewest() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // checkpointed changelog offset > newest offset (e.g. changelog topic got changed)\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"21\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to retain or delete\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from current oldest to current newest)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetOlderThanOldest",
  "sourceCode" : "/**\r\n * This can happen if the changelog topic gets compacted and the local store offset was written prior to the\r\n * compaction. If so, we do a full restore.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetOlderThanOldest() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // checkpointed changelog offset > newest offset (e.g. changelog topic got changed)\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"10\", \"20\", \"21\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to retain or delete\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from current oldest to current newest)\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"20\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetInRangeButMaybeCompacted",
  "sourceCode" : "/**\r\n * This can happen if the changelog offset is valid but the checkpoint is older than min compaction lag ms. E.g., when\r\n * the job/container shut down and restarted after a long time.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfCheckpointedOffsetInRangeButMaybeCompacted() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // checkpointed changelog offset > newest offset (e.g. changelog topic got changed)\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"10\", \"20\", \"21\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    // checkpoint id older than default min.compaction.lag.ms\r\n    CheckpointId checkpointId = CheckpointId.deserialize(\"0-0\");\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to retain or delete\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from current oldest to current newest)\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"20\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_FullTrimIfNullCheckpointedOffsetAndNotRetainExistingChangelogState",
  "sourceCode" : "/**\r\n * We need to trim the changelog topic to handle the scenario where container wrote some messages to store and\r\n * changelog, but died before the first commit (leaving checkpointed changelog offset as null).\r\n *\r\n * Retain existing state flag exists to support cases when user is turning on transactional support for the first\r\n * time and does not have an existing checkpointed changelog offset. Retain existing state flag allows them to\r\n * carry over the existing changelog state after a full bootstrap. Flag should be turned off after the first deploy.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_FullTrimIfNullCheckpointedOffsetAndNotRetainExistingChangelogState() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = null;\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"false\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to delete or retain\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for restore (full trim == restore from oldest to null)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertNull(storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfNullCheckpointedOffsetAndRetainExistingChangelogState",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForLoggedNonPersistentStore_FullRestoreIfNullCheckpointedOffsetAndRetainExistingChangelogState() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    // non-persistent store\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(false);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = null;\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"true\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that there is nothing to delete or retain\r\n    assertEquals(0, storeActions.storeDirsToDelete.size());\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for restore (full trim == restore from oldest to null)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_RestoreToCheckpointedOffsetIfNoStoreCheckpoints",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_RestoreToCheckpointedOffsetIfNoStoreCheckpoints() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    assertEquals(1, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    // ensure that we restore from the oldest changelog offset to checkpointed changelog offset\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_RestoreToCheckpointedOffsetIfInvalidStoreCheckpoints",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_RestoreToCheckpointedOffsetIfInvalidStoreCheckpoints() {\r\n    // in these tests, stale == local offset within range of current oldest and newest, but not equal to checkpointed\r\n    // invalid == store offset is corrupted / store is stale (older than delete retention) etc.\r\n    // hence in these tests a store checkpoint can be not-stale yet invalid\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    // not stale\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(// invalid store\r\n    false);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(// invalid store\r\n    false);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all current dir and checkpoint dirs are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no store checkpoint is marked for retention\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for restore from oldest offset to checkpointed offset\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_RestoreDeltaIfStaleStoreCheckpoint",
  "sourceCode" : "@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_RestoreDeltaIfStaleStoreCheckpoint() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    String newerCheckpointDirLocalOffset = \"4\";\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that both the current dir and older stale checkpoint dir are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    // ensure that the newer (but still stale) store checkpoint is marked for retention\r\n    assertEquals(mockStoreNewerCheckpointDir, storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we mark the store for restore from newer (but still stale) local offset to checkpointed offset\r\n    assertEquals(newerCheckpointDirLocalOffset, storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_NoRestoreButTrimIfUpToDateStoreCheckpoint",
  "sourceCode" : "/**\r\n * Ensure that we do a trim even if the local offset == checkpointed changelog offset, since there may be\r\n * additional messages in the changelog since the last commit that we need to revert.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_NoRestoreButTrimIfUpToDateStoreCheckpoint() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreUpToDateCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreUpToDateCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreUpToDateCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreUpToDateCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, changelog1CheckpointedOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, \"3\"));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that both the current dir and older checkpoint dir are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    // ensure that the up-to-date store checkpoint is marked for retention\r\n    assertEquals(mockStoreUpToDateCheckpointDir, storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we mark the store for restore even if local offset == checkpointed offset\r\n    // this is required even if there are no messages to restore, since there may be message we need to trim\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_DeleteStoreCheckpointIfLocalOffsetHigherThanCheckpointed",
  "sourceCode" : "/**\r\n * This can happen if container failed after checkpointing store but before writing newest changelog offset to\r\n * checkpoint topic. In this case, the previously checkpointed (older) store directory should be used.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_DeleteStoreCheckpointIfLocalOffsetHigherThanCheckpointed() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// greater than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, \"10\"));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, changelog1CheckpointedOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that both the current dir and newer checkpoint dir are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that the older store checkpoint is marked for retention\r\n    assertEquals(mockStoreOlderCheckpointDir, storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we mark the store for restore even if local offset == checkpointed offset\r\n    // this is required since there may be message we need to trim\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_RetainOneCheckpointIfMultipleCheckpointsWithSameOffset",
  "sourceCode" : "/**\r\n * This can happen if no new messages were written to the store between commits. There may be more than one store\r\n * checkpoint if container fails during commit after creating a checkpoint but before deleting the old one.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_RetainOneCheckpointIfMultipleCheckpointsWithSameOffset() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    ImmutableMap<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = ImmutableMap.of(store1Name, kafkaStateCheckpointMarker);\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// equal to checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, changelog1CheckpointedOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// also equal to checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, changelog1CheckpointedOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that both the current dir and one of the checkpoint dirs are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    // ensure that the one of the store checkpoint is marked for retention\r\n    assertNotNull(storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we mark the store for restore even if local offset == checkpointed offset\r\n    // this is required since there may be message we need to trim\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(changelog1CheckpointedOffset, storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullRestoreIfNullCheckpointedOffsetAndRetainExistingChangelogState",
  "sourceCode" : "/**\r\n * We need to trim the changelog topic to handle the scenario where container wrote some messages to store and\r\n * changelog, but died before the first commit (leaving checkpointed changelog offset as null).\r\n *\r\n * Retain existing state flag exists to support cases when user is turning on transactional support for the first\r\n * time and does not have an existing checkpointed changelog offset. Retain existing state flag allows them to\r\n * carry over the existing changelog state after a full bootstrap. Flag should be turned off after the first deploy.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullRestoreIfNullCheckpointedOffsetAndRetainExistingChangelogState() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = null;\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"true\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all the store dirs (current or checkpoint) are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no directories are retained\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from oldest to newest)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullTrimIfNullCheckpointedOffsetAndNotRetainExistingState",
  "sourceCode" : "/**\r\n * We need to trim the changelog topic to handle the scenario where container wrote some messages to store and\r\n * changelog, but died before the first commit (leaving checkpointed changelog offset as null).\r\n *\r\n * Retain existing state flag exists to support cases when user is turning on transactional support for the first\r\n * time and does not have an existing checkpointed changelog offset. Retain existing state flag allows them to\r\n * carry over the existing changelog state after a full bootstrap. Flag should be turned off after the first deploy.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullTrimIfNullCheckpointedOffsetAndNotRetainExistingState() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = null;\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"false\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all the store dirs (current or checkpoint) are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no directories are retained\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for restore (full trim == restore from oldest to null)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertNull(storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullRestoreIfEqualCheckpointedOldestAndNewestOffset",
  "sourceCode" : "/**\r\n * This is the case when the changelog topic is empty but not new. E.g., if wrote 100 messages,\r\n * then deleted 100 messages, and after compaction oldest == newest == checkpointed. In this case\r\n * full restore does not do anything since there is nothing to restore or trim, but the code path will\r\n * leave us in a consistent state with the appropriate stores deleted and retained.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullRestoreIfEqualCheckpointedOldestAndNewestOffset() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"5\", \"5\", \"6\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    // should not matter\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"true\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreCheckpointDir = mock(File.class);\r\n    String checkpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, checkpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        if (offset1 == null || offset2 == null) {\r\n            return -1;\r\n        }\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that current and old checkpoint dirs are marked for deletion\r\n    assertEquals(1, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    // ensure that latest checkpoint dir is retained\r\n    assertEquals(mockStoreCheckpointDir, storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we do a full restore (on the empty topic)\r\n    assertEquals(\"5\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"5\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullRestoreIfNullCheckpointedAndOldestOffset",
  "sourceCode" : "/**\r\n * This can be the case if the changelog topic is empty (although KafkaSystemAdmin returns 0 as the oldest offset\r\n * instead of null for empty topics).\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullRestoreIfNullCheckpointedAndOldestOffset() {\r\n    // full restore == clear existing state\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // SSPMetadata contract allows for (and recommends) null as the oldest offset for empty streams.\r\n    // KafkaSystemAdmin does not follow this convention and returns 0 instead, but we should test for this\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(null, null, null);\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = null;\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    HashMap<String, String> configMap = new HashMap<>();\r\n    // should not matter\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"true\");\r\n    Config mockConfig = new MapConfig(configMap);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        if (offset1 == null || offset2 == null) {\r\n            return -1;\r\n        }\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all the store dirs (current or checkpoint) are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no directories are retained\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we do a full restore (on the empty topic)\r\n    assertNull(storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertNull(storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullRestoreIfCheckpointedOffsetOlderThanOldest",
  "sourceCode" : "/**\r\n * This can happen if the changelog topic gets compacted and the local store offset was written prior to the\r\n * compaction. If so, we do a full restore.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullRestoreIfCheckpointedOffsetOlderThanOldest() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // oldest offset > checkpointed changelog offset\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"11\", \"20\", \"21\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all the store dirs (current or checkpoint) are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no directories are retained\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from current oldest to current newest)\r\n    assertEquals(\"11\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"20\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_RestoreFromLocalToNewestIfCheckpointedOffsetInRangeButMaybeCompacted",
  "sourceCode" : "/**\r\n * This can happen if the changelog offset is valid but the checkpoint is older than min compaction lag ms. E.g., when\r\n * the job/container shut down and restarted after a long time.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_RestoreFromLocalToNewestIfCheckpointedOffsetInRangeButMaybeCompacted() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // checkpointed changelog offset is valid\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"4\", \"20\", \"21\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"5\";\r\n    // checkpoint timestamp older than default min compaction lag\r\n    CheckpointId checkpointId = CheckpointId.deserialize(\"0-0\");\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"3\";\r\n    String newerCheckpointDirLocalOffset = \"5\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that the current store dir and older checkpoint dir are marked for deletion\r\n    assertEquals(2, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    // ensure that newer checkpoint dir is retained\r\n    assertEquals(1, storeActions.storeDirsToRetain.size());\r\n    assertEquals(mockStoreNewerCheckpointDir, storeActions.storeDirsToRetain.get(store1Name));\r\n    // ensure that we mark the store for restore to head (from local checkpoint to current newest)\r\n    assertEquals(\"5\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"20\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testGetStoreActionsForLoggedPersistentStore_FullRestoreIfCheckpointedOffsetNewerThanNewest",
  "sourceCode" : "/**\r\n * This can happen if the changelog topic was manually deleted and recreated, and the checkpointed/local changelog\r\n * offset is not valid anymore.\r\n */\r\n@Test\r\npublic void testGetStoreActionsForLoggedPersistentStore_FullRestoreIfCheckpointedOffsetNewerThanNewest() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine);\r\n    String changelog1SystemName = \"system1\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelog1SystemName, changelog1StreamName);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    // checkpointed changelog offset > newest offset (e.g. changelog topic got changed)\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    String changelog1CheckpointedOffset = \"21\";\r\n    CheckpointId checkpointId = CheckpointId.create();\r\n    KafkaStateCheckpointMarker kafkaStateCheckpointMarker = new KafkaStateCheckpointMarker(changelog1SSP, changelog1CheckpointedOffset);\r\n    Map<String, KafkaStateCheckpointMarker> mockCheckpointedChangelogOffset = new HashMap<String, KafkaStateCheckpointMarker>() {\r\n\r\n        {\r\n            put(store1Name, kafkaStateCheckpointMarker);\r\n        }\r\n    };\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogOffsets = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(changelog1SSP.getSystem())).thenReturn(mockSystemAdmin);\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    Config mockConfig = mock(Config.class);\r\n    Clock mockClock = mock(Clock.class);\r\n    File mockCurrentStoreDir = mock(File.class);\r\n    File mockStoreNewerCheckpointDir = mock(File.class);\r\n    File mockStoreOlderCheckpointDir = mock(File.class);\r\n    String olderCheckpointDirLocalOffset = \"5\";\r\n    String newerCheckpointDirLocalOffset = \"15\";\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(mockCurrentStoreDir);\r\n    when(mockStorageManagerUtil.getTaskStoreCheckpointDirs(eq(mockLoggedStoreBaseDir), eq(store1Name), eq(taskName), any())).thenReturn(ImmutableList.of(mockStoreNewerCheckpointDir, mockStoreOlderCheckpointDir));\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreNewerCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    when(mockStorageManagerUtil.isLoggedStoreValid(eq(store1Name), eq(mockStoreOlderCheckpointDir), any(), eq(mockStoreChangelogs), eq(mockTaskModel), any(), eq(mockStoreEngines))).thenReturn(true);\r\n    Set<SystemStreamPartition> mockChangelogSSPs = ImmutableSet.of(changelog1SSP);\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreNewerCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(ImmutableMap.of(changelog1SSP, newerCheckpointDirLocalOffset));\r\n    when(mockStorageManagerUtil.readOffsetFile(eq(mockStoreOlderCheckpointDir), eq(mockChangelogSSPs), eq(false))).thenReturn(// less than checkpointed offset (5)\r\n    ImmutableMap.of(changelog1SSP, olderCheckpointDirLocalOffset));\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    StoreActions storeActions = TransactionalStateTaskRestoreManager.getStoreActions(mockTaskModel, mockStoreEngines, mockStoreChangelogs, mockCheckpointedChangelogOffset, checkpointId, mockCurrentChangelogOffsets, mockSystemAdmins, mockStorageManagerUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir, mockConfig, mockClock);\r\n    // ensure that all the store dirs (current or checkpoint) are marked for deletion\r\n    assertEquals(3, storeActions.storeDirsToDelete.get(store1Name).size());\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockCurrentStoreDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreOlderCheckpointDir));\r\n    assertTrue(storeActions.storeDirsToDelete.get(store1Name).contains(mockStoreNewerCheckpointDir));\r\n    // ensure that no directories are retained\r\n    assertEquals(0, storeActions.storeDirsToRetain.size());\r\n    // ensure that we mark the store for full restore (from current oldest to current newest)\r\n    assertEquals(\"0\", storeActions.storesToRestore.get(store1Name).startingOffset);\r\n    assertEquals(\"10\", storeActions.storesToRestore.get(store1Name).endingOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testSetupStoreDirs",
  "sourceCode" : "@Test\r\npublic void testSetupStoreDirs() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    StorageEngine store1Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore1Properties = mock(StoreProperties.class);\r\n    when(store1Engine.getStoreProperties()).thenReturn(mockStore1Properties);\r\n    when(mockStore1Properties.isLoggedStore()).thenReturn(true);\r\n    when(mockStore1Properties.isPersistedToDisk()).thenReturn(true);\r\n    String store2Name = \"store2\";\r\n    StorageEngine store2Engine = mock(StorageEngine.class);\r\n    StoreProperties mockStore2Properties = mock(StoreProperties.class);\r\n    when(store2Engine.getStoreProperties()).thenReturn(mockStore2Properties);\r\n    // non-logged store\r\n    when(mockStore2Properties.isLoggedStore()).thenReturn(false);\r\n    when(mockStore2Properties.isPersistedToDisk()).thenReturn(true);\r\n    Map<String, StorageEngine> mockStoreEngines = ImmutableMap.of(store1Name, store1Engine, store2Name, store2Engine);\r\n    File mockStore1DirToRetain = mock(File.class);\r\n    // there will be no dir to retain for non-logged persistent stores\r\n    ImmutableMap<String, File> storeDirsToRetain = ImmutableMap.of(store1Name, mockStore1DirToRetain);\r\n    ListMultimap<String, File> storeDirsToDelete = ArrayListMultimap.create();\r\n    File mockStore1CurrentDir = mock(File.class);\r\n    Path mockStore1CurrentDirPath = mock(Path.class);\r\n    when(mockStore1CurrentDir.toPath()).thenReturn(mockStore1CurrentDirPath);\r\n    File mockStore2CurrentDir = mock(File.class);\r\n    Path mockStore2CurrentDirPath = mock(Path.class);\r\n    when(mockStore2CurrentDir.toPath()).thenReturn(mockStore2CurrentDirPath);\r\n    File mockStore1DirToDelete1 = mock(File.class);\r\n    File mockStore1DirToDelete2 = mock(File.class);\r\n    File mockStore2DirToDelete1 = mock(File.class);\r\n    File mockStore2DirToDelete2 = mock(File.class);\r\n    storeDirsToDelete.put(store1Name, mockStore1CurrentDir);\r\n    storeDirsToDelete.put(store1Name, mockStore1DirToDelete1);\r\n    storeDirsToDelete.put(store1Name, mockStore1DirToDelete2);\r\n    storeDirsToDelete.put(store2Name, mockStore2CurrentDir);\r\n    storeDirsToDelete.put(store2Name, mockStore2DirToDelete1);\r\n    storeDirsToDelete.put(store2Name, mockStore2DirToDelete2);\r\n    StoreActions storeActions = new StoreActions(storeDirsToRetain, storeDirsToDelete, ImmutableMap.of());\r\n    StorageManagerUtil mockStorageManagerUtil = mock(StorageManagerUtil.class);\r\n    FileUtil mockFileUtil = mock(FileUtil.class);\r\n    File mockLoggedStoreBaseDir = mock(File.class);\r\n    File mockNonLoggedStoreBaseDir = mock(File.class);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockLoggedStoreBaseDir), eq(store1Name), any(), any())).thenReturn(mockStore1CurrentDir);\r\n    when(mockStorageManagerUtil.getTaskStoreDir(eq(mockNonLoggedStoreBaseDir), eq(store2Name), any(), any())).thenReturn(mockStore2CurrentDir);\r\n    when(mockFileUtil.exists(eq(mockStore1CurrentDirPath))).thenReturn(false);\r\n    when(mockFileUtil.exists(eq(mockStore2CurrentDirPath))).thenReturn(// will not be true in reality since current dir is always cleared, but return true for testing\r\n    true);\r\n    TransactionalStateTaskRestoreManager.setupStoreDirs(mockTaskModel, mockStoreEngines, storeActions, mockStorageManagerUtil, mockFileUtil, mockLoggedStoreBaseDir, mockNonLoggedStoreBaseDir);\r\n    // verify that store directories to delete are deleted\r\n    verify(mockFileUtil, times(1)).rm(mockStore1CurrentDir);\r\n    verify(mockFileUtil, times(1)).rm(mockStore1DirToDelete1);\r\n    verify(mockFileUtil, times(1)).rm(mockStore1DirToDelete2);\r\n    verify(mockFileUtil, times(1)).rm(mockStore2CurrentDir);\r\n    verify(mockFileUtil, times(1)).rm(mockStore2DirToDelete1);\r\n    verify(mockFileUtil, times(1)).rm(mockStore2DirToDelete2);\r\n    // verify that store checkpoint directories to retain are moved to (empty) current dirs only for store 1\r\n    // setupStoreDirs doesn't guarantee that the dir is empty by itself, but the dir will be part of dirs to delete.\r\n    verify(mockStorageManagerUtil, times(1)).restoreCheckpointFiles(any(), any());\r\n    verify(mockFileUtil, times(1)).exists(mockStore1CurrentDirPath);\r\n    verify(mockFileUtil, times(1)).createDirectories(mockStore1CurrentDirPath);\r\n    verify(mockFileUtil, times(1)).exists(mockStore2CurrentDirPath);\r\n    // should not be called since exists == true\r\n    verify(mockFileUtil, never()).createDirectories(mockStore2CurrentDirPath);\r\n    verifyNoMoreInteractions(mockFileUtil);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testRegisterStartingOffsets",
  "sourceCode" : "@Test\r\npublic void testRegisterStartingOffsets() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    String store1Name = \"store1\";\r\n    String changelogSystemName = \"system\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    String store2Name = \"store2\";\r\n    String changelog2StreamName = \"store2Changelog\";\r\n    String store3Name = \"store3\";\r\n    String changelog3StreamName = \"store3Changelog\";\r\n    String store4Name = \"store4\";\r\n    String changelog4StreamName = \"store4Changelog\";\r\n    // tests restore for store 1 and store 2 but not store 3\r\n    Map<String, RestoreOffsets> mockRestoreOffsets = ImmutableMap.of(// tests starting offset == oldest (i.e. restore is inclusive)\r\n    store1Name, // tests starting offset == oldest (i.e. restore is inclusive)\r\n    new RestoreOffsets(\"0\", \"5\"), // tests starting offset != oldest (i.e. restore from next offset)\r\n    store2Name, // tests starting offset != oldest (i.e. restore from next offset)\r\n    new RestoreOffsets(\"15\", \"20\"), store4Name, // tests that null ending offsets are OK (should trim)\r\n    new RestoreOffsets(\"31\", null));\r\n    StoreActions mockStoreActions = new StoreActions(ImmutableMap.of(), ArrayListMultimap.create(), mockRestoreOffsets);\r\n    SystemStream changelog1SystemStream = new SystemStream(changelogSystemName, changelog1StreamName);\r\n    SystemStream changelog2SystemStream = new SystemStream(changelogSystemName, changelog2StreamName);\r\n    SystemStream changelog3SystemStream = new SystemStream(changelogSystemName, changelog3StreamName);\r\n    SystemStream changelog4SystemStream = new SystemStream(changelogSystemName, changelog4StreamName);\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream, store2Name, changelog2SystemStream, store3Name, changelog3SystemStream, store4Name, changelog4SystemStream);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    SystemStreamPartition changelog2SSP = new SystemStreamPartition(changelog2SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog2SSPMetadata = new SystemStreamPartitionMetadata(\"11\", \"20\", \"21\");\r\n    SystemStreamPartition changelog3SSP = new SystemStreamPartition(changelog3SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog3SSPMetadata = new SystemStreamPartitionMetadata(\"21\", \"30\", \"31\");\r\n    SystemStreamPartition changelog4SSP = new SystemStreamPartition(changelog4SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog4SSPMetadata = new SystemStreamPartitionMetadata(\"31\", \"40\", \"41\");\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogSSPMetadata = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata, changelog2SSP, changelog2SSPMetadata, changelog3SSP, changelog3SSPMetadata, changelog4SSP, changelog4SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(eq(changelogSystemName))).thenReturn(mockSystemAdmin);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    Mockito.when(mockSystemAdmin.getOffsetsAfter(any())).thenAnswer((Answer<Map<SystemStreamPartition, String>>) invocation -> {\r\n        Map<SystemStreamPartition, String> offsets = (Map<SystemStreamPartition, String>) invocation.getArguments()[0];\r\n        Map<SystemStreamPartition, String> nextOffsets = new HashMap<>();\r\n        offsets.forEach((ssp, offset) -> nextOffsets.put(ssp, Long.toString(Long.valueOf(offset) + 1)));\r\n        return nextOffsets;\r\n    });\r\n    SystemConsumer mockSystemConsumer = mock(SystemConsumer.class);\r\n    Map<String, SystemConsumer> mockStoreConsumers = ImmutableMap.of(store1Name, mockSystemConsumer, store2Name, mockSystemConsumer, store3Name, mockSystemConsumer, store4Name, mockSystemConsumer);\r\n    TransactionalStateTaskRestoreManager.registerStartingOffsets(mockTaskModel, mockStoreActions, mockStoreChangelogs, mockSystemAdmins, mockStoreConsumers, mockCurrentChangelogSSPMetadata);\r\n    // verify that we first register upcoming offsets for each changelog ssp\r\n    verify(mockSystemConsumer, times(1)).register(changelog1SSP, \"11\");\r\n    verify(mockSystemConsumer, times(1)).register(changelog2SSP, \"21\");\r\n    verify(mockSystemConsumer, times(1)).register(changelog3SSP, \"31\");\r\n    verify(mockSystemConsumer, times(1)).register(changelog4SSP, \"41\");\r\n    // then verify that we override the starting offsets for changelog 1 and 2\r\n    // ensure that starting offset is inclusive if oldest\r\n    verify(mockSystemConsumer, times(1)).register(changelog1SSP, \"0\");\r\n    // and that it is next offset if not oldest\r\n    verify(mockSystemConsumer, times(1)).register(changelog2SSP, \"16\");\r\n    // and that null ending offset is ok\r\n    verify(mockSystemConsumer, times(1)).register(changelog4SSP, \"31\");\r\n    verifyNoMoreInteractions(mockSystemConsumer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskRestoreManager.java",
  "methodName" : "testRegisterStartingOffsetsThrowsIfStartingGreaterThanEnding",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testRegisterStartingOffsetsThrowsIfStartingGreaterThanEnding() {\r\n    TaskModel mockTaskModel = mock(TaskModel.class);\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    when(mockTaskModel.getTaskName()).thenReturn(taskName);\r\n    Partition taskChangelogPartition = new Partition(0);\r\n    when(mockTaskModel.getChangelogPartition()).thenReturn(taskChangelogPartition);\r\n    // tests starting offset > ending offset\r\n    Map<String, RestoreOffsets> mockRestoreOffsets = ImmutableMap.of(\"store1\", new RestoreOffsets(\"5\", \"0\"));\r\n    StoreActions mockStoreActions = new StoreActions(ImmutableMap.of(), ArrayListMultimap.create(), mockRestoreOffsets);\r\n    String store1Name = \"store1\";\r\n    String changelogSystemName = \"system\";\r\n    String changelog1StreamName = \"store1Changelog\";\r\n    SystemStream changelog1SystemStream = new SystemStream(changelogSystemName, changelog1StreamName);\r\n    Map<String, SystemStream> mockStoreChangelogs = ImmutableMap.of(store1Name, changelog1SystemStream);\r\n    SystemStreamPartition changelog1SSP = new SystemStreamPartition(changelog1SystemStream, taskChangelogPartition);\r\n    SystemStreamPartitionMetadata changelog1SSPMetadata = new SystemStreamPartitionMetadata(\"0\", \"10\", \"11\");\r\n    Map<SystemStreamPartition, SystemStreamPartitionMetadata> mockCurrentChangelogSSPMetadata = ImmutableMap.of(changelog1SSP, changelog1SSPMetadata);\r\n    SystemAdmins mockSystemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin mockSystemAdmin = mock(SystemAdmin.class);\r\n    when(mockSystemAdmins.getSystemAdmin(eq(changelogSystemName))).thenReturn(mockSystemAdmin);\r\n    Mockito.when(mockSystemAdmin.offsetComparator(anyString(), anyString())).thenAnswer((Answer<Integer>) invocation -> {\r\n        String offset1 = (String) invocation.getArguments()[0];\r\n        String offset2 = (String) invocation.getArguments()[1];\r\n        return Long.valueOf(offset1).compareTo(Long.valueOf(offset2));\r\n    });\r\n    Mockito.when(mockSystemAdmin.getOffsetsAfter(any())).thenAnswer((Answer<Map<SystemStreamPartition, String>>) invocation -> {\r\n        Map<SystemStreamPartition, String> offsets = (Map<SystemStreamPartition, String>) invocation.getArguments()[0];\r\n        Map<SystemStreamPartition, String> nextOffsets = new HashMap<>();\r\n        offsets.forEach((ssp, offset) -> nextOffsets.put(ssp, Long.toString(Long.valueOf(offset) + 1)));\r\n        return nextOffsets;\r\n    });\r\n    SystemConsumer mockSystemConsumer = mock(SystemConsumer.class);\r\n    Map<String, SystemConsumer> mockStoreConsumers = ImmutableMap.of(\"store1\", mockSystemConsumer);\r\n    TransactionalStateTaskRestoreManager.registerStartingOffsets(mockTaskModel, mockStoreActions, mockStoreChangelogs, mockSystemAdmins, mockStoreConsumers, mockCurrentChangelogSSPMetadata);\r\n    fail(\"Should have thrown an exception since starting offset > ending offset\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemoryManager.java",
  "methodName" : "testGetSystemStreamMetadata",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetadata() {\r\n    this.inMemoryManager.initializeStream(new StreamSpec(STREAM0, STREAM0, SYSTEM, 1));\r\n    this.inMemoryManager.initializeStream(new StreamSpec(STREAM1, STREAM1, SYSTEM, 1));\r\n    // add some other stream which we won't request metadata for\r\n    this.inMemoryManager.initializeStream(new StreamSpec(\"otherStream\", \"otherStream\", SYSTEM, 1));\r\n    // empty stream\r\n    SystemStreamMetadata systemStreamMetadata0 = new SystemStreamMetadata(STREAM0, ImmutableMap.of(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(null, null, \"0\")));\r\n    assertEquals(ImmutableMap.of(STREAM0, systemStreamMetadata0), this.inMemoryManager.getSystemStreamMetadata(SYSTEM, ImmutableSet.of(STREAM0)));\r\n    // add a message in\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(SYSTEM, STREAM0, new Partition(0));\r\n    this.inMemoryManager.put(ssp0, \"key00\", \"message00\");\r\n    systemStreamMetadata0 = new SystemStreamMetadata(STREAM0, ImmutableMap.of(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", \"0\", \"1\")));\r\n    assertEquals(ImmutableMap.of(STREAM0, systemStreamMetadata0), this.inMemoryManager.getSystemStreamMetadata(SYSTEM, ImmutableSet.of(STREAM0)));\r\n    // add a second message to the first stream and add one message to the second stream\r\n    this.inMemoryManager.put(ssp0, \"key01\", \"message01\");\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(SYSTEM, STREAM1, new Partition(0));\r\n    this.inMemoryManager.put(ssp1, \"key10\", \"message10\");\r\n    systemStreamMetadata0 = new SystemStreamMetadata(STREAM0, ImmutableMap.of(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", \"1\", \"2\")));\r\n    SystemStreamMetadata systemStreamMetadata1 = new SystemStreamMetadata(STREAM1, ImmutableMap.of(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", \"0\", \"1\")));\r\n    // also test a batch call for multiple streams here\r\n    assertEquals(ImmutableMap.of(STREAM0, systemStreamMetadata0, STREAM1, systemStreamMetadata1), this.inMemoryManager.getSystemStreamMetadata(SYSTEM, ImmutableSet.of(STREAM0, STREAM1)));\r\n    // test END_OF_STREAM doesn't alter new or upcoming offset\r\n    this.inMemoryManager.put(ssp0, \"key02\", new EndOfStreamMessage());\r\n    systemStreamMetadata0 = new SystemStreamMetadata(STREAM0, ImmutableMap.of(new Partition(0), new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", \"1\", \"2\")));\r\n    assertEquals(ImmutableMap.of(STREAM0, systemStreamMetadata0), this.inMemoryManager.getSystemStreamMetadata(SYSTEM, ImmutableSet.of(STREAM0)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemoryManager.java",
  "methodName" : "testPoll",
  "sourceCode" : "@Test\r\npublic void testPoll() {\r\n    this.inMemoryManager.initializeStream(new StreamSpec(STREAM0, STREAM0, SYSTEM, 1));\r\n    this.inMemoryManager.initializeStream(new StreamSpec(STREAM1, STREAM1, SYSTEM, 1));\r\n    // add some other stream which we won't request metadata for\r\n    this.inMemoryManager.initializeStream(new StreamSpec(\"otherStream\", \"otherStream\", SYSTEM, 1));\r\n    // empty stream\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(SYSTEM, STREAM0, new Partition(0));\r\n    assertEquals(ImmutableMap.of(ssp0, ImmutableList.of()), this.inMemoryManager.poll(Collections.singletonMap(ssp0, \"0\")));\r\n    // add a message in\r\n    this.inMemoryManager.put(ssp0, \"key00\", \"message00\");\r\n    Map<SystemStreamPartition, List<IncomingMessageEnvelope>> polledMessages = this.inMemoryManager.poll(Collections.singletonMap(ssp0, \"0\"));\r\n    assertEquals(1, polledMessages.get(ssp0).size());\r\n    assertIncomingMessageEnvelope(\"key00\", \"message00\", \"0\", ssp0, polledMessages.get(ssp0).get(0));\r\n    // add a second message to the first stream\r\n    this.inMemoryManager.put(ssp0, \"key01\", \"message01\");\r\n    // verify multiple messages returned\r\n    polledMessages = this.inMemoryManager.poll(ImmutableMap.of(ssp0, \"0\"));\r\n    assertEquals(2, polledMessages.get(ssp0).size());\r\n    assertIncomingMessageEnvelope(\"key00\", \"message00\", \"0\", ssp0, polledMessages.get(ssp0).get(0));\r\n    assertIncomingMessageEnvelope(\"key01\", \"message01\", \"1\", ssp0, polledMessages.get(ssp0).get(1));\r\n    // make sure only read messages starting from the offset that is not the oldest offset\r\n    polledMessages = this.inMemoryManager.poll(ImmutableMap.of(ssp0, \"1\"));\r\n    assertEquals(1, polledMessages.get(ssp0).size());\r\n    assertIncomingMessageEnvelope(\"key01\", \"message01\", \"1\", ssp0, polledMessages.get(ssp0).get(0));\r\n    // add a message to the second stream to test a batch call\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(SYSTEM, STREAM1, new Partition(0));\r\n    this.inMemoryManager.put(ssp1, \"key10\", \"message10\");\r\n    polledMessages = this.inMemoryManager.poll(ImmutableMap.of(ssp0, \"1\", ssp1, \"0\"));\r\n    assertEquals(1, polledMessages.get(ssp0).size());\r\n    assertIncomingMessageEnvelope(\"key01\", \"message01\", \"1\", ssp0, polledMessages.get(ssp0).get(0));\r\n    assertEquals(1, polledMessages.get(ssp1).size());\r\n    assertIncomingMessageEnvelope(\"key10\", \"message10\", \"0\", ssp1, polledMessages.get(ssp1).get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testMessageFlow",
  "sourceCode" : "@Test\r\npublic void testMessageFlow() {\r\n    PageViewEvent event1 = new PageViewEvent(TEST_MEMBER_X, PAGE_ID_X, System.currentTimeMillis());\r\n    PageViewEvent event2 = new PageViewEvent(TEST_MEMBER_Y, PAGE_ID_Y, System.currentTimeMillis());\r\n    produceMessages(event1, event2);\r\n    Set<SystemStreamPartition> sspsToPoll = IntStream.range(0, PARTITION_COUNT).mapToObj(partition -> new SystemStreamPartition(SYSTEM_STREAM, new Partition(partition))).collect(Collectors.toSet());\r\n    List<PageViewEvent> results = consumeMessages(sspsToPoll);\r\n    assertEquals(2, results.size());\r\n    assertTrue(results.contains(event1));\r\n    assertTrue(results.contains(event2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testConsumerRespectsOffset",
  "sourceCode" : "@Test\r\npublic void testConsumerRespectsOffset() {\r\n    PageViewEvent event = new PageViewEvent(TEST_MEMBER_X, PAGE_ID_X, System.currentTimeMillis());\r\n    PageViewEvent event1 = new PageViewEvent(TEST_MEMBER_Y, PAGE_ID_Y, System.currentTimeMillis());\r\n    produceMessages(event);\r\n    SystemConsumer consumer = systemFactory.getConsumer(SYSTEM_NAME, config, mockRegistry);\r\n    Set<SystemStreamPartition> sspsToPoll = IntStream.range(0, PARTITION_COUNT).mapToObj(partition -> new SystemStreamPartition(SYSTEM_STREAM, new Partition(partition))).collect(Collectors.toSet());\r\n    // register the consumer for ssps\r\n    for (SystemStreamPartition ssp : sspsToPoll) {\r\n        consumer.register(ssp, \"0\");\r\n    }\r\n    List<PageViewEvent> results = consumeMessages(consumer, sspsToPoll);\r\n    assertEquals(1, results.size());\r\n    assertTrue(results.contains(event));\r\n    // nothing to poll\r\n    results = consumeMessages(consumer, sspsToPoll);\r\n    assertEquals(0, results.size());\r\n    produceMessages(event1);\r\n    // got new message. check if the offset has progressed\r\n    results = consumeMessages(consumer, sspsToPoll);\r\n    assertEquals(1, results.size());\r\n    assertTrue(results.contains(event1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testEndOfStreamMessageWithTask",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamMessageWithTask() {\r\n    EndOfStreamMessage eos = new EndOfStreamMessage(\"test-task\");\r\n    produceMessages(eos);\r\n    Set<SystemStreamPartition> sspsToPoll = IntStream.range(0, PARTITION_COUNT).mapToObj(partition -> new SystemStreamPartition(SYSTEM_STREAM, new Partition(partition))).collect(Collectors.toSet());\r\n    List<IncomingMessageEnvelope> results = consumeRawMessages(sspsToPoll);\r\n    assertEquals(1, results.size());\r\n    assertEquals(\"test-task\", ((EndOfStreamMessage) results.get(0).getMessage()).getTaskName());\r\n    assertFalse(results.get(0).isEndOfStream());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testEndOfStreamMessageWithoutTask",
  "sourceCode" : "@Test\r\npublic void testEndOfStreamMessageWithoutTask() {\r\n    EndOfStreamMessage eos = new EndOfStreamMessage();\r\n    produceMessages(eos);\r\n    Set<SystemStreamPartition> sspsToPoll = IntStream.range(0, PARTITION_COUNT).mapToObj(partition -> new SystemStreamPartition(SYSTEM_STREAM, new Partition(partition))).collect(Collectors.toSet());\r\n    List<IncomingMessageEnvelope> results = consumeRawMessages(sspsToPoll);\r\n    assertEquals(1, results.size());\r\n    assertNull(((EndOfStreamMessage) results.get(0).getMessage()).getTaskName());\r\n    assertTrue(results.get(0).isEndOfStream());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testNullMessageWithValidMessageKey",
  "sourceCode" : "@Test\r\npublic void testNullMessageWithValidMessageKey() {\r\n    final String messageKey = \"validKey\";\r\n    SystemProducer systemProducer = systemFactory.getProducer(SYSTEM_NAME, config, mockRegistry);\r\n    systemProducer.send(SOURCE, new OutgoingMessageEnvelope(SYSTEM_STREAM, messageKey, null));\r\n    SystemConsumer consumer = systemFactory.getConsumer(SYSTEM_NAME, config, mockRegistry);\r\n    Set<SystemStreamPartition> sspsToPoll = IntStream.range(0, PARTITION_COUNT).mapToObj(partition -> new SystemStreamPartition(SYSTEM_STREAM, new Partition(partition))).collect(Collectors.toSet());\r\n    // register the consumer for ssps\r\n    for (SystemStreamPartition ssp : sspsToPoll) {\r\n        consumer.register(ssp, \"0\");\r\n    }\r\n    List<IncomingMessageEnvelope> results = consumeRawMessages(consumer, sspsToPoll);\r\n    assertEquals(1, results.size());\r\n    assertEquals(results.get(0).getKey(), messageKey);\r\n    assertNull(results.get(0).getMessage());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystem.java",
  "methodName" : "testNullMessageWithNullKey",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNullMessageWithNullKey() {\r\n    SystemProducer systemProducer = systemFactory.getProducer(SYSTEM_NAME, config, mockRegistry);\r\n    systemProducer.send(SOURCE, new OutgoingMessageEnvelope(SYSTEM_STREAM, null));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystemAdmin.java",
  "methodName" : "testOffsetComparator",
  "sourceCode" : "@Test\r\npublic void testOffsetComparator() {\r\n    assertEquals(0, inMemorySystemAdmin.offsetComparator(null, null).intValue());\r\n    assertEquals(-1, inMemorySystemAdmin.offsetComparator(null, \"0\").intValue());\r\n    assertEquals(1, inMemorySystemAdmin.offsetComparator(\"0\", null).intValue());\r\n    assertEquals(-1, inMemorySystemAdmin.offsetComparator(\"0\", \"1\").intValue());\r\n    assertEquals(0, inMemorySystemAdmin.offsetComparator(\"0\", \"0\").intValue());\r\n    assertEquals(1, inMemorySystemAdmin.offsetComparator(\"1\", \"0\").intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystemConsumer.java",
  "methodName" : "testPoll",
  "sourceCode" : "@Test\r\npublic void testPoll() throws InterruptedException {\r\n    this.inMemorySystemConsumer.register(SSP0, \"1\");\r\n    this.inMemorySystemConsumer.register(SSP1, \"1\");\r\n    IncomingMessageEnvelope ime01 = new IncomingMessageEnvelope(SSP0, \"1\", \"key01\", \"message01\");\r\n    IncomingMessageEnvelope ime02 = new IncomingMessageEnvelope(SSP0, \"2\", \"key02\", \"message02\");\r\n    Map<SystemStreamPartition, String> pollRequest = ImmutableMap.of(SSP0, \"1\");\r\n    when(this.inMemoryManager.poll(pollRequest)).// poll for SSP0 only, return no messages\r\n    thenReturn(ImmutableMap.of(SSP0, ImmutableList.of())).// poll for SSP0 only, return some messages; still same offset request since got no messages last time\r\n    thenReturn(ImmutableMap.of(SSP0, ImmutableList.of(ime01, ime02)));\r\n    // poll for SSP0 and SSP1; SSP0 should have a new offset now\r\n    pollRequest = ImmutableMap.of(SSP0, \"3\", SSP1, \"1\");\r\n    IncomingMessageEnvelope ime03 = new IncomingMessageEnvelope(SSP0, \"3\", \"key03\", \"message03\");\r\n    IncomingMessageEnvelope ime10 = new IncomingMessageEnvelope(SSP1, \"1\", \"key10\", \"message10\");\r\n    when(this.inMemoryManager.poll(pollRequest)).thenReturn(ImmutableMap.of(SSP0, ImmutableList.of(ime03), SSP1, ImmutableList.of(ime10)));\r\n    assertEquals(ImmutableMap.of(SSP0, ImmutableList.of()), this.inMemorySystemConsumer.poll(ImmutableSet.of(SSP0), 1000));\r\n    assertEquals(ImmutableMap.of(SSP0, ImmutableList.of(ime01, ime02)), this.inMemorySystemConsumer.poll(ImmutableSet.of(SSP0), 1000));\r\n    assertEquals(ImmutableMap.of(SSP0, ImmutableList.of(ime03), SSP1, ImmutableList.of(ime10)), this.inMemorySystemConsumer.poll(ImmutableSet.of(SSP0, SSP1), 1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystemConsumer.java",
  "methodName" : "testPollRegisterNullOffset",
  "sourceCode" : "@Test\r\npublic void testPollRegisterNullOffset() throws InterruptedException {\r\n    this.inMemorySystemConsumer.register(SSP0, null);\r\n    IncomingMessageEnvelope ime0 = new IncomingMessageEnvelope(SSP0, \"0\", \"key0\", \"message0\");\r\n    IncomingMessageEnvelope ime1 = new IncomingMessageEnvelope(SSP0, \"1\", \"key1\", \"message1\");\r\n    Map<SystemStreamPartition, String> pollRequest = ImmutableMap.of(SSP0, \"0\");\r\n    when(this.inMemoryManager.poll(pollRequest)).thenReturn(ImmutableMap.of(SSP0, ImmutableList.of(ime0)));\r\n    pollRequest = ImmutableMap.of(SSP0, \"1\");\r\n    when(this.inMemoryManager.poll(pollRequest)).thenReturn(ImmutableMap.of(SSP0, ImmutableList.of(ime1)));\r\n    assertEquals(ImmutableMap.of(SSP0, ImmutableList.of(ime0)), this.inMemorySystemConsumer.poll(ImmutableSet.of(SSP0), 1000));\r\n    assertEquals(ImmutableMap.of(SSP0, ImmutableList.of(ime1)), this.inMemorySystemConsumer.poll(ImmutableSet.of(SSP0), 1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\inmemory\\TestInMemorySystemProducer.java",
  "methodName" : "testPartition",
  "sourceCode" : "/**\r\n * Test keys of type byte[] goes to the same partition if they have the same contents.\r\n */\r\n@Test\r\npublic void testPartition() {\r\n    doReturn(1000).when(inMemoryManager).getPartitionCountForSystemStream(any());\r\n    doAnswer(new Answer<Void>() {\r\n\r\n        int partitionOfFirstMessage = -1;\r\n\r\n        int partitionOfSecondMessage = -2;\r\n\r\n        @Override\r\n        public Void answer(InvocationOnMock invocation) throws Throwable {\r\n            SystemStreamPartition ssp = invocation.getArgumentAt(0, SystemStreamPartition.class);\r\n            if (partitionOfFirstMessage == -1) {\r\n                partitionOfFirstMessage = ssp.getPartition().getPartitionId();\r\n            } else {\r\n                partitionOfSecondMessage = ssp.getPartition().getPartitionId();\r\n                Assert.assertEquals(partitionOfFirstMessage, partitionOfSecondMessage);\r\n                testFinished = true;\r\n            }\r\n            return null;\r\n        }\r\n    }).when(inMemoryManager).put(any(), any(), any());\r\n    byte[] key1 = new byte[] { 1, 2, 3 };\r\n    byte[] key2 = new byte[] { 1, 2, 3 };\r\n    SystemStream systemStream = new SystemStream(\"TestSystem\", \"TestStream\");\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope1 = new OutgoingMessageEnvelope(systemStream, key1, null);\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope2 = new OutgoingMessageEnvelope(systemStream, key2, null);\r\n    inMemorySystemProducer.send(\"TestSource\", outgoingMessageEnvelope1);\r\n    inMemorySystemProducer.send(\"TestSource\", outgoingMessageEnvelope2);\r\n    Assert.assertTrue(testFinished);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataWithPrefetch",
  "sourceCode" : "/**\r\n * Given that there are sspsToPrefetch, getMetadata should call the admin (when necessary) to get the metadata for the\r\n * requested and \"prefetch\" SSPs. It should also cache the data.\r\n */\r\n@Test\r\npublic void testGetMetadataWithPrefetch() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SystemStreamPartition otherSSP = buildSSP(1);\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(ssp, otherSSP));\r\n    // t = 10: first read, t = 11: first write\r\n    when(clock.currentTimeMillis()).thenReturn(10L, 11L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP))).thenReturn(ImmutableMap.of(ssp, sspMetadata(1), otherSSP, sspMetadata(2)));\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n    // stay within TTL: use cached data\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    assertEquals(sspMetadata(2), cache.getMetadata(otherSSP));\r\n    // still only one call to the admin from the initial fill\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n    // now entries are stale\r\n    when(clock.currentTimeMillis()).thenReturn(12 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP))).thenReturn(ImmutableMap.of(ssp, sspMetadata(10), otherSSP, sspMetadata(11)));\r\n    // flip the order; prefetching should still be done correctly\r\n    assertEquals(sspMetadata(11), cache.getMetadata(otherSSP));\r\n    assertEquals(sspMetadata(10), cache.getMetadata(ssp));\r\n    verify(systemAdmin, times(2)).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataEmptyMetadata",
  "sourceCode" : "/**\r\n * Given that an SSP has empty metadata, getMetadata should return and cache that.\r\n */\r\n@Test\r\npublic void testGetMetadataEmptyMetadata() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(ssp));\r\n    // t = 10: first read, t = 11: first write\r\n    when(clock.currentTimeMillis()).thenReturn(10L, 11L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of());\r\n    assertNull(cache.getMetadata(ssp));\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp));\r\n    // stay within TTL: use cached data\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    assertNull(cache.getMetadata(ssp));\r\n    // still only one call to the admin from the initial fill\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp));\r\n    // now entries are stale\r\n    when(clock.currentTimeMillis()).thenReturn(12 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of());\r\n    assertNull(cache.getMetadata(ssp));\r\n    verify(systemAdmin, times(2)).getSSPMetadata(ImmutableSet.of(ssp));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataMultipleSystemsForPrefetch",
  "sourceCode" : "/**\r\n * Given that the there are sspsToPrefetch with systems that do not match the requested SSP, getMetadata should not\r\n * prefetch all sspsToPrefetch.\r\n */\r\n@Test\r\npublic void testGetMetadataMultipleSystemsForPrefetch() {\r\n    // add one more extended system admin so we can have two of them for this test\r\n    SystemAdmin otherSystemAdmin = mock(SystemAdmin.class);\r\n    String otherSystem = \"otherSystem\";\r\n    when(systemAdmins.getSystemAdmin(otherSystem)).thenReturn(otherSystemAdmin);\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    // different system should not get prefetched\r\n    SystemStreamPartition sspOtherSystem = new SystemStreamPartition(otherSystem, \"otherStream\", new Partition(1));\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(ssp, sspOtherSystem));\r\n    // t = 10: first read for ssp, t = 11: first write for ssp\r\n    when(clock.currentTimeMillis()).thenReturn(10L, 11L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(1)));\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    // does not call for sspOtherSystem\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp));\r\n    // t = 12: first read for sspOtherSystem, t = 13: first write for sspOtherSystem\r\n    when(clock.currentTimeMillis()).thenReturn(12L, 13L);\r\n    when(otherSystemAdmin.getSSPMetadata(ImmutableSet.of(sspOtherSystem))).thenReturn(ImmutableMap.of(sspOtherSystem, sspMetadata(2)));\r\n    assertEquals(sspMetadata(2), cache.getMetadata(sspOtherSystem));\r\n    // does not call for ssp\r\n    verify(otherSystemAdmin).getSSPMetadata(ImmutableSet.of(sspOtherSystem));\r\n    // now entries are stale, do another round of individual fetches\r\n    when(clock.currentTimeMillis()).thenReturn(14 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(10)));\r\n    assertEquals(sspMetadata(10), cache.getMetadata(ssp));\r\n    verify(systemAdmin, times(2)).getSSPMetadata(ImmutableSet.of(ssp));\r\n    when(otherSystemAdmin.getSSPMetadata(ImmutableSet.of(sspOtherSystem))).thenReturn(ImmutableMap.of(sspOtherSystem, sspMetadata(11)));\r\n    assertEquals(sspMetadata(11), cache.getMetadata(sspOtherSystem));\r\n    verify(otherSystemAdmin, times(2)).getSSPMetadata(ImmutableSet.of(sspOtherSystem));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataNoSSPsToPrefetch",
  "sourceCode" : "/**\r\n * Given that there are no sspsToPrefetch, getMetadata should still fetch and cache metadata for a requested SSP.\r\n */\r\n@Test\r\npublic void testGetMetadataNoSSPsToPrefetch() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of());\r\n    // t = 10: first read, t = 11: first write\r\n    when(clock.currentTimeMillis()).thenReturn(10L, 11L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(1)));\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp));\r\n    // stay within TTL: use cached data\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    // now entry is stale\r\n    when(clock.currentTimeMillis()).thenReturn(12 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(10)));\r\n    assertEquals(sspMetadata(10), cache.getMetadata(ssp));\r\n    verify(systemAdmin, times(2)).getSSPMetadata(ImmutableSet.of(ssp));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataRequestedSSPNotInSSPsToPrefetch",
  "sourceCode" : "/**\r\n * Given that the sspsToPrefetch does not contain the requested SSP, getMetadata should still fetch and cache metadata\r\n * for it.\r\n */\r\n@Test\r\npublic void testGetMetadataRequestedSSPNotInSSPsToPrefetch() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SystemStreamPartition otherSSP = buildSSP(1);\r\n    // do not include ssp in sspsToPrefetch\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(otherSSP));\r\n    // t = 10: first read, t = 11: first write\r\n    when(clock.currentTimeMillis()).thenReturn(10L, 11L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP))).thenReturn(ImmutableMap.of(ssp, sspMetadata(1), otherSSP, sspMetadata(2)));\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    // still will fetch metadata for both\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n    // stay within TTL: use cached data\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    assertEquals(sspMetadata(1), cache.getMetadata(ssp));\r\n    assertEquals(sspMetadata(2), cache.getMetadata(otherSSP));\r\n    // still only one call to the admin from the initial fill\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n    // now entries are stale\r\n    when(clock.currentTimeMillis()).thenReturn(12 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(10), otherSSP, sspMetadata(11)));\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(otherSSP))).thenReturn(ImmutableMap.of(otherSSP, sspMetadata(11)));\r\n    // call for otherSSP first; no prefetching since ssp is not in sspsToPrefetch\r\n    assertEquals(sspMetadata(11), cache.getMetadata(otherSSP));\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(otherSSP));\r\n    // call for ssp also has no prefetching since the otherSSP metadata is fresh at this point\r\n    assertEquals(sspMetadata(10), cache.getMetadata(ssp));\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp));\r\n    // still only one call for both at the same time from the initial fill\r\n    verify(systemAdmin).getSSPMetadata(ImmutableSet.of(ssp, otherSSP));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataConcurrentAccess",
  "sourceCode" : "/**\r\n * Given concurrent access to getMetadata, there should be only single calls to fetch metadata.\r\n */\r\n@Test\r\npublic void testGetMetadataConcurrentAccess() throws ExecutionException, InterruptedException {\r\n    int numPartitions = 50;\r\n    // initial fetch\r\n    when(clock.currentTimeMillis()).thenReturn(10L);\r\n    Set<SystemStreamPartition> ssps = IntStream.range(0, numPartitions).mapToObj(TestSSPMetadataCache::buildSSP).collect(Collectors.toSet());\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ssps);\r\n    ExecutorService executorService = Executors.newFixedThreadPool(10);\r\n    when(systemAdmin.getSSPMetadata(ssps)).thenAnswer(invocation -> {\r\n        // have the admin call wait so that it forces the threads to overlap on the lock\r\n        Thread.sleep(500);\r\n        return IntStream.range(0, numPartitions).boxed().collect(Collectors.toMap(TestSSPMetadataCache::buildSSP, i -> sspMetadata((long) i)));\r\n    });\r\n    // send concurrent requests for metadata\r\n    List<Future<SystemStreamMetadata.SystemStreamPartitionMetadata>> getMetadataFutures = IntStream.range(0, numPartitions).mapToObj(i -> executorService.submit(() -> cache.getMetadata(buildSSP(i)))).collect(Collectors.toList());\r\n    for (int i = 0; i < numPartitions; i++) {\r\n        assertEquals(sspMetadata(i), getMetadataFutures.get(i).get());\r\n    }\r\n    // should only see one call to fetch metadata\r\n    verify(systemAdmin).getSSPMetadata(ssps);\r\n    // make entries stale\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    getMetadataFutures = IntStream.range(0, numPartitions).mapToObj(i -> executorService.submit(() -> cache.getMetadata(buildSSP(i)))).collect(Collectors.toList());\r\n    for (int i = 0; i < numPartitions; i++) {\r\n        assertEquals(sspMetadata(i), getMetadataFutures.get(i).get());\r\n    }\r\n    // should see two total calls to fetch metadata\r\n    verify(systemAdmin, times(2)).getSSPMetadata(ssps);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataExceptionFirstFetch",
  "sourceCode" : "/**\r\n * Given that the admin throws an exception when trying to get the metadata for the first time, getMetadata should\r\n * propagate the exception.\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testGetMetadataExceptionFirstFetch() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(ssp));\r\n    when(clock.currentTimeMillis()).thenReturn(10L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenThrow(new SamzaException());\r\n    cache.getMetadata(ssp);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\system\\TestSSPMetadataCache.java",
  "methodName" : "testGetMetadataExceptionAfterSuccessfulFetch",
  "sourceCode" : "/**\r\n * Given that the admin throws an exception when trying to get the metadata after a successful fetch, getMetadata\r\n * should propagate the exception.\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testGetMetadataExceptionAfterSuccessfulFetch() {\r\n    SystemStreamPartition ssp = buildSSP(0);\r\n    SSPMetadataCache cache = buildSSPMetadataCache(ImmutableSet.of(ssp));\r\n    // do a successful fetch first\r\n    when(clock.currentTimeMillis()).thenReturn(10L);\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenReturn(ImmutableMap.of(ssp, sspMetadata(1)));\r\n    cache.getMetadata(ssp);\r\n    // throw an exception on the next fetch\r\n    when(clock.currentTimeMillis()).thenReturn(11 + CACHE_TTL.toMillis());\r\n    when(systemAdmin.getSSPMetadata(ImmutableSet.of(ssp))).thenThrow(new SamzaException());\r\n    cache.getMetadata(ssp);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchProcessor.java",
  "methodName" : "testCreate",
  "sourceCode" : "@Test\r\npublic void testCreate() {\r\n    final ReadWriteUpdateTable<Integer, Integer, Integer> table = mock(ReadWriteUpdateTable.class);\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = createBatchProcessor(table, 3, Integer.MAX_VALUE);\r\n    // The batch processor initially has no operation.\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    batchProcessor.processPutDeleteOrUpdateOperations(new PutOperation<>(1, 1));\r\n    // The batch processor now has one operation.\r\n    Assert.assertEquals(1, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchProcessor.java",
  "methodName" : "testUpdateAndLookup",
  "sourceCode" : "@Test\r\npublic void testUpdateAndLookup() {\r\n    final ReadWriteUpdateTable<Integer, Integer, Integer> table = mock(ReadWriteUpdateTable.class);\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = createBatchProcessor(table, Integer.MAX_VALUE, Integer.MAX_VALUE);\r\n    int numberOfPuts = 10;\r\n    for (int i = 0; i < numberOfPuts; i++) {\r\n        batchProcessor.processPutDeleteOrUpdateOperations(new PutOperation<>(i, i));\r\n    }\r\n    // verify that the number of addBatch operations is correct.\r\n    Assert.assertEquals(numberOfPuts, batchProcessor.size());\r\n    // Verify that the value is correct for each key.\r\n    for (int i = 0; i < numberOfPuts; i++) {\r\n        final Operation<Integer, Integer, Integer> operation = batchProcessor.getLatestPutUpdateOrDelete(i);\r\n        Assert.assertEquals(i, operation.getKey().intValue());\r\n        Assert.assertEquals(i, operation.getValue().intValue());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchProcessor.java",
  "methodName" : "testBatchOperationTriggeredByBatchSize",
  "sourceCode" : "@Test\r\npublic void testBatchOperationTriggeredByBatchSize() {\r\n    final int maxBatchSize = 3;\r\n    final CountDownLatch batchCompletionTriggerLatch = new CountDownLatch(1);\r\n    final Supplier<Void> tableUpdateSupplier = () -> {\r\n        try {\r\n            batchCompletionTriggerLatch.await();\r\n        } catch (InterruptedException e) {\r\n            // ignore\r\n        }\r\n        return null;\r\n    };\r\n    final ReadWriteUpdateTable<Integer, Integer, Integer> table = mock(ReadWriteUpdateTable.class);\r\n    when(table.putAllAsync(anyList())).thenReturn(CompletableFuture.supplyAsync(tableUpdateSupplier));\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = createBatchProcessor(table, maxBatchSize, Integer.MAX_VALUE);\r\n    List<CompletableFuture<Void>> futureList = new ArrayList<>();\r\n    // One batch will be created and sent to the remote table after the for-loop.\r\n    for (int i = 0; i < maxBatchSize; i++) {\r\n        futureList.add(batchProcessor.processPutDeleteOrUpdateOperations(new PutOperation<>(i, i)));\r\n    }\r\n    for (int i = 0; i < maxBatchSize; i++) {\r\n        Assert.assertFalse(futureList.get(i).isDone());\r\n    }\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    // Complete the async call to the underlying table\r\n    batchCompletionTriggerLatch.countDown();\r\n    // The latch should eventually trigger completion to the future returned by the batch processor\r\n    CompletableFuture.allOf(futureList.toArray(new CompletableFuture[0])).join();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchProcessor.java",
  "methodName" : "testBatchOperationTriggeredByTimer",
  "sourceCode" : "@Test\r\npublic void testBatchOperationTriggeredByTimer() {\r\n    final int maxBatchDelayMs = 100;\r\n    final int putOperationCount = 100;\r\n    final ReadWriteUpdateTable<Integer, Integer, Integer> table = mock(ReadWriteUpdateTable.class);\r\n    when(table.putAllAsync(any())).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(table.deleteAllAsync(anyList())).thenReturn(CompletableFuture.completedFuture(null));\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = createBatchProcessor(table, Integer.MAX_VALUE, maxBatchDelayMs);\r\n    for (int i = 0; i < putOperationCount; i++) {\r\n        batchProcessor.processPutDeleteOrUpdateOperations(new PutOperation<>(i, i));\r\n    }\r\n    // There's one batch with infinite maximum size, it has 100ms maximum delay.\r\n    Assert.assertEquals(putOperationCount, batchProcessor.size());\r\n    try {\r\n        sleep(maxBatchDelayMs * 2);\r\n    } catch (InterruptedException e) {\r\n        // ignore\r\n    }\r\n    // After the timer fired, a new batch will be created.\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testPutAsync",
  "sourceCode" : "@Test\r\npublic void testPutAsync() {\r\n    final List<CompletableFuture<Void>> futures = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        futures.add(asyncBatchingTable.putAsync(i, i));\r\n    }\r\n    sleep();\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = asyncBatchingTable.getBatchProcessor();\r\n    // Verify that all async puts are finished.\r\n    futures.forEach(future -> Assert.assertTrue(future.isDone()));\r\n    verify(table, times(1)).putAllAsync(any());\r\n    // There should be no operations in the batch processor.\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    asyncBatchingTable.putAsync(BATCH_SIZE, BATCH_SIZE);\r\n    // Now batch size should be 1.\r\n    Assert.assertEquals(1, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testPutAllAsync",
  "sourceCode" : "@Test\r\npublic void testPutAllAsync() {\r\n    final List<Entry<Integer, Integer>> entries = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        entries.add(new Entry<>(i, i));\r\n    }\r\n    CompletableFuture<Void> future = asyncBatchingTable.putAllAsync(entries);\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = asyncBatchingTable.getBatchProcessor();\r\n    sleep();\r\n    // Verify that putAllAsync is finished.\r\n    Assert.assertTrue(future.isDone());\r\n    // There should be no pending operations.\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    // The addBatchUpdates batch operations propagates to the table.\r\n    verify(table, times(1)).putAllAsync(anyList());\r\n    // This new addBatchUpdates will make the batch size to be 1.\r\n    asyncBatchingTable.putAsync(BATCH_SIZE, BATCH_SIZE);\r\n    Assert.assertEquals(1, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testUpdateAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAsync() {\r\n    // Use CompleteBatch instead of CompactBatch as CompactBatch doesn't support updates\r\n    asyncBatchingTable = new AsyncBatchingTable(\"id\", table, new CompleteBatchProvider().withMaxBatchSize(BATCH_SIZE).withMaxBatchDelay(BATCH_DELAY), Executors.newSingleThreadScheduledExecutor());\r\n    asyncBatchingTable.createBatchProcessor(() -> 0, mock(BatchMetrics.class));\r\n    // mocking of updateAsync and updateAllAsync of AsyncReadWriteTable table done in setup method\r\n    final List<CompletableFuture<Void>> futures = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        futures.add(asyncBatchingTable.updateAsync(i, i));\r\n    }\r\n    sleep();\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = asyncBatchingTable.getBatchProcessor();\r\n    // Verify that all async updates are finished.\r\n    futures.forEach(future -> Assert.assertTrue(future.isDone()));\r\n    verify(table, times(1)).updateAllAsync(anyList());\r\n    // There should be no operations in the batch processor.\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    asyncBatchingTable.updateAsync(1, 1);\r\n    asyncBatchingTable.updateAsync(2, 2);\r\n    // Now batch size should be 2.\r\n    Assert.assertEquals(2, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testUpdateAllAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAllAsync() {\r\n    // Use CompleteBatch instead of CompactBatch as CompactBatch doesn't support updates\r\n    asyncBatchingTable = new AsyncBatchingTable(\"id\", table, new CompleteBatchProvider().withMaxBatchSize(BATCH_SIZE).withMaxBatchDelay(BATCH_DELAY), Executors.newSingleThreadScheduledExecutor());\r\n    asyncBatchingTable.createBatchProcessor(() -> 0, mock(BatchMetrics.class));\r\n    // mocking of updateAsync and updateAllAsync of AsyncReadWriteTable table done in setup method\r\n    final List<Entry<Integer, Integer>> updates = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        updates.add(new Entry<>(i, i));\r\n    }\r\n    CompletableFuture<Void> future = asyncBatchingTable.updateAllAsync(updates);\r\n    final BatchProcessor<Integer, Integer, Integer> batchProcessor = asyncBatchingTable.getBatchProcessor();\r\n    sleep();\r\n    // Verify that updateAllAsync is finished.\r\n    Assert.assertTrue(future.isDone());\r\n    Assert.assertEquals(0, batchProcessor.size());\r\n    // The addBatchUpdates batch operations propagates to the table.\r\n    verify(table, times(1)).updateAllAsync(anyList());\r\n    // This new addBatchUpdates will make the batch size to be 1.\r\n    asyncBatchingTable.updateAsync(BATCH_SIZE, BATCH_SIZE);\r\n    Assert.assertEquals(1, batchProcessor.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testGetAsync",
  "sourceCode" : "@Test\r\npublic void testGetAsync() throws ExecutionException, InterruptedException {\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        asyncBatchingTable.putAsync(i, i);\r\n    }\r\n    sleep();\r\n    final List<CompletableFuture<Integer>> futures = new ArrayList<>(BATCH_SIZE);\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        futures.add(asyncBatchingTable.getAsync(i));\r\n    }\r\n    sleep();\r\n    for (Integer i = 0; i < BATCH_SIZE; i++) {\r\n        Assert.assertTrue(futures.get(i).isDone());\r\n        Assert.assertEquals(i, futures.get(i).get());\r\n    }\r\n    verify(table, times(1)).getAllAsync(anyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testGetAllAsync",
  "sourceCode" : "@Test\r\npublic void testGetAllAsync() throws ExecutionException, InterruptedException {\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        asyncBatchingTable.putAsync(i, i);\r\n    }\r\n    sleep();\r\n    final List<Integer> keys = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        keys.add(new Integer(i));\r\n    }\r\n    CompletableFuture<Map<Integer, Integer>> future = asyncBatchingTable.getAllAsync(keys);\r\n    sleep();\r\n    Assert.assertTrue(future.isDone());\r\n    Assert.assertEquals(BATCH_SIZE, future.get().size());\r\n    verify(table, times(1)).getAllAsync(anyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testDeleteAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAsync() throws Exception {\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        asyncBatchingTable.putAsync(i, i);\r\n    }\r\n    sleep();\r\n    // The 1st batch is done.\r\n    verify(table, times(1)).putAllAsync(anyList());\r\n    final List<CompletableFuture<Void>> completableFutures = new ArrayList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        completableFutures.add(asyncBatchingTable.deleteAsync(i));\r\n    }\r\n    sleep();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        Assert.assertEquals(null, completableFutures.get(i).get());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\batching\\TestBatchTable.java",
  "methodName" : "testDeleteAllAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsync() throws Exception {\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        asyncBatchingTable.putAsync(i, i);\r\n    }\r\n    sleep();\r\n    final List<Integer> keys = new LinkedList<>();\r\n    for (int i = 0; i < BATCH_SIZE; i++) {\r\n        keys.add(new Integer(i));\r\n    }\r\n    final CompletableFuture<Void> future = asyncBatchingTable.deleteAllAsync(keys);\r\n    sleep();\r\n    Assert.assertTrue(future.isDone());\r\n    final CompletableFuture<Map<Integer, Integer>> getAllFuture = asyncBatchingTable.getAllAsync(keys);\r\n    sleep();\r\n    Assert.assertTrue(getAllFuture.isDone());\r\n    getAllFuture.get().forEach((k, v) -> Assert.assertEquals(null, v));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testSerializeSimple",
  "sourceCode" : "@Test\r\npublic void testSerializeSimple() {\r\n    doTestSerialize(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testSerializeWithCacheInstance",
  "sourceCode" : "@Test\r\npublic void testSerializeWithCacheInstance() {\r\n    String tableId = \"guavaCacheId\";\r\n    GuavaCacheTableDescriptor guavaTableDesc = new GuavaCacheTableDescriptor(tableId).withCache(CacheBuilder.newBuilder().build());\r\n    Map<String, String> tableConfig = guavaTableDesc.toConfig(new MapConfig());\r\n    assertExists(GuavaCacheTableDescriptor.GUAVA_CACHE, tableId, tableConfig);\r\n    doTestSerialize(guavaTableDesc);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testCacheOpsWriteThrough",
  "sourceCode" : "@Test\r\npublic void testCacheOpsWriteThrough() {\r\n    doTestCacheOps(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testCacheOpsWriteAround",
  "sourceCode" : "@Test\r\npublic void testCacheOpsWriteAround() {\r\n    doTestCacheOps(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testNonexistentKeyInTable",
  "sourceCode" : "@Test\r\npublic void testNonexistentKeyInTable() {\r\n    ReadWriteUpdateTable<String, String, String> table = mock(ReadWriteUpdateTable.class);\r\n    doReturn(CompletableFuture.completedFuture(null)).when(table).getAsync(any());\r\n    ReadWriteUpdateTable<String, String, String> cache = getMockCache().getLeft();\r\n    CachingTable<String, String, String> cachingTable = new CachingTable<>(\"myTable\", table, cache, false);\r\n    initTables(cachingTable);\r\n    Assert.assertNull(cachingTable.get(\"abc\"));\r\n    verify(cache, times(1)).get(any());\r\n    Assert.assertNull(cache.get(\"abc\"));\r\n    verify(cache, times(0)).put(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testKeyEviction",
  "sourceCode" : "@Test\r\npublic void testKeyEviction() {\r\n    ReadWriteUpdateTable<String, String, Void> table = mock(ReadWriteUpdateTable.class);\r\n    doReturn(CompletableFuture.completedFuture(\"3\")).when(table).getAsync(any());\r\n    ReadWriteUpdateTable<String, String, Void> cache = mock(ReadWriteUpdateTable.class);\r\n    // no handler added to mock cache so get/put are noop, this can simulate eviction\r\n    CachingTable<String, String, Void> cachingTable = new CachingTable<>(\"myTable\", table, cache, false);\r\n    initTables(cachingTable);\r\n    cachingTable.get(\"abc\");\r\n    verify(table, times(1)).getAsync(any());\r\n    // get() should go to table again\r\n    cachingTable.get(\"abc\");\r\n    verify(table, times(2)).getAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testGuavaCacheAndRemoteTable",
  "sourceCode" : "/**\r\n * Testing caching in a more realistic scenario with Guava cache + remote table\r\n */\r\n@Test\r\npublic void testGuavaCacheAndRemoteTable() throws Exception {\r\n    String tableId = \"testGuavaCacheAndRemoteTable\";\r\n    Cache<String, String> guavaCache = CacheBuilder.newBuilder().initialCapacity(100).build();\r\n    final ReadWriteUpdateTable<String, String, String> guavaTable = new GuavaCacheTable<>(tableId + \"-cache\", guavaCache);\r\n    // It is okay to share rateLimitHelper and async helper for read/write in test\r\n    TableRateLimiter<String, String> readRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableRateLimiter<String, String> writeRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableRateLimiter<String, String> updateRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    final RemoteTable<String, String, String> remoteTable = new RemoteTable<>(tableId + \"-remote\", readFn, writeFn, readRateLimitHelper, writeRateLimitHelper, updateRateLimitHelper, Executors.newSingleThreadExecutor(), null, null, null, null, null, Executors.newSingleThreadExecutor());\r\n    final CachingTable<String, String, String> cachingTable = new CachingTable<>(tableId, remoteTable, guavaTable, false);\r\n    initTables(cachingTable, guavaTable, remoteTable);\r\n    // 4 per readable table (12)\r\n    // 8 per read/write table (24)\r\n    verify(metricsRegistry, times(36)).newCounter(any(), anyString());\r\n    // 3 per readable table (9)\r\n    // 8 per read/write table (24)\r\n    // 1 per remote readable table (1)\r\n    // 2 per remote read/write table (2)\r\n    verify(metricsRegistry, times(36)).newTimer(any(), anyString());\r\n    // 1 per guava table (1)\r\n    // 3 per caching table (2)\r\n    verify(metricsRegistry, times(4)).newGauge(anyString(), any());\r\n    // GET\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any());\r\n    Assert.assertEquals(cachingTable.getAsync(\"foo\").get(), \"bar\");\r\n    // Ensure cache is updated\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo\"), \"bar\");\r\n    // PUT\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any());\r\n    cachingTable.putAsync(\"foo\", \"baz\").get();\r\n    // Ensure cache is updated\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo\"), \"baz\");\r\n    // DELETE\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any());\r\n    cachingTable.deleteAsync(\"foo\").get();\r\n    // Ensure cache is updated\r\n    Assert.assertNull(guavaCache.getIfPresent(\"foo\"));\r\n    // GET-ALL\r\n    Map<String, String> records = new HashMap<>();\r\n    records.put(\"foo1\", \"bar1\");\r\n    records.put(\"foo2\", \"bar2\");\r\n    doReturn(CompletableFuture.completedFuture(records)).when(readFn).getAllAsync(any());\r\n    Assert.assertEquals(cachingTable.getAllAsync(Arrays.asList(\"foo1\", \"foo2\")).get(), records);\r\n    // Ensure cache is updated\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo1\"), \"bar1\");\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo2\"), \"bar2\");\r\n    // GET-ALL with partial miss\r\n    doReturn(CompletableFuture.completedFuture(Collections.singletonMap(\"foo3\", \"bar3\"))).when(readFn).getAllAsync(any());\r\n    records = cachingTable.getAllAsync(Arrays.asList(\"foo1\", \"foo2\", \"foo3\")).get();\r\n    Assert.assertEquals(records.get(\"foo3\"), \"bar3\");\r\n    // Ensure cache is updated\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo3\"), \"bar3\");\r\n    // Calling again for the same keys should not trigger IO, ie. no exception is thrown\r\n    CompletableFuture<String> exFuture = new CompletableFuture<>();\r\n    exFuture.completeExceptionally(new RuntimeException(\"Test exception\"));\r\n    doReturn(exFuture).when(readFn).getAllAsync(any());\r\n    cachingTable.getAllAsync(Arrays.asList(\"foo1\", \"foo2\", \"foo3\")).get();\r\n    // Partial results should throw\r\n    try {\r\n        cachingTable.getAllAsync(Arrays.asList(\"foo1\", \"foo2\", \"foo5\")).get();\r\n        Assert.fail();\r\n    } catch (Exception e) {\r\n    }\r\n    // PUT-ALL\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(any());\r\n    List<Entry<String, String>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(\"foo1\", \"bar111\"));\r\n    entries.add(new Entry<>(\"foo2\", \"bar222\"));\r\n    cachingTable.putAllAsync(entries).get();\r\n    // Ensure cache is updated\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo1\"), \"bar111\");\r\n    Assert.assertEquals(guavaCache.getIfPresent(\"foo2\"), \"bar222\");\r\n    // PUT-ALL with delete\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(any());\r\n    entries = new ArrayList<>();\r\n    entries.add(new Entry<>(\"foo1\", \"bar111\"));\r\n    entries.add(new Entry<>(\"foo2\", null));\r\n    cachingTable.putAllAsync(entries).get();\r\n    // Ensure cache is updated\r\n    Assert.assertNull(guavaCache.getIfPresent(\"foo2\"));\r\n    // At this point, foo1 and foo3 should still exist\r\n    Assert.assertNotNull(guavaCache.getIfPresent(\"foo1\"));\r\n    Assert.assertNotNull(guavaCache.getIfPresent(\"foo3\"));\r\n    // DELETE-ALL\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(any());\r\n    cachingTable.deleteAllAsync(Arrays.asList(\"foo1\", \"foo3\")).get();\r\n    // Ensure foo1 and foo3 are gone\r\n    Assert.assertNull(guavaCache.getIfPresent(\"foo1\"));\r\n    Assert.assertNull(guavaCache.getIfPresent(\"foo3\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\caching\\TestCachingTable.java",
  "methodName" : "testTimerDisabled",
  "sourceCode" : "@Test\r\npublic void testTimerDisabled() throws Exception {\r\n    String tableId = \"testTimerDisabled\";\r\n    Cache<String, String> guavaCache = CacheBuilder.newBuilder().initialCapacity(100).build();\r\n    final ReadWriteUpdateTable<String, String, String> guavaTable = new GuavaCacheTable<>(tableId, guavaCache);\r\n    TableRateLimiter<String, String> readRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableRateLimiter<String, String> writeRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableRateLimiter<String, String> updateRateLimitHelper = mock(TableRateLimiter.class);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(CompletableFuture.completedFuture(\"\")).when(readFn).getAsync(any());\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any());\r\n    final RemoteTable<String, String, String> remoteTable = new RemoteTable<>(tableId, readFn, writeFn, readRateLimitHelper, writeRateLimitHelper, updateRateLimitHelper, Executors.newSingleThreadExecutor(), null, null, null, null, null, Executors.newSingleThreadExecutor());\r\n    final CachingTable<String, String, String> cachingTable = new CachingTable<>(tableId, remoteTable, guavaTable, false);\r\n    initTables(true, cachingTable, guavaTable, remoteTable);\r\n    cachingTable.get(\"\");\r\n    cachingTable.getAsync(\"\").get();\r\n    cachingTable.getAll(Collections.emptyList());\r\n    cachingTable.getAllAsync(Collections.emptyList());\r\n    cachingTable.flush();\r\n    cachingTable.put(\"\", \"\");\r\n    cachingTable.putAsync(\"\", \"\");\r\n    cachingTable.putAll(Collections.emptyList());\r\n    cachingTable.putAllAsync(Collections.emptyList());\r\n    cachingTable.delete(\"\");\r\n    cachingTable.deleteAsync(\"\");\r\n    cachingTable.deleteAll(Collections.emptyList());\r\n    cachingTable.deleteAllAsync(Collections.emptyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testMinimal",
  "sourceCode" : "@Test\r\npublic void testMinimal() {\r\n    Config jobConfig = createJobConfig();\r\n    Assert.assertEquals(2, jobConfig.size());\r\n    Map<String, String> tableConfig = createTableDescriptor().toConfig(jobConfig);\r\n    Assert.assertNotNull(tableConfig);\r\n    Assert.assertEquals(1, tableConfig.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testTableProviderFactoryConfig",
  "sourceCode" : "@Test\r\npublic void testTableProviderFactoryConfig() {\r\n    Map<String, String> tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertEquals(1, tableConfig.size());\r\n    Assert.assertEquals(MockTableProviderFactory.class.getName(), tableConfig.get(String.format(JavaTableConfig.TABLE_PROVIDER_FACTORY, TABLE_ID)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testStoreConfig",
  "sourceCode" : "@Test\r\npublic void testStoreConfig() {\r\n    Map<String, String> tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertEquals(1, tableConfig.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testChangelogDisabled",
  "sourceCode" : "@Test\r\npublic void testChangelogDisabled() {\r\n    Map<String, String> tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertEquals(1, tableConfig.size());\r\n    Assert.assertFalse(tableConfig.containsKey(String.format(StorageConfig.CHANGELOG_STREAM, TABLE_ID)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testChangelogEnabled",
  "sourceCode" : "@Test\r\npublic void testChangelogEnabled() {\r\n    Map<String, String> tableConfig = createTableDescriptor().withChangelogEnabled().toConfig(createJobConfig());\r\n    Assert.assertEquals(2, tableConfig.size());\r\n    Assert.assertEquals(\"test-job-10-table-t1\", String.format(tableConfig.get(String.format(StorageConfig.CHANGELOG_STREAM, TABLE_ID))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testChangelogEnabledWithCustomParameters",
  "sourceCode" : "@Test\r\npublic void testChangelogEnabledWithCustomParameters() {\r\n    Map<String, String> tableConfig = createTableDescriptor().withChangelogStream(\"my-stream\").withChangelogReplicationFactor(100).toConfig(createJobConfig());\r\n    Assert.assertEquals(3, tableConfig.size());\r\n    Assert.assertEquals(\"my-stream\", String.format(tableConfig.get(String.format(StorageConfig.CHANGELOG_STREAM, TABLE_ID))));\r\n    Assert.assertEquals(\"100\", String.format(tableConfig.get(String.format(StorageConfig.CHANGELOG_REPLICATION_FACTOR, TABLE_ID))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testChangelogWithoutJobName",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testChangelogWithoutJobName() {\r\n    Map<String, String> jobConfig = new HashMap<>();\r\n    jobConfig.put(\"job.id\", JOB_ID);\r\n    createTableDescriptor().withChangelogEnabled().toConfig(new MapConfig(jobConfig));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\descriptors\\TestLocalTableDescriptor.java",
  "methodName" : "testChangelogWithoutJobId",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testChangelogWithoutJobId() {\r\n    Map<String, String> jobConfig = new HashMap<>();\r\n    jobConfig.put(\"job.name\", JOB_NAME);\r\n    createTableDescriptor().withChangelogEnabled().toConfig(new MapConfig(jobConfig));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testNotNullTableId",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullTableId() {\r\n    new AsyncRateLimitedTable(null, mock(AsyncReadWriteUpdateTable.class), mock(TableRateLimiter.class), mock(TableRateLimiter.class), mock(TableRateLimiter.class), mock(ScheduledExecutorService.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testNotNullTable",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullTable() {\r\n    new AsyncRateLimitedTable(\"t1\", null, mock(TableRateLimiter.class), mock(TableRateLimiter.class), mock(TableRateLimiter.class), mock(ScheduledExecutorService.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testNotNullRateLimitingExecutor",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullRateLimitingExecutor() {\r\n    new AsyncRateLimitedTable(\"t1\", mock(AsyncReadWriteUpdateTable.class), mock(TableRateLimiter.class), mock(TableRateLimiter.class), mock(TableRateLimiter.class), null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testNotNullAtLeastOneRateLimiter",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testNotNullAtLeastOneRateLimiter() {\r\n    new AsyncRateLimitedTable(\"t1\", mock(AsyncReadWriteUpdateTable.class), null, null, null, mock(ScheduledExecutorService.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testGetAsync",
  "sourceCode" : "@Test\r\npublic void testGetAsync() {\r\n    Assert.assertEquals(\"bar\", readTable.getAsync(\"foo\").join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    verify(readFn, times(0)).getAsync(any(), any());\r\n    verify(readRateLimiter, times(1)).throttle(anyString());\r\n    verify(readRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(readRateLimiter, times(0)).throttle(any(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyWritePartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testGetAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testGetAsyncWithArgs() {\r\n    Assert.assertEquals(\"bar\", readTable.getAsync(\"foo\", 1).join());\r\n    verify(readFn, times(0)).getAsync(any());\r\n    verify(readFn, times(1)).getAsync(any(), any());\r\n    verify(readRateLimiter, times(1)).throttle(anyString(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(readRateLimiter, times(0)).throttle(any(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyWritePartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testGetAllAsync",
  "sourceCode" : "@Test\r\npublic void testGetAllAsync() {\r\n    Assert.assertEquals(readMap, readTable.getAllAsync(Arrays.asList(\"\")).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    verify(readFn, times(0)).getAllAsync(any(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyString());\r\n    verify(readRateLimiter, times(1)).throttle(anyCollection());\r\n    verify(readRateLimiter, times(0)).throttle(any(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyWritePartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testGetAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testGetAllAsyncWithArgs() {\r\n    Assert.assertEquals(readMap, readTable.getAllAsync(Arrays.asList(\"\"), \"\").join());\r\n    verify(readFn, times(0)).getAllAsync(any());\r\n    verify(readFn, times(1)).getAllAsync(any(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyString());\r\n    verify(readRateLimiter, times(1)).throttle(anyCollection(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyString(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyWritePartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testReadAsync",
  "sourceCode" : "@Test\r\npublic void testReadAsync() {\r\n    Assert.assertEquals(5, readTable.readAsync(1, 2).join());\r\n    verify(readFn, times(1)).readAsync(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttle(anyString());\r\n    verify(readRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(readRateLimiter, times(0)).throttle(any(), any());\r\n    verify(readRateLimiter, times(1)).throttle(anyInt(), any());\r\n    verify(readRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyWritePartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testPutAsync",
  "sourceCode" : "@Test\r\npublic void testPutAsync() {\r\n    writeTable.putAsync(\"foo\", \"bar\").join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    verify(writeFn, times(0)).putAsync(any(), any(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testPutAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testPutAsyncWithArgs() {\r\n    writeTable.putAsync(\"foo\", \"bar\", 1).join();\r\n    verify(writeFn, times(0)).putAsync(any(), any());\r\n    verify(writeFn, times(1)).putAsync(any(), any(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyString(), anyString(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testPutAllAsync",
  "sourceCode" : "@Test\r\npublic void testPutAllAsync() {\r\n    writeTable.putAllAsync(Arrays.asList(new Entry(\"1\", \"2\"))).join();\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(1)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testPutAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testPutAllAsyncWithArgs() {\r\n    writeTable.putAllAsync(Arrays.asList(new Entry(\"1\", \"2\")), Arrays.asList(1)).join();\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(1)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testUpdateAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAsync() {\r\n    writeTable.updateAsync(\"foo\", \"bar\").join();\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testUpdateAllAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAllAsync() {\r\n    writeTable.updateAllAsync(Arrays.asList(new Entry(\"1\", \"2\"))).join();\r\n    verify(writeFn, times(1)).updateAllAsync(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(1)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testDeleteAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAsync() {\r\n    writeTable.deleteAsync(\"foo\").join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    verify(writeFn, times(0)).deleteAsync(any(), any());\r\n    verify(writeRateLimiter, times(1)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testDeleteAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testDeleteAsyncWithArgs() {\r\n    writeTable.deleteAsync(\"foo\", 1).join();\r\n    verify(writeFn, times(0)).deleteAsync(any());\r\n    verify(writeFn, times(1)).deleteAsync(any(), any());\r\n    verify(writeRateLimiter, times(1)).throttle(anyString(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testDeleteAllAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsync() {\r\n    writeTable.deleteAllAsync(Arrays.asList(\"1\", \"2\")).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testDeleteAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsyncWithArgs() {\r\n    writeTable.deleteAllAsync(Arrays.asList(\"1\", \"2\"), 1).join();\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyCollection(), any());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyInt(), any());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testWriteAsync",
  "sourceCode" : "@Test\r\npublic void testWriteAsync() {\r\n    Assert.assertEquals(5, writeTable.writeAsync(1, 2).join());\r\n    verify(writeFn, times(1)).writeAsync(anyInt(), any());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString());\r\n    verify(writeRateLimiter, times(0)).throttle(anyCollection());\r\n    verify(writeRateLimiter, times(0)).throttle(anyString(), anyString());\r\n    verify(writeRateLimiter, times(1)).throttle(anyInt(), any());\r\n    verify(writeRateLimiter, times(0)).throttleRecords(anyCollection());\r\n    verifyReadPartNotCalled();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\ratelimit\\TestAsyncRateLimitedTable.java",
  "methodName" : "testFlushAndClose",
  "sourceCode" : "@Test\r\npublic void testFlushAndClose() {\r\n    TableRateLimiter readRateLimiter = mock(TableRateLimiter.class);\r\n    TableRateLimiter writeRateLimiter = mock(TableRateLimiter.class);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRateLimitedTable table = new AsyncRateLimitedTable(\"t1\", delegate, readRateLimiter, writeRateLimiter, writeRateLimiter, schedExec);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    table.flush();\r\n    verify(writeFn, times(1)).flush();\r\n    table.close();\r\n    verify(readFn, times(1)).close();\r\n    verify(writeFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testValidateOnlyReadOrWriteFn",
  "sourceCode" : "@Test\r\npublic void testValidateOnlyReadOrWriteFn() {\r\n    // Only read defined\r\n    String tableId = \"1\";\r\n    RemoteTableDescriptor desc = new RemoteTableDescriptor(tableId).withReadFunction(createMockTableReadFunction()).withReadRateLimiterDisabled();\r\n    Map<String, String> tableConfig = desc.toConfig(new MapConfig());\r\n    Assert.assertNotNull(tableConfig);\r\n    // Only write defined\r\n    String tableId2 = \"2\";\r\n    RemoteTableDescriptor desc2 = new RemoteTableDescriptor(tableId2).withWriteFunction(createMockTableWriteFunction()).withWriteRateLimiterDisabled();\r\n    tableConfig = desc2.toConfig(new MapConfig());\r\n    Assert.assertNotNull(tableConfig);\r\n    // Neither read or write defined (Failure case)\r\n    String tableId3 = \"3\";\r\n    RemoteTableDescriptor desc3 = new RemoteTableDescriptor(tableId3);\r\n    try {\r\n        desc3.toConfig(new MapConfig());\r\n        Assert.fail(\"Should not allow neither readFn or writeFn defined\");\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(e instanceof IllegalArgumentException);\r\n        Assert.assertTrue(e.getMessage().contains(\"Must have one of TableReadFunction or TableWriteFunction\"));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeSimple",
  "sourceCode" : "@Test\r\npublic void testSerializeSimple() {\r\n    doTestSerialize(null, null, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeWithLimiter",
  "sourceCode" : "@Test\r\npublic void testSerializeWithLimiter() {\r\n    doTestSerialize(createMockRateLimiter(), new CountingCreditFunction(), new CountingCreditFunction());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeWithLimiterAndReadCredFn",
  "sourceCode" : "@Test\r\npublic void testSerializeWithLimiterAndReadCredFn() {\r\n    doTestSerialize(createMockRateLimiter(), (k, v, args) -> 1, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeWithLimiterAndWriteCredFn",
  "sourceCode" : "@Test\r\npublic void testSerializeWithLimiterAndWriteCredFn() {\r\n    doTestSerialize(createMockRateLimiter(), null, (k, v, args) -> 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeNullWriteFunction",
  "sourceCode" : "@Test\r\npublic void testSerializeNullWriteFunction() {\r\n    String tableId = \"1\";\r\n    RemoteTableDescriptor desc = new RemoteTableDescriptor(tableId).withReadFunction(createMockTableReadFunction()).withRateLimiterDisabled();\r\n    Map<String, String> tableConfig = desc.toConfig(new MapConfig());\r\n    assertExists(RemoteTableDescriptor.READ_FN, tableId, tableConfig);\r\n    assertEquals(null, RemoteTableDescriptor.WRITE_FN, tableId, tableConfig);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSerializeNullReadFunction",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testSerializeNullReadFunction() {\r\n    RemoteTableDescriptor desc = new RemoteTableDescriptor(\"1\");\r\n    Map<String, String> tableConfig = desc.toConfig(new MapConfig());\r\n    Assert.assertTrue(tableConfig.containsKey(RemoteTableDescriptor.READ_FN));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testSpecifyBothRateAndRateLimiter",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testSpecifyBothRateAndRateLimiter() {\r\n    RemoteTableDescriptor desc = new RemoteTableDescriptor(\"1\");\r\n    desc.withReadFunction(createMockTableReadFunction());\r\n    desc.withReadRateLimit(100);\r\n    desc.withRateLimiter(createMockRateLimiter(), null, null);\r\n    desc.toConfig(new MapConfig());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testTablePartToConfigDefault",
  "sourceCode" : "@Test\r\npublic void testTablePartToConfigDefault() {\r\n    TableReadFunction readFn = createMockTableReadFunction();\r\n    when(readFn.toConfig(any(), any())).thenReturn(createConfigPair(1));\r\n    Map<String, String> tableConfig = new RemoteTableDescriptor(\"1\").withReadFunction(readFn).withReadRateLimit(100).toConfig(new MapConfig());\r\n    verify(readFn, times(1)).toConfig(any(), any());\r\n    Assert.assertEquals(\"v1\", tableConfig.get(\"tables.1.io.read.func.k1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testTablePartToConfig",
  "sourceCode" : "@Test\r\npublic void testTablePartToConfig() {\r\n    int key = 0;\r\n    TableReadFunction readFn = createMockTableReadFunction();\r\n    when(readFn.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    TableWriteFunction writeFn = createMockTableWriteFunction();\r\n    when(writeFn.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    RateLimiter rateLimiter = createMockRateLimiter();\r\n    when(((TablePart) rateLimiter).toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    CreditFunction readCredFn = createMockCreditFunction();\r\n    when(readCredFn.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    CreditFunction writeCredFn = createMockCreditFunction();\r\n    when(writeCredFn.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    TableRetryPolicy readRetryPolicy = createMockTableRetryPolicy();\r\n    when(readRetryPolicy.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    TableRetryPolicy writeRetryPolicy = createMockTableRetryPolicy();\r\n    when(writeRetryPolicy.toConfig(any(), any())).thenReturn(createConfigPair(key));\r\n    Map<String, String> tableConfig = new RemoteTableDescriptor(\"1\").withReadFunction(readFn).withWriteFunction(writeFn).withRateLimiter(rateLimiter, readCredFn, writeCredFn).withReadRetryPolicy(readRetryPolicy).withWriteRetryPolicy(writeRetryPolicy).toConfig(new MapConfig());\r\n    verify(readFn, times(1)).toConfig(any(), any());\r\n    verify(writeFn, times(1)).toConfig(any(), any());\r\n    verify((TablePart) rateLimiter, times(1)).toConfig(any(), any());\r\n    verify(readCredFn, times(1)).toConfig(any(), any());\r\n    verify(writeCredFn, times(1)).toConfig(any(), any());\r\n    verify(readRetryPolicy, times(1)).toConfig(any(), any());\r\n    verify(writeRetryPolicy, times(1)).toConfig(any(), any());\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.read.func.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.write.func.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.ratelimiter.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.read.credit.func.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.write.credit.func.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.read.retry.policy.k0\"), \"v0\");\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.write.retry.policy.k0\"), \"v0\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testTableRetryPolicyToConfig",
  "sourceCode" : "@Test\r\npublic void testTableRetryPolicyToConfig() {\r\n    Map<String, String> tableConfig = new RemoteTableDescriptor(\"1\").withReadFunction(createMockTableReadFunction()).withReadRetryPolicy(new TableRetryPolicy()).withRateLimiterDisabled().toConfig(new MapConfig());\r\n    Assert.assertEquals(tableConfig.get(\"tables.1.io.read.retry.policy.TableRetryPolicy\"), \"{\\\"exponentialFactor\\\":0.0,\\\"backoffType\\\":\\\"NONE\\\",\\\"retryPredicate\\\":{}}\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testReadWriteRateLimitToConfig",
  "sourceCode" : "@Test\r\npublic void testReadWriteRateLimitToConfig() {\r\n    Map<String, String> tableConfig = new RemoteTableDescriptor(\"1\").withReadFunction(createMockTableReadFunction()).withReadRetryPolicy(new TableRetryPolicy()).withWriteRateLimit(1000).withReadRateLimit(2000).toConfig(new MapConfig());\r\n    Assert.assertEquals(String.valueOf(2000), tableConfig.get(\"tables.1.io.read.credits\"));\r\n    Assert.assertEquals(String.valueOf(1000), tableConfig.get(\"tables.1.io.write.credits\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionNoRateLimit",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionNoRateLimit() {\r\n    doTestDeserializeReadFunctionAndLimiter(false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterWrite",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterWrite() {\r\n    doTestDeserializeReadFunctionAndLimiter(false, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterRead",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterRead() {\r\n    doTestDeserializeReadFunctionAndLimiter(false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterReadWrite",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterReadWrite() {\r\n    doTestDeserializeReadFunctionAndLimiter(false, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterRateOnlyWrite",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterRateOnlyWrite() {\r\n    doTestDeserializeReadFunctionAndLimiter(true, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterRateOnlyRead",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterRateOnlyRead() {\r\n    doTestDeserializeReadFunctionAndLimiter(true, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\descriptors\\TestRemoteTableDescriptor.java",
  "methodName" : "testDeserializeReadFunctionAndLimiterRateOnlyReadWrite",
  "sourceCode" : "@Test\r\npublic void testDeserializeReadFunctionAndLimiterRateOnlyReadWrite() {\r\n    doTestDeserializeReadFunctionAndLimiter(true, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testGetAsync",
  "sourceCode" : "@Test\r\npublic void testGetAsync() {\r\n    int times = 0;\r\n    roTable.getAsync(1);\r\n    verify(readFn, times(++times)).getAsync(any());\r\n    rwTable.getAsync(1);\r\n    verify(readFn, times(++times)).getAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testGetAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testGetAsyncWithArgs() {\r\n    int times = 0;\r\n    roTable.getAsync(1, 1);\r\n    verify(readFn, times(++times)).getAsync(any(), any());\r\n    rwTable.getAsync(1, 1);\r\n    verify(readFn, times(++times)).getAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testGetAllAsync",
  "sourceCode" : "@Test\r\npublic void testGetAllAsync() {\r\n    int times = 0;\r\n    roTable.getAllAsync(Arrays.asList(1, 2));\r\n    verify(readFn, times(++times)).getAllAsync(any());\r\n    rwTable.getAllAsync(Arrays.asList(1, 2));\r\n    verify(readFn, times(++times)).getAllAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testGetAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testGetAllAsyncWithArgs() {\r\n    int times = 0;\r\n    roTable.getAllAsync(Arrays.asList(1, 2), Arrays.asList(0, 0));\r\n    verify(readFn, times(++times)).getAllAsync(any(), any());\r\n    rwTable.getAllAsync(Arrays.asList(1, 2), Arrays.asList(0, 0));\r\n    verify(readFn, times(++times)).getAllAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testReadAsync",
  "sourceCode" : "@Test\r\npublic void testReadAsync() {\r\n    int times = 0;\r\n    roTable.readAsync(1, 2, 3);\r\n    verify(readFn, times(++times)).readAsync(anyInt(), any(), any());\r\n    rwTable.readAsync(1, 2, 3);\r\n    verify(readFn, times(++times)).readAsync(anyInt(), any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testPutAsync",
  "sourceCode" : "@Test\r\npublic void testPutAsync() {\r\n    verifyFailure(() -> roTable.putAsync(1, 2));\r\n    rwTable.putAsync(1, 2);\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testPutAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testPutAsyncWithArgs() {\r\n    verifyFailure(() -> roTable.putAsync(1, 2, 3));\r\n    rwTable.putAsync(1, 2, 3);\r\n    verify(writeFn, times(1)).putAsync(any(), any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testPutAllAsync",
  "sourceCode" : "@Test\r\npublic void testPutAllAsync() {\r\n    verifyFailure(() -> roTable.putAllAsync(Arrays.asList(new Entry(1, 2))));\r\n    rwTable.putAllAsync(Arrays.asList(new Entry(1, 2)));\r\n    verify(writeFn, times(1)).putAllAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testPutAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testPutAllAsyncWithArgs() {\r\n    verifyFailure(() -> roTable.putAllAsync(Arrays.asList(new Entry(1, 2)), Arrays.asList(0, 0)));\r\n    rwTable.putAllAsync(Arrays.asList(new Entry(1, 2)), Arrays.asList(0, 0));\r\n    verify(writeFn, times(1)).putAllAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testUpdateAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAsync() {\r\n    verifyFailure(() -> roTable.updateAsync(1, 2));\r\n    rwTable.updateAsync(1, 2);\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testUpdateAllAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAllAsync() {\r\n    List<Entry<Integer, Integer>> updates = Arrays.asList(new Entry<>(1, 100), new Entry<>(2, 200));\r\n    verifyFailure(() -> roTable.updateAllAsync(updates));\r\n    rwTable.updateAllAsync(updates);\r\n    verify(writeFn, times(1)).updateAllAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testDeleteAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAsync() {\r\n    verifyFailure(() -> roTable.deleteAsync(1));\r\n    rwTable.deleteAsync(1);\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testDeleteAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testDeleteAsyncWithArgs() {\r\n    verifyFailure(() -> roTable.deleteAsync(1, 2));\r\n    rwTable.deleteAsync(1, 2);\r\n    verify(writeFn, times(1)).deleteAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testDeleteAllAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsync() {\r\n    verifyFailure(() -> roTable.deleteAllAsync(Arrays.asList(1)));\r\n    rwTable.deleteAllAsync(Arrays.asList(1, 2));\r\n    verify(writeFn, times(1)).deleteAllAsync(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testDeleteAllAsyncWithArgs",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsyncWithArgs() {\r\n    verifyFailure(() -> roTable.deleteAllAsync(Arrays.asList(1), Arrays.asList(2)));\r\n    rwTable.deleteAllAsync(Arrays.asList(1, 2), Arrays.asList(2));\r\n    verify(writeFn, times(1)).deleteAllAsync(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testWriteAsync",
  "sourceCode" : "@Test\r\npublic void testWriteAsync() {\r\n    verifyFailure(() -> roTable.writeAsync(1, 2, 3));\r\n    rwTable.writeAsync(1, 2, 3);\r\n    verify(writeFn, times(1)).writeAsync(anyInt(), any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testClose",
  "sourceCode" : "@Test\r\npublic void testClose() {\r\n    roTable.close();\r\n    verify(readFn, times(1)).close();\r\n    verify(writeFn, times(0)).close();\r\n    rwTable.close();\r\n    verify(readFn, times(2)).close();\r\n    verify(writeFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    roTable.flush();\r\n    verify(writeFn, times(0)).flush();\r\n    rwTable.flush();\r\n    verify(writeFn, times(1)).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestAsyncRemoteTable.java",
  "methodName" : "testFailOnNullReadFnAndWriteFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFailOnNullReadFnAndWriteFn() {\r\n    new AsyncRemoteTable(null, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testFailOnNullReadFnAndWriteFn",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFailOnNullReadFnAndWriteFn() {\r\n    getTable(\"id\", null, null, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testSucceedValidationOnNullReadFn",
  "sourceCode" : "@Test\r\npublic void testSucceedValidationOnNullReadFn() {\r\n    RemoteTable<String, String, Void> table = getTable(\"tableId\", null, mock(TableWriteFunction.class), false);\r\n    Assert.assertNotNull(table);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testInit",
  "sourceCode" : "@Test\r\npublic void testInit() {\r\n    String tableId = \"testInit\";\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    RemoteTable<String, String, String> table = getTable(tableId, readFn, writeFn, true);\r\n    // AsyncRetriableTable\r\n    AsyncReadWriteUpdateTable innerTable = TestUtils.getFieldValue(table, \"asyncTable\");\r\n    Assert.assertTrue(innerTable instanceof AsyncRetriableTable);\r\n    Assert.assertNotNull(TestUtils.getFieldValue(innerTable, \"readRetryMetrics\"));\r\n    Assert.assertNotNull(TestUtils.getFieldValue(innerTable, \"writeRetryMetrics\"));\r\n    // AsyncRateLimitedTable\r\n    innerTable = TestUtils.getFieldValue(innerTable, \"table\");\r\n    Assert.assertTrue(innerTable instanceof AsyncRateLimitedTable);\r\n    // AsyncRemoteTable\r\n    innerTable = TestUtils.getFieldValue(innerTable, \"table\");\r\n    Assert.assertTrue(innerTable instanceof AsyncRemoteTable);\r\n    // Verify table functions are initialized\r\n    verify(readFn, times(1)).init(any(), any());\r\n    verify(writeFn, times(1)).init(any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGet",
  "sourceCode" : "@Test\r\npublic void testGet() {\r\n    doTestGet(true, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAsync",
  "sourceCode" : "@Test\r\npublic void testGetAsync() {\r\n    doTestGet(false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testGetAsyncError() {\r\n    doTestGet(false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAsyncErrorRetried",
  "sourceCode" : "@Test\r\npublic void testGetAsyncErrorRetried() {\r\n    doTestGet(false, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetMultipleTables",
  "sourceCode" : "@Test\r\npublic void testGetMultipleTables() {\r\n    TableReadFunction<String, String> readFn1 = mock(TableReadFunction.class);\r\n    TableReadFunction<String, String> readFn2 = mock(TableReadFunction.class);\r\n    // Sync is backed by async so needs to mock the async method\r\n    doReturn(CompletableFuture.completedFuture(\"bar1\")).when(readFn1).getAsync(anyString());\r\n    doReturn(CompletableFuture.completedFuture(\"bar2\")).when(readFn1).getAsync(anyString());\r\n    RemoteTable<String, String, Void> table1 = getTable(\"testGetMultipleTables-1\", readFn1, null, false);\r\n    RemoteTable<String, String, Void> table2 = getTable(\"testGetMultipleTables-2\", readFn2, null, false);\r\n    CompletableFuture<String> future1 = table1.getAsync(\"foo1\");\r\n    CompletableFuture<String> future2 = table2.getAsync(\"foo2\");\r\n    CompletableFuture.allOf(future1, future2).thenAccept(u -> {\r\n        Assert.assertEquals(future1.join(), \"bar1\");\r\n        Assert.assertEquals(future2.join(), \"bar1\");\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testRead",
  "sourceCode" : "@Test\r\npublic void testRead() {\r\n    doTestRead(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testReadAsync",
  "sourceCode" : "@Test\r\npublic void testReadAsync() {\r\n    doTestRead(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testReadAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testReadAsyncError() {\r\n    doTestRead(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPut",
  "sourceCode" : "@Test\r\npublic void testPut() {\r\n    doTestPut(true, false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutDelete",
  "sourceCode" : "@Test\r\npublic void testPutDelete() {\r\n    doTestPut(true, false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAsync",
  "sourceCode" : "@Test\r\npublic void testPutAsync() {\r\n    doTestPut(false, false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAsyncDelete",
  "sourceCode" : "@Test\r\npublic void testPutAsyncDelete() {\r\n    doTestPut(false, false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testPutAsyncError() {\r\n    doTestPut(false, true, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAsyncErrorRetried",
  "sourceCode" : "@Test\r\npublic void testPutAsyncErrorRetried() {\r\n    doTestPut(false, true, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdate",
  "sourceCode" : "@Test\r\npublic void testUpdate() {\r\n    doTestUpdate(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdateAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAsync() {\r\n    doTestUpdate(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdateAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testUpdateAsyncError() {\r\n    doTestUpdate(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDelete",
  "sourceCode" : "@Test\r\npublic void testDelete() {\r\n    doTestDelete(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDeleteAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAsync() {\r\n    doTestDelete(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDeleteAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testDeleteAsyncError() {\r\n    doTestDelete(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAll",
  "sourceCode" : "@Test\r\npublic void testGetAll() {\r\n    doTestGetAll(true, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAllAsync",
  "sourceCode" : "@Test\r\npublic void testGetAllAsync() {\r\n    doTestGetAll(false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAllAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testGetAllAsyncError() {\r\n    doTestGetAll(false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetAllPartialResult",
  "sourceCode" : "// Partial result is an acceptable scenario\r\n@Test\r\npublic void testGetAllPartialResult() {\r\n    doTestGetAll(false, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAll",
  "sourceCode" : "@Test\r\npublic void testPutAll() {\r\n    doTestPutAll(true, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAllHasDelete",
  "sourceCode" : "@Test\r\npublic void testPutAllHasDelete() {\r\n    doTestPutAll(true, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAllAsync",
  "sourceCode" : "@Test\r\npublic void testPutAllAsync() {\r\n    doTestPutAll(false, false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAllAsyncHasDelete",
  "sourceCode" : "@Test\r\npublic void testPutAllAsyncHasDelete() {\r\n    doTestPutAll(false, false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutAllAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testPutAllAsyncError() {\r\n    doTestPutAll(false, true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdateAll",
  "sourceCode" : "@Test\r\npublic void testUpdateAll() {\r\n    doTestUpdateAll(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdateAllAsync",
  "sourceCode" : "@Test\r\npublic void testUpdateAllAsync() {\r\n    doTestUpdateAll(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testUpdateAllAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testUpdateAllAsyncError() {\r\n    doTestUpdateAll(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDeleteAll",
  "sourceCode" : "@Test\r\npublic void testDeleteAll() {\r\n    doTestDeleteAll(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDeleteAllAsync",
  "sourceCode" : "@Test\r\npublic void testDeleteAllAsync() {\r\n    doTestDeleteAll(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testDeleteAllAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testDeleteAllAsyncError() {\r\n    doTestDeleteAll(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testWrite",
  "sourceCode" : "@Test\r\npublic void testWrite() {\r\n    doTestWrite(true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testWriteAsync",
  "sourceCode" : "@Test\r\npublic void testWriteAsync() {\r\n    doTestWrite(false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testWriteAsyncError",
  "sourceCode" : "@Test(expected = RuntimeException.class)\r\npublic void testWriteAsyncError() {\r\n    doTestWrite(false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    TableWriteFunction<String, String, Void> writeFn = mock(TableWriteFunction.class);\r\n    RemoteTable<String, String, Void> table = getTable(\"testFlush\", mock(TableReadFunction.class), writeFn, false);\r\n    table.flush();\r\n    verify(writeFn, times(1)).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetWithCallbackExecutor",
  "sourceCode" : "@Test\r\npublic void testGetWithCallbackExecutor() {\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    // Sync is backed by async so needs to mock the async method\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(anyString());\r\n    RemoteTable<String, String, Void> table = getTable(\"testGetWithCallbackExecutor\", readFn, null, Executors.newSingleThreadExecutor(), false);\r\n    Thread testThread = Thread.currentThread();\r\n    table.getAsync(\"foo\").thenAccept(result -> {\r\n        Assert.assertEquals(\"bar\", result);\r\n        // Must be executed on the executor thread\r\n        Assert.assertNotSame(testThread, Thread.currentThread());\r\n    });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testGetDelegation",
  "sourceCode" : "@Test\r\npublic void testGetDelegation() {\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any(), any());\r\n    Map<String, String> result = new HashMap<>();\r\n    result.put(\"foo\", \"bar\");\r\n    doReturn(CompletableFuture.completedFuture(result)).when(readFn).getAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(result)).when(readFn).getAllAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(5)).when(readFn).readAsync(anyInt(), any());\r\n    RemoteTable<String, String, Void> table = getTable(\"testGetDelegation\", readFn, null, Executors.newSingleThreadExecutor(), true);\r\n    verify(readFn, times(1)).init(any(), any());\r\n    // GetAsync\r\n    verify(readFn, times(0)).getAsync(any());\r\n    verify(readFn, times(0)).getAsync(any(), any());\r\n    assertEquals(\"bar\", table.getAsync(\"foo\").join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    verify(readFn, times(0)).getAsync(any(), any());\r\n    assertEquals(\"bar\", table.getAsync(\"foo\", 1).join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    verify(readFn, times(1)).getAsync(any(), any());\r\n    // GetAllAsync\r\n    verify(readFn, times(0)).getAllAsync(any());\r\n    verify(readFn, times(0)).getAllAsync(any(), any());\r\n    assertEquals(result, table.getAllAsync(Arrays.asList(\"foo\")).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    verify(readFn, times(0)).getAllAsync(any(), any());\r\n    assertEquals(result, table.getAllAsync(Arrays.asList(\"foo\"), Arrays.asList(1)).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    verify(readFn, times(1)).getAllAsync(any(), any());\r\n    // ReadAsync\r\n    verify(readFn, times(0)).readAsync(anyInt(), any());\r\n    assertEquals(5, table.readAsync(1, 2).join());\r\n    verify(readFn, times(1)).readAsync(anyInt(), any());\r\n    table.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\remote\\TestRemoteTable.java",
  "methodName" : "testPutUpdateAndDeleteDelegation",
  "sourceCode" : "@Test\r\npublic void testPutUpdateAndDeleteDelegation() {\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(anyCollection());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(anyCollection(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAllAsync(anyCollection());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(anyCollection());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(anyCollection(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).writeAsync(anyInt(), any());\r\n    RemoteTable<String, String, String> table = getTable(\"testGetDelegation\", readFn, writeFn, Executors.newSingleThreadExecutor(), true);\r\n    verify(readFn, times(1)).init(any(), any());\r\n    // PutAsync\r\n    verify(writeFn, times(0)).putAsync(any(), any());\r\n    verify(writeFn, times(0)).putAsync(any(), any(), any());\r\n    table.putAsync(\"roo\", \"bar\").join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    verify(writeFn, times(0)).putAsync(any(), any(), any());\r\n    table.putAsync(\"foo\", \"bar\", 3).join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    verify(writeFn, times(1)).putAsync(any(), any(), any());\r\n    // PutAllAsync\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection(), any());\r\n    table.putAllAsync(Arrays.asList(new Entry(\"foo\", \"bar\"))).join();\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection(), any());\r\n    table.putAllAsync(Arrays.asList(new Entry(\"foo\", \"bar\")), 2).join();\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection(), any());\r\n    // UpdateAsync\r\n    verify(writeFn, times(0)).updateAsync(any(), any());\r\n    table.updateAsync(\"foo\", \"bar\").join();\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n    // UpdateAllAsync\r\n    verify(writeFn, times(0)).updateAllAsync(anyCollection());\r\n    table.updateAllAsync(Arrays.asList(new Entry<>(\"foo\", \"bar\"))).join();\r\n    verify(writeFn, times(1)).updateAllAsync(anyCollection());\r\n    // DeleteAsync\r\n    verify(writeFn, times(0)).deleteAsync(any());\r\n    verify(writeFn, times(0)).deleteAsync(any(), any());\r\n    table.deleteAsync(\"foo\").join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    verify(writeFn, times(0)).deleteAsync(any(), any());\r\n    table.deleteAsync(\"foo\", 2).join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    verify(writeFn, times(1)).deleteAsync(any(), any());\r\n    // DeleteAllAsync\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection(), any());\r\n    table.deleteAllAsync(Arrays.asList(\"foo\")).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection(), any());\r\n    table.deleteAllAsync(Arrays.asList(\"foo\"), Arrays.asList(2)).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection(), any());\r\n    // WriteAsync\r\n    verify(writeFn, times(0)).writeAsync(anyInt(), any());\r\n    table.writeAsync(1, 2).join();\r\n    verify(writeFn, times(1)).writeAsync(anyInt(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testNotNullTableId",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullTableId() {\r\n    new AsyncRetriableTable(null, mock(AsyncReadWriteUpdateTable.class), mock(TableRetryPolicy.class), mock(TableRetryPolicy.class), mock(ScheduledExecutorService.class), mock(TableReadFunction.class), mock(TableWriteFunction.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testNotNullTable",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullTable() {\r\n    new AsyncRetriableTable(\"t1\", null, mock(TableRetryPolicy.class), mock(TableRetryPolicy.class), mock(ScheduledExecutorService.class), mock(TableReadFunction.class), mock(TableWriteFunction.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testNotNullExecutorService",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNotNullExecutorService() {\r\n    new AsyncRetriableTable(\"t1\", mock(AsyncReadWriteUpdateTable.class), mock(TableRetryPolicy.class), mock(TableRetryPolicy.class), null, mock(TableReadFunction.class), mock(TableWriteFunction.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testNotAllRetryPolicyAreNull",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testNotAllRetryPolicyAreNull() {\r\n    new AsyncRetriableTable(\"t1\", mock(AsyncReadWriteUpdateTable.class), null, null, mock(ScheduledExecutorService.class), mock(TableReadFunction.class), mock(TableWriteFunction.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetDelegation",
  "sourceCode" : "@Test\r\npublic void testGetDelegation() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any(), any());\r\n    Map<String, String> result = new HashMap<>();\r\n    result.put(\"foo\", \"bar\");\r\n    doReturn(CompletableFuture.completedFuture(result)).when(readFn).getAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(result)).when(readFn).getAllAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(5)).when(readFn).readAsync(anyInt(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    verify(readFn, times(0)).init(any(), any());\r\n    // GetAsync\r\n    verify(readFn, times(0)).getAsync(any());\r\n    verify(readFn, times(0)).getAsync(any(), any());\r\n    assertEquals(\"bar\", table.getAsync(\"foo\").join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    verify(readFn, times(0)).getAsync(any(), any());\r\n    assertEquals(\"bar\", table.getAsync(\"foo\", 1).join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    verify(readFn, times(1)).getAsync(any(), any());\r\n    // GetAllAsync\r\n    verify(readFn, times(0)).getAllAsync(any());\r\n    verify(readFn, times(0)).getAllAsync(any(), any());\r\n    assertEquals(result, table.getAllAsync(Arrays.asList(\"foo\")).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    verify(readFn, times(0)).getAllAsync(any(), any());\r\n    assertEquals(result, table.getAllAsync(Arrays.asList(\"foo\"), Arrays.asList(1)).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    verify(readFn, times(1)).getAllAsync(any(), any());\r\n    // ReadAsync\r\n    verify(readFn, times(0)).readAsync(anyInt(), any());\r\n    assertEquals(5, table.readAsync(1, 2).join());\r\n    verify(readFn, times(1)).readAsync(anyInt(), any());\r\n    table.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetWithoutRetry",
  "sourceCode" : "@Test\r\npublic void testGetWithoutRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    doReturn(true).when(readFn).isRetriable(any());\r\n    doReturn(CompletableFuture.completedFuture(\"bar\")).when(readFn).getAsync(any());\r\n    Map<String, String> result = new HashMap<>();\r\n    result.put(\"foo\", \"bar\");\r\n    doReturn(CompletableFuture.completedFuture(result)).when(readFn).getAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    int times = 0;\r\n    table.init(TestRemoteTable.getMockContext());\r\n    verify(readFn, times(0)).init(any(), any());\r\n    assertEquals(\"bar\", table.getAsync(\"foo\").join());\r\n    verify(readFn, times(1)).getAsync(any());\r\n    assertEquals(++times, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(result, table.getAllAsync(Arrays.asList(\"foo\")).join());\r\n    verify(readFn, times(1)).getAllAsync(any());\r\n    assertEquals(++times, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.retryTimer.getSnapshot().getMax());\r\n    assertEquals(0, table.readRetryMetrics.permFailureCount.getCount());\r\n    assertNull(table.writeRetryMetrics);\r\n    table.close();\r\n    verify(readFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetWithRetryDisabled",
  "sourceCode" : "@Test\r\npublic void testGetWithRetryDisabled() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    policy.withStopAfterDelay(Duration.ofMillis(100));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(false).when(readFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(readFn).getAsync(anyString());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.getAsync(\"foo\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(readFn, times(1)).getAsync(any());\r\n    assertEquals(0, table.readRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.permFailureCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.retryTimer.getSnapshot().getMax());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetAllWithOneRetry",
  "sourceCode" : "@Test\r\npublic void testGetAllWithOneRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(true).when(readFn).isRetriable(any());\r\n    AtomicInteger times = new AtomicInteger();\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"foo1\", \"bar1\");\r\n    map.put(\"foo2\", \"bar2\");\r\n    doAnswer(invocation -> {\r\n        CompletableFuture<Map<String, String>> future = new CompletableFuture();\r\n        if (times.get() > 0) {\r\n            future.complete(map);\r\n        } else {\r\n            times.incrementAndGet();\r\n            future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n        }\r\n        return future;\r\n    }).when(readFn).getAllAsync(anyCollection());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    assertEquals(map, table.getAllAsync(Arrays.asList(\"foo1\", \"foo2\")).join());\r\n    verify(readFn, times(2)).getAllAsync(any());\r\n    assertEquals(1, table.readRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.readRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetWithPermFailureOnTimeout",
  "sourceCode" : "@Test\r\npublic void testGetWithPermFailureOnTimeout() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(5));\r\n    policy.withStopAfterDelay(Duration.ofMillis(100));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(true).when(readFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(readFn).getAsync(anyString());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.getAsync(\"foo\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(readFn, atLeast(3)).getAsync(any());\r\n    assertTrue(table.readRetryMetrics.retryCount.getCount() >= 3);\r\n    assertEquals(0, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(1, table.readRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.readRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testGetWithPermFailureOnMaxCount",
  "sourceCode" : "@Test\r\npublic void testGetWithPermFailureOnMaxCount() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(5));\r\n    policy.withStopAfterAttempts(10);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    doReturn(true).when(readFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(readFn).getAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, null);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, policy, null, schedExec, readFn, null);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.getAsync(\"foo\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(readFn, atLeast(11)).getAsync(any());\r\n    assertEquals(10, table.readRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.readRetryMetrics.successCount.getCount());\r\n    assertEquals(1, table.readRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.readRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutUpdateAndDeleteDelegation",
  "sourceCode" : "@Test\r\npublic void testPutUpdateAndDeleteDelegation() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).writeAsync(anyInt(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    // PutAsync\r\n    verify(writeFn, times(0)).putAsync(any(), any());\r\n    verify(writeFn, times(0)).putAsync(any(), any(), any());\r\n    table.putAsync(1, 2).join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    verify(writeFn, times(0)).putAsync(any(), any(), any());\r\n    table.putAsync(1, 2, 3).join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    verify(writeFn, times(1)).putAsync(any(), any(), any());\r\n    // PutAllAsync\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection(), any());\r\n    table.putAllAsync(Arrays.asList(1)).join();\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).putAllAsync(anyCollection(), any());\r\n    table.putAllAsync(Arrays.asList(1), Arrays.asList(1)).join();\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).putAllAsync(anyCollection(), any());\r\n    // UpdateAsync\r\n    verify(writeFn, times(0)).updateAsync(any(), any());\r\n    table.updateAsync(1, 2).join();\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n    // UpdateAllAsync\r\n    verify(writeFn, times(0)).updateAllAsync(anyCollection());\r\n    table.updateAllAsync(Arrays.asList(new Entry<>(1, 2))).join();\r\n    verify(writeFn, times(1)).updateAllAsync(anyCollection());\r\n    // DeleteAsync\r\n    verify(writeFn, times(0)).deleteAsync(any());\r\n    verify(writeFn, times(0)).deleteAsync(any(), any());\r\n    table.deleteAsync(1).join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    verify(writeFn, times(0)).deleteAsync(any(), any());\r\n    table.deleteAsync(1, 2).join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    verify(writeFn, times(1)).deleteAsync(any(), any());\r\n    // DeleteAllAsync\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection(), any());\r\n    table.deleteAllAsync(Arrays.asList(1)).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(0)).deleteAllAsync(anyCollection(), any());\r\n    table.deleteAllAsync(Arrays.asList(1), Arrays.asList(2)).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection());\r\n    verify(writeFn, times(1)).deleteAllAsync(anyCollection(), any());\r\n    // WriteAsync\r\n    verify(writeFn, times(0)).writeAsync(anyInt(), any());\r\n    table.writeAsync(1, 2).join();\r\n    verify(writeFn, times(1)).writeAsync(anyInt(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutWithoutRetry",
  "sourceCode" : "@Test\r\npublic void testPutWithoutRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).putAllAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAsync(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).deleteAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    int times = 0;\r\n    table.init(TestRemoteTable.getMockContext());\r\n    verify(readFn, times(0)).init(any(), any());\r\n    verify(writeFn, times(0)).init(any(), any());\r\n    table.putAsync(\"foo\", \"bar\").join();\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    table.putAllAsync(Arrays.asList(new Entry(\"1\", \"2\"))).join();\r\n    verify(writeFn, times(1)).putAllAsync(any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    table.deleteAsync(\"1\").join();\r\n    verify(writeFn, times(1)).deleteAsync(any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    table.deleteAllAsync(Arrays.asList(\"1\", \"2\")).join();\r\n    verify(writeFn, times(1)).deleteAllAsync(any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryTimer.getSnapshot().getMax());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertNull(table.readRetryMetrics);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutWithRetryDisabled",
  "sourceCode" : "@Test\r\npublic void testPutWithRetryDisabled() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    policy.withStopAfterDelay(Duration.ofMillis(100));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(false).when(writeFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(writeFn).putAsync(any(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.putAsync(\"foo\", \"bar\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(writeFn, times(1)).putAsync(any(), any());\r\n    assertEquals(0, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryTimer.getSnapshot().getMax());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutAllWithOneRetry",
  "sourceCode" : "@Test\r\npublic void testPutAllWithOneRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    AtomicInteger times = new AtomicInteger();\r\n    doAnswer(invocation -> {\r\n        CompletableFuture<Map<String, String>> future = new CompletableFuture();\r\n        if (times.get() > 0) {\r\n            future.complete(null);\r\n        } else {\r\n            times.incrementAndGet();\r\n            future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n        }\r\n        return future;\r\n    }).when(writeFn).putAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    table.putAllAsync(Arrays.asList(new Entry(1, 2))).join();\r\n    verify(writeFn, times(2)).putAllAsync(any());\r\n    assertEquals(1, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutWithPermFailureOnTimeout",
  "sourceCode" : "@Test\r\npublic void testPutWithPermFailureOnTimeout() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(5));\r\n    policy.withStopAfterDelay(Duration.ofMillis(100));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(readFn).getAsync(anyString());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.putAsync(\"foo\", \"bar\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(writeFn, atLeast(3)).putAsync(any(), any());\r\n    assertTrue(table.writeRetryMetrics.retryCount.getCount() >= 3);\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(1, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testPutWithPermFailureOnMaxCount",
  "sourceCode" : "@Test\r\npublic void testPutWithPermFailureOnMaxCount() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(5));\r\n    policy.withStopAfterAttempts(10);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(writeFn).putAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.putAllAsync(Arrays.asList(new Entry(1, 2))).join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(writeFn, atLeast(11)).putAllAsync(any());\r\n    assertEquals(10, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(1, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testUpdateWithoutRetry",
  "sourceCode" : "@Test\r\npublic void testUpdateWithoutRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAsync(any(), any());\r\n    doReturn(CompletableFuture.completedFuture(null)).when(writeFn).updateAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    int times = 0;\r\n    table.init(TestRemoteTable.getMockContext());\r\n    verify(readFn, times(0)).init(any(), any());\r\n    verify(writeFn, times(0)).init(any(), any());\r\n    table.updateAsync(\"foo\", \"bar\").join();\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    table.updateAllAsync(Arrays.asList(new Entry<>(\"1\", \"2\"))).join();\r\n    verify(writeFn, times(1)).updateAllAsync(any());\r\n    assertEquals(++times, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryTimer.getSnapshot().getMax());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertNull(table.readRetryMetrics);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testUpdateWithRetryDisabled",
  "sourceCode" : "@Test\r\npublic void testUpdateWithRetryDisabled() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    policy.withStopAfterDelay(Duration.ofMillis(100));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(false).when(writeFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(writeFn).updateAsync(any(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.updateAsync(\"foo\", \"bar\").join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(writeFn, times(1)).updateAsync(any(), any());\r\n    assertEquals(0, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.retryTimer.getSnapshot().getMax());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testUpdateWithOneRetry",
  "sourceCode" : "@Test\r\npublic void testUpdateWithOneRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    AtomicInteger times = new AtomicInteger();\r\n    doAnswer(invocation -> {\r\n        CompletableFuture<Map<String, String>> future = new CompletableFuture();\r\n        if (times.get() > 0) {\r\n            future.complete(null);\r\n        } else {\r\n            times.incrementAndGet();\r\n            future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n        }\r\n        return future;\r\n    }).when(writeFn).updateAsync(any(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    table.updateAsync(1, 2).join();\r\n    verify(writeFn, times(2)).updateAsync(any(), any());\r\n    assertEquals(1, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testUpdateAllWithOneRetry",
  "sourceCode" : "@Test\r\npublic void testUpdateAllWithOneRetry() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(10));\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    AtomicInteger times = new AtomicInteger();\r\n    doAnswer(invocation -> {\r\n        CompletableFuture<Map<String, String>> future = new CompletableFuture();\r\n        if (times.get() > 0) {\r\n            future.complete(null);\r\n        } else {\r\n            times.incrementAndGet();\r\n            future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n        }\r\n        return future;\r\n    }).when(writeFn).updateAllAsync(any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    table.updateAllAsync(Arrays.asList(new Entry(1, 2))).join();\r\n    verify(writeFn, times(2)).updateAllAsync(any());\r\n    assertEquals(1, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testUpdateWithPermFailureOnMaxCount",
  "sourceCode" : "@Test\r\npublic void testUpdateWithPermFailureOnMaxCount() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(5));\r\n    policy.withStopAfterAttempts(5);\r\n    TableReadFunction<String, String> readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writeFn = mock(TableWriteFunction.class);\r\n    doReturn(true).when(writeFn).isRetriable(any());\r\n    CompletableFuture<String> future = new CompletableFuture();\r\n    future.completeExceptionally(new RuntimeException(\"test exception\"));\r\n    doReturn(future).when(writeFn).updateAsync(any(), any());\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.init(TestRemoteTable.getMockContext());\r\n    try {\r\n        table.updateAsync(1, 2).join();\r\n        fail();\r\n    } catch (Throwable t) {\r\n    }\r\n    verify(writeFn, atLeast(6)).updateAsync(any(), any());\r\n    assertEquals(5, table.writeRetryMetrics.retryCount.getCount());\r\n    assertEquals(0, table.writeRetryMetrics.successCount.getCount());\r\n    assertEquals(1, table.writeRetryMetrics.permFailureCount.getCount());\r\n    assertTrue(table.writeRetryMetrics.retryTimer.getSnapshot().getMax() > 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestAsyncRetriableTable.java",
  "methodName" : "testFlushAndClose",
  "sourceCode" : "@Test\r\npublic void testFlushAndClose() {\r\n    TableRetryPolicy policy = new TableRetryPolicy();\r\n    policy.withFixedBackoff(Duration.ofMillis(100));\r\n    TableReadFunction readFn = mock(TableReadFunction.class);\r\n    TableWriteFunction writeFn = mock(TableWriteFunction.class);\r\n    AsyncReadWriteUpdateTable delegate = new AsyncRemoteTable(readFn, writeFn);\r\n    AsyncRetriableTable table = new AsyncRetriableTable(\"t1\", delegate, null, policy, schedExec, readFn, writeFn);\r\n    table.flush();\r\n    verify(writeFn, times(1)).flush();\r\n    table.close();\r\n    verify(readFn, times(1)).close();\r\n    verify(writeFn, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestTableRetryPolicy.java",
  "methodName" : "testNoRetry",
  "sourceCode" : "@Test\r\npublic void testNoRetry() {\r\n    TableRetryPolicy retryPolicy = new TableRetryPolicy();\r\n    Assert.assertEquals(TableRetryPolicy.BackoffType.NONE, retryPolicy.getBackoffType());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestTableRetryPolicy.java",
  "methodName" : "testFixedRetry",
  "sourceCode" : "@Test\r\npublic void testFixedRetry() {\r\n    TableRetryPolicy retryPolicy = new TableRetryPolicy();\r\n    retryPolicy.withFixedBackoff(Duration.ofMillis(1000));\r\n    retryPolicy.withJitter(Duration.ofMillis(100));\r\n    retryPolicy.withStopAfterAttempts(4);\r\n    Assert.assertEquals(TableRetryPolicy.BackoffType.FIXED, retryPolicy.getBackoffType());\r\n    RetryPolicy fsRetry = FailsafeAdapter.valueOf(retryPolicy);\r\n    Assert.assertEquals(1000, fsRetry.getDelay().toMillis());\r\n    Assert.assertEquals(100, fsRetry.getJitter().toMillis());\r\n    Assert.assertEquals(4, fsRetry.getMaxRetries());\r\n    Assert.assertNotNull(retryPolicy.getRetryPredicate());\r\n    Assert.assertEquals(\"{\\\"sleepTime\\\":{\\\"seconds\\\":1,\\\"nanos\\\":0},\\\"exponentialFactor\\\":0.0,\" + \"\\\"jitter\\\":{\\\"seconds\\\":0,\\\"nanos\\\":100000000},\\\"maxAttempts\\\":4,\\\"backoffType\\\":\\\"FIXED\\\",\" + \"\\\"retryPredicate\\\":{}}\", retryPolicy.toConfig(null, null).get(\"TableRetryPolicy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestTableRetryPolicy.java",
  "methodName" : "testRandomRetry",
  "sourceCode" : "@Test\r\npublic void testRandomRetry() {\r\n    TableRetryPolicy retryPolicy = new TableRetryPolicy();\r\n    retryPolicy.withRandomBackoff(Duration.ofMillis(1000), Duration.ofMillis(2000));\r\n    // no-op\r\n    retryPolicy.withJitter(Duration.ofMillis(100));\r\n    Assert.assertEquals(TableRetryPolicy.BackoffType.RANDOM, retryPolicy.getBackoffType());\r\n    RetryPolicy fsRetry = FailsafeAdapter.valueOf(retryPolicy);\r\n    Assert.assertEquals(1000, fsRetry.getDelayMin().toMillis());\r\n    Assert.assertEquals(2000, fsRetry.getDelayMax().toMillis());\r\n    Assert.assertEquals(\"{\\\"randomMin\\\":{\\\"seconds\\\":1,\\\"nanos\\\":0},\\\"randomMax\\\":{\\\"seconds\\\":2,\\\"nanos\\\":0},\" + \"\\\"exponentialFactor\\\":0.0,\\\"backoffType\\\":\\\"RANDOM\\\",\\\"retryPredicate\\\":{}}\", retryPolicy.toConfig(null, null).get(\"TableRetryPolicy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestTableRetryPolicy.java",
  "methodName" : "testExponentialRetry",
  "sourceCode" : "@Test\r\npublic void testExponentialRetry() {\r\n    TableRetryPolicy retryPolicy = new TableRetryPolicy();\r\n    retryPolicy.withExponentialBackoff(Duration.ofMillis(1000), Duration.ofMillis(2000), 1.5);\r\n    retryPolicy.withJitter(Duration.ofMillis(100));\r\n    Assert.assertEquals(TableRetryPolicy.BackoffType.EXPONENTIAL, retryPolicy.getBackoffType());\r\n    RetryPolicy fsRetry = FailsafeAdapter.valueOf(retryPolicy);\r\n    Assert.assertEquals(1000, fsRetry.getDelay().toMillis());\r\n    Assert.assertEquals(2000, fsRetry.getMaxDelay().toMillis());\r\n    Assert.assertEquals(1.5, fsRetry.getDelayFactor(), 0.001);\r\n    Assert.assertEquals(100, fsRetry.getJitter().toMillis());\r\n    Assert.assertEquals(\"{\\\"sleepTime\\\":{\\\"seconds\\\":1,\\\"nanos\\\":0},\\\"exponentialFactor\\\":1.5,\" + \"\\\"exponentialMaxSleep\\\":{\\\"seconds\\\":2,\\\"nanos\\\":0},\\\"jitter\\\":{\\\"seconds\\\":0,\\\"nanos\\\":100000000},\" + \"\\\"backoffType\\\":\\\"EXPONENTIAL\\\",\\\"retryPredicate\\\":{}}\", retryPolicy.toConfig(null, null).get(\"TableRetryPolicy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\retry\\TestTableRetryPolicy.java",
  "methodName" : "testCustomRetryPredicate",
  "sourceCode" : "@Test\r\npublic void testCustomRetryPredicate() {\r\n    TableRetryPolicy retryPolicy = new TableRetryPolicy();\r\n    retryPolicy.withRetryPredicate((e) -> e instanceof IllegalArgumentException);\r\n    Assert.assertTrue(retryPolicy.getRetryPredicate().test(new IllegalArgumentException()));\r\n    Assert.assertFalse(retryPolicy.getRetryPredicate().test(new NullPointerException()));\r\n    Assert.assertEquals(\"{\\\"exponentialFactor\\\":0.0,\\\"backoffType\\\":\\\"NONE\\\",\\\"retryPredicate\\\":{}}\", retryPolicy.toConfig(null, null).get(\"TableRetryPolicy\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\TestTableConfigGenerator.java",
  "methodName" : "testWithSerdes",
  "sourceCode" : "@Test\r\npublic void testWithSerdes() {\r\n    List<TableDescriptor> descriptors = Arrays.asList(new MockLocalTableDescriptor(\"t1\", KVSerde.of(new StringSerde(), new IntegerSerde())), new MockLocalTableDescriptor(\"t2\", KVSerde.of(new StringSerde(), new IntegerSerde())));\r\n    Config jobConfig = new MapConfig(TableConfigGenerator.generateSerdeConfig(descriptors));\r\n    JavaTableConfig javaTableConfig = new JavaTableConfig(jobConfig);\r\n    assertNotNull(javaTableConfig.getKeySerde(\"t1\"));\r\n    assertNotNull(javaTableConfig.getMsgSerde(\"t1\"));\r\n    assertNotNull(javaTableConfig.getKeySerde(\"t2\"));\r\n    assertNotNull(javaTableConfig.getMsgSerde(\"t2\"));\r\n    MapConfig tableConfig = new MapConfig(TableConfigGenerator.generate(jobConfig, descriptors));\r\n    javaTableConfig = new JavaTableConfig(tableConfig);\r\n    assertNotNull(javaTableConfig.getTableProviderFactory(\"t1\"));\r\n    assertNotNull(javaTableConfig.getTableProviderFactory(\"t2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\TestTableManager.java",
  "methodName" : "testInitByConfig",
  "sourceCode" : "@Test\r\npublic void testInitByConfig() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(String.format(JavaTableConfig.TABLE_PROVIDER_FACTORY, TABLE_ID), DummyTableProviderFactory.class.getName());\r\n    map.put(String.format(\"tables.%s.some.config\", TABLE_ID), \"xyz\");\r\n    addKeySerde(map);\r\n    addValueSerde(map);\r\n    doTestInit(map);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\TestTableManager.java",
  "methodName" : "testInitFailsWithoutProviderFactory",
  "sourceCode" : "@Test(expected = Exception.class)\r\npublic void testInitFailsWithoutProviderFactory() {\r\n    Map<String, String> map = new HashMap<>();\r\n    addKeySerde(map);\r\n    addValueSerde(map);\r\n    doTestInit(map);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\table\\TestTableManager.java",
  "methodName" : "testInitFailsWithoutInitializingLocalStores",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testInitFailsWithoutInitializingLocalStores() {\r\n    TableManager tableManager = new TableManager(new MapConfig(new HashMap<>()));\r\n    tableManager.getTable(\"dummy\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestAsyncStreamAdapter.java",
  "methodName" : "testAdapterWithoutThreadPool",
  "sourceCode" : "@Test\r\npublic void testAdapterWithoutThreadPool() throws Exception {\r\n    taskAdaptor = new AsyncStreamTaskAdapter(task, null);\r\n    TestCallbackListener listener = new TestCallbackListener();\r\n    TaskCallback callback = new TaskCallbackImpl(listener, null, envelope, null, 0L, 0L);\r\n    taskAdaptor.init(null);\r\n    assertTrue(task.inited);\r\n    taskAdaptor.processAsync(null, null, null, callback);\r\n    assertTrue(task.processed);\r\n    assertTrue(listener.callbackComplete);\r\n    e = new Exception(\"dummy exception\");\r\n    taskAdaptor.processAsync(null, null, null, callback);\r\n    assertTrue(listener.callbackFailure);\r\n    taskAdaptor.window(null, null);\r\n    assertTrue(task.windowed);\r\n    taskAdaptor.close();\r\n    assertTrue(task.closed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestAsyncStreamAdapter.java",
  "methodName" : "testAdapterWithThreadPool",
  "sourceCode" : "@Test\r\npublic void testAdapterWithThreadPool() throws Exception {\r\n    TestCallbackListener listener1 = new TestCallbackListener();\r\n    TaskCallback callback1 = new TaskCallbackImpl(listener1, null, envelope, null, 0L, 0L);\r\n    TestCallbackListener listener2 = new TestCallbackListener();\r\n    TaskCallback callback2 = new TaskCallbackImpl(listener2, null, envelope, null, 1L, 0L);\r\n    ExecutorService executor = Executors.newFixedThreadPool(2);\r\n    taskAdaptor = new AsyncStreamTaskAdapter(task, executor);\r\n    taskAdaptor.processAsync(null, null, null, callback1);\r\n    taskAdaptor.processAsync(null, null, null, callback2);\r\n    executor.awaitTermination(1, TimeUnit.SECONDS);\r\n    assertTrue(listener1.callbackComplete);\r\n    assertTrue(listener2.callbackComplete);\r\n    e = new Exception(\"dummy exception\");\r\n    taskAdaptor.processAsync(null, null, null, callback1);\r\n    taskAdaptor.processAsync(null, null, null, callback2);\r\n    executor.awaitTermination(1, TimeUnit.SECONDS);\r\n    assertTrue(listener1.callbackFailure);\r\n    assertTrue(listener2.callbackFailure);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestCoordinatorRequests.java",
  "methodName" : "testUpdateCommit",
  "sourceCode" : "@Test\r\npublic void testUpdateCommit() {\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskA);\r\n    coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    coordinatorRequests.update(coordinator);\r\n    assertTrue(coordinatorRequests.commitRequests().contains(taskA));\r\n    coordinator = new ReadableCoordinator(taskC);\r\n    coordinator.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    coordinatorRequests.update(coordinator);\r\n    assertTrue(coordinatorRequests.commitRequests().contains(taskC));\r\n    assertFalse(coordinatorRequests.commitRequests().contains(taskB));\r\n    assertTrue(coordinatorRequests.commitRequests().size() == 2);\r\n    coordinator.commit(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n    coordinatorRequests.update(coordinator);\r\n    assertTrue(coordinatorRequests.commitRequests().contains(taskB));\r\n    assertTrue(coordinatorRequests.commitRequests().size() == 3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestCoordinatorRequests.java",
  "methodName" : "testUpdateShutdownOnConsensus",
  "sourceCode" : "@Test\r\npublic void testUpdateShutdownOnConsensus() {\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskA);\r\n    coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    coordinatorRequests.update(coordinator);\r\n    assertFalse(coordinatorRequests.shouldShutdownNow());\r\n    coordinator = new ReadableCoordinator(taskB);\r\n    coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    coordinatorRequests.update(coordinator);\r\n    assertFalse(coordinatorRequests.shouldShutdownNow());\r\n    coordinator = new ReadableCoordinator(taskC);\r\n    coordinator.shutdown(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    coordinatorRequests.update(coordinator);\r\n    assertTrue(coordinatorRequests.shouldShutdownNow());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestCoordinatorRequests.java",
  "methodName" : "testUpdateShutdownNow",
  "sourceCode" : "@Test\r\npublic void testUpdateShutdownNow() {\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskA);\r\n    coordinator.shutdown(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n    coordinatorRequests.update(coordinator);\r\n    assertTrue(coordinatorRequests.shouldShutdownNow());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestDefaultTaskExecutorFactory.java",
  "methodName" : "testGetTaskExecutor",
  "sourceCode" : "@Test\r\npublic void testGetTaskExecutor() {\r\n    DefaultTaskExecutorFactory factory = new DefaultTaskExecutorFactory();\r\n    Map<String, String> mapConfig = new HashMap<>();\r\n    int poolSize = 12;\r\n    mapConfig.put(JOB_CONTAINER_THREAD_POOL_SIZE, String.valueOf(poolSize));\r\n    Config config = new MapConfig(mapConfig);\r\n    ExecutorService executor = factory.getTaskExecutor(config);\r\n    assertEquals(poolSize, ((ThreadPoolExecutor) executor).getCorePoolSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestDefaultTaskExecutorFactory.java",
  "methodName" : "testGetTaskExecutorFactory",
  "sourceCode" : "@Test\r\npublic void testGetTaskExecutorFactory() {\r\n    Map<String, String> mapConfig = new HashMap<>();\r\n    mapConfig.put(JOB_CONTAINER_TASK_EXECUTOR_FACTORY, MockTaskExecutorFactory.class.getName());\r\n    JobConfig config = new JobConfig(new MapConfig(mapConfig));\r\n    String taskExecutorFactoryClassName = config.getTaskExecutorFactory();\r\n    TaskExecutorFactory taskExecutorFactory = ReflectionUtil.getObj(taskExecutorFactoryClassName, TaskExecutorFactory.class);\r\n    Assert.assertTrue(taskExecutorFactory instanceof MockTaskExecutorFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestDefaultTaskExecutorFactory.java",
  "methodName" : "getOperatorTaskFactoryWithContainerThreadPoolGreaterThanOne",
  "sourceCode" : "@Test\r\npublic void getOperatorTaskFactoryWithContainerThreadPoolGreaterThanOne() {\r\n    DefaultTaskExecutorFactory factory = new DefaultTaskExecutorFactory();\r\n    int poolSize = 5;\r\n    Map<String, String> config = ImmutableMap.of(JOB_CONTAINER_THREAD_POOL_SIZE, Integer.toString(poolSize));\r\n    ExecutorService operatorExecutor = factory.getOperatorExecutor(mock(TaskName.class), new MapConfig(config));\r\n    assertEquals(poolSize, ((ThreadPoolExecutor) operatorExecutor).getCorePoolSize());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestDefaultTaskExecutorFactory.java",
  "methodName" : "getOperatorTaskFactoryWithThreadContainerPoolLessThanOne",
  "sourceCode" : "@Test\r\npublic void getOperatorTaskFactoryWithThreadContainerPoolLessThanOne() {\r\n    DefaultTaskExecutorFactory factory = new DefaultTaskExecutorFactory();\r\n    Map<String, String> config = ImmutableMap.of(JOB_CONTAINER_THREAD_POOL_SIZE, \"-1\");\r\n    ExecutorService operatorExecutor = factory.getOperatorExecutor(mock(TaskName.class), new MapConfig(config));\r\n    assertFalse(operatorExecutor instanceof ThreadPoolExecutor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestDefaultTaskExecutorFactory.java",
  "methodName" : "getOperatorTaskFactoryForSameTask",
  "sourceCode" : "@Test\r\npublic void getOperatorTaskFactoryForSameTask() {\r\n    DefaultTaskExecutorFactory factory = new DefaultTaskExecutorFactory();\r\n    TaskName taskName = mock(TaskName.class);\r\n    Map<String, String> config = ImmutableMap.of(JOB_CONTAINER_THREAD_POOL_SIZE, \"0\");\r\n    ExecutorService taskExecutor = factory.getOperatorExecutor(taskName, new MapConfig(config));\r\n    assertEquals(\"Expected same executor instance to be returned for the same task within a JVM instance\", taskExecutor, factory.getOperatorExecutor(taskName, new MapConfig(config)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestStreamOperatorTask.java",
  "methodName" : "testCloseDuringInitializationErrors",
  "sourceCode" : "@Test\r\npublic void testCloseDuringInitializationErrors() throws Exception {\r\n    Context context = mock(Context.class);\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(context.getJobContext()).thenReturn(jobContext);\r\n    doThrow(new RuntimeException(\"Failed to get config\")).when(jobContext).getConfig();\r\n    StreamOperatorTask operatorTask = new StreamOperatorTask(mock(OperatorSpecGraph.class), mock(Clock.class));\r\n    try {\r\n        operatorTask.init(context);\r\n    } catch (RuntimeException e) {\r\n        if (e instanceof NullPointerException) {\r\n            fail(\"Unexpected null pointer exception\");\r\n        }\r\n    }\r\n    operatorTask.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestStreamOperatorTask.java",
  "methodName" : "testExceptionsInProcessInvokesTaskCallback",
  "sourceCode" : "/**\r\n * Pass an invalid IME to processAsync. Any exceptions in processAsync should still get propagated through the\r\n * task callback.\r\n */\r\n@Test\r\npublic void testExceptionsInProcessInvokesTaskCallback() throws InterruptedException {\r\n    ExecutorService taskThreadPool = Executors.newFixedThreadPool(2);\r\n    TaskCallback mockTaskCallback = mock(TaskCallback.class);\r\n    MessageCollector mockMessageCollector = mock(MessageCollector.class);\r\n    TaskCoordinator mockTaskCoordinator = mock(TaskCoordinator.class);\r\n    StreamOperatorTask operatorTask = new StreamOperatorTask(mock(OperatorSpecGraph.class));\r\n    operatorTask.setTaskThreadPool(taskThreadPool);\r\n    CountDownLatch failureLatch = new CountDownLatch(1);\r\n    doAnswer(ctx -> {\r\n        failureLatch.countDown();\r\n        return null;\r\n    }).when(mockTaskCallback).failure(anyObject());\r\n    operatorTask.processAsync(mock(IncomingMessageEnvelope.class), mockMessageCollector, mockTaskCoordinator, mockTaskCallback);\r\n    failureLatch.await();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestStreamOperatorTask.java",
  "methodName" : "testExceptionIfInputOperatorMissing",
  "sourceCode" : "/**\r\n * Tests if the appropriate SamzaException is propagated to the TaskCallback if there is no InputOperator for a given\r\n * SystemStream in the OperatorGraph.\r\n */\r\n@Test\r\npublic void testExceptionIfInputOperatorMissing() throws NoSuchFieldException, IllegalAccessException {\r\n    IncomingMessageEnvelope mockIme = mock(IncomingMessageEnvelope.class, RETURNS_DEEP_STUBS);\r\n    SystemStream testSystemStream = new SystemStream(\"foo\", \"bar\");\r\n    when(mockIme.getSystemStreamPartition().getSystemStream()).thenReturn(testSystemStream);\r\n    OperatorImplGraph mockOperatorImplGraph = mock(OperatorImplGraph.class);\r\n    when(mockOperatorImplGraph.getInputOperator(anyObject())).thenReturn(null);\r\n    StreamOperatorTask operatorTask = new StreamOperatorTask(mock(OperatorSpecGraph.class));\r\n    operatorTask.setOperatorImplGraph(mockOperatorImplGraph);\r\n    TaskCallback mockTaskCallback = mock(TaskCallback.class);\r\n    operatorTask.processAsync(mockIme, mock(MessageCollector.class), mock(TaskCoordinator.class), mockTaskCallback);\r\n    ArgumentCaptor<Throwable> throwableCaptor = ArgumentCaptor.forClass(Throwable.class);\r\n    verify(mockTaskCallback, only()).failure(throwableCaptor.capture());\r\n    assertEquals(throwableCaptor.getValue().getClass(), SamzaException.class);\r\n    String expectedErrMessage = String.format(\"InputOperator not found in OperatorGraph for %s. The available input\" + \" operators are: %s. Please check SystemStream configuration for the `SystemConsumer` and/or task.inputs\" + \" task configuration.\", testSystemStream, mockOperatorImplGraph.getAllInputOperators());\r\n    assertEquals(throwableCaptor.getValue().getMessage(), expectedErrMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackImpl.java",
  "methodName" : "testComplete",
  "sourceCode" : "@Test\r\npublic void testComplete() {\r\n    callback.complete();\r\n    assertEquals(1L, completeCount.get());\r\n    assertEquals(0L, failureCount.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackImpl.java",
  "methodName" : "testFailure",
  "sourceCode" : "@Test\r\npublic void testFailure() {\r\n    callback.failure(new Exception(\"dummy exception\"));\r\n    assertEquals(0L, completeCount.get());\r\n    assertEquals(1L, failureCount.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackImpl.java",
  "methodName" : "testCallbackMultipleComplete",
  "sourceCode" : "@Test\r\npublic void testCallbackMultipleComplete() {\r\n    callback.complete();\r\n    assertEquals(1L, completeCount.get());\r\n    callback.complete();\r\n    assertEquals(1L, failureCount.get());\r\n    assertTrue(throwable instanceof IllegalStateException);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackImpl.java",
  "methodName" : "testCallbackFailureAfterComplete",
  "sourceCode" : "@Test\r\npublic void testCallbackFailureAfterComplete() {\r\n    callback.complete();\r\n    assertEquals(1L, completeCount.get());\r\n    callback.failure(new Exception(\"dummy exception\"));\r\n    assertEquals(1L, failureCount.get());\r\n    assertTrue(throwable instanceof IllegalStateException);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackImpl.java",
  "methodName" : "testMultithreadedCallbacks",
  "sourceCode" : "@Test\r\npublic void testMultithreadedCallbacks() throws Exception {\r\n    final CyclicBarrier barrier = new CyclicBarrier(2);\r\n    ExecutorService executor = Executors.newFixedThreadPool(2);\r\n    for (int i = 0; i < 2; i++) {\r\n        executor.submit(new Runnable() {\r\n\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    barrier.await();\r\n                    callback.complete();\r\n                } catch (Exception e) {\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        });\r\n    }\r\n    executor.awaitTermination(1, TimeUnit.SECONDS);\r\n    assertEquals(1L, completeCount.get());\r\n    assertEquals(1L, failureCount.get());\r\n    assertTrue(throwable instanceof IllegalStateException);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testCreateCallback",
  "sourceCode" : "@Test\r\npublic void testCreateCallback() {\r\n    TaskCallbackImpl callback = callbackManager.createCallback(new TaskName(\"Partition 0\"), mock(IncomingMessageEnvelope.class), null);\r\n    assertTrue(callback.matchSeqNum(0));\r\n    callback = callbackManager.createCallback(new TaskName(\"Partition 0\"), mock(IncomingMessageEnvelope.class), null);\r\n    assertTrue(callback.matchSeqNum(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testCreateDrainCallback",
  "sourceCode" : "@Test\r\npublic void testCreateDrainCallback() {\r\n    TaskCallbackImpl callback = callbackManager.createCallback(new TaskName(\"Partition 0\"), mock(IncomingMessageEnvelope.class), null, -1);\r\n    assertTrue(callback.matchSeqNum(0));\r\n    callback = callbackManager.createCallback(new TaskName(\"Partition 0\"), mock(IncomingMessageEnvelope.class), null, -1);\r\n    assertTrue(callback.matchSeqNum(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testUpdateCallbackInOrder",
  "sourceCode" : "@Test\r\npublic void testUpdateCallbackInOrder() {\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"kafka\", \"topic\", new Partition(0));\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskName);\r\n    IncomingMessageEnvelope envelope0 = new IncomingMessageEnvelope(ssp, \"0\", null, null);\r\n    TaskCallbackImpl callback0 = new TaskCallbackImpl(listener, taskName, envelope0, coordinator, 0, 0);\r\n    List<TaskCallbackImpl> callbacksToUpdate = callbackManager.updateCallback(callback0);\r\n    assertEquals(1, callbacksToUpdate.size());\r\n    TaskCallbackImpl callback = callbacksToUpdate.get(0);\r\n    assertTrue(callback.matchSeqNum(0));\r\n    assertEquals(ssp, callback.getSystemStreamPartition());\r\n    assertEquals(\"0\", callback.getOffset());\r\n    IncomingMessageEnvelope envelope1 = new IncomingMessageEnvelope(ssp, \"1\", null, null);\r\n    TaskCallbackImpl callback1 = new TaskCallbackImpl(listener, taskName, envelope1, coordinator, 1, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback1);\r\n    assertEquals(1, callbacksToUpdate.size());\r\n    callback = callbacksToUpdate.get(0);\r\n    assertTrue(callback.matchSeqNum(1));\r\n    assertEquals(ssp, callback.getSystemStreamPartition());\r\n    assertEquals(\"1\", callback.getOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testUpdateCallbackOutofOrder",
  "sourceCode" : "@Test\r\npublic void testUpdateCallbackOutofOrder() {\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"kafka\", \"topic\", new Partition(0));\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskName);\r\n    // simulate out of order\r\n    IncomingMessageEnvelope envelope2 = new IncomingMessageEnvelope(ssp, \"2\", null, null);\r\n    TaskCallbackImpl callback2 = new TaskCallbackImpl(listener, taskName, envelope2, coordinator, 2, 0);\r\n    List<TaskCallbackImpl> callbacksToUpdate = callbackManager.updateCallback(callback2);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope1 = new IncomingMessageEnvelope(ssp, \"1\", null, null);\r\n    TaskCallbackImpl callback1 = new TaskCallbackImpl(listener, taskName, envelope1, coordinator, 1, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback1);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope0 = new IncomingMessageEnvelope(ssp, \"0\", null, null);\r\n    TaskCallbackImpl callback0 = new TaskCallbackImpl(listener, taskName, envelope0, coordinator, 0, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback0);\r\n    assertEquals(3, callbacksToUpdate.size());\r\n    TaskCallbackImpl callback = callbacksToUpdate.get(0);\r\n    assertTrue(callback.matchSeqNum(0));\r\n    assertEquals(ssp, callback.getSystemStreamPartition());\r\n    assertEquals(\"0\", callback.getOffset());\r\n    callback = callbacksToUpdate.get(1);\r\n    assertTrue(callback.matchSeqNum(1));\r\n    assertEquals(ssp, callback.getSystemStreamPartition());\r\n    assertEquals(\"1\", callback.getOffset());\r\n    callback = callbacksToUpdate.get(2);\r\n    assertTrue(callback.matchSeqNum(2));\r\n    assertEquals(ssp, callback.getSystemStreamPartition());\r\n    assertEquals(\"2\", callback.getOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testUpdateCallbackWithCoordinatorRequests",
  "sourceCode" : "@Test\r\npublic void testUpdateCallbackWithCoordinatorRequests() {\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"kafka\", \"topic\", new Partition(0));\r\n    // simulate out of order\r\n    IncomingMessageEnvelope envelope2 = new IncomingMessageEnvelope(ssp, \"2\", null, null);\r\n    ReadableCoordinator coordinator2 = new ReadableCoordinator(taskName);\r\n    coordinator2.shutdown(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n    TaskCallbackImpl callback2 = new TaskCallbackImpl(listener, taskName, envelope2, coordinator2, 2, 0);\r\n    List<TaskCallbackImpl> callbacksToUpdate = callbackManager.updateCallback(callback2);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope1 = new IncomingMessageEnvelope(ssp, \"1\", null, null);\r\n    ReadableCoordinator coordinator1 = new ReadableCoordinator(taskName);\r\n    coordinator1.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    TaskCallbackImpl callback1 = new TaskCallbackImpl(listener, taskName, envelope1, coordinator1, 1, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback1);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope0 = new IncomingMessageEnvelope(ssp, \"0\", null, null);\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskName);\r\n    TaskCallbackImpl callback0 = new TaskCallbackImpl(listener, taskName, envelope0, coordinator, 0, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback0);\r\n    assertEquals(2, callbacksToUpdate.size());\r\n    //Check for envelope0\r\n    TaskCallbackImpl taskCallback = callbacksToUpdate.get(0);\r\n    assertTrue(taskCallback.matchSeqNum(0));\r\n    assertEquals(ssp, taskCallback.getSystemStreamPartition());\r\n    assertEquals(\"0\", taskCallback.getOffset());\r\n    //Check for envelope1\r\n    taskCallback = callbacksToUpdate.get(1);\r\n    assertTrue(taskCallback.matchSeqNum(1));\r\n    assertEquals(ssp, taskCallback.getSystemStreamPartition());\r\n    assertEquals(\"1\", taskCallback.getOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskCallbackManager.java",
  "methodName" : "testUpdateShouldReturnAllCompletedCallbacksTillTheCommitRequestDefined",
  "sourceCode" : "@Test\r\npublic void testUpdateShouldReturnAllCompletedCallbacksTillTheCommitRequestDefined() {\r\n    TaskName taskName = new TaskName(\"Partition 0\");\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(\"kafka\", \"topic\", new Partition(0));\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(\"kafka\", \"topic\", new Partition(0));\r\n    // Callback for Envelope3 contains commit request.\r\n    IncomingMessageEnvelope envelope3 = new IncomingMessageEnvelope(ssp2, \"0\", null, null);\r\n    ReadableCoordinator coordinator3 = new ReadableCoordinator(taskName);\r\n    coordinator3.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    TaskCallbackImpl callback3 = new TaskCallbackImpl(listener, taskName, envelope3, coordinator3, 3, 0);\r\n    List<TaskCallbackImpl> callbacksToUpdate = callbackManager.updateCallback(callback3);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope2 = new IncomingMessageEnvelope(ssp1, \"2\", null, null);\r\n    ReadableCoordinator coordinator2 = new ReadableCoordinator(taskName);\r\n    coordinator2.shutdown(TaskCoordinator.RequestScope.ALL_TASKS_IN_CONTAINER);\r\n    TaskCallbackImpl callback2 = new TaskCallbackImpl(listener, taskName, envelope2, coordinator2, 2, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback2);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    IncomingMessageEnvelope envelope1 = new IncomingMessageEnvelope(ssp1, \"1\", null, null);\r\n    ReadableCoordinator coordinator1 = new ReadableCoordinator(taskName);\r\n    coordinator1.commit(TaskCoordinator.RequestScope.CURRENT_TASK);\r\n    TaskCallbackImpl callback1 = new TaskCallbackImpl(listener, taskName, envelope1, coordinator1, 1, 0);\r\n    callbacksToUpdate = callbackManager.updateCallback(callback1);\r\n    assertTrue(callbacksToUpdate.isEmpty());\r\n    // Callback for Envelope0 contains commit request.\r\n    IncomingMessageEnvelope envelope0 = new IncomingMessageEnvelope(ssp1, \"0\", null, null);\r\n    ReadableCoordinator coordinator = new ReadableCoordinator(taskName);\r\n    TaskCallbackImpl callback0 = new TaskCallbackImpl(listener, taskName, envelope0, coordinator, 0, 0);\r\n    // Check for both Envelope1, Envelope2, Envelope3 in callbacks to commit.\r\n    // Two callbacks belonging to different system partition and has commitRequest defined is returned.\r\n    callbacksToUpdate = callbackManager.updateCallback(callback0);\r\n    assertEquals(2, callbacksToUpdate.size());\r\n    TaskCallbackImpl callback = callbacksToUpdate.get(0);\r\n    assertTrue(callback.matchSeqNum(0));\r\n    assertEquals(envelope0.getSystemStreamPartition(), callback.getSystemStreamPartition());\r\n    assertEquals(envelope0.getOffset(), callback.getOffset());\r\n    callback = callbacksToUpdate.get(1);\r\n    assertTrue(callback.matchSeqNum(1));\r\n    assertEquals(envelope1.getSystemStreamPartition(), callback.getSystemStreamPartition());\r\n    assertEquals(envelope1.getOffset(), callback.getOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testStreamTaskClass",
  "sourceCode" : "@Test\r\npublic void testStreamTaskClass() {\r\n    TaskFactory retFactory = TaskFactoryUtil.getTaskFactory(MockStreamTask.class.getName());\r\n    assertTrue(retFactory instanceof StreamTaskFactory);\r\n    assertTrue(((StreamTaskFactory) retFactory).createInstance() instanceof MockStreamTask);\r\n    try {\r\n        TaskFactoryUtil.getTaskFactory(\"no.such.class\");\r\n        fail(\"Should have failed w/ no.such.class\");\r\n    } catch (ConfigException cfe) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testAsyncStreamTask",
  "sourceCode" : "@Test\r\npublic void testAsyncStreamTask() {\r\n    TaskFactory retFactory = TaskFactoryUtil.getTaskFactory(MockAsyncStreamTask.class.getName());\r\n    assertTrue(retFactory instanceof AsyncStreamTaskFactory);\r\n    assertTrue(((AsyncStreamTaskFactory) retFactory).createInstance() instanceof MockAsyncStreamTask);\r\n    try {\r\n        TaskFactoryUtil.getTaskFactory(\"no.such.class\");\r\n        fail(\"Should have failed w/ no.such.class\");\r\n    } catch (ConfigException cfe) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testFinalizeTaskFactory",
  "sourceCode" : "@Test\r\npublic void testFinalizeTaskFactory() throws NoSuchFieldException, IllegalAccessException {\r\n    TaskFactory mockFactory = mock(TaskFactory.class);\r\n    try {\r\n        TaskFactoryUtil.finalizeTaskFactory(mockFactory, null);\r\n        fail(\"Should have failed with validation\");\r\n    } catch (SamzaException se) {\r\n        // expected\r\n    }\r\n    StreamTaskFactory mockStreamFactory = mock(StreamTaskFactory.class);\r\n    ExecutorService mockThreadPool = mock(ExecutorService.class);\r\n    TaskFactory retFactory = TaskFactoryUtil.finalizeTaskFactory(mockStreamFactory, mockThreadPool);\r\n    assertTrue(retFactory instanceof AsyncStreamTaskFactory);\r\n    assertTrue(((AsyncStreamTaskFactory) retFactory).createInstance() instanceof AsyncStreamTaskAdapter);\r\n    AsyncStreamTaskAdapter taskAdapter = (AsyncStreamTaskAdapter) ((AsyncStreamTaskFactory) retFactory).createInstance();\r\n    Field executorSrvFld = AsyncStreamTaskAdapter.class.getDeclaredField(\"executor\");\r\n    executorSrvFld.setAccessible(true);\r\n    ExecutorService executor = (ExecutorService) executorSrvFld.get(taskAdapter);\r\n    assertEquals(executor, mockThreadPool);\r\n    AsyncStreamTaskFactory mockAsyncStreamFactory = mock(AsyncStreamTaskFactory.class);\r\n    retFactory = TaskFactoryUtil.finalizeTaskFactory(mockAsyncStreamFactory, null);\r\n    assertEquals(retFactory, mockAsyncStreamFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testGetTaskFactoryWithStreamAppDescriptor",
  "sourceCode" : "// test getTaskFactory with StreamApplicationDescriptor\r\n@Test\r\npublic void testGetTaskFactoryWithStreamAppDescriptor() {\r\n    StreamApplicationDescriptorImpl mockStreamApp = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpecGraph mockSpecGraph = mock(OperatorSpecGraph.class);\r\n    when(mockStreamApp.getOperatorSpecGraph()).thenReturn(mockSpecGraph);\r\n    TaskFactory streamTaskFactory = TaskFactoryUtil.getTaskFactory(mockStreamApp);\r\n    assertTrue(streamTaskFactory instanceof AsyncStreamTaskFactory);\r\n    AsyncStreamTask streamTask = ((AsyncStreamTaskFactory) streamTaskFactory).createInstance();\r\n    assertTrue(streamTask instanceof StreamOperatorTask);\r\n    verify(mockSpecGraph).clone();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testGetTaskFactoryWithTaskAppDescriptor",
  "sourceCode" : "// test getTaskFactory with TaskApplicationDescriptor\r\n@Test\r\npublic void testGetTaskFactoryWithTaskAppDescriptor() {\r\n    TaskApplicationDescriptorImpl mockTaskApp = mock(TaskApplicationDescriptorImpl.class);\r\n    TaskFactory mockTaskFactory = mock(TaskFactory.class);\r\n    when(mockTaskApp.getTaskFactory()).thenReturn(mockTaskFactory);\r\n    TaskFactory taskFactory = TaskFactoryUtil.getTaskFactory(mockTaskApp);\r\n    assertEquals(mockTaskFactory, taskFactory);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\task\\TestTaskFactoryUtil.java",
  "methodName" : "testGetTaskFactoryWithInvalidAddDescriptorImpl",
  "sourceCode" : "// test getTaskFactory with invalid ApplicationDescriptorImpl\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetTaskFactoryWithInvalidAddDescriptorImpl() {\r\n    ApplicationDescriptorImpl mockInvalidApp = mock(ApplicationDescriptorImpl.class);\r\n    TaskFactoryUtil.getTaskFactory(mockInvalidApp);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testRewriteConfig",
  "sourceCode" : "@Test\r\npublic void testRewriteConfig() {\r\n    Map<String, String> baseConfigMap = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE);\r\n    // no rewriters\r\n    Map<String, String> fullConfig = new HashMap<>(baseConfigMap);\r\n    assertEquals(fullConfig, ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n    // rewriter that adds property\r\n    fullConfig = new HashMap<>(baseConfigMap);\r\n    fullConfig.put(JobConfig.CONFIG_REWRITERS, REWRITER_NAME);\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), NewPropertyRewriter.class.getName());\r\n    Map<String, String> expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(NEW_CONFIG_KEY, CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n    // rewriter that updates property\r\n    fullConfig = new HashMap<>(baseConfigMap);\r\n    fullConfig.put(JobConfig.CONFIG_REWRITERS, REWRITER_NAME);\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), UpdatePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(CONFIG_KEY, CONFIG_VALUE + CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n    // rewriter that removes property\r\n    fullConfig = new HashMap<>(baseConfigMap);\r\n    fullConfig.put(JobConfig.CONFIG_REWRITERS, REWRITER_NAME);\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), DeletePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.remove(CONFIG_KEY);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n    // only apply rewriters from rewriters list\r\n    fullConfig = new HashMap<>(baseConfigMap);\r\n    fullConfig.put(JobConfig.CONFIG_REWRITERS, OTHER_REWRITER_NAME);\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), NewPropertyRewriter.class.getName());\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, OTHER_REWRITER_NAME), UpdatePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(CONFIG_KEY, CONFIG_VALUE + CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n    // two rewriters; second rewriter overwrites configs from first\r\n    fullConfig = new HashMap<>(baseConfigMap);\r\n    fullConfig.put(JobConfig.CONFIG_REWRITERS, REWRITER_NAME + \",\" + OTHER_REWRITER_NAME);\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), NewPropertyRewriter.class.getName());\r\n    fullConfig.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, OTHER_REWRITER_NAME), UpdatePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(NEW_CONFIG_KEY, CONFIG_VALUE + CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.rewriteConfig(new MapConfig(fullConfig)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testRewriteConfigConfigRewritersEmptyString",
  "sourceCode" : "/**\r\n * This fails because Util will interpret the empty string value as a single rewriter which has the empty string as a\r\n * name, and there is no rewriter class config for a rewriter name which is the empty string.\r\n * TODO: should this be fixed to interpret the empty string as an empty list?\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testRewriteConfigConfigRewritersEmptyString() {\r\n    Config config = new MapConfig(ImmutableMap.of(JobConfig.CONFIG_REWRITERS, \"\"));\r\n    ConfigUtil.rewriteConfig(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testRewriteConfigNoClassForConfigRewriterName",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testRewriteConfigNoClassForConfigRewriterName() {\r\n    Config config = new MapConfig(ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, JobConfig.CONFIG_REWRITERS, \"unknownRewriter\"));\r\n    ConfigUtil.rewriteConfig(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testRewriteConfigRewriterClassDoesNotExist",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testRewriteConfigRewriterClassDoesNotExist() {\r\n    Config config = new MapConfig(ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, JobConfig.CONFIG_REWRITERS, REWRITER_NAME, String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), \"not_a_class\"));\r\n    ConfigUtil.rewriteConfig(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testApplyRewriter",
  "sourceCode" : "@Test\r\npublic void testApplyRewriter() {\r\n    // new property\r\n    Map<String, String> fullConfig = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), NewPropertyRewriter.class.getName());\r\n    Map<String, String> expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(NEW_CONFIG_KEY, CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.applyRewriter(new MapConfig(fullConfig), REWRITER_NAME));\r\n    // update property\r\n    fullConfig = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), UpdatePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.put(CONFIG_KEY, CONFIG_VALUE + CONFIG_VALUE);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.applyRewriter(new MapConfig(fullConfig), REWRITER_NAME));\r\n    // remove property\r\n    fullConfig = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), DeletePropertyRewriter.class.getName());\r\n    expectedConfigMap = new HashMap<>(fullConfig);\r\n    expectedConfigMap.remove(CONFIG_KEY);\r\n    assertEquals(new MapConfig(expectedConfigMap), ConfigUtil.applyRewriter(new MapConfig(fullConfig), REWRITER_NAME));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testApplyRewriterNoClassForConfigRewriterName",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testApplyRewriterNoClassForConfigRewriterName() {\r\n    Map<String, String> fullConfig = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE);\r\n    ConfigUtil.applyRewriter(new MapConfig(fullConfig), REWRITER_NAME);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testApplyRewriterClassDoesNotExist",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testApplyRewriterClassDoesNotExist() {\r\n    Map<String, String> fullConfig = ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), \"not_a_class\");\r\n    Config expectedConfig = new MapConfig(ImmutableMap.of(CONFIG_KEY, CONFIG_VALUE, NEW_CONFIG_KEY, CONFIG_VALUE));\r\n    assertEquals(expectedConfig, ConfigUtil.applyRewriter(new MapConfig(fullConfig), REWRITER_NAME));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testLoadConfigWithoutLoader",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testLoadConfigWithoutLoader() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(JobConfig.JOB_NAME, \"new-test-job\");\r\n    ConfigUtil.loadConfig(new MapConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestConfigUtil.java",
  "methodName" : "testLoadConfigWithOverridesAndRewrites",
  "sourceCode" : "@Test\r\npublic void testLoadConfigWithOverridesAndRewrites() {\r\n    Map<String, String> config = new HashMap<>();\r\n    config.put(JobConfig.CONFIG_LOADER_FACTORY, PropertiesConfigLoaderFactory.class.getCanonicalName());\r\n    config.put(JobConfig.JOB_NAME, \"new-test-job\");\r\n    config.put(JobConfig.CONFIG_REWRITERS, REWRITER_NAME);\r\n    config.put(CONFIG_KEY, CONFIG_VALUE);\r\n    config.put(String.format(JobConfig.CONFIG_REWRITER_CLASS, REWRITER_NAME), NewPropertyRewriter.class.getName());\r\n    config.put(PropertiesConfigLoaderFactory.CONFIG_LOADER_PROPERTIES_PREFIX + \"path\", getClass().getResource(\"/test.properties\").getPath());\r\n    Config actual = ConfigUtil.loadConfig(new MapConfig(config));\r\n    assertEquals(\"org.apache.samza.job.MockJobFactory\", actual.get(\"job.factory.class\"));\r\n    assertEquals(\"bar\", actual.get(\"foo\"));\r\n    // overridden value\r\n    assertEquals(\"new-test-job\", actual.get(\"job.name\"));\r\n    // rewritten value\r\n    assertEquals(CONFIG_VALUE, actual.get(NEW_CONFIG_KEY));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestDefaultCoordinatorStreamConfigFactory.java",
  "methodName" : "testBuildCoordinatorStreamConfigWithJobName",
  "sourceCode" : "@Test\r\npublic void testBuildCoordinatorStreamConfigWithJobName() {\r\n    Map<String, String> mapConfig = new HashMap<>();\r\n    mapConfig.put(\"job.name\", \"testName\");\r\n    mapConfig.put(\"job.id\", \"testId\");\r\n    mapConfig.put(\"job.coordinator.system\", \"testSamza\");\r\n    mapConfig.put(\"test.only\", \"nothing\");\r\n    mapConfig.put(\"systems.testSamza.test\", \"test\");\r\n    Config config = factory.buildCoordinatorStreamConfig(new MapConfig(mapConfig));\r\n    Map<String, String> expectedMap = new HashMap<>();\r\n    expectedMap.put(\"job.name\", \"testName\");\r\n    expectedMap.put(\"job.id\", \"testId\");\r\n    expectedMap.put(\"systems.testSamza.test\", \"test\");\r\n    expectedMap.put(JobConfig.JOB_COORDINATOR_SYSTEM, \"testSamza\");\r\n    expectedMap.put(JobConfig.MONITOR_PARTITION_CHANGE_FREQUENCY_MS, \"300000\");\r\n    assertEquals(config, new MapConfig(expectedMap));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestDefaultCoordinatorStreamConfigFactory.java",
  "methodName" : "testBuildCoordinatorStreamConfigWithoutJobName",
  "sourceCode" : "@Test(expected = ConfigException.class)\r\npublic void testBuildCoordinatorStreamConfigWithoutJobName() {\r\n    Map<String, String> mapConfig = new HashMap<>();\r\n    mapConfig.put(\"job.id\", \"testId\");\r\n    mapConfig.put(\"job.coordinator.system\", \"testSamza\");\r\n    mapConfig.put(\"test.only\", \"nothing\");\r\n    mapConfig.put(\"systems.testSamza.test\", \"test\");\r\n    factory.buildCoordinatorStreamConfig(new MapConfig(mapConfig));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestDiagnosticsUtil.java",
  "methodName" : "testBuildDiagnosticsManager",
  "sourceCode" : "@Test\r\npublic void testBuildDiagnosticsManager() {\r\n    Config config = new MapConfig(buildTestConfigs());\r\n    JobModel mockJobModel = mock(JobModel.class);\r\n    SystemFactory systemFactory = mock(SystemFactory.class);\r\n    SystemProducer mockProducer = mock(SystemProducer.class);\r\n    when(systemFactory.getProducer(anyString(), any(Config.class), any(MetricsRegistry.class), anyString())).thenReturn(mockProducer);\r\n    PowerMockito.mockStatic(ReflectionUtil.class);\r\n    when(ReflectionUtil.getObj(SYSTEM_FACTORY, SystemFactory.class)).thenReturn(systemFactory);\r\n    Optional<DiagnosticsManager> diagnosticsManager = DiagnosticsUtil.buildDiagnosticsManager(JOB_NAME, JOB_ID, mockJobModel, CONTAINER_ID, Optional.of(ENV_ID), Optional.of(SAMZA_EPOCH_ID), config);\r\n    Assert.assertTrue(diagnosticsManager.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testAcquire",
  "sourceCode" : "@Test\r\n@Ignore(\"Flaky Test: Test fails in travis.\")\r\npublic void testAcquire() {\r\n    RateLimiter rateLimiter = new EmbeddedTaggedRateLimiter(TARGET_RATE);\r\n    initRateLimiter(rateLimiter);\r\n    int count = 0;\r\n    long start = System.currentTimeMillis();\r\n    while (System.currentTimeMillis() - start < TEST_INTERVAL) {\r\n        rateLimiter.acquire(INCREMENT);\r\n        count += INCREMENT;\r\n    }\r\n    long rate = count * 1000 / TEST_INTERVAL;\r\n    verifyRate(rate);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testAcquireWithTimeout",
  "sourceCode" : "@Test\r\n@Ignore(\"Flaky Test.\")\r\npublic void testAcquireWithTimeout() {\r\n    RateLimiter rateLimiter = new EmbeddedTaggedRateLimiter(TARGET_RATE);\r\n    initRateLimiter(rateLimiter);\r\n    boolean hasSeenZeros = false;\r\n    int count = 0;\r\n    int callCount = 0;\r\n    long start = System.currentTimeMillis();\r\n    while (System.currentTimeMillis() - start < TEST_INTERVAL) {\r\n        ++callCount;\r\n        int availableCredits = rateLimiter.acquire(INCREMENT, 20, MILLISECONDS);\r\n        if (availableCredits <= 0) {\r\n            hasSeenZeros = true;\r\n        } else {\r\n            count += INCREMENT;\r\n        }\r\n    }\r\n    long rate = count * 1000 / TEST_INTERVAL;\r\n    verifyRate(rate);\r\n    junit.framework.Assert.assertTrue(Math.abs(callCount - TARGET_RATE_PER_TASK * TEST_INTERVAL / 1000 / INCREMENT) <= 2);\r\n    junit.framework.Assert.assertFalse(hasSeenZeros);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testFailsWhenUninitialized",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testFailsWhenUninitialized() {\r\n    new EmbeddedTaggedRateLimiter(100).acquire(1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testFailsWhenUsingTags",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFailsWhenUsingTags() {\r\n    RateLimiter rateLimiter = new EmbeddedTaggedRateLimiter(10);\r\n    initRateLimiter(rateLimiter);\r\n    Map<String, Integer> tagToCredits = new HashMap<>();\r\n    tagToCredits.put(\"red\", 1);\r\n    tagToCredits.put(\"green\", 1);\r\n    rateLimiter.acquire(tagToCredits);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testAcquireTagged",
  "sourceCode" : "@Test\r\npublic void testAcquireTagged() {\r\n    RateLimiter rateLimiter = createRateLimiter();\r\n    Map<String, Integer> tagToCount = new HashMap<>();\r\n    tagToCount.put(\"red\", 0);\r\n    tagToCount.put(\"green\", 0);\r\n    Map<String, Integer> tagToCredits = new HashMap<>();\r\n    tagToCredits.put(\"red\", INCREMENT);\r\n    tagToCredits.put(\"green\", INCREMENT);\r\n    long start = System.currentTimeMillis();\r\n    while (System.currentTimeMillis() - start < TEST_INTERVAL) {\r\n        rateLimiter.acquire(tagToCredits);\r\n        tagToCount.put(\"red\", tagToCount.get(\"red\") + INCREMENT);\r\n        tagToCount.put(\"green\", tagToCount.get(\"green\") + INCREMENT);\r\n    }\r\n    {\r\n        long rate = tagToCount.get(\"red\") * 1000 / TEST_INTERVAL;\r\n        verifyRate(rate, TARGET_RATE_PER_TASK_RED);\r\n    }\r\n    {\r\n        // Note: due to blocking, green is capped at red's QPS\r\n        long rate = tagToCount.get(\"green\") * 1000 / TEST_INTERVAL;\r\n        verifyRate(rate, TARGET_RATE_PER_TASK_RED);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testAcquireWithTimeoutTagged",
  "sourceCode" : "@Test\r\npublic void testAcquireWithTimeoutTagged() {\r\n    RateLimiter rateLimiter = createRateLimiter();\r\n    Map<String, Integer> tagToCount = new HashMap<>();\r\n    tagToCount.put(\"red\", 0);\r\n    tagToCount.put(\"green\", 0);\r\n    Map<String, Integer> tagToCredits = new HashMap<>();\r\n    tagToCredits.put(\"red\", INCREMENT);\r\n    tagToCredits.put(\"green\", INCREMENT);\r\n    long start = System.currentTimeMillis();\r\n    while (System.currentTimeMillis() - start < TEST_INTERVAL) {\r\n        Map<String, Integer> resultMap = rateLimiter.acquire(tagToCredits, 20, MILLISECONDS);\r\n        tagToCount.put(\"red\", tagToCount.get(\"red\") + resultMap.get(\"red\"));\r\n        tagToCount.put(\"green\", tagToCount.get(\"green\") + resultMap.get(\"green\"));\r\n    }\r\n    {\r\n        long rate = tagToCount.get(\"red\") * 1000 / TEST_INTERVAL;\r\n        verifyRate(rate, TARGET_RATE_PER_TASK_RED);\r\n    }\r\n    {\r\n        // Note: due to blocking, green is capped at red's QPS\r\n        long rate = tagToCount.get(\"green\") * 1000 / TEST_INTERVAL;\r\n        verifyRate(rate, TARGET_RATE_PER_TASK_RED);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testFailsWhenUninitializedTagged",
  "sourceCode" : "@Test(expected = IllegalStateException.class)\r\npublic void testFailsWhenUninitializedTagged() {\r\n    Map<String, Integer> tagToTargetRateMap = new HashMap<>();\r\n    tagToTargetRateMap.put(\"red\", 1000);\r\n    tagToTargetRateMap.put(\"green\", 2000);\r\n    new EmbeddedTaggedRateLimiter(tagToTargetRateMap).acquire(tagToTargetRateMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestEmbeddedTaggedRateLimiter.java",
  "methodName" : "testFailsWhenNotUsingTags",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testFailsWhenNotUsingTags() {\r\n    Map<String, Integer> tagToCredits = new HashMap<>();\r\n    tagToCredits.put(\"red\", 1);\r\n    tagToCredits.put(\"green\", 1);\r\n    RateLimiter rateLimiter = new EmbeddedTaggedRateLimiter(tagToCredits);\r\n    initRateLimiter(rateLimiter);\r\n    rateLimiter.acquire(1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testAllOf",
  "sourceCode" : "/**\r\n * Test all futures in all collections complete before allOf completes.\r\n * Test completes exceptionally if any complete exceptionally.\r\n * Test works with heterogeneous value types.\r\n * Test works with heterogeneous collection types.\r\n * Test works with completion stages as well as completable futures.\r\n */\r\n@Test\r\npublic void testAllOf() {\r\n    // verify that there is no short circuiting\r\n    CompletableFuture<String> future1 = new CompletableFuture<>();\r\n    CompletableFuture<String> future2 = new CompletableFuture<>();\r\n    CompletableFuture<String> future3 = new CompletableFuture<>();\r\n    CompletableFuture<Integer> future4 = new CompletableFuture<>();\r\n    ImmutableList<CompletableFuture<?>> collection1 = ImmutableList.of(future1, future2);\r\n    ImmutableSet<CompletionStage<?>> collection2 = ImmutableSet.of(future3, future4);\r\n    CompletableFuture<Void> allFuture = FutureUtil.allOf(collection1, collection2);\r\n    future1.complete(\"1\");\r\n    assertFalse(allFuture.isDone());\r\n    RuntimeException ex2 = new RuntimeException(\"2\");\r\n    future2.completeExceptionally(ex2);\r\n    assertFalse(allFuture.isDone());\r\n    assertFalse(allFuture.isCompletedExceptionally());\r\n    future3.complete(\"3\");\r\n    assertFalse(allFuture.isDone());\r\n    assertFalse(allFuture.isCompletedExceptionally());\r\n    future4.complete(4);\r\n    assertTrue(allFuture.isDone());\r\n    assertTrue(allFuture.isCompletedExceptionally());\r\n    try {\r\n        allFuture.join();\r\n    } catch (Exception e) {\r\n        assertEquals(ex2, FutureUtil.unwrapExceptions(CompletionException.class, e));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testAllOfIgnoringErrorsCompletesSuccessfullyIfNoErrors",
  "sourceCode" : "@Test\r\npublic void testAllOfIgnoringErrorsCompletesSuccessfullyIfNoErrors() {\r\n    CompletableFuture<String> future1 = new CompletableFuture<>();\r\n    CompletableFuture<String> future2 = new CompletableFuture<>();\r\n    CompletableFuture<Void> allFuture = FutureUtil.allOf(t -> false, future1, future2);\r\n    future1.complete(\"1\");\r\n    assertFalse(allFuture.isDone());\r\n    future2.complete(\"2\");\r\n    assertTrue(allFuture.isDone());\r\n    assertFalse(allFuture.isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testAllOfIgnoringErrorsCompletesSuccessfullyIfOnlyIgnoredErrors",
  "sourceCode" : "@Test\r\npublic void testAllOfIgnoringErrorsCompletesSuccessfullyIfOnlyIgnoredErrors() {\r\n    CompletableFuture<String> future1 = new CompletableFuture<>();\r\n    CompletableFuture<String> future2 = new CompletableFuture<>();\r\n    CompletableFuture<Void> allFuture = FutureUtil.allOf(t -> true, future1, future2);\r\n    future1.complete(\"1\");\r\n    assertFalse(allFuture.isDone());\r\n    RuntimeException ex2 = new RuntimeException(\"2\");\r\n    future2.completeExceptionally(ex2);\r\n    assertTrue(allFuture.isDone());\r\n    assertFalse(allFuture.isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testAllOfIgnoringErrorsCompletesExceptionallyIfNonIgnoredErrors",
  "sourceCode" : "@Test\r\npublic void testAllOfIgnoringErrorsCompletesExceptionallyIfNonIgnoredErrors() {\r\n    // also test that each future is checked individually\r\n    CompletableFuture<String> future1 = new CompletableFuture<>();\r\n    CompletableFuture<String> future2 = new CompletableFuture<>();\r\n    Predicate<Throwable> mockPredicate = mock(Predicate.class);\r\n    when(mockPredicate.test(any())).thenReturn(true).thenReturn(false);\r\n    CompletableFuture<Void> allFuture = FutureUtil.allOf(mockPredicate, future1, future2);\r\n    future1.completeExceptionally(new SamzaException());\r\n    assertFalse(allFuture.isDone());\r\n    RuntimeException ex2 = new RuntimeException(\"2\");\r\n    future2.completeExceptionally(ex2);\r\n    assertTrue(allFuture.isDone());\r\n    assertTrue(allFuture.isCompletedExceptionally());\r\n    verify(mockPredicate, times(2)).test(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testFutureOfMapCompletesExceptionallyIfAValueFutureCompletesExceptionally",
  "sourceCode" : "@Test\r\npublic void testFutureOfMapCompletesExceptionallyIfAValueFutureCompletesExceptionally() {\r\n    Map<String, CompletableFuture<String>> map = new HashMap<>();\r\n    map.put(\"1\", CompletableFuture.completedFuture(\"1\"));\r\n    map.put(\"2\", FutureUtil.failedFuture(new SamzaException()));\r\n    assertTrue(FutureUtil.toFutureOfMap(map).isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testFutureOfMapCompletesSuccessfullyIfNoErrors",
  "sourceCode" : "@Test\r\npublic void testFutureOfMapCompletesSuccessfullyIfNoErrors() {\r\n    Map<String, CompletableFuture<String>> map = new HashMap<>();\r\n    map.put(\"1\", CompletableFuture.completedFuture(\"1\"));\r\n    map.put(\"2\", CompletableFuture.completedFuture(\"2\"));\r\n    CompletableFuture<Map<String, String>> result = FutureUtil.toFutureOfMap(t -> true, map);\r\n    assertTrue(result.isDone());\r\n    assertFalse(result.isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testFutureOfMapCompletesSuccessfullyIfOnlyIgnoredErrors",
  "sourceCode" : "@Test\r\npublic void testFutureOfMapCompletesSuccessfullyIfOnlyIgnoredErrors() {\r\n    Map<String, CompletableFuture<String>> map = new HashMap<>();\r\n    map.put(\"1\", CompletableFuture.completedFuture(\"1\"));\r\n    map.put(\"2\", FutureUtil.failedFuture(new SamzaException()));\r\n    CompletableFuture<Map<String, String>> result = FutureUtil.toFutureOfMap(t -> FutureUtil.unwrapExceptions(CompletionException.class, t) instanceof SamzaException, map);\r\n    assertTrue(result.isDone());\r\n    result.join();\r\n    assertFalse(result.isCompletedExceptionally());\r\n    assertEquals(\"1\", result.join().get(\"1\"));\r\n    assertFalse(result.join().containsKey(\"2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testFutureOfMapCompletesExceptionallyIfAnyNonIgnoredErrors",
  "sourceCode" : "@Test\r\npublic void testFutureOfMapCompletesExceptionallyIfAnyNonIgnoredErrors() {\r\n    Map<String, CompletableFuture<String>> map = new HashMap<>();\r\n    map.put(\"1\", FutureUtil.failedFuture(new RuntimeException()));\r\n    SamzaException samzaException = new SamzaException();\r\n    map.put(\"2\", FutureUtil.failedFuture(samzaException));\r\n    Predicate<Throwable> mockPredicate = mock(Predicate.class);\r\n    when(mockPredicate.test(any())).thenReturn(true).thenReturn(false);\r\n    CompletableFuture<Map<String, String>> result = FutureUtil.toFutureOfMap(mockPredicate, map);\r\n    assertTrue(result.isDone());\r\n    assertTrue(result.isCompletedExceptionally());\r\n    // verify that each failed value future is tested\r\n    verify(mockPredicate, times(2)).test(any());\r\n    try {\r\n        result.join();\r\n        fail(\"Should have thrown an exception.\");\r\n    } catch (Exception e) {\r\n        assertEquals(samzaException, FutureUtil.unwrapExceptions(CompletionException.class, e));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testUnwrapExceptionUnwrapsMultipleExceptions",
  "sourceCode" : "@Test\r\npublic void testUnwrapExceptionUnwrapsMultipleExceptions() {\r\n    IllegalArgumentException cause = new IllegalArgumentException();\r\n    Throwable t = new SamzaException(new SamzaException(cause));\r\n    Throwable unwrappedThrowable = FutureUtil.unwrapExceptions(SamzaException.class, t);\r\n    assertEquals(cause, unwrappedThrowable);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testUnwrapExceptionReturnsOriginalExceptionIfNoWrapper",
  "sourceCode" : "@Test\r\npublic void testUnwrapExceptionReturnsOriginalExceptionIfNoWrapper() {\r\n    IllegalArgumentException cause = new IllegalArgumentException();\r\n    Throwable unwrappedThrowable = FutureUtil.unwrapExceptions(SamzaException.class, cause);\r\n    assertEquals(cause, unwrappedThrowable);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testUnwrapExceptionReturnsNullIfNoNonWrapperCause",
  "sourceCode" : "@Test\r\npublic void testUnwrapExceptionReturnsNullIfNoNonWrapperCause() {\r\n    Throwable t = new SamzaException(new SamzaException());\r\n    Throwable unwrappedThrowable = FutureUtil.unwrapExceptions(SamzaException.class, t);\r\n    assertNull(unwrappedThrowable);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestFutureUtil.java",
  "methodName" : "testUnwrapExceptionReturnsNullIfOriginalExceptionIsNull",
  "sourceCode" : "@Test\r\npublic void testUnwrapExceptionReturnsNullIfOriginalExceptionIsNull() {\r\n    Throwable unwrappedThrowable = FutureUtil.unwrapExceptions(SamzaException.class, null);\r\n    assertNull(unwrappedThrowable);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestMathUtils.java",
  "methodName" : "testGcdWithNullInputs",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGcdWithNullInputs() {\r\n    MathUtil.gcd(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestMathUtils.java",
  "methodName" : "testGcdWithEmptyInputs",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGcdWithEmptyInputs() {\r\n    MathUtil.gcd(Collections.emptyList());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestMathUtils.java",
  "methodName" : "testGcdWithValidInputs",
  "sourceCode" : "@Test\r\npublic void testGcdWithValidInputs() {\r\n    // gcd(x, x) = x\r\n    assertEquals(2, MathUtil.gcd(ImmutableList.of(2L, 2L)));\r\n    assertEquals(15, MathUtil.gcd(ImmutableList.of(15L)));\r\n    assertEquals(1, MathUtil.gcd(ImmutableList.of(1L)));\r\n    // gcd(0,x) = x\r\n    assertEquals(2, MathUtil.gcd(ImmutableList.of(2L, 0L)));\r\n    // gcd(1,x) = 1\r\n    assertEquals(1, MathUtil.gcd(ImmutableList.of(10L, 20L, 30L, 40L, 50L, 1L)));\r\n    // other happy path test cases\r\n    assertEquals(10, MathUtil.gcd(ImmutableList.of(10L, 20L, 30L, 40L, 50L, 0L)));\r\n    assertEquals(10, MathUtil.gcd(ImmutableList.of(10L, 20L, 30L, 40L, 50L)));\r\n    assertEquals(5, MathUtil.gcd(ImmutableList.of(25L, 35L, 45L, 55L)));\r\n    assertEquals(1, MathUtil.gcd(ImmutableList.of(25L, 35L, 45L, 55L, 13L)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestMathUtils.java",
  "methodName" : "testClampAdd",
  "sourceCode" : "@Test\r\npublic void testClampAdd() {\r\n    assertEquals(0, MathUtil.clampAdd(0, 0));\r\n    assertEquals(2, MathUtil.clampAdd(1, 1));\r\n    assertEquals(-2, MathUtil.clampAdd(-1, -1));\r\n    assertEquals(Long.MAX_VALUE, MathUtil.clampAdd(Long.MAX_VALUE, 0));\r\n    assertEquals(Long.MAX_VALUE - 1, MathUtil.clampAdd(Long.MAX_VALUE, -1));\r\n    assertEquals(Long.MAX_VALUE, MathUtil.clampAdd(Long.MAX_VALUE, 1));\r\n    assertEquals(Long.MAX_VALUE, MathUtil.clampAdd(Long.MAX_VALUE, Long.MAX_VALUE));\r\n    assertEquals(Long.MIN_VALUE, MathUtil.clampAdd(Long.MIN_VALUE, 0));\r\n    assertEquals(Long.MIN_VALUE, MathUtil.clampAdd(Long.MIN_VALUE, -1));\r\n    assertEquals(Long.MIN_VALUE + 1, MathUtil.clampAdd(Long.MIN_VALUE, 1));\r\n    assertEquals(Long.MIN_VALUE, MathUtil.clampAdd(Long.MIN_VALUE, Long.MIN_VALUE));\r\n    assertEquals(-1, MathUtil.clampAdd(Long.MAX_VALUE, Long.MIN_VALUE));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObj",
  "sourceCode" : "@Test\r\npublic void testGetObj() {\r\n    assertTrue(ReflectionUtil.getObj(ArrayList.class.getName(), List.class) instanceof ArrayList);\r\n    assertEquals(new ArrayList<>(), ReflectionUtil.getObj(ArrayList.class.getName(), List.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjNoClass",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjNoClass() {\r\n    ReflectionUtil.getObj(\"not.a.class\", Set.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjInvalidConstructor",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjInvalidConstructor() {\r\n    ReflectionUtil.getObj(WithTwoArgConstructor.class.getName(), Object.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgs",
  "sourceCode" : "@Test\r\npublic void testGetObjWithArgs() {\r\n    assertEquals(new WithTwoArgConstructor(\"hello\", ImmutableList.of(\"hello\", \"world\")), ReflectionUtil.getObjWithArgs(WithTwoArgConstructor.class.getName(), WithTwoArgConstructor.class, ReflectionUtil.constructorArgument(\"hello\", String.class), ReflectionUtil.constructorArgument(ImmutableList.of(\"hello\", \"world\"), List.class)));\r\n    // should still work if pass no args, since should use empty constructor\r\n    assertTrue(ReflectionUtil.getObjWithArgs(ArrayList.class.getName(), List.class) instanceof ArrayList);\r\n    assertEquals(new ArrayList<>(), ReflectionUtil.getObjWithArgs(ArrayList.class.getName(), List.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgsNoClass",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjWithArgsNoClass() {\r\n    ReflectionUtil.getObjWithArgs(\"not.a.class\", Set.class, ReflectionUtil.constructorArgument(10, Integer.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgsWrongArgumentCount",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjWithArgsWrongArgumentCount() {\r\n    ReflectionUtil.getObjWithArgs(WithTwoArgConstructor.class.getName(), Object.class, ReflectionUtil.constructorArgument(\"hello world\", String.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgsWrongArgumentTypes",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjWithArgsWrongArgumentTypes() {\r\n    ReflectionUtil.getObjWithArgs(WithTwoArgConstructor.class.getName(), Object.class, ReflectionUtil.constructorArgument(\"hello world\", String.class), ReflectionUtil.constructorArgument(ImmutableList.of(\"hello\", \"world\"), ImmutableList.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgsZeroArgsNoClass",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjWithArgsZeroArgsNoClass() {\r\n    ReflectionUtil.getObjWithArgs(\"not.a.class\", Set.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestReflectionUtil.java",
  "methodName" : "testGetObjWithArgsZeroArgsInvalidConstructor",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetObjWithArgsZeroArgsInvalidConstructor() {\r\n    ReflectionUtil.getObjWithArgs(WithTwoArgConstructor.class.getName(), Object.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestShutdownUtil.java",
  "methodName" : "testBoundedShutdown",
  "sourceCode" : "@Test\r\npublic void testBoundedShutdown() throws Exception {\r\n    long longTimeout = Duration.ofSeconds(60).toMillis();\r\n    long shortTimeout = Duration.ofMillis(100).toMillis();\r\n    Runnable shortRunnable = () -> {\r\n        try {\r\n            Thread.sleep(shortTimeout);\r\n        } catch (Exception e) {\r\n            Assert.fail(e.getMessage());\r\n        }\r\n    };\r\n    long start = System.currentTimeMillis();\r\n    Assert.assertTrue(\"expect the shutdown task to terminate\", ShutdownUtil.boundedShutdown(Collections.singletonList(shortRunnable), \"testLongTimeout\", longTimeout));\r\n    long end = System.currentTimeMillis();\r\n    Assert.assertTrue(\"boundedShutdown should complete if the shutdown function completes earlier\", (end - start) < longTimeout / 2);\r\n    Runnable longRunnable = () -> {\r\n        try {\r\n            Thread.sleep(longTimeout);\r\n        } catch (Exception e) {\r\n            Assert.fail(e.getMessage());\r\n        }\r\n    };\r\n    start = System.currentTimeMillis();\r\n    Assert.assertFalse(\"expect the shutdown task to be unfinished\", ShutdownUtil.boundedShutdown(Collections.singletonList(longRunnable), \"testShortTimeout\", shortTimeout));\r\n    end = System.currentTimeMillis();\r\n    Assert.assertTrue(\"boundedShutdown should complete even if the shutdown function takes long time\", (end - start) < longTimeout / 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithPhysicalNameInConfig",
  "sourceCode" : "// The physical name should be pulled from the StreamConfig.PHYSICAL_NAME property value.\r\n@Test\r\npublic void testGetStreamWithPhysicalNameInConfig() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_PHYSICAL_NAME, spec.getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithoutPhysicalNameInConfig",
  "sourceCode" : "// The streamId should be used as the physicalName when the physical name is not specified.\r\n// NOTE: its either this, set to null, or exception. This seems better for backward compatibility and API brevity.\r\n@Test\r\npublic void testGetStreamWithoutPhysicalNameInConfig() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(STREAM_ID, spec.getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithSystemAtStreamScopeInConfig",
  "sourceCode" : "// If the system is specified at the stream scope, use it\r\n@Test\r\npublic void testGetStreamWithSystemAtStreamScopeInConfig() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithSystemAtDefaultScopeInConfig",
  "sourceCode" : "// If system isn't specified at stream scope, use the default system\r\n@Test\r\npublic void testGetStreamWithSystemAtDefaultScopeInConfig() {\r\n    Config config = addConfigs(buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME), JobConfig.JOB_DEFAULT_SYSTEM, TEST_DEFAULT_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_DEFAULT_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithSystemAtBothScopesInConfig",
  "sourceCode" : "// Stream scope should override default scope\r\n@Test\r\npublic void testGetStreamWithSystemAtBothScopesInConfig() {\r\n    Config config = addConfigs(buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM), JobConfig.JOB_DEFAULT_SYSTEM, TEST_DEFAULT_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamWithOutSystemInConfig",
  "sourceCode" : "// System is required. Throw if it cannot be determined.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamWithOutSystemInConfig() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamPropertiesPassthrough",
  "sourceCode" : "// The properties in the config \"streams.{streamId}.*\" should be passed through to the spec.\r\n@Test\r\npublic void testGetStreamPropertiesPassthrough() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM, \"systemProperty1\", \"systemValue1\", \"systemProperty2\", \"systemValue2\", \"systemProperty3\", \"systemValue3\");\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    Map<String, String> properties = spec.getConfig();\r\n    assertEquals(3, properties.size());\r\n    assertEquals(\"systemValue1\", properties.get(\"systemProperty1\"));\r\n    assertEquals(\"systemValue2\", properties.get(\"systemProperty2\"));\r\n    assertEquals(\"systemValue3\", properties.get(\"systemProperty3\"));\r\n    assertEquals(\"systemValue1\", spec.get(\"systemProperty1\"));\r\n    assertEquals(\"systemValue2\", spec.get(\"systemProperty2\"));\r\n    assertEquals(\"systemValue3\", spec.get(\"systemProperty3\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamSamzaPropertiesOmitted",
  "sourceCode" : "// The samza properties (which are invalid for the underlying system) should be filtered out.\r\n@Test\r\npublic void testGetStreamSamzaPropertiesOmitted() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM, \"systemProperty1\", \"systemValue1\", \"systemProperty2\", \"systemValue2\", \"systemProperty3\", \"systemValue3\");\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    Map<String, String> properties = spec.getConfig();\r\n    assertEquals(3, properties.size());\r\n    assertNull(properties.get(String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID)));\r\n    assertNull(properties.get(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID)));\r\n    assertNull(spec.get(String.format(StreamConfig.PHYSICAL_NAME_FOR_STREAM_ID, STREAM_ID)));\r\n    assertNull(spec.get(String.format(StreamConfig.SYSTEM_FOR_STREAM_ID, STREAM_ID)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testStreamConfigOverrides",
  "sourceCode" : "@Test\r\npublic void testStreamConfigOverrides() {\r\n    final String sysStreamPrefix = String.format(\"systems.%s.streams.%s.\", TEST_SYSTEM, TEST_PHYSICAL_NAME);\r\n    Config config = addConfigs(buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM, \"systemProperty1\", \"systemValue1\", \"systemProperty2\", \"systemValue2\", \"systemProperty3\", \"systemValue3\"), sysStreamPrefix + \"systemProperty4\", \"systemValue4\", sysStreamPrefix + \"systemProperty2\", \"systemValue8\");\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    Map<String, String> properties = spec.getConfig();\r\n    assertEquals(4, properties.size());\r\n    assertEquals(\"systemValue4\", properties.get(\"systemProperty4\"));\r\n    assertEquals(\"systemValue2\", properties.get(\"systemProperty2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testStreamConfigOverridesWithSystemDefaults",
  "sourceCode" : "// Verify that we use a default specified with systems.x.default.stream.*, if specified\r\n@Test\r\npublic void testStreamConfigOverridesWithSystemDefaults() {\r\n    Config config = addConfigs(buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM, \"segment.bytes\", \"5309\"), // System default property\r\n    String.format(\"systems.%s.default.stream.replication.factor\", TEST_SYSTEM), // System default property\r\n    \"4\", String.format(\"systems.%s.default.stream.segment.bytest\", TEST_SYSTEM), \"867\");\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    Map<String, String> properties = spec.getConfig();\r\n    assertEquals(3, properties.size());\r\n    // Uses system default\r\n    assertEquals(\"4\", properties.get(\"replication.factor\"));\r\n    // Overrides system default\r\n    assertEquals(\"5309\", properties.get(\"segment.bytes\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamPhysicalNameArgSimple",
  "sourceCode" : "// When the physicalName argument is passed explicitly it should be used, regardless of whether it is also in the config\r\n@Test\r\npublic void testGetStreamPhysicalNameArgSimple() {\r\n    Config config = buildStreamConfig(STREAM_ID, // This should be ignored because of the explicit arg\r\n    StreamConfig.PHYSICAL_NAME, // This should be ignored because of the explicit arg\r\n    TEST_PHYSICAL_NAME2, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(STREAM_ID, spec.getId());\r\n    assertEquals(TEST_PHYSICAL_NAME2, spec.getPhysicalName());\r\n    assertEquals(TEST_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamPhysicalNameArgSpecialCharacters",
  "sourceCode" : "// Special characters are allowed for the physical name\r\n@Test\r\npublic void testGetStreamPhysicalNameArgSpecialCharacters() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME_SPECIAL_CHARS, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(TEST_PHYSICAL_NAME_SPECIAL_CHARS, spec.getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamPhysicalNameArgNull",
  "sourceCode" : "// Null is allowed for the physical name\r\n@Test\r\npublic void testGetStreamPhysicalNameArgNull() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, null, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertNull(spec.getPhysicalName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamSystemNameArgValid",
  "sourceCode" : "// When the system name is provided explicitly, it should be used, regardless of whether it's also in the config\r\n@Test\r\npublic void testGetStreamSystemNameArgValid() {\r\n    Config config = buildStreamConfig(STREAM_ID, // This should be ignored because of the explicit arg\r\n    StreamConfig.PHYSICAL_NAME, // This should be ignored because of the explicit arg\r\n    TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, // This too\r\n    TEST_SYSTEM);\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n    assertEquals(STREAM_ID, spec.getId());\r\n    assertEquals(TEST_PHYSICAL_NAME, spec.getPhysicalName());\r\n    assertEquals(TEST_SYSTEM, spec.getSystemName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamSystemNameArgInvalid",
  "sourceCode" : "// Special characters are NOT allowed for system name, because it's used as an identifier in the config.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamSystemNameArgInvalid() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, TEST_SYSTEM_INVALID);\r\n    StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamSystemNameArgEmpty",
  "sourceCode" : "// Empty strings are NOT allowed for system name, because it's used as an identifier in the config.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamSystemNameArgEmpty() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, \"\");\r\n    StreamSpec spec = StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamSystemNameArgNull",
  "sourceCode" : "// Null is not allowed IllegalArgumentException system name.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamSystemNameArgNull() {\r\n    Config config = buildStreamConfig(STREAM_ID, StreamConfig.PHYSICAL_NAME, TEST_PHYSICAL_NAME, StreamConfig.SYSTEM, null);\r\n    StreamUtil.getStreamSpec(STREAM_ID, new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamStreamIdInvalid",
  "sourceCode" : "// Special characters are NOT allowed for streamId, because it's used as an identifier in the config.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamStreamIdInvalid() {\r\n    Config config = buildStreamConfig(STREAM_ID_INVALID, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamUtil.getStreamSpec(STREAM_ID_INVALID, new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamStreamIdEmpty",
  "sourceCode" : "// Empty strings are NOT allowed for streamId, because it's used as an identifier in the config.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamStreamIdEmpty() {\r\n    Config config = buildStreamConfig(\"\", StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamUtil.getStreamSpec(\"\", new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestStreamUtil.java",
  "methodName" : "testGetStreamStreamIdNull",
  "sourceCode" : "// Null is not allowed for streamId.\r\n@Test(expected = IllegalArgumentException.class)\r\npublic void testGetStreamStreamIdNull() {\r\n    Config config = buildStreamConfig(null, StreamConfig.SYSTEM, TEST_SYSTEM);\r\n    StreamUtil.getStreamSpec(null, new StreamConfig(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testInitialState",
  "sourceCode" : "@Test\r\npublic void testInitialState() {\r\n    ThrottlingExecutor throttler = new ThrottlingExecutor(MAX_NANOS);\r\n    assertEquals(0, throttler.getPendingNanos());\r\n    assertEquals(1.0, throttler.getWorkFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testSetWorkRate",
  "sourceCode" : "@Test\r\npublic void testSetWorkRate() {\r\n    executor.setWorkFactor(1.0);\r\n    assertEquals(1.0, executor.getWorkFactor());\r\n    executor.setWorkFactor(0.5);\r\n    assertEquals(0.5, executor.getWorkFactor());\r\n    executor.setWorkFactor(ThrottlingExecutor.MIN_WORK_FACTOR);\r\n    assertEquals(ThrottlingExecutor.MIN_WORK_FACTOR, executor.getWorkFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testLessThan0PercentWorkRate",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testLessThan0PercentWorkRate() {\r\n    new ThrottlingExecutor(MAX_NANOS).setWorkFactor(-0.1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testGreaterThan100PercentWorkRate",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGreaterThan100PercentWorkRate() {\r\n    new ThrottlingExecutor(MAX_NANOS).setWorkFactor(1.1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "test100PercentWorkRate",
  "sourceCode" : "@Test\r\npublic void test100PercentWorkRate() throws InterruptedException {\r\n    setWorkTime(TimeUnit.MILLISECONDS.toNanos(5));\r\n    executor.execute(NO_OP);\r\n    assertEquals(0L, executor.getPendingNanos());\r\n    // At 100% work rate sleep should not be called\r\n    Mockito.verify(executor, Mockito.never()).sleep(Mockito.anyLong());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "test50PercentWorkRate",
  "sourceCode" : "@Test\r\npublic void test50PercentWorkRate() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    setWorkTime(workTimeNanos);\r\n    // Sleep time is same as work time at 50% work rate\r\n    setActualSleepTime(workTimeNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(workTimeNanos);\r\n    assertEquals(0L, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testMinWorkRate",
  "sourceCode" : "@Test\r\npublic void testMinWorkRate() throws InterruptedException {\r\n    final double workFactor = ThrottlingExecutor.MIN_WORK_FACTOR;\r\n    executor.setWorkFactor(workFactor);\r\n    // The math to work out how much to multiply work time to get expected delay time\r\n    double workToDelayFactor = (1.0 - workFactor) / workFactor;\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long delayTimeNanos = (long) (workToDelayFactor * workTimeNanos);\r\n    setWorkTime(workTimeNanos);\r\n    setActualSleepTime(delayTimeNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(delayTimeNanos);\r\n    assertEquals(0, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testSleepOvershoot",
  "sourceCode" : "@Test\r\npublic void testSleepOvershoot() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long expectedDelayNanos = workTimeNanos;\r\n    final long actualDelayTimeNanos = TimeUnit.MILLISECONDS.toNanos(6);\r\n    setWorkTime(workTimeNanos);\r\n    setActualSleepTime(actualDelayTimeNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(expectedDelayNanos);\r\n    assertEquals(expectedDelayNanos - actualDelayTimeNanos, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testSleepUndershoot",
  "sourceCode" : "@Test\r\npublic void testSleepUndershoot() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long expectedDelayNanos = workTimeNanos;\r\n    final long actualDelayNanos = TimeUnit.MILLISECONDS.toNanos(4);\r\n    setWorkTime(workTimeNanos);\r\n    setActualSleepTime(actualDelayNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(expectedDelayNanos);\r\n    assertEquals(expectedDelayNanos - actualDelayNanos, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testApplyPendingSleepNanos",
  "sourceCode" : "@Test\r\npublic void testApplyPendingSleepNanos() throws InterruptedException {\r\n    // This verifies that the executor tries to re-apply pending sleep time on the next execution.\r\n    executor.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long actualDelayNanos1 = TimeUnit.MILLISECONDS.toNanos(4);\r\n    final long actualDelayNanos2 = TimeUnit.MILLISECONDS.toNanos(6);\r\n    // First execution\r\n    setWorkTime(workTimeNanos);\r\n    setActualSleepTime(actualDelayNanos1);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(workTimeNanos);\r\n    assertEquals(workTimeNanos - actualDelayNanos1, executor.getPendingNanos());\r\n    // Second execution\r\n    setWorkTime(workTimeNanos);\r\n    setActualSleepTime(actualDelayNanos2);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(workTimeNanos);\r\n    assertEquals(0L, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testClampDelayMillis",
  "sourceCode" : "@Test\r\npublic void testClampDelayMillis() throws InterruptedException {\r\n    final long maxDelayMillis = 10;\r\n    final long maxDelayNanos = TimeUnit.MILLISECONDS.toNanos(maxDelayMillis);\r\n    executor = Mockito.spy(new ThrottlingExecutor(maxDelayMillis, clock));\r\n    executor.setWorkFactor(0.5);\r\n    // Note work time exceeds maxDelayMillis\r\n    setWorkTime(TimeUnit.MILLISECONDS.toNanos(100));\r\n    setActualSleepTime(maxDelayNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(maxDelayNanos);\r\n    assertEquals(0L, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testDecreaseWorkFactor",
  "sourceCode" : "@Test\r\npublic void testDecreaseWorkFactor() {\r\n    executor.setWorkFactor(0.5);\r\n    executor.setPendingNanos(5000);\r\n    executor.setWorkFactor(0.3);\r\n    assertEquals(5000, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testOverflowOfSleepNanos",
  "sourceCode" : "@Test\r\npublic void testOverflowOfSleepNanos() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    executor.setPendingNanos(Long.MAX_VALUE);\r\n    assertEquals(Long.MAX_VALUE, executor.getPendingNanos());\r\n    // At a 50% work factor we'd expect work and sleep to match. As they don't, the function will\r\n    // try to increment the pending sleep nanos, which could (but should not) result in overflow.\r\n    setWorkTime(5000);\r\n    setActualSleepTime(Long.MAX_VALUE);\r\n    executor.execute(NO_OP);\r\n    // Expect sleep nanos to be clamped to the maximum long value\r\n    verifySleepTime(Long.MAX_VALUE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testNegativePendingNanos",
  "sourceCode" : "@Test\r\npublic void testNegativePendingNanos() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    executor.setPendingNanos(-1000);\r\n    assertEquals(-1000, executor.getPendingNanos());\r\n    // Note: we do not expect the delay time to be used because work time + pending delay is\r\n    // negative.\r\n    setWorkTime(500);\r\n    executor.execute(NO_OP);\r\n    // Sleep should not be called with negative pending nanos\r\n    Mockito.verify(executor, Mockito.never()).sleep(Mockito.anyLong());\r\n    assertEquals(-1000 + 500, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingExecutor.java",
  "methodName" : "testNegativePendingNanosGoesPositive",
  "sourceCode" : "@Test\r\npublic void testNegativePendingNanosGoesPositive() throws InterruptedException {\r\n    executor.setWorkFactor(0.5);\r\n    long startPendingNanos = -1000;\r\n    executor.setPendingNanos(startPendingNanos);\r\n    assertEquals(-1000, executor.getPendingNanos());\r\n    setWorkTime(1250);\r\n    setActualSleepTime(1250 + startPendingNanos);\r\n    executor.execute(NO_OP);\r\n    verifySleepTime(1250 + startPendingNanos);\r\n    assertEquals(0, executor.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testInitialState",
  "sourceCode" : "@Test\r\npublic void testInitialState() {\r\n    ThrottlingExecutor throttler = new ThrottlingExecutor(MAX_NANOS);\r\n    assertEquals(0, throttler.getPendingNanos());\r\n    assertEquals(1.0, throttler.getWorkFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testSetWorkRate",
  "sourceCode" : "@Test\r\npublic void testSetWorkRate() {\r\n    throttler.setWorkFactor(1.0);\r\n    assertEquals(1.0, throttler.getWorkFactor());\r\n    throttler.setWorkFactor(0.5);\r\n    assertEquals(0.5, throttler.getWorkFactor());\r\n    throttler.setWorkFactor(Throttleable.MIN_WORK_FACTOR);\r\n    assertEquals(Throttleable.MIN_WORK_FACTOR, throttler.getWorkFactor());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testLessThan0PercentWorkRate",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testLessThan0PercentWorkRate() {\r\n    new ThrottlingExecutor(MAX_NANOS).setWorkFactor(-0.1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testGreaterThan100PercentWorkRate",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGreaterThan100PercentWorkRate() {\r\n    new ThrottlingExecutor(MAX_NANOS).setWorkFactor(1.1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "test100PercentWorkRate",
  "sourceCode" : "@Test\r\npublic void test100PercentWorkRate() throws InterruptedException {\r\n    throttler.schedule(NO_OP, 1000);\r\n    assertEquals(0L, throttler.getPendingNanos());\r\n    // At 100% work rate schedule should not be called\r\n    Mockito.verify(scheduledExecutorService, Mockito.never()).schedule(Mockito.any(Runnable.class), Mockito.anyLong(), Mockito.any(TimeUnit.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "test50PercentWorkRate",
  "sourceCode" : "@Test\r\npublic void test50PercentWorkRate() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    setActualDelay(workTimeNanos);\r\n    throttler.schedule(NO_OP, workTimeNanos);\r\n    // Delay time is same as work time at 50% work rate\r\n    verifyRequestedDelay(workTimeNanos);\r\n    assertEquals(0L, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testMinWorkRate",
  "sourceCode" : "@Test\r\npublic void testMinWorkRate() throws InterruptedException {\r\n    final double workFactor = Throttleable.MIN_WORK_FACTOR;\r\n    throttler.setWorkFactor(workFactor);\r\n    // The math to work out how much to multiply work time to get expected delay time\r\n    double workToDelayFactor = (1.0 - workFactor) / workFactor;\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long delayTimeNanos = (long) (workToDelayFactor * workTimeNanos);\r\n    setActualDelay(delayTimeNanos);\r\n    throttler.schedule(NO_OP, workTimeNanos);\r\n    verifyRequestedDelay(delayTimeNanos);\r\n    assertEquals(0, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testDelayOvershoot",
  "sourceCode" : "@Test\r\npublic void testDelayOvershoot() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long expectedDelayNanos = workTimeNanos;\r\n    final long actualDelayNanos = TimeUnit.MILLISECONDS.toNanos(6);\r\n    setActualDelay(actualDelayNanos);\r\n    throttler.schedule(NO_OP, workTimeNanos);\r\n    verifyRequestedDelay(expectedDelayNanos);\r\n    assertEquals(expectedDelayNanos - actualDelayNanos, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testDelayUndershoot",
  "sourceCode" : "@Test\r\npublic void testDelayUndershoot() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    final long workTimeNanos = TimeUnit.MILLISECONDS.toNanos(5);\r\n    final long expectedDelayNanos = workTimeNanos;\r\n    final long actualDelayNanos = TimeUnit.MILLISECONDS.toNanos(4);\r\n    setActualDelay(actualDelayNanos);\r\n    throttler.schedule(NO_OP, workTimeNanos);\r\n    verifyRequestedDelay(expectedDelayNanos);\r\n    assertEquals(expectedDelayNanos - actualDelayNanos, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testClampDelayMillis",
  "sourceCode" : "@Test\r\npublic void testClampDelayMillis() throws InterruptedException {\r\n    final long maxDelayMillis = 10;\r\n    final long maxDelayNanos = TimeUnit.MILLISECONDS.toNanos(maxDelayMillis);\r\n    ThrottlingScheduler throttler = new ThrottlingScheduler(maxDelayMillis, scheduledExecutorService, clock);\r\n    throttler.setWorkFactor(0.5);\r\n    setActualDelay(maxDelayNanos);\r\n    // Note work time exceeds maxDelayMillis\r\n    throttler.schedule(NO_OP, TimeUnit.MILLISECONDS.toNanos(100));\r\n    verifyRequestedDelay(maxDelayNanos);\r\n    assertEquals(0L, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testDecreaseWorkFactor",
  "sourceCode" : "@Test\r\npublic void testDecreaseWorkFactor() {\r\n    throttler.setWorkFactor(0.5);\r\n    throttler.setPendingNanos(5000);\r\n    throttler.setWorkFactor(0.3);\r\n    assertEquals(5000, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testOverflowOfDelayNanos",
  "sourceCode" : "@Test\r\npublic void testOverflowOfDelayNanos() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    throttler.setPendingNanos(Long.MAX_VALUE);\r\n    assertEquals(Long.MAX_VALUE, throttler.getPendingNanos());\r\n    // At a 50% work factor we'd expect work and delay to match. The function will try\r\n    // to increment the pending delay nanos, which could (but should not) result in overflow.\r\n    long workDurationNs = 5000;\r\n    setActualDelay(workDurationNs);\r\n    throttler.schedule(NO_OP, workDurationNs);\r\n    verifyRequestedDelay(workDurationNs);\r\n    // Expect delay nanos to be clamped during accumulation, and decreased by expected delay at the end.\r\n    assertEquals(Long.MAX_VALUE - workDurationNs, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testNegativePendingNanos",
  "sourceCode" : "@Test\r\npublic void testNegativePendingNanos() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    throttler.setPendingNanos(-1000);\r\n    assertEquals(-1000, throttler.getPendingNanos());\r\n    // Note: we do not expect the delay time to be used because work time + pending delay is\r\n    // negative.\r\n    throttler.schedule(NO_OP, 500);\r\n    // Should not be delayed with negative pending nanos\r\n    Mockito.verify(scheduledExecutorService, Mockito.never()).schedule(Mockito.any(Runnable.class), Mockito.anyLong(), Mockito.any(TimeUnit.class));\r\n    assertEquals(-1000 + 500, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestThrottlingScheduler.java",
  "methodName" : "testNegativePendingNanosGoesPositive",
  "sourceCode" : "@Test\r\npublic void testNegativePendingNanosGoesPositive() throws InterruptedException {\r\n    throttler.setWorkFactor(0.5);\r\n    long startPendingNanos = -1000;\r\n    throttler.setPendingNanos(startPendingNanos);\r\n    assertEquals(-1000, throttler.getPendingNanos());\r\n    setActualDelay(1250);\r\n    // We request a delay greater than the starting pending delay.\r\n    throttler.schedule(NO_OP, 1250);\r\n    verifyRequestedDelay(1250);\r\n    // Final pending delay should equal initial pending delay since we delay\r\n    // for the exact requested amount.\r\n    assertEquals(startPendingNanos, throttler.getPendingNanos());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testEnvVarEscape",
  "sourceCode" : "@Test\r\npublic void testEnvVarEscape() {\r\n    // no special characters in original\r\n    String noSpecialCharacters = \"hello world 123 .?! '\";\r\n    assertEquals(noSpecialCharacters, Util.envVarEscape(noSpecialCharacters));\r\n    String withSpecialCharacters = \"quotation \\\" backslash \\\\ grave accent `\";\r\n    String escaped = \"quotation \\\\\\\" backslash \\\\\\\\ grave accent \\\\`\";\r\n    assertEquals(escaped, Util.envVarEscape(withSpecialCharacters));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetSamzaVersion",
  "sourceCode" : "/**\r\n *  It's difficult to explicitly test having an actual version and using the fallback, due to the usage of methods of\r\n *  Class.\r\n */\r\n@Test\r\npublic void testGetSamzaVersion() {\r\n    String utilImplementationVersion = Util.class.getPackage().getImplementationVersion();\r\n    String expectedVersion = (utilImplementationVersion != null) ? utilImplementationVersion : Util.FALLBACK_VERSION;\r\n    assertEquals(expectedVersion, Util.getSamzaVersion());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetTaskClassVersion",
  "sourceCode" : "/**\r\n *  It's difficult to explicitly test having an actual version and using the fallback, due to the usage of methods of\r\n *  Class.\r\n */\r\n@Test\r\npublic void testGetTaskClassVersion() {\r\n    // cannot find app nor task\r\n    assertEquals(Util.FALLBACK_VERSION, Util.getTaskClassVersion(new MapConfig()));\r\n    // only app\r\n    String appClassVersion = MyAppClass.class.getPackage().getImplementationVersion();\r\n    String expectedAppClassVersion = (appClassVersion != null) ? appClassVersion : Util.FALLBACK_VERSION;\r\n    Config config = new MapConfig(ImmutableMap.of(ApplicationConfig.APP_CLASS, MyAppClass.class.getName()));\r\n    assertEquals(expectedAppClassVersion, Util.getTaskClassVersion(config));\r\n    // only task\r\n    String taskClassVersion = MyTaskClass.class.getPackage().getImplementationVersion();\r\n    String expectedTaskClassVersion = (taskClassVersion != null) ? taskClassVersion : Util.FALLBACK_VERSION;\r\n    config = new MapConfig(ImmutableMap.of(TaskConfig.TASK_CLASS, MyTaskClass.class.getName()));\r\n    assertEquals(expectedTaskClassVersion, Util.getTaskClassVersion(config));\r\n    // both app and task; choose app\r\n    config = new MapConfig(ImmutableMap.of(ApplicationConfig.APP_CLASS, MyAppClass.class.getName(), // shouldn't even try to load the task class\r\n    TaskConfig.TASK_CLASS, \"this_is_not_a_class\"));\r\n    assertEquals(expectedAppClassVersion, Util.getTaskClassVersion(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetLocalHostNotLoopbackAddress",
  "sourceCode" : "@Test\r\npublic void testGetLocalHostNotLoopbackAddress() throws UnknownHostException {\r\n    mockStatic(InetAddress.class);\r\n    InetAddress inetAddressLocalHost = mock(InetAddress.class);\r\n    when(inetAddressLocalHost.isLoopbackAddress()).thenReturn(false);\r\n    when(InetAddress.getLocalHost()).thenReturn(inetAddressLocalHost);\r\n    assertEquals(inetAddressLocalHost, Util.getLocalHost());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetLocalHostLoopbackAddressNoExternalAddressFound",
  "sourceCode" : "@Test\r\npublic void testGetLocalHostLoopbackAddressNoExternalAddressFound() throws Exception {\r\n    mockStatic(InetAddress.class, NetworkInterface.class);\r\n    InetAddress inetAddressLocalHost = mock(InetAddress.class);\r\n    when(inetAddressLocalHost.isLoopbackAddress()).thenReturn(true);\r\n    when(InetAddress.getLocalHost()).thenReturn(inetAddressLocalHost);\r\n    // network interfaces return addresses which are not external\r\n    InetAddress linkLocalAddress = mock(InetAddress.class);\r\n    when(linkLocalAddress.isLinkLocalAddress()).thenReturn(true);\r\n    InetAddress loopbackAddress = mock(InetAddress.class);\r\n    when(loopbackAddress.isLinkLocalAddress()).thenReturn(false);\r\n    when(loopbackAddress.isLoopbackAddress()).thenReturn(true);\r\n    NetworkInterface networkInterface0 = mock(NetworkInterface.class);\r\n    when(networkInterface0.getInetAddresses()).thenReturn(Collections.enumeration(Arrays.asList(linkLocalAddress, loopbackAddress)));\r\n    NetworkInterface networkInterface1 = mock(NetworkInterface.class);\r\n    when(networkInterface1.getInetAddresses()).thenReturn(Collections.enumeration(Collections.singletonList(loopbackAddress)));\r\n    when(NetworkInterface.getNetworkInterfaces()).thenReturn(Collections.enumeration(Arrays.asList(networkInterface0, networkInterface1)));\r\n    assertEquals(inetAddressLocalHost, Util.getLocalHost());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetLocalHostExternalInet4Address",
  "sourceCode" : "@Test\r\npublic void testGetLocalHostExternalInet4Address() throws Exception {\r\n    mockStatic(InetAddress.class, NetworkInterface.class);\r\n    InetAddress inetAddressLocalHost = mock(InetAddress.class);\r\n    when(inetAddressLocalHost.isLoopbackAddress()).thenReturn(true);\r\n    when(InetAddress.getLocalHost()).thenReturn(inetAddressLocalHost);\r\n    InetAddress linkLocalAddress = mock(InetAddress.class);\r\n    when(linkLocalAddress.isLinkLocalAddress()).thenReturn(true);\r\n    Inet4Address externalInet4Address = mock(Inet4Address.class);\r\n    when(externalInet4Address.isLinkLocalAddress()).thenReturn(false);\r\n    when(externalInet4Address.isLoopbackAddress()).thenReturn(false);\r\n    byte[] externalInet4AddressBytes = new byte[] { 0, 1, 2, 3 };\r\n    when(externalInet4Address.getAddress()).thenReturn(externalInet4AddressBytes);\r\n    // not Inet4Address\r\n    InetAddress otherExternalAddress = mock(InetAddress.class);\r\n    when(otherExternalAddress.isLinkLocalAddress()).thenReturn(false);\r\n    when(otherExternalAddress.isLoopbackAddress()).thenReturn(false);\r\n    NetworkInterface networkInterfaceLinkLocal = mock(NetworkInterface.class);\r\n    when(networkInterfaceLinkLocal.getInetAddresses()).thenReturn(Collections.enumeration(Collections.singletonList(linkLocalAddress)));\r\n    NetworkInterface networkInterfaceExternal = mock(NetworkInterface.class);\r\n    when(networkInterfaceExternal.getInetAddresses()).thenReturn(Collections.enumeration(Arrays.asList(otherExternalAddress, externalInet4Address)));\r\n    when(NetworkInterface.getNetworkInterfaces()).thenReturn(Collections.enumeration(Arrays.asList(networkInterfaceLinkLocal, networkInterfaceExternal)));\r\n    InetAddress finalInetAddress = mock(InetAddress.class);\r\n    when(InetAddress.getByAddress(aryEq(externalInet4AddressBytes))).thenReturn(finalInetAddress);\r\n    assertEquals(finalInetAddress, Util.getLocalHost());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\util\\TestUtil.java",
  "methodName" : "testGetLocalHostExternalAddressNotInet4Address",
  "sourceCode" : "@Test\r\npublic void testGetLocalHostExternalAddressNotInet4Address() throws Exception {\r\n    mockStatic(InetAddress.class, NetworkInterface.class);\r\n    InetAddress inetAddressLocalHost = mock(InetAddress.class);\r\n    when(inetAddressLocalHost.isLoopbackAddress()).thenReturn(true);\r\n    when(InetAddress.getLocalHost()).thenReturn(inetAddressLocalHost);\r\n    byte[] externalAddressBytes = new byte[] { 0, 1, 2, 3, 4, 5 };\r\n    InetAddress externalAddress = mock(InetAddress.class);\r\n    when(externalAddress.isLinkLocalAddress()).thenReturn(false);\r\n    when(externalAddress.isLoopbackAddress()).thenReturn(false);\r\n    when(externalAddress.getAddress()).thenReturn(externalAddressBytes);\r\n    NetworkInterface networkInterface = mock(NetworkInterface.class);\r\n    when(networkInterface.getInetAddresses()).thenReturn(Collections.enumeration(Collections.singletonList(externalAddress)));\r\n    when(NetworkInterface.getNetworkInterfaces()).thenReturn(Collections.enumeration(Collections.singletonList(networkInterface)));\r\n    InetAddress finalInetAddress = mock(InetAddress.class);\r\n    when(InetAddress.getByAddress(aryEq(externalAddressBytes))).thenReturn(finalInetAddress);\r\n    assertEquals(finalInetAddress, Util.getLocalHost());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestScheduleAfterDebounceTime.java",
  "methodName" : "testSchedule",
  "sourceCode" : "@Test\r\npublic void testSchedule() throws InterruptedException {\r\n    ScheduleAfterDebounceTime scheduledQueue = new ScheduleAfterDebounceTime(TEST_PROCESSOR_ID);\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final TestObj testObj = new TestScheduleAfterDebounceTime.TestObj();\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST1\", WAIT_TIME, () -> {\r\n        testObj.inc();\r\n        latch.countDown();\r\n    });\r\n    // action is delayed\r\n    Assert.assertEquals(0, testObj.get());\r\n    boolean result = latch.await(WAIT_TIME * 2, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(\"Latch timed-out and task was not scheduled on time.\", result);\r\n    Assert.assertEquals(1, testObj.get());\r\n    scheduledQueue.stopScheduler();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestScheduleAfterDebounceTime.java",
  "methodName" : "testCancelAndSchedule",
  "sourceCode" : "@Test\r\npublic void testCancelAndSchedule() throws InterruptedException {\r\n    ScheduleAfterDebounceTime scheduledQueue = new ScheduleAfterDebounceTime(TEST_PROCESSOR_ID);\r\n    final CountDownLatch test1Latch = new CountDownLatch(1);\r\n    final TestObj testObj = new TestScheduleAfterDebounceTime.TestObj();\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST1\", WAIT_TIME, testObj::inc);\r\n    // next schedule should cancel the previous one with the same name\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST1\", 2 * WAIT_TIME, () -> {\r\n        testObj.inc();\r\n        test1Latch.countDown();\r\n    });\r\n    final TestObj testObj2 = new TestScheduleAfterDebounceTime.TestObj();\r\n    // this schedule should not cancel the previous one, because it has different name\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST2\", WAIT_TIME, testObj2::inc);\r\n    boolean result = test1Latch.await(4 * WAIT_TIME, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(\"Latch timed-out. Scheduled tasks were not run correctly.\", result);\r\n    Assert.assertEquals(1, testObj.get());\r\n    Assert.assertEquals(1, testObj2.get());\r\n    scheduledQueue.stopScheduler();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestScheduleAfterDebounceTime.java",
  "methodName" : "testRunnableWithExceptionInvokesCallback",
  "sourceCode" : "@Test\r\npublic void testRunnableWithExceptionInvokesCallback() throws InterruptedException {\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final Throwable[] taskCallbackException = new Exception[1];\r\n    ScheduleAfterDebounceTime scheduledQueue = new ScheduleAfterDebounceTime(TEST_PROCESSOR_ID);\r\n    scheduledQueue.setScheduledTaskCallback(throwable -> {\r\n        taskCallbackException[0] = throwable;\r\n        latch.countDown();\r\n    });\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST1\", WAIT_TIME, () -> {\r\n        throw new RuntimeException(\"From the runnable!\");\r\n    });\r\n    final TestObj testObj = new TestObj();\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST2\", WAIT_TIME * 2, testObj::inc);\r\n    boolean result = latch.await(5 * WAIT_TIME, TimeUnit.MILLISECONDS);\r\n    Assert.assertTrue(\"Latch timed-out.\", result);\r\n    Assert.assertEquals(0, testObj.get());\r\n    Assert.assertEquals(RuntimeException.class, taskCallbackException[0].getClass());\r\n    scheduledQueue.stopScheduler();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestScheduleAfterDebounceTime.java",
  "methodName" : "testNewTasksScheduledAfterShutdownDoesNotThrowException",
  "sourceCode" : "@Test\r\npublic void testNewTasksScheduledAfterShutdownDoesNotThrowException() throws InterruptedException {\r\n    ScheduleAfterDebounceTime scheduledQueue = new ScheduleAfterDebounceTime(TEST_PROCESSOR_ID);\r\n    scheduledQueue.stopScheduler();\r\n    scheduledQueue.scheduleAfterDebounceTime(\"TEST1\", 2 * WAIT_TIME, () -> Assert.fail(\"New event should not be scheduled\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkBarrierForVersionUpgrade.java",
  "methodName" : "testZkBarrierForVersionUpgrade",
  "sourceCode" : "@Test\r\npublic void testZkBarrierForVersionUpgrade() {\r\n    String barrierId = String.format(\"%s/%s\", zkUtils1.getKeyBuilder().getRootPath(), RandomStringUtils.randomAlphabetic(4));\r\n    List<String> processors = ImmutableList.of(\"p1\", \"p2\");\r\n    CountDownLatch latch = new CountDownLatch(2);\r\n    TestZkBarrierListener listener = new TestZkBarrierListener(latch, State.DONE);\r\n    ZkBarrierForVersionUpgrade processor1Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils, listener, debounceTimer);\r\n    ZkBarrierForVersionUpgrade processor2Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils1, listener, debounceTimer);\r\n    processor1Barrier.create(BARRIER_VERSION, processors);\r\n    String barrierState = zkUtils.getZkClient().readData(barrierId + \"/barrier_1/barrier_state\");\r\n    assertEquals(State.NEW, State.valueOf(barrierState));\r\n    processor1Barrier.join(BARRIER_VERSION, \"p1\");\r\n    processor2Barrier.join(BARRIER_VERSION, \"p2\");\r\n    boolean result = false;\r\n    try {\r\n        result = latch.await(10000, TimeUnit.MILLISECONDS);\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertTrue(\"Barrier failed to complete within test timeout.\", result);\r\n    List<String> children = zkUtils.getZkClient().getChildren(barrierId + \"/barrier_1/barrier_participants\");\r\n    barrierState = zkUtils.getZkClient().readData(barrierId + \"/barrier_1/barrier_state\");\r\n    assertEquals(State.DONE, State.valueOf(barrierState));\r\n    assertNotNull(children);\r\n    assertEquals(\"Unexpected barrier state. Didn't find two processors.\", 2, children.size());\r\n    assertEquals(\"Unexpected barrier state. Didn't find the expected members.\", processors, children);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkBarrierForVersionUpgrade.java",
  "methodName" : "testZkBarrierForVersionUpgradeWithTimeOut",
  "sourceCode" : "@Test\r\npublic void testZkBarrierForVersionUpgradeWithTimeOut() {\r\n    String barrierId = String.format(\"%s/%s\", zkUtils1.getKeyBuilder().getRootPath(), RandomStringUtils.randomAlphabetic(4));\r\n    List<String> processors = ImmutableList.of(\"p1\", \"p2\", \"p3\");\r\n    CountDownLatch latch = new CountDownLatch(2);\r\n    TestZkBarrierListener listener = new TestZkBarrierListener(latch, State.TIMED_OUT);\r\n    ZkBarrierForVersionUpgrade processor1Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils, listener, debounceTimer);\r\n    ZkBarrierForVersionUpgrade processor2Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils1, listener, debounceTimer);\r\n    processor1Barrier.create(BARRIER_VERSION, processors);\r\n    processor1Barrier.join(BARRIER_VERSION, \"p1\");\r\n    processor2Barrier.join(BARRIER_VERSION, \"p2\");\r\n    processor1Barrier.expire(BARRIER_VERSION);\r\n    boolean result = false;\r\n    try {\r\n        result = latch.await(10000, TimeUnit.MILLISECONDS);\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertTrue(\"Barrier Timeout test failed to complete within test timeout.\", result);\r\n    List<String> children = zkUtils.getZkClient().getChildren(barrierId + \"/barrier_1/barrier_participants\");\r\n    String barrierState = zkUtils.getZkClient().readData(barrierId + \"/barrier_1/barrier_state\");\r\n    assertEquals(State.TIMED_OUT, State.valueOf(barrierState));\r\n    assertNotNull(children);\r\n    assertEquals(\"Unexpected barrier state. Didn't find two processors.\", 2, children.size());\r\n    assertEquals(\"Unexpected barrier state. Didn't find the expected members.\", ImmutableList.of(\"p1\", \"p2\"), children);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkBarrierForVersionUpgrade.java",
  "methodName" : "testShouldDiscardBarrierUpdateEventsAfterABarrierIsMarkedAsDone",
  "sourceCode" : "@Test\r\npublic void testShouldDiscardBarrierUpdateEventsAfterABarrierIsMarkedAsDone() {\r\n    String barrierId = String.format(\"%s/%s\", zkUtils1.getKeyBuilder().getRootPath(), RandomStringUtils.randomAlphabetic(4));\r\n    List<String> processors = ImmutableList.of(\"p1\", \"p2\");\r\n    CountDownLatch latch = new CountDownLatch(2);\r\n    TestZkBarrierListener listener = new TestZkBarrierListener(latch, State.DONE);\r\n    ZkBarrierForVersionUpgrade processor1Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils, listener, debounceTimer);\r\n    ZkBarrierForVersionUpgrade processor2Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils1, listener, debounceTimer);\r\n    processor1Barrier.create(BARRIER_VERSION, processors);\r\n    processor1Barrier.join(BARRIER_VERSION, \"p1\");\r\n    processor2Barrier.join(BARRIER_VERSION, \"p2\");\r\n    boolean result = false;\r\n    try {\r\n        result = latch.await(10000, TimeUnit.MILLISECONDS);\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertTrue(\"Barrier Timeout test failed to complete within test timeout.\", result);\r\n    processor1Barrier.expire(BARRIER_VERSION);\r\n    String barrierState = zkUtils.getZkClient().readData(barrierId + \"/barrier_1/barrier_state\");\r\n    assertEquals(State.DONE, State.valueOf(barrierState));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkBarrierForVersionUpgrade.java",
  "methodName" : "testShouldDiscardBarrierUpdateEventsAfterABarrierIsMarkedAsTimedOut",
  "sourceCode" : "@Test\r\npublic void testShouldDiscardBarrierUpdateEventsAfterABarrierIsMarkedAsTimedOut() {\r\n    String barrierId = String.format(\"%s/%s\", zkUtils1.getKeyBuilder().getRootPath(), RandomStringUtils.randomAlphabetic(4));\r\n    List<String> processors = ImmutableList.of(\"p1\", \"p2\", \"p3\");\r\n    CountDownLatch latch = new CountDownLatch(2);\r\n    TestZkBarrierListener listener = new TestZkBarrierListener(latch, State.TIMED_OUT);\r\n    ZkBarrierForVersionUpgrade processor1Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils, listener, debounceTimer);\r\n    ZkBarrierForVersionUpgrade processor2Barrier = new ZkBarrierForVersionUpgrade(barrierId, zkUtils1, listener, debounceTimer);\r\n    processor1Barrier.create(BARRIER_VERSION, processors);\r\n    processor1Barrier.join(BARRIER_VERSION, \"p1\");\r\n    processor2Barrier.join(BARRIER_VERSION, \"p2\");\r\n    processor1Barrier.expire(BARRIER_VERSION);\r\n    boolean result = false;\r\n    try {\r\n        result = latch.await(10000, TimeUnit.MILLISECONDS);\r\n    } catch (InterruptedException e) {\r\n        e.printStackTrace();\r\n    }\r\n    assertTrue(\"Barrier Timeout test failed to complete within test timeout.\", result);\r\n    processor1Barrier.join(BARRIER_VERSION, \"p3\");\r\n    String barrierState = zkUtils.getZkClient().readData(barrierId + \"/barrier_1/barrier_state\");\r\n    assertEquals(State.TIMED_OUT, State.valueOf(barrierState));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkClusterMembership.java",
  "methodName" : "testMembershipSingleProcessor",
  "sourceCode" : "@Test\r\npublic void testMembershipSingleProcessor() {\r\n    // happy path for single processor\r\n    ZkClusterMembership clusterMembership = new ZkClusterMembership(\"p1\", zkUtils1);\r\n    String processorId;\r\n    assertEquals(\"ClusterMembership has participants before any processor registered.\", 0, clusterMembership.getNumberOfProcessors());\r\n    processorId = clusterMembership.registerProcessor();\r\n    assertEquals(\"ClusterMembership does not have participants after a processor registered.\", 1, clusterMembership.getNumberOfProcessors());\r\n    clusterMembership.unregisterProcessor(processorId);\r\n    assertEquals(\"ClusterMembership has participants after the single processor unregistered.\", 0, clusterMembership.getNumberOfProcessors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkClusterMembership.java",
  "methodName" : "testMembershipTwoProcessors",
  "sourceCode" : "@Test\r\npublic void testMembershipTwoProcessors() {\r\n    // Two processors register. Check if second processor registering gets 2 as number of processors.\r\n    ZkClusterMembership clusterMembership1 = new ZkClusterMembership(\"p1\", zkUtils1);\r\n    ZkClusterMembership clusterMembership2 = new ZkClusterMembership(\"p2\", zkUtils1);\r\n    String processorId1;\r\n    String processorId2;\r\n    assertEquals(\"ClusterMembership has participants before any processor registered.\", 0, clusterMembership1.getNumberOfProcessors());\r\n    processorId1 = clusterMembership1.registerProcessor();\r\n    assertEquals(\"ClusterMembership does not have participants after one processor registered.\", 1, clusterMembership1.getNumberOfProcessors());\r\n    processorId2 = clusterMembership2.registerProcessor();\r\n    assertEquals(\"ClusterMembership does not have 2 participants after two processor registered.\", 2, clusterMembership2.getNumberOfProcessors());\r\n    clusterMembership1.unregisterProcessor(processorId1);\r\n    clusterMembership2.unregisterProcessor(processorId2);\r\n    assertEquals(\"ClusterMembership has participants after both processors unregistered.\", 0, clusterMembership1.getNumberOfProcessors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkClusterMembership.java",
  "methodName" : "testMembershipFirstProcessorUnregister",
  "sourceCode" : "@Test\r\npublic void testMembershipFirstProcessorUnregister() {\r\n    // First processor unregisters. Check if second processor registering gets 1 as number of processors.\r\n    ZkClusterMembership clusterMembership1 = new ZkClusterMembership(\"p1\", zkUtils1);\r\n    ZkClusterMembership clusterMembership2 = new ZkClusterMembership(\"p2\", zkUtils1);\r\n    String processorId1;\r\n    String processorId2;\r\n    assertEquals(\"ClusterMembership has participants before any processor registered.\", 0, clusterMembership1.getNumberOfProcessors());\r\n    processorId1 = clusterMembership1.registerProcessor();\r\n    assertEquals(\"ClusterMembership does not have participants after one processor registered.\", 1, clusterMembership1.getNumberOfProcessors());\r\n    clusterMembership1.unregisterProcessor(processorId1);\r\n    processorId2 = clusterMembership2.registerProcessor();\r\n    assertEquals(\"ClusterMembership does not have 1 participant1 after second processor registered.\", 1, clusterMembership2.getNumberOfProcessors());\r\n    clusterMembership2.unregisterProcessor(processorId2);\r\n    assertEquals(\"ClusterMembership has participants after both processors unregistered.\", 0, clusterMembership2.getNumberOfProcessors());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkDistributedLock.java",
  "methodName" : "testLockSingleProcessor",
  "sourceCode" : "@Test\r\npublic void testLockSingleProcessor() {\r\n    String lockId = \"FAKE_LOCK_ID_1\";\r\n    ZkDistributedLock lock1 = new ZkDistributedLock(\"p1\", zkUtils1, lockId);\r\n    assertEquals(\"Lock has participants before any processor tried to lock.\", 0, getParticipants(zkUtils1, lockId).size());\r\n    boolean lock1Status = lock1.lock(Duration.ofMillis(10000));\r\n    assertEquals(\"Lock does not have 1 participant after first processor tries to lock.\", 1, getParticipants(zkUtils1, lockId).size());\r\n    assertEquals(\"1st processor requesting to lock did not acquire the lock.\", true, lock1Status);\r\n    lock1.unlock();\r\n    assertEquals(\"Lock does have 1 participant after first processor tries to unlock.\", 0, getParticipants(zkUtils1, lockId).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkDistributedLock.java",
  "methodName" : "testLockTwoProcessors",
  "sourceCode" : "@Test\r\npublic void testLockTwoProcessors() {\r\n    // second processor should acquire lock after first one unlocks\r\n    String lockId = \"FAKE_LOCK_ID_2\";\r\n    ZkDistributedLock lock1 = new ZkDistributedLock(\"p1\", zkUtils1, lockId);\r\n    ZkDistributedLock lock2 = new ZkDistributedLock(\"p2\", zkUtils2, lockId);\r\n    assertEquals(\"Lock has participants before any processor tried to lock.\", 0, getParticipants(zkUtils1, lockId).size());\r\n    boolean lock1Status = lock1.lock(Duration.ofMillis(10000));\r\n    assertEquals(\"First processor requesting to lock did not acquire the lock.\", true, lock1Status);\r\n    lock1.unlock();\r\n    boolean lock2Status = lock2.lock(Duration.ofMillis(10000));\r\n    assertEquals(\"Second processor requesting to lock did not acquire the lock.\", true, lock2Status);\r\n    lock2.unlock();\r\n    assertEquals(\"Lock does have participants after processors unlocked.\", 0, getParticipants(zkUtils1, lockId).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkDistributedLock.java",
  "methodName" : "testLockFirstProcessorClosing",
  "sourceCode" : "@Test\r\npublic void testLockFirstProcessorClosing() {\r\n    // first processor dies before unlock then second processor should acquire\r\n    String lockId = \"FAKE_LOCK_ID_3\";\r\n    ZkDistributedLock lock1 = new ZkDistributedLock(\"p1\", zkUtils1, lockId);\r\n    ZkDistributedLock lock2 = new ZkDistributedLock(\"p2\", zkUtils2, lockId);\r\n    assertEquals(\"Lock has participants before any processor tried to lock!\", 0, getParticipants(zkUtils1, lockId).size());\r\n    boolean lock1Status = lock1.lock(Duration.ofMillis(10000));\r\n    assertEquals(\"First processor requesting to lock did not acquire the lock.\", true, lock1Status);\r\n    // first processor dies before unlock\r\n    zkUtils1.close();\r\n    boolean lock2Status = lock2.lock(Duration.ofMillis(10000));\r\n    assertEquals(\"Second processor requesting to lock did not acquire the lock.\", true, lock2Status);\r\n    lock2.unlock();\r\n    assertEquals(\"Lock does have participants after processors unlocked.\", 0, getParticipants(zkUtils2, lockId).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testCheckAndExpireWithMultipleRebalances",
  "sourceCode" : "@Test\r\npublic void testCheckAndExpireWithMultipleRebalances() {\r\n    final TaskName taskName = new TaskName(\"task1\");\r\n    final ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    final JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    final JobModel jobModelVersion1 = mock(JobModel.class);\r\n    final JobModel jobModelVersion2 = mock(JobModel.class);\r\n    final JobModel jobModelVersion3 = jobModelVersion1;\r\n    when(mockContainerModel.getTasks()).thenReturn(ImmutableMap.of(taskName, mock(TaskModel.class)));\r\n    when(jobModelVersion3.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel));\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    zkJobCoordinator.setActiveJobModel(jobModelVersion1);\r\n    /*\r\n     * The following mimics the scenario where new work assignment(V2) is proposed by the leader and the work assignment\r\n     * differs from the active work assignment(V1) and hence results in job model expiration\r\n     */\r\n    zkJobCoordinator.checkAndExpireJobModel(jobModelVersion2);\r\n    verify(mockListener, times(1)).onJobModelExpired();\r\n    assertTrue(\"JobModelExpired should be true for work assignment changes\", zkJobCoordinator.getJobModelExpired());\r\n    assertEquals(\"Active job model shouldn't be updated\", jobModelVersion1, zkJobCoordinator.getActiveJobModel());\r\n    /*\r\n     * The following mimics the scenario where leader kicked off another rebalance where the new work assignment(V3)\r\n     * is same as the old work assignment(V1) and doesn't trigger job model expiration. We check the interactions w/\r\n     * the listener to ensure job model expiration isn't invoked. However, the previous rebalance should have already\r\n     * triggered job model expiration and set the job model expired flag to true\r\n     */\r\n    zkJobCoordinator.checkAndExpireJobModel(jobModelVersion1);\r\n    verifyNoMoreInteractions(mockListener);\r\n    assertTrue(\"JobModelExpired should remain unchanged\", zkJobCoordinator.getJobModelExpired());\r\n    assertEquals(\"Active job model shouldn't be updated\", jobModelVersion1, zkJobCoordinator.getActiveJobModel());\r\n    /*\r\n     * The following mimics the scenario where the new work assignment(V3) proposed by the leader is accepted and\r\n     * on new job model is invoked. Even though the work assignment remains the same w/ the active job model version,\r\n     * onNewJobModel is invoked on the listener as an intermediate rebalance expired the old work assignment(V1)\r\n     */\r\n    zkJobCoordinator.onNewJobModel(jobModelVersion3);\r\n    verify(mockListener, times(1)).onNewJobModel(PROCESSOR_ID, jobModelVersion3);\r\n    verify(zkUtils, times(1)).writeTaskLocality(any(), any());\r\n    assertEquals(\"Active job model should be updated to new job model\", zkJobCoordinator.getActiveJobModel(), jobModelVersion3);\r\n    assertFalse(\"JobModelExpired should be set to false after onNewJobModel\", zkJobCoordinator.getJobModelExpired());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testCheckAndExpireWithNoChangeInWorkAssignment",
  "sourceCode" : "@Test\r\npublic void testCheckAndExpireWithNoChangeInWorkAssignment() {\r\n    BiConsumer<ZkUtils, JobCoordinatorListener> verificationMethod = (ignored, coordinatorListener) -> verifyZeroInteractions(coordinatorListener);\r\n    testNoChangesInWorkAssignmentHelper(ZkJobCoordinator::checkAndExpireJobModel, verificationMethod);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testCheckAndExpireWithChangeInWorkAssignment",
  "sourceCode" : "@Test\r\npublic void testCheckAndExpireWithChangeInWorkAssignment() {\r\n    final String processorId = \"testProcessor\";\r\n    JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(processorId, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    zkJobCoordinator.checkAndExpireJobModel(mock(JobModel.class));\r\n    verify(mockListener, times(1)).onJobModelExpired();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testCheckAndExpireJobModelWithNullJobModel",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testCheckAndExpireJobModelWithNullJobModel() {\r\n    final String processorId = \"testProcessor\";\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(processorId, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.checkAndExpireJobModel(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testOnNewJobModelWithChangeInWorkAssignment",
  "sourceCode" : "@Test\r\npublic void testOnNewJobModelWithChangeInWorkAssignment() {\r\n    final TaskName taskName = new TaskName(\"task1\");\r\n    final ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    final JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    final JobModel mockJobModel = mock(JobModel.class);\r\n    when(mockContainerModel.getTasks()).thenReturn(ImmutableMap.of(taskName, mock(TaskModel.class)));\r\n    when(mockJobModel.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel));\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    zkJobCoordinator.setJobModelExpired(true);\r\n    zkJobCoordinator.onNewJobModel(mockJobModel);\r\n    verify(zkUtils, times(1)).writeTaskLocality(eq(taskName), any());\r\n    verify(mockListener, times(1)).onNewJobModel(PROCESSOR_ID, mockJobModel);\r\n    assertEquals(\"Active job model should be updated with the new job model\", mockJobModel, zkJobCoordinator.getActiveJobModel());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testOnNewJobModelWithNoChangesInWorkAssignment",
  "sourceCode" : "@Test\r\npublic void testOnNewJobModelWithNoChangesInWorkAssignment() {\r\n    BiConsumer<ZkUtils, JobCoordinatorListener> verificationMethod = (zkUtils, coordinatorListener) -> {\r\n        verify(zkUtils, times(0)).writeTaskLocality(any(), any());\r\n        verifyZeroInteractions(coordinatorListener);\r\n    };\r\n    testNoChangesInWorkAssignmentHelper(ZkJobCoordinator::onNewJobModel, verificationMethod);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testOnNewJobModelWithNullJobModel",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testOnNewJobModelWithNullJobModel() {\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.onNewJobModel(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testJobModelVersionChangeWithChangeInWorkAssignment",
  "sourceCode" : "/**\r\n * Test job model version changed changes to work assignment. In this scenario, existing work should\r\n * be stopped a.k.a processor should stop the container through the listener. The processor then proceeds to join\r\n * the barrier to notify its acceptance on the proposed job model.\r\n */\r\n@Test\r\npublic void testJobModelVersionChangeWithChangeInWorkAssignment() throws Exception {\r\n    BiConsumer<ZkBarrierForVersionUpgrade, JobCoordinatorListener> verificationMethod = (barrier, listener) -> {\r\n        verify(listener, times(1)).onJobModelExpired();\r\n        verify(barrier, times(1)).join(TEST_JOB_MODEL_VERSION, PROCESSOR_ID);\r\n    };\r\n    testJobModelVersionChangeHelper(null, mock(JobModel.class), verificationMethod);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testJobModelVersionChangeWithNoChangeInWorkAssignment",
  "sourceCode" : "/**\r\n * Test job model version changed without any changes to work assignment. In this scenario, existing work should\r\n * not be stopped a.k.a processor shouldn't stop the container. However, the processor proceeds to join the barrier\r\n * to notify its acceptance on the proposed job model.\r\n */\r\n@Test\r\npublic void testJobModelVersionChangeWithNoChangeInWorkAssignment() throws Exception {\r\n    final JobModel jobModel = mock(JobModel.class);\r\n    BiConsumer<ZkBarrierForVersionUpgrade, JobCoordinatorListener> verificationMethod = (barrier, listener) -> {\r\n        verifyZeroInteractions(listener);\r\n        verify(barrier, times(1)).join(TEST_JOB_MODEL_VERSION, PROCESSOR_ID);\r\n    };\r\n    testJobModelVersionChangeHelper(jobModel, jobModel, verificationMethod);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testShouldRemoveBufferedEventsInDebounceQueueOnSessionExpiration",
  "sourceCode" : "@Test\r\npublic void testShouldRemoveBufferedEventsInDebounceQueueOnSessionExpiration() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(new JobModel(new MapConfig(), new HashMap<>()));\r\n    ScheduleAfterDebounceTime mockDebounceTimer = Mockito.mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    zkJobCoordinator.debounceTimer = mockDebounceTimer;\r\n    zkJobCoordinator.zkSessionMetrics = new ZkSessionMetrics(new MetricsRegistryMap());\r\n    final ZkSessionStateChangedListener zkSessionStateChangedListener = zkJobCoordinator.new ZkSessionStateChangedListener();\r\n    zkSessionStateChangedListener.handleStateChanged(Watcher.Event.KeeperState.Expired);\r\n    verify(zkUtils).incGeneration();\r\n    verify(mockDebounceTimer).cancelAllScheduledActions();\r\n    verify(mockDebounceTimer).scheduleAfterDebounceTime(eq(\"ZK_SESSION_EXPIRED\"), eq(0L), Mockito.any(Runnable.class));\r\n    Assert.assertEquals(1, zkJobCoordinator.zkSessionMetrics.zkSessionExpirations.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testZookeeperSessionMetricsAreUpdatedCorrectly",
  "sourceCode" : "@Test\r\npublic void testZookeeperSessionMetricsAreUpdatedCorrectly() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(new JobModel(new MapConfig(), new HashMap<>()));\r\n    ScheduleAfterDebounceTime mockDebounceTimer = Mockito.mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    zkJobCoordinator.debounceTimer = mockDebounceTimer;\r\n    zkJobCoordinator.zkSessionMetrics = new ZkSessionMetrics(new MetricsRegistryMap());\r\n    final ZkSessionStateChangedListener zkSessionStateChangedListener = zkJobCoordinator.new ZkSessionStateChangedListener();\r\n    zkSessionStateChangedListener.handleStateChanged(Watcher.Event.KeeperState.Disconnected);\r\n    zkSessionStateChangedListener.handleStateChanged(Watcher.Event.KeeperState.SyncConnected);\r\n    zkSessionStateChangedListener.handleStateChanged(Watcher.Event.KeeperState.AuthFailed);\r\n    Assert.assertEquals(1, zkJobCoordinator.zkSessionMetrics.zkSessionErrors.getCount());\r\n    zkSessionStateChangedListener.handleSessionEstablishmentError(new SamzaException(\"Test exception\"));\r\n    Assert.assertEquals(1, zkJobCoordinator.zkSessionMetrics.zkSessionDisconnects.getCount());\r\n    Assert.assertEquals(1, zkJobCoordinator.zkSessionMetrics.zkSyncConnected.getCount());\r\n    Assert.assertEquals(2, zkJobCoordinator.zkSessionMetrics.zkSessionErrors.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testShouldStopPartitionCountMonitorOnSessionExpiration",
  "sourceCode" : "@Test\r\npublic void testShouldStopPartitionCountMonitorOnSessionExpiration() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(new JobModel(new MapConfig(), new HashMap<>()));\r\n    ScheduleAfterDebounceTime mockDebounceTimer = Mockito.mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    StreamPartitionCountMonitor monitor = Mockito.mock(StreamPartitionCountMonitor.class);\r\n    zkJobCoordinator.debounceTimer = mockDebounceTimer;\r\n    zkJobCoordinator.streamPartitionCountMonitor = monitor;\r\n    ZkSessionStateChangedListener zkSessionStateChangedListener = zkJobCoordinator.new ZkSessionStateChangedListener();\r\n    zkSessionStateChangedListener.handleStateChanged(Watcher.Event.KeeperState.Expired);\r\n    Mockito.verify(monitor).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testShouldStartPartitionCountMonitorOnBecomingLeader",
  "sourceCode" : "@Test\r\npublic void testShouldStartPartitionCountMonitorOnBecomingLeader() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(new JobModel(new MapConfig(), new HashMap<>()));\r\n    ScheduleAfterDebounceTime mockDebounceTimer = Mockito.mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    StreamPartitionCountMonitor monitor = Mockito.mock(StreamPartitionCountMonitor.class);\r\n    zkJobCoordinator.debounceTimer = mockDebounceTimer;\r\n    zkJobCoordinator.streamPartitionCountMonitor = monitor;\r\n    doReturn(monitor).when(zkJobCoordinator).getPartitionCountMonitor();\r\n    ZkJobCoordinator.LeaderElectorListenerImpl listener = zkJobCoordinator.new LeaderElectorListenerImpl();\r\n    listener.onBecomingLeader();\r\n    Mockito.verify(monitor).start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testShouldStopPartitionCountMonitorWhenStoppingTheJobCoordinator",
  "sourceCode" : "@Test\r\npublic void testShouldStopPartitionCountMonitorWhenStoppingTheJobCoordinator() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(new JobModel(new MapConfig(), new HashMap<>()));\r\n    ScheduleAfterDebounceTime mockDebounceTimer = Mockito.mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    StreamPartitionCountMonitor monitor = Mockito.mock(StreamPartitionCountMonitor.class);\r\n    zkJobCoordinator.debounceTimer = mockDebounceTimer;\r\n    zkJobCoordinator.streamPartitionCountMonitor = monitor;\r\n    zkJobCoordinator.stop();\r\n    Mockito.verify(monitor).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testStartWithActiveJobModelDisabled",
  "sourceCode" : "@Test\r\npublic void testStartWithActiveJobModelDisabled() {\r\n    final ScheduleAfterDebounceTime mockDebounceTimer = mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setLeaderElector(mock(ZkLeaderElector.class));\r\n    zkJobCoordinator.setDebounceTimer(mockDebounceTimer);\r\n    zkJobCoordinator.start();\r\n    verifyZeroInteractions(mockDebounceTimer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testStartWithActiveJobModelEnabled",
  "sourceCode" : "@Test\r\npublic void testStartWithActiveJobModelEnabled() {\r\n    final ScheduleAfterDebounceTime mockDebounceTimer = mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, config, new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setLeaderElector(mock(ZkLeaderElector.class));\r\n    zkJobCoordinator.setDebounceTimer(mockDebounceTimer);\r\n    zkJobCoordinator.start();\r\n    verify(mockDebounceTimer, times(1)).scheduleAfterDebounceTime(eq(ZkJobCoordinator.START_WORK_WITH_LAST_ACTIVE_JOB_MODEL), anyLong(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testStartWorkWithLastActiveJobModel",
  "sourceCode" : "@Test\r\npublic void testStartWorkWithLastActiveJobModel() {\r\n    final TaskName taskName = new TaskName(\"task1\");\r\n    final ContainerModel mockContainerModel = mock(ContainerModel.class);\r\n    final JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    final JobModel mockJobModel = mock(JobModel.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    when(mockContainerModel.getTasks()).thenReturn(ImmutableMap.of(taskName, mock(TaskModel.class)));\r\n    when(mockJobModel.getContainers()).thenReturn(ImmutableMap.of(PROCESSOR_ID, mockContainerModel));\r\n    when(zkUtils.getLastActiveJobModelVersion()).thenReturn(TEST_JOB_MODEL_VERSION);\r\n    when(zkUtils.getJobModelVersion()).thenReturn(TEST_JOB_MODEL_VERSION);\r\n    doReturn(mockJobModel).when(zkJobCoordinator).readJobModelFromMetadataStore(TEST_JOB_MODEL_VERSION);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    zkJobCoordinator.startWorkWithLastActiveJobModel();\r\n    verify(mockListener, times(1)).onJobModelExpired();\r\n    verify(zkUtils, times(1)).writeTaskLocality(eq(taskName), any());\r\n    verify(mockListener, times(1)).onNewJobModel(PROCESSOR_ID, mockJobModel);\r\n    assertEquals(\"Active job model should be updated with the new job model\", mockJobModel, zkJobCoordinator.getActiveJobModel());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testStartWorkWithLastActiveJobModelShouldNotStartContainer",
  "sourceCode" : "@Test\r\npublic void testStartWorkWithLastActiveJobModelShouldNotStartContainer() {\r\n    final JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    when(zkUtils.getLastActiveJobModelVersion()).thenReturn(TEST_JOB_MODEL_VERSION);\r\n    when(zkUtils.getJobModelVersion()).thenReturn(LATEST_JOB_MODEL_VERSION);\r\n    zkJobCoordinator.startWorkWithLastActiveJobModel();\r\n    verifyZeroInteractions(mockListener);\r\n    assertNull(\"Expected active job model to be null\", zkJobCoordinator.getActiveJobModel());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testStartWorkWithLastActiveJobModelWithNullActiveJobModelVersion",
  "sourceCode" : "@Test\r\npublic void testStartWorkWithLastActiveJobModelWithNullActiveJobModelVersion() {\r\n    final JobCoordinatorListener mockListener = mock(JobCoordinatorListener.class);\r\n    ZkJobCoordinator zkJobCoordinator = new ZkJobCoordinator(PROCESSOR_ID, new MapConfig(), new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore);\r\n    zkJobCoordinator.setListener(mockListener);\r\n    zkJobCoordinator.startWorkWithLastActiveJobModel();\r\n    verifyZeroInteractions(mockListener);\r\n    assertNull(\"Expected active job model to be null\", zkJobCoordinator.getActiveJobModel());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testLoadMetadataResources",
  "sourceCode" : "@Test\r\npublic void testLoadMetadataResources() throws IOException {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(jobModel);\r\n    StartpointManager mockStartpointManager = Mockito.mock(StartpointManager.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, config, new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    doReturn(mockStartpointManager).when(zkJobCoordinator).createStartpointManager();\r\n    MetadataResourceUtil mockMetadataResourceUtil = mock(MetadataResourceUtil.class);\r\n    doReturn(mockMetadataResourceUtil).when(zkJobCoordinator).createMetadataResourceUtil(any(), any(Config.class));\r\n    verifyZeroInteractions(mockStartpointManager);\r\n    zkJobCoordinator.loadMetadataResources(jobModel);\r\n    verify(mockMetadataResourceUtil).createResources();\r\n    verify(mockStartpointManager).start();\r\n    verify(mockStartpointManager).fanOut(any());\r\n    verify(mockStartpointManager).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testDoOnProcessorChange",
  "sourceCode" : "@Test\r\npublic void testDoOnProcessorChange() {\r\n    when(zkUtils.getJobModel(TEST_JOB_MODEL_VERSION)).thenReturn(jobModel);\r\n    StartpointManager mockStartpointManager = Mockito.mock(StartpointManager.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, config, new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    doReturn(mockStartpointManager).when(zkJobCoordinator).createStartpointManager();\r\n    doReturn(jobModel).when(zkJobCoordinator).generateNewJobModel(any());\r\n    doNothing().when(zkJobCoordinator).loadMetadataResources(jobModel);\r\n    zkJobCoordinator.doOnProcessorChange();\r\n    verify(zkUtils).publishJobModelVersion(anyString(), anyString());\r\n    verify(zkJobCoordinator).loadMetadataResources(eq(jobModel));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkJobCoordinator.java",
  "methodName" : "testDoOnProcessorChangeWithNoChangesToWorkAssignment",
  "sourceCode" : "@Test\r\npublic void testDoOnProcessorChangeWithNoChangesToWorkAssignment() {\r\n    ZkBarrierForVersionUpgrade mockBarrier = mock(ZkBarrierForVersionUpgrade.class);\r\n    ScheduleAfterDebounceTime mockDebounceTimer = mock(ScheduleAfterDebounceTime.class);\r\n    ZkJobCoordinator zkJobCoordinator = Mockito.spy(new ZkJobCoordinator(PROCESSOR_ID, config, new NoOpMetricsRegistry(), zkUtils, zkMetadataStore, coordinatorStreamStore));\r\n    zkJobCoordinator.setActiveJobModel(jobModel);\r\n    zkJobCoordinator.setDebounceTimer(mockDebounceTimer);\r\n    zkJobCoordinator.setZkBarrierUpgradeForVersion(mockBarrier);\r\n    doReturn(jobModel).when(zkJobCoordinator).generateNewJobModel(any());\r\n    zkJobCoordinator.doOnProcessorChange();\r\n    verify(zkUtils, times(0)).publishJobModelVersion(anyString(), anyString());\r\n    verifyZeroInteractions(mockBarrier);\r\n    verifyZeroInteractions(mockDebounceTimer);\r\n    verify(zkJobCoordinator, times(0)).loadMetadataResources(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkKeyBuilder.java",
  "methodName" : "pathPrefixCannotBeNull",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void pathPrefixCannotBeNull() {\r\n    new ZkKeyBuilder(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkKeyBuilder.java",
  "methodName" : "pathPrefixCannotBeEmpty",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void pathPrefixCannotBeEmpty() {\r\n    new ZkKeyBuilder(\"    \");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkKeyBuilder.java",
  "methodName" : "testProcessorsPath",
  "sourceCode" : "@Test\r\npublic void testProcessorsPath() {\r\n    ZkKeyBuilder builder = new ZkKeyBuilder(\"test\");\r\n    Assert.assertEquals(\"/test/\" + ZkKeyBuilder.PROCESSORS_PATH, builder.getProcessorsPath());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkKeyBuilder.java",
  "methodName" : "testParseIdFromPath",
  "sourceCode" : "@Test\r\npublic void testParseIdFromPath() {\r\n    Assert.assertEquals(\"1\", ZkKeyBuilder.parseIdFromPath(\"/test/processors/\" + \"1\"));\r\n    Assert.assertNull(ZkKeyBuilder.parseIdFromPath(null));\r\n    Assert.assertNull(ZkKeyBuilder.parseIdFromPath(\"\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkKeyBuilder.java",
  "methodName" : "testJobModelPath",
  "sourceCode" : "@Test\r\npublic void testJobModelPath() {\r\n    ZkKeyBuilder builder = new ZkKeyBuilder(\"test\");\r\n    Assert.assertEquals(\"/test/\" + ZkKeyBuilder.JOBMODEL_GENERATION_PATH + \"/jobModelVersion\", builder.getJobModelVersionPath());\r\n    Assert.assertEquals(\"/test/\" + ZkKeyBuilder.JOBMODEL_GENERATION_PATH + \"/jobModels\", builder.getJobModelPathPrefix());\r\n    String version = \"2\";\r\n    Assert.assertEquals(\"/test/\" + ZkKeyBuilder.JOBMODEL_GENERATION_PATH + \"/jobModels/\" + version, builder.getJobModelPath(version));\r\n    Assert.assertEquals(\"/test/\" + ZkKeyBuilder.JOBMODEL_GENERATION_PATH + \"/\" + ZkKeyBuilder.JOB_MODEL_UPGRADE_BARRIER_PATH + \"/versionBarriers\", builder.getJobModelVersionBarrierPrefix());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testLeaderElectionRegistersProcessor",
  "sourceCode" : "@Test\r\npublic void testLeaderElectionRegistersProcessor() {\r\n    List<String> activeProcessors = new ArrayList<String>() {\r\n\r\n        {\r\n            add(\"0000000000\");\r\n        }\r\n    };\r\n    ZkUtils mockZkUtils = mock(ZkUtils.class);\r\n    when(mockZkUtils.registerProcessorAndGetId(any())).thenReturn(KEY_BUILDER.getProcessorsPath() + \"/0000000000\");\r\n    when(mockZkUtils.getSortedActiveProcessorsZnodes()).thenReturn(activeProcessors);\r\n    Mockito.doNothing().when(mockZkUtils).validatePaths(any(String[].class));\r\n    ZkKeyBuilder kb = mock(ZkKeyBuilder.class);\r\n    when(kb.getProcessorsPath()).thenReturn(\"\");\r\n    when(mockZkUtils.getKeyBuilder()).thenReturn(kb);\r\n    ZkLeaderElector leaderElector = new ZkLeaderElector(\"1\", mockZkUtils, null);\r\n    BooleanResult isLeader = new BooleanResult();\r\n    leaderElector.setLeaderElectorListener(() -> isLeader.res = true);\r\n    leaderElector.tryBecomeLeader();\r\n    Assert.assertTrue(TestZkUtils.testWithDelayBackOff(() -> isLeader.res, 2, 100));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testUnregisteredProcessorInLeaderElection",
  "sourceCode" : "@Test\r\npublic void testUnregisteredProcessorInLeaderElection() {\r\n    String processorId = \"1\";\r\n    ZkUtils mockZkUtils = mock(ZkUtils.class);\r\n    when(mockZkUtils.getSortedActiveProcessorsZnodes()).thenReturn(new ArrayList<String>());\r\n    Mockito.doNothing().when(mockZkUtils).validatePaths(any(String[].class));\r\n    ZkKeyBuilder kb = mock(ZkKeyBuilder.class);\r\n    when(kb.getProcessorsPath()).thenReturn(\"\");\r\n    when(mockZkUtils.getKeyBuilder()).thenReturn(kb);\r\n    ZkLeaderElector leaderElector = new ZkLeaderElector(processorId, mockZkUtils, null);\r\n    leaderElector.setLeaderElectorListener(() -> {\r\n    });\r\n    try {\r\n        leaderElector.tryBecomeLeader();\r\n        Assert.fail(\"Was expecting leader election to fail!\");\r\n    } catch (SamzaException e) {\r\n        // No-op Expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testLeaderElection",
  "sourceCode" : "/**\r\n * Test starts 3 processors and verifies the state of the Zk tree after all processors participate in LeaderElection\r\n */\r\n@Test\r\npublic void testLeaderElection() {\r\n    BooleanResult isLeader1 = new BooleanResult();\r\n    BooleanResult isLeader2 = new BooleanResult();\r\n    BooleanResult isLeader3 = new BooleanResult();\r\n    // Processor-1\r\n    ZkUtils zkUtils1 = getZkUtilsWithNewClient();\r\n    ZkLeaderElector leaderElector1 = new ZkLeaderElector(\"1\", zkUtils1);\r\n    leaderElector1.setLeaderElectorListener(() -> isLeader1.res = true);\r\n    // Processor-2\r\n    ZkUtils zkUtils2 = getZkUtilsWithNewClient();\r\n    ZkLeaderElector leaderElector2 = new ZkLeaderElector(\"2\", zkUtils2);\r\n    leaderElector2.setLeaderElectorListener(() -> isLeader2.res = true);\r\n    // Processor-3\r\n    ZkUtils zkUtils3 = getZkUtilsWithNewClient();\r\n    ZkLeaderElector leaderElector3 = new ZkLeaderElector(\"3\", zkUtils3);\r\n    leaderElector3.setLeaderElectorListener(() -> isLeader3.res = true);\r\n    Assert.assertEquals(0, testZkUtils.getSortedActiveProcessorsZnodes().size());\r\n    leaderElector1.tryBecomeLeader();\r\n    leaderElector2.tryBecomeLeader();\r\n    leaderElector3.tryBecomeLeader();\r\n    Assert.assertTrue(TestZkUtils.testWithDelayBackOff(() -> isLeader1.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader2.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader3.res, 2, 100));\r\n    Assert.assertEquals(3, testZkUtils.getSortedActiveProcessorsZnodes().size());\r\n    // Clean up\r\n    zkUtils1.close();\r\n    zkUtils2.close();\r\n    zkUtils3.close();\r\n    Assert.assertEquals(new ArrayList<String>(), testZkUtils.getSortedActiveProcessorsZnodes());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testLeaderFailure",
  "sourceCode" : "/**\r\n * Tests that Leader Failure automatically promotes the next successor to become the leader\r\n */\r\n@Test\r\npublic void testLeaderFailure() {\r\n    /**\r\n     * electionLatch and count together verify that:\r\n     * 1. the registered listeners are actually invoked by the ZkClient on the correct path\r\n     * 2. for a single participant failure, at-most 1 other participant is notified\r\n     */\r\n    final CountDownLatch electionLatch = new CountDownLatch(1);\r\n    final AtomicInteger count = new AtomicInteger(0);\r\n    BooleanResult isLeader1 = new BooleanResult();\r\n    BooleanResult isLeader2 = new BooleanResult();\r\n    BooleanResult isLeader3 = new BooleanResult();\r\n    // Processor-1\r\n    ZkUtils zkUtils1 = getZkUtilsWithNewClient();\r\n    zkUtils1.registerProcessorAndGetId(new ProcessorData(\"processor1\", \"1\"));\r\n    ZkLeaderElector leaderElector1 = new ZkLeaderElector(\"processor1\", zkUtils1, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            count.incrementAndGet();\r\n        }\r\n    });\r\n    leaderElector1.setLeaderElectorListener(() -> isLeader1.res = true);\r\n    // Processor-2\r\n    ZkUtils zkUtils2 = getZkUtilsWithNewClient();\r\n    final String path2 = zkUtils2.registerProcessorAndGetId(new ProcessorData(\"processor2\", \"2\"));\r\n    ZkLeaderElector leaderElector2 = new ZkLeaderElector(\"processor2\", zkUtils2, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            String registeredIdStr = ZkKeyBuilder.parseIdFromPath(path2);\r\n            Assert.assertNotNull(registeredIdStr);\r\n            String predecessorIdStr = ZkKeyBuilder.parseIdFromPath(dataPath);\r\n            Assert.assertNotNull(predecessorIdStr);\r\n            try {\r\n                int selfId = Integer.parseInt(registeredIdStr);\r\n                int predecessorId = Integer.parseInt(predecessorIdStr);\r\n                Assert.assertEquals(1, selfId - predecessorId);\r\n            } catch (Exception e) {\r\n                LOG.error(e.getLocalizedMessage());\r\n            }\r\n            count.incrementAndGet();\r\n            electionLatch.countDown();\r\n        }\r\n    });\r\n    leaderElector2.setLeaderElectorListener(() -> isLeader2.res = true);\r\n    // Processor-3\r\n    ZkUtils zkUtils3 = getZkUtilsWithNewClient();\r\n    zkUtils3.registerProcessorAndGetId(new ProcessorData(\"processor3\", \"3\"));\r\n    ZkLeaderElector leaderElector3 = new ZkLeaderElector(\"processor3\", zkUtils3, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            count.incrementAndGet();\r\n        }\r\n    });\r\n    leaderElector3.setLeaderElectorListener(() -> isLeader3.res = true);\r\n    // Join Leader Election\r\n    leaderElector1.tryBecomeLeader();\r\n    leaderElector2.tryBecomeLeader();\r\n    leaderElector3.tryBecomeLeader();\r\n    Assert.assertTrue(TestZkUtils.testWithDelayBackOff(() -> isLeader1.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader2.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader3.res, 2, 100));\r\n    Assert.assertTrue(leaderElector1.amILeader());\r\n    Assert.assertFalse(leaderElector2.amILeader());\r\n    Assert.assertFalse(leaderElector3.amILeader());\r\n    List<String> currentActiveProcessors = zkUtils1.getSortedActiveProcessorsZnodes();\r\n    Assert.assertEquals(3, currentActiveProcessors.size());\r\n    // Leader Failure\r\n    zkUtils1.close();\r\n    currentActiveProcessors.remove(0);\r\n    try {\r\n        Assert.assertTrue(electionLatch.await(5, TimeUnit.SECONDS));\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Interrupted while waiting for leaderElection listener callback to complete!\");\r\n    }\r\n    Assert.assertEquals(1, count.get());\r\n    Assert.assertEquals(currentActiveProcessors, zkUtils2.getSortedActiveProcessorsZnodes());\r\n    // Clean up\r\n    zkUtils2.close();\r\n    zkUtils3.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testNonLeaderFailure",
  "sourceCode" : "/**\r\n * Tests that a non-leader failure updates the Zk tree and participants' state correctly\r\n */\r\n@Test\r\npublic void testNonLeaderFailure() {\r\n    /**\r\n     * electionLatch and count together verify that:\r\n     * 1. the registered listeners are actually invoked by the ZkClient on the correct path\r\n     * 2. for a single participant failure, at-most 1 other participant is notified\r\n     */\r\n    final CountDownLatch electionLatch = new CountDownLatch(1);\r\n    final AtomicInteger count = new AtomicInteger(0);\r\n    BooleanResult isLeader1 = new BooleanResult();\r\n    BooleanResult isLeader2 = new BooleanResult();\r\n    BooleanResult isLeader3 = new BooleanResult();\r\n    // Processor-1\r\n    ZkUtils zkUtils1 = getZkUtilsWithNewClient();\r\n    zkUtils1.registerProcessorAndGetId(new ProcessorData(\"processor1\", \"1\"));\r\n    ZkLeaderElector leaderElector1 = new ZkLeaderElector(\"processor1\", zkUtils1, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            count.incrementAndGet();\r\n        }\r\n    });\r\n    leaderElector1.setLeaderElectorListener(() -> isLeader1.res = true);\r\n    // Processor-2\r\n    ZkUtils zkUtils2 = getZkUtilsWithNewClient();\r\n    zkUtils2.registerProcessorAndGetId(new ProcessorData(\"processor2\", \"2\"));\r\n    ZkLeaderElector leaderElector2 = new ZkLeaderElector(\"processor2\", zkUtils2, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            count.incrementAndGet();\r\n        }\r\n    });\r\n    leaderElector2.setLeaderElectorListener(() -> isLeader2.res = true);\r\n    // Processor-3\r\n    ZkUtils zkUtils3 = getZkUtilsWithNewClient();\r\n    final String path3 = zkUtils3.registerProcessorAndGetId(new ProcessorData(\"processor3\", \"3\"));\r\n    ZkLeaderElector leaderElector3 = new ZkLeaderElector(\"processor3\", zkUtils3, new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            String registeredIdStr = ZkKeyBuilder.parseIdFromPath(path3);\r\n            Assert.assertNotNull(registeredIdStr);\r\n            String predecessorIdStr = ZkKeyBuilder.parseIdFromPath(dataPath);\r\n            Assert.assertNotNull(predecessorIdStr);\r\n            try {\r\n                int selfId = Integer.parseInt(registeredIdStr);\r\n                int predecessorId = Integer.parseInt(predecessorIdStr);\r\n                Assert.assertEquals(1, selfId - predecessorId);\r\n            } catch (Exception e) {\r\n                Assert.fail(\"Exception in LeaderElectionListener!\");\r\n            }\r\n            count.incrementAndGet();\r\n            electionLatch.countDown();\r\n        }\r\n    });\r\n    leaderElector3.setLeaderElectorListener(() -> isLeader3.res = true);\r\n    // Join Leader Election\r\n    leaderElector1.tryBecomeLeader();\r\n    leaderElector2.tryBecomeLeader();\r\n    leaderElector3.tryBecomeLeader();\r\n    Assert.assertTrue(TestZkUtils.testWithDelayBackOff(() -> isLeader1.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader2.res, 2, 100));\r\n    Assert.assertFalse(TestZkUtils.testWithDelayBackOff(() -> isLeader3.res, 2, 100));\r\n    List<String> currentActiveProcessors = zkUtils1.getSortedActiveProcessorsZnodes();\r\n    Assert.assertEquals(3, currentActiveProcessors.size());\r\n    zkUtils2.close();\r\n    currentActiveProcessors.remove(1);\r\n    try {\r\n        Assert.assertTrue(electionLatch.await(5, TimeUnit.SECONDS));\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Interrupted while waiting for leaderElection listener callback to complete!\");\r\n    }\r\n    Assert.assertEquals(1, count.get());\r\n    Assert.assertEquals(currentActiveProcessors, zkUtils1.getSortedActiveProcessorsZnodes());\r\n    // Clean up\r\n    zkUtils1.close();\r\n    zkUtils3.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkLeaderElector.java",
  "methodName" : "testAmILeader",
  "sourceCode" : "@Test\r\npublic void testAmILeader() {\r\n    BooleanResult isLeader1 = new BooleanResult();\r\n    BooleanResult isLeader2 = new BooleanResult();\r\n    // Processor-1\r\n    ZkUtils zkUtils1 = getZkUtilsWithNewClient();\r\n    ZkLeaderElector leaderElector1 = new ZkLeaderElector(\"1\", zkUtils1);\r\n    leaderElector1.setLeaderElectorListener(() -> isLeader1.res = true);\r\n    // Processor-2\r\n    ZkUtils zkUtils2 = getZkUtilsWithNewClient();\r\n    ZkLeaderElector leaderElector2 = new ZkLeaderElector(\"2\", zkUtils2);\r\n    leaderElector2.setLeaderElectorListener(() -> isLeader2.res = true);\r\n    // Before Leader Election\r\n    Assert.assertFalse(leaderElector1.amILeader());\r\n    Assert.assertFalse(leaderElector2.amILeader());\r\n    leaderElector1.tryBecomeLeader();\r\n    leaderElector2.tryBecomeLeader();\r\n    // After Leader Election\r\n    Assert.assertTrue(leaderElector1.amILeader());\r\n    Assert.assertFalse(leaderElector2.amILeader());\r\n    zkUtils1.close();\r\n    zkUtils2.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkMetadataStore.java",
  "methodName" : "testReadAfterWrite",
  "sourceCode" : "@Test\r\npublic void testReadAfterWrite() throws Exception {\r\n    String key = \"test-key1\";\r\n    byte[] value = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    Assert.assertNull(zkMetadataStore.get(key));\r\n    zkMetadataStore.put(key, value);\r\n    Assert.assertTrue(Arrays.equals(value, zkMetadataStore.get(key)));\r\n    Assert.assertEquals(1, zkMetadataStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkMetadataStore.java",
  "methodName" : "testReadAfterDelete",
  "sourceCode" : "@Test\r\npublic void testReadAfterDelete() throws Exception {\r\n    String key = \"test-key1\";\r\n    byte[] value = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    Assert.assertNull(zkMetadataStore.get(key));\r\n    zkMetadataStore.put(key, value);\r\n    Assert.assertTrue(Arrays.equals(value, zkMetadataStore.get(key)));\r\n    zkMetadataStore.delete(key);\r\n    Assert.assertNull(zkMetadataStore.get(key));\r\n    Assert.assertEquals(0, zkMetadataStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkMetadataStore.java",
  "methodName" : "testReadOfNonExistentKey",
  "sourceCode" : "@Test\r\npublic void testReadOfNonExistentKey() {\r\n    Assert.assertNull(zkMetadataStore.get(\"randomKey\"));\r\n    Assert.assertEquals(0, zkMetadataStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkMetadataStore.java",
  "methodName" : "testMultipleUpdatesForSameKey",
  "sourceCode" : "@Test\r\npublic void testMultipleUpdatesForSameKey() throws Exception {\r\n    String key = \"test-key1\";\r\n    byte[] value = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    byte[] value1 = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    zkMetadataStore.put(key, value);\r\n    zkMetadataStore.put(key, value1);\r\n    Assert.assertTrue(Arrays.equals(value1, zkMetadataStore.get(key)));\r\n    Assert.assertEquals(1, zkMetadataStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkMetadataStore.java",
  "methodName" : "testAllEntries",
  "sourceCode" : "@Test\r\npublic void testAllEntries() throws Exception {\r\n    String key = \"test-key1\";\r\n    String key1 = \"test-key2\";\r\n    String key2 = \"test-key3\";\r\n    byte[] value = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    byte[] value1 = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    byte[] value2 = getRandomByteArray(VALUE_SIZE_IN_BYTES);\r\n    zkMetadataStore.put(key, value);\r\n    zkMetadataStore.put(key1, value1);\r\n    zkMetadataStore.put(key2, value2);\r\n    ImmutableMap<String, byte[]> expected = ImmutableMap.of(key, value, key1, value1, key2, value2);\r\n    Assert.assertEquals(expected.size(), zkMetadataStore.all().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkNamespace.java",
  "methodName" : "testValidateFailZkNameSpace1LevelPath",
  "sourceCode" : "@Test\r\npublic void testValidateFailZkNameSpace1LevelPath() {\r\n    try {\r\n        String zkConnect = \"127.0.0.1:\" + zkServer.getPort() + \"/zkNameSpace\";\r\n        initZk(zkConnect);\r\n        ZkCoordinationUtilsFactory.validateZkNameSpace(zkConnect, zkClient);\r\n        Assert.fail(\"1.Should fail with exception, because namespace doesn't exist\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    } finally {\r\n        tearDownZk();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkNamespace.java",
  "methodName" : "testValidateFailZkNameSpace2LevelPath",
  "sourceCode" : "@Test\r\npublic void testValidateFailZkNameSpace2LevelPath() {\r\n    try {\r\n        String zkConnect = \"127.0.0.1:\" + zkServer.getPort() + \"/zkNameSpace/xyz\";\r\n        initZk(zkConnect);\r\n        ZkCoordinationUtilsFactory.validateZkNameSpace(zkConnect, zkClient);\r\n        Assert.fail(\"2.Should fail with exception, because namespace doesn't exist\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    } finally {\r\n        tearDownZk();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkNamespace.java",
  "methodName" : "testValidateFailZkNameSpaceEmptyPath",
  "sourceCode" : "@Test\r\npublic void testValidateFailZkNameSpaceEmptyPath() {\r\n    // should succeed, because no namespace provided\r\n    String zkConnect = \"127.0.0.1:\" + zkServer.getPort() + \"\";\r\n    initZk(zkConnect);\r\n    ZkCoordinationUtilsFactory.validateZkNameSpace(zkConnect, zkClient);\r\n    tearDownZk();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkNamespace.java",
  "methodName" : "testValidateNotFailZkNameSpace",
  "sourceCode" : "@Test\r\npublic void testValidateNotFailZkNameSpace() {\r\n    // now positive tests - with existing namespace\r\n    testDoNotFailIfNameSpacePresent(\"/zkNameSpace1\");\r\n    testDoNotFailIfNameSpacePresent(\"/zkNameSpace1/xyz1\");\r\n    testDoNotFailIfNameSpacePresent(\"\");\r\n    tearDownZk();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkProcessorLatch.java",
  "methodName" : "testLatchSizeOne",
  "sourceCode" : "@Test\r\npublic void testLatchSizeOne() {\r\n    final int latchSize = 1;\r\n    final String latchId = \"latchSizeOne\";\r\n    ExecutorService pool = Executors.newFixedThreadPool(3);\r\n    Future processor = pool.submit(getParticipantRunnable(latchSize, latchId, \"participant1\"));\r\n    try {\r\n        processor.get(30, TimeUnit.SECONDS);\r\n    } catch (Exception e) {\r\n        Assert.fail(\"failed to get future.\" + e.getLocalizedMessage());\r\n    } finally {\r\n        pool.shutdownNow();\r\n    }\r\n    try {\r\n        List<String> latchParticipants = testZkUtils.getZkClient().getChildren(String.format(\"%s/%s_%s\", KEY_BUILDER.getRootPath(), ZkProcessorLatch.LATCH_PATH, latchId));\r\n        Assert.assertNotNull(latchParticipants);\r\n        Assert.assertEquals(1, latchParticipants.size());\r\n        Assert.assertEquals(\"0000000000\", latchParticipants.get(0));\r\n    } catch (Exception e) {\r\n        Assert.fail(\"Failed to read the latch status from ZK directly\" + e.getLocalizedMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkProcessorLatch.java",
  "methodName" : "testLatchSizeOneWithTwoParticipants",
  "sourceCode" : "@Test\r\npublic void testLatchSizeOneWithTwoParticipants() {\r\n    final int latchSize = 1;\r\n    final String latchId = \"testLatchSizeOneWithTwoParticipants\";\r\n    ExecutorService pool = Executors.newFixedThreadPool(3);\r\n    Future f1 = pool.submit(() -> {\r\n        String participant1 = \"participant1\";\r\n        ZkUtils zkUtils = getZkUtilsWithNewClient(participant1);\r\n        zkUtils.connect();\r\n        Latch latch = new ZkProcessorLatch(latchSize, latchId, participant1, zkUtils);\r\n        //latch.countDown(); only one thread counts down\r\n        try {\r\n            latch.await(30, TimeUnit.SECONDS);\r\n        } catch (TimeoutException e) {\r\n            Assert.fail(String.format(\"await timed out from  %s - %s\", participant1, e.getLocalizedMessage()));\r\n        } finally {\r\n            zkUtils.close();\r\n        }\r\n    });\r\n    Future f2 = pool.submit(getParticipantRunnable(latchSize, latchId, \"participant2\"));\r\n    try {\r\n        f1.get(30, TimeUnit.SECONDS);\r\n        f2.get(30, TimeUnit.SECONDS);\r\n    } catch (Exception e) {\r\n        Assert.fail(\"failed to get future.\" + e.getLocalizedMessage());\r\n    } finally {\r\n        pool.shutdownNow();\r\n    }\r\n    try {\r\n        List<String> latchParticipants = testZkUtils.getZkClient().getChildren(String.format(\"%s/%s_%s\", KEY_BUILDER.getRootPath(), ZkProcessorLatch.LATCH_PATH, latchId));\r\n        Assert.assertNotNull(latchParticipants);\r\n        Assert.assertEquals(1, latchParticipants.size());\r\n        Assert.assertEquals(\"0000000000\", latchParticipants.get(0));\r\n    } catch (Exception e) {\r\n        Assert.fail(\"Failed to read the latch status from ZK directly\" + e.getLocalizedMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkProcessorLatch.java",
  "methodName" : "testLatchSizeN",
  "sourceCode" : "@Test\r\npublic void testLatchSizeN() {\r\n    final int latchSize = 3;\r\n    final String latchId = \"testLatchSizeN\";\r\n    ExecutorService pool = Executors.newFixedThreadPool(3);\r\n    Future f1 = pool.submit(getParticipantRunnable(latchSize, latchId, \"participant1\"));\r\n    Future f2 = pool.submit(getParticipantRunnable(latchSize, latchId, \"participant2\"));\r\n    Future f3 = pool.submit(getParticipantRunnable(latchSize, latchId, \"participant3\"));\r\n    try {\r\n        f1.get(30, TimeUnit.SECONDS);\r\n        f2.get(30, TimeUnit.SECONDS);\r\n        f3.get(30, TimeUnit.SECONDS);\r\n    } catch (Exception e) {\r\n        Assert.fail(\"failed to get future.\" + e.getLocalizedMessage());\r\n    } finally {\r\n        pool.shutdownNow();\r\n    }\r\n    try {\r\n        List<String> latchParticipants = testZkUtils.getZkClient().getChildren(String.format(\"%s/%s_%s\", KEY_BUILDER.getRootPath(), ZkProcessorLatch.LATCH_PATH, latchId));\r\n        Assert.assertNotNull(latchParticipants);\r\n        Assert.assertEquals(3, latchParticipants.size());\r\n    } catch (Exception e) {\r\n        Assert.fail(\"Failed to read the latch status from ZK directly\" + e.getLocalizedMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkProcessorLatch.java",
  "methodName" : "testLatchExpires",
  "sourceCode" : "@Test\r\npublic void testLatchExpires() {\r\n    final String latchId = \"testLatchExpires\";\r\n    final int latchSize = 3;\r\n    Latch latch = new ZkProcessorLatch(latchSize, latchId, \"test\", testZkUtils);\r\n    try {\r\n        latch.countDown();\r\n        latch.await(5, TimeUnit.SECONDS);\r\n    } catch (TimeoutException e) {\r\n        // expected\r\n    } catch (Exception e) {\r\n        Assert.fail(String.format(\"Expected only TimeoutException! Received %s\", e));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkStringSerializer.java",
  "methodName" : "testStringSerialization",
  "sourceCode" : "@Test\r\npublic void testStringSerialization() throws Exception {\r\n    ZkStringSerializer serde = new ZkStringSerializer();\r\n    Assert.assertEquals(null, serde.serialize(null));\r\n    Assert.assertEquals(null, serde.deserialize(null));\r\n    String fooBar = \"37\";\r\n    byte[] fooBarBytes = serde.serialize(fooBar);\r\n    Assert.assertArrayEquals(fooBar.getBytes(\"UTF-8\"), fooBarBytes);\r\n    Assert.assertEquals(fooBar, serde.deserialize(fooBarBytes));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testRegisterProcessorId",
  "sourceCode" : "@Test\r\npublic void testRegisterProcessorId() {\r\n    String assignedPath = zkUtils.registerProcessorAndGetId(new ProcessorData(\"host\", \"1\"));\r\n    Assert.assertTrue(assignedPath.startsWith(KEY_BUILDER.getProcessorsPath()));\r\n    // Calling registerProcessorId again should return the same ephemeralPath as long as the session is valid\r\n    Assert.assertTrue(zkUtils.registerProcessorAndGetId(new ProcessorData(\"host\", \"1\")).equals(assignedPath));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testGetActiveProcessors",
  "sourceCode" : "@Test\r\npublic void testGetActiveProcessors() {\r\n    Assert.assertEquals(0, zkUtils.getSortedActiveProcessorsZnodes().size());\r\n    zkUtils.registerProcessorAndGetId(new ProcessorData(\"processorData\", \"1\"));\r\n    Assert.assertEquals(1, zkUtils.getSortedActiveProcessorsZnodes().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testGetActiveProcessorIdShouldReturnEmptyForNonExistingZookeeperNodes",
  "sourceCode" : "@Test\r\npublic void testGetActiveProcessorIdShouldReturnEmptyForNonExistingZookeeperNodes() {\r\n    List<String> processorsIDs = zkUtils.getActiveProcessorsIDs(ImmutableList.of(\"node1\", \"node2\"));\r\n    Assert.assertEquals(0, processorsIDs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testReadAfterWriteTaskLocality",
  "sourceCode" : "@Test\r\npublic void testReadAfterWriteTaskLocality() {\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"));\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-2\"), new LocationId(\"LocationId-2\"));\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"), new TaskName(\"task-2\"), new LocationId(\"LocationId-2\"));\r\n    Assert.assertEquals(taskLocality, zkUtils.readTaskLocality());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testReadWhenTaskLocalityDoesNotExist",
  "sourceCode" : "@Test\r\npublic void testReadWhenTaskLocalityDoesNotExist() {\r\n    Map<TaskName, LocationId> taskLocality = zkUtils.readTaskLocality();\r\n    Assert.assertEquals(0, taskLocality.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testWriteTaskLocalityShouldUpdateTheExistingValue",
  "sourceCode" : "@Test\r\npublic void testWriteTaskLocalityShouldUpdateTheExistingValue() {\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"));\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"));\r\n    Assert.assertEquals(taskLocality, zkUtils.readTaskLocality());\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-1\"), new LocationId(\"LocationId-2\"));\r\n    taskLocality = ImmutableMap.of(new TaskName(\"task-1\"), new LocationId(\"LocationId-2\"));\r\n    Assert.assertEquals(taskLocality, zkUtils.readTaskLocality());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testReadTaskLocalityShouldReturnAllTheExistingLocalityValue",
  "sourceCode" : "@Test\r\npublic void testReadTaskLocalityShouldReturnAllTheExistingLocalityValue() {\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"));\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-2\"), new LocationId(\"LocationId-2\"));\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-3\"), new LocationId(\"LocationId-3\"));\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-4\"), new LocationId(\"LocationId-4\"));\r\n    zkUtils.writeTaskLocality(new TaskName(\"task-5\"), new LocationId(\"LocationId-5\"));\r\n    Map<TaskName, LocationId> taskLocality = ImmutableMap.of(new TaskName(\"task-1\"), new LocationId(\"LocationId-1\"), new TaskName(\"task-2\"), new LocationId(\"LocationId-2\"), new TaskName(\"task-3\"), new LocationId(\"LocationId-3\"), new TaskName(\"task-4\"), new LocationId(\"LocationId-4\"), new TaskName(\"task-5\"), new LocationId(\"LocationId-5\"));\r\n    Assert.assertEquals(taskLocality, zkUtils.readTaskLocality());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testGetAllProcessorNodesShouldReturnEmptyForNonExistingZookeeperNodes",
  "sourceCode" : "@Test\r\npublic void testGetAllProcessorNodesShouldReturnEmptyForNonExistingZookeeperNodes() {\r\n    List<ZkUtils.ProcessorNode> processorsIDs = zkUtils.getAllProcessorNodes();\r\n    Assert.assertEquals(0, processorsIDs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testZKProtocolVersion",
  "sourceCode" : "@Test\r\npublic void testZKProtocolVersion() {\r\n    // first time connect, version should be set to ZkUtils.ZK_PROTOCOL_VERSION\r\n    ZkLeaderElector le = new ZkLeaderElector(\"1\", zkUtils);\r\n    zkUtils.validateZkVersion();\r\n    String root = zkUtils.getKeyBuilder().getRootPath();\r\n    String ver = zkUtils.getZkClient().readData(root);\r\n    Assert.assertEquals(ZkUtils.ZK_PROTOCOL_VERSION, ver);\r\n    // do it again (in case original value was null\r\n    zkUtils.validateZkVersion();\r\n    ver = zkUtils.getZkClient().readData(root);\r\n    Assert.assertEquals(ZkUtils.ZK_PROTOCOL_VERSION, ver);\r\n    // now negative case\r\n    zkUtils.getZkClient().writeData(root, \"2.0\");\r\n    try {\r\n        zkUtils.validateZkVersion();\r\n        Assert.fail(\"Expected to fail because of version mismatch 2.0 vs 1.0\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    }\r\n    // validate future values, let's say that current version should be 3.0\r\n    try {\r\n        Field f = zkUtils.getClass().getDeclaredField(\"ZK_PROTOCOL_VERSION\");\r\n        FieldUtils.removeFinalModifier(f);\r\n        f.set(null, \"3.0\");\r\n    } catch (Exception e) {\r\n        System.out.println(e);\r\n        Assert.fail();\r\n    }\r\n    try {\r\n        zkUtils.validateZkVersion();\r\n        Assert.fail(\"Expected to fail because of version mismatch 2.0 vs 3.0\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testGetProcessorsIDs",
  "sourceCode" : "@Test\r\npublic void testGetProcessorsIDs() {\r\n    Assert.assertEquals(0, zkUtils.getSortedActiveProcessorsIDs().size());\r\n    zkUtils.registerProcessorAndGetId(new ProcessorData(\"host1\", \"1\"));\r\n    List<String> l = zkUtils.getSortedActiveProcessorsIDs();\r\n    Assert.assertEquals(1, l.size());\r\n    new ZkUtils(KEY_BUILDER, zkClient, CONNECTION_TIMEOUT_MS, SESSION_TIMEOUT_MS, new NoOpMetricsRegistry()).registerProcessorAndGetId(new ProcessorData(\"host2\", \"2\"));\r\n    l = zkUtils.getSortedActiveProcessorsIDs();\r\n    Assert.assertEquals(2, l.size());\r\n    Assert.assertEquals(\" ID1 didn't match\", \"1\", l.get(0));\r\n    Assert.assertEquals(\" ID2 didn't match\", \"2\", l.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testSubscribeToJobModelVersionChange",
  "sourceCode" : "@Test\r\npublic void testSubscribeToJobModelVersionChange() {\r\n    ZkKeyBuilder keyBuilder = new ZkKeyBuilder(\"test\");\r\n    String root = keyBuilder.getRootPath();\r\n    zkClient.deleteRecursive(root);\r\n    class Result {\r\n\r\n        String res = \"\";\r\n\r\n        public String getRes() {\r\n            return res;\r\n        }\r\n\r\n        public void updateRes(String newRes) {\r\n            res = newRes;\r\n        }\r\n    }\r\n    Assert.assertFalse(zkUtils.exists(root));\r\n    // create the paths\r\n    zkUtils.validatePaths(new String[] { root, keyBuilder.getJobModelVersionPath(), keyBuilder.getProcessorsPath() });\r\n    Assert.assertTrue(zkUtils.exists(root));\r\n    Assert.assertTrue(zkUtils.exists(keyBuilder.getJobModelVersionPath()));\r\n    Assert.assertTrue(zkUtils.exists(keyBuilder.getProcessorsPath()));\r\n    final Result res = new Result();\r\n    // define the callback\r\n    IZkDataListener dataListener = new IZkDataListener() {\r\n\r\n        @Override\r\n        public void handleDataChange(String dataPath, Object data) throws Exception {\r\n            res.updateRes((String) data);\r\n        }\r\n\r\n        @Override\r\n        public void handleDataDeleted(String dataPath) throws Exception {\r\n            Assert.fail(\"Data wasn't deleted;\");\r\n        }\r\n    };\r\n    // subscribe\r\n    zkClient.subscribeDataChanges(keyBuilder.getJobModelVersionPath(), dataListener);\r\n    zkClient.subscribeDataChanges(keyBuilder.getProcessorsPath(), dataListener);\r\n    // update\r\n    zkClient.writeData(keyBuilder.getJobModelVersionPath(), \"newVersion\");\r\n    // verify\r\n    Assert.assertTrue(testWithDelayBackOff(() -> \"newVersion\".equals(res.getRes()), 2, 1000));\r\n    // update again\r\n    zkClient.writeData(keyBuilder.getProcessorsPath(), \"newProcessor\");\r\n    Assert.assertTrue(testWithDelayBackOff(() -> \"newProcessor\".equals(res.getRes()), 2, 1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testRegisterProcessorAndGetIdShouldFailForDuplicateProcessorRegistration",
  "sourceCode" : "/**\r\n * Create two duplicate processors with same processorId.\r\n * Second creation should fail with exception.\r\n */\r\n@Test\r\npublic void testRegisterProcessorAndGetIdShouldFailForDuplicateProcessorRegistration() {\r\n    final String testHostName = \"localhost\";\r\n    final String testProcessId = \"testProcessorId\";\r\n    ProcessorData processorData1 = new ProcessorData(testHostName, testProcessId);\r\n    // Register processor 1 which is not duplicate, this registration should succeed.\r\n    zkUtils.registerProcessorAndGetId(processorData1);\r\n    ZkUtils zkUtils1 = getZkUtils();\r\n    zkUtils1.connect();\r\n    ProcessorData duplicateProcessorData = new ProcessorData(testHostName, testProcessId);\r\n    // Registration of the duplicate processor should fail.\r\n    expectedException.expect(SamzaException.class);\r\n    zkUtils1.registerProcessorAndGetId(duplicateProcessorData);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testPublishNewJobModel",
  "sourceCode" : "@Test\r\npublic void testPublishNewJobModel() {\r\n    ZkKeyBuilder keyBuilder = new ZkKeyBuilder(\"test\");\r\n    String root = keyBuilder.getRootPath();\r\n    zkClient.deleteRecursive(root);\r\n    String version = \"1\";\r\n    String oldVersion = \"0\";\r\n    zkUtils.validatePaths(new String[] { root, keyBuilder.getJobModelPathPrefix(), keyBuilder.getJobModelVersionPath() });\r\n    zkUtils.publishJobModelVersion(oldVersion, version);\r\n    Assert.assertEquals(version, zkUtils.getJobModelVersion());\r\n    String newerVersion = Long.toString(Long.valueOf(version) + 1);\r\n    zkUtils.publishJobModelVersion(version, newerVersion);\r\n    Assert.assertEquals(newerVersion, zkUtils.getJobModelVersion());\r\n    try {\r\n        //invalid new version\r\n        zkUtils.publishJobModelVersion(oldVersion, \"10\");\r\n        Assert.fail(\"publish invalid version should've failed\");\r\n    } catch (SamzaException e) {\r\n        // expected\r\n    }\r\n    // create job model\r\n    Map<String, String> configMap = new HashMap<>();\r\n    Map<String, ContainerModel> containers = new HashMap<>();\r\n    MapConfig config = new MapConfig(configMap);\r\n    JobModel jobModel = new JobModel(config, containers);\r\n    zkUtils.publishJobModel(version, jobModel);\r\n    Assert.assertEquals(jobModel, zkUtils.getJobModel(version));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testCleanUpZkJobModels",
  "sourceCode" : "@Test\r\npublic void testCleanUpZkJobModels() {\r\n    String root = zkUtils.getKeyBuilder().getJobModelPathPrefix();\r\n    System.out.println(\"root=\" + root);\r\n    zkUtils.getZkClient().createPersistent(root, true);\r\n    // generate multiple version\r\n    for (int i = 101; i < 110; i++) {\r\n        zkUtils.publishJobModel(String.valueOf(i), null);\r\n    }\r\n    // clean all of the versions except 5 most recent ones\r\n    zkUtils.deleteOldJobModels(5);\r\n    Assert.assertEquals(Arrays.asList(\"105\", \"106\", \"107\", \"108\", \"109\"), zkUtils.getZkClient().getChildren(root));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testCleanUpZkBarrierVersion",
  "sourceCode" : "@Test\r\npublic void testCleanUpZkBarrierVersion() {\r\n    String root = zkUtils.getKeyBuilder().getJobModelVersionBarrierPrefix();\r\n    zkUtils.getZkClient().createPersistent(root, true);\r\n    ZkBarrierForVersionUpgrade barrier = new ZkBarrierForVersionUpgrade(root, zkUtils, null, null);\r\n    for (int i = 200; i < 210; i++) {\r\n        barrier.create(String.valueOf(i), new ArrayList<>(Arrays.asList(i + \"a\", i + \"b\", i + \"c\")));\r\n    }\r\n    zkUtils.deleteOldBarrierVersions(5);\r\n    List<String> zNodeIds = zkUtils.getZkClient().getChildren(root);\r\n    Collections.sort(zNodeIds);\r\n    Assert.assertEquals(Arrays.asList(\"barrier_205\", \"barrier_206\", \"barrier_207\", \"barrier_208\", \"barrier_209\"), zNodeIds);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testCleanUpZk",
  "sourceCode" : "@Test\r\npublic void testCleanUpZk() {\r\n    String pathA = \"/path/testA\";\r\n    String pathB = \"/path/testB\";\r\n    zkUtils.getZkClient().createPersistent(pathA, true);\r\n    zkUtils.getZkClient().createPersistent(pathB, true);\r\n    // Create 100 nodes\r\n    for (int i = 0; i < 20; i++) {\r\n        String p1 = pathA + \"/\" + i;\r\n        zkUtils.getZkClient().createPersistent(p1, true);\r\n        zkUtils.getZkClient().createPersistent(p1 + \"/something1\", true);\r\n        zkUtils.getZkClient().createPersistent(p1 + \"/something2\", true);\r\n        String p2 = pathB + \"/some_\" + i;\r\n        zkUtils.getZkClient().createPersistent(p2, true);\r\n        zkUtils.getZkClient().createPersistent(p2 + \"/something1\", true);\r\n        zkUtils.getZkClient().createPersistent(p2 + \"/something2\", true);\r\n    }\r\n    List<String> zNodeIds = new ArrayList<>();\r\n    // empty list\r\n    zkUtils.deleteOldVersionPath(pathA, zNodeIds, 10, new Comparator<String>() {\r\n\r\n        @Override\r\n        public int compare(String o1, String o2) {\r\n            return o1.compareTo(o2);\r\n        }\r\n    });\r\n    zNodeIds = zkUtils.getZkClient().getChildren(pathA);\r\n    zkUtils.deleteOldVersionPath(pathA, zNodeIds, 10, new Comparator<String>() {\r\n\r\n        @Override\r\n        public int compare(String o1, String o2) {\r\n            return Integer.valueOf(o1) - Integer.valueOf(o2);\r\n        }\r\n    });\r\n    for (int i = 0; i < 10; i++) {\r\n        // should be gone\r\n        String p1 = pathA + \"/\" + i;\r\n        Assert.assertFalse(\"path \" + p1 + \" exists\", zkUtils.getZkClient().exists(p1));\r\n    }\r\n    for (int i = 10; i < 20; i++) {\r\n        // should be gone\r\n        String p1 = pathA + \"/\" + i;\r\n        Assert.assertTrue(\"path \" + p1 + \" exists\", zkUtils.getZkClient().exists(p1));\r\n    }\r\n    zNodeIds = zkUtils.getZkClient().getChildren(pathB);\r\n    zkUtils.deleteOldVersionPath(pathB, zNodeIds, 1, new Comparator<String>() {\r\n\r\n        @Override\r\n        public int compare(String o1, String o2) {\r\n            return Integer.valueOf(o1.substring(o1.lastIndexOf(\"_\") + 1)) - Integer.valueOf(o2.substring(o2.lastIndexOf(\"_\") + 1));\r\n        }\r\n    });\r\n    for (int i = 0; i < 19; i++) {\r\n        // should be gone\r\n        String p1 = pathB + \"/\" + i;\r\n        Assert.assertFalse(\"path \" + p1 + \" exists\", zkUtils.getZkClient().exists(p1));\r\n    }\r\n    for (int i = 19; i < 20; i++) {\r\n        // should be gone\r\n        String p1 = pathB + \"/some_\" + i;\r\n        Assert.assertTrue(\"path \" + p1 + \" exists\", zkUtils.getZkClient().exists(p1));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testgetNextJobModelVersion",
  "sourceCode" : "@Test\r\npublic void testgetNextJobModelVersion() {\r\n    // Set up the Zk base paths for testing.\r\n    ZkKeyBuilder keyBuilder = new ZkKeyBuilder(\"test\");\r\n    String root = keyBuilder.getRootPath();\r\n    zkClient.deleteRecursive(root);\r\n    zkUtils.validatePaths(new String[] { root, keyBuilder.getJobModelPathPrefix(), keyBuilder.getJobModelVersionPath() });\r\n    String version = \"1\";\r\n    String oldVersion = \"0\";\r\n    // Set zkNode JobModelVersion to 1.\r\n    zkUtils.publishJobModelVersion(oldVersion, version);\r\n    Assert.assertEquals(version, zkUtils.getJobModelVersion());\r\n    // Publish JobModel with a higher version (2).\r\n    zkUtils.publishJobModel(\"2\", new JobModel(new MapConfig(), new HashMap<>()));\r\n    // Get on the JobModel version should return 2, taking into account the published version 2.\r\n    Assert.assertEquals(\"3\", zkUtils.getNextJobModelVersion(zkUtils.getJobModelVersion()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testDeleteProcessorNodeShouldDeleteTheCorrectProcessorNode",
  "sourceCode" : "@Test\r\npublic void testDeleteProcessorNodeShouldDeleteTheCorrectProcessorNode() {\r\n    String testProcessorId1 = \"processorId1\";\r\n    String testProcessorId2 = \"processorId2\";\r\n    ZkUtils zkUtils = getZkUtils();\r\n    ZkUtils zkUtils1 = getZkUtils();\r\n    zkUtils.registerProcessorAndGetId(new ProcessorData(\"host1\", testProcessorId1));\r\n    zkUtils1.registerProcessorAndGetId(new ProcessorData(\"host2\", testProcessorId2));\r\n    zkUtils.deleteProcessorNode(testProcessorId1);\r\n    List<String> expectedProcessors = ImmutableList.of(testProcessorId2);\r\n    List<String> actualProcessors = zkUtils.getSortedActiveProcessorsIDs();\r\n    Assert.assertEquals(expectedProcessors, actualProcessors);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testCloseShouldRetryOnceOnInterruptedException",
  "sourceCode" : "@Test\r\npublic void testCloseShouldRetryOnceOnInterruptedException() {\r\n    ZkClient zkClient = mock(ZkClient.class);\r\n    ZkUtils zkUtils = new ZkUtils(KEY_BUILDER, zkClient, CONNECTION_TIMEOUT_MS, SESSION_TIMEOUT_MS, new NoOpMetricsRegistry());\r\n    Mockito.doThrow(new ZkInterruptedException(new InterruptedException())).doAnswer(invocation -> null).when(zkClient).close();\r\n    zkUtils.close();\r\n    Mockito.verify(zkClient, Mockito.times(2)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\java\\org\\apache\\samza\\zk\\TestZkUtils.java",
  "methodName" : "testCloseShouldTearDownZkConnectionOnInterruptedException",
  "sourceCode" : "@Test\r\npublic void testCloseShouldTearDownZkConnectionOnInterruptedException() throws Exception {\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    // Establish connection with the zookeeper server.\r\n    ZkClient zkClient = mock(ZkClient.class);\r\n    ZkUtils zkUtils = new ZkUtils(KEY_BUILDER, zkClient, CONNECTION_TIMEOUT_MS, SESSION_TIMEOUT_MS, new NoOpMetricsRegistry());\r\n    Thread threadToInterrupt = new Thread(() -> {\r\n        try {\r\n            latch.await();\r\n        } catch (InterruptedException e) {\r\n            Thread.currentThread().interrupt();\r\n        }\r\n        zkUtils.close();\r\n    });\r\n    threadToInterrupt.start();\r\n    threadToInterrupt.interrupt();\r\n    threadToInterrupt.join();\r\n    verify(zkClient, times(1)).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\coordinator\\TestInputRegexMonitor.java",
  "methodName" : "testStartStop",
  "sourceCode" : "@Test\r\npublic void testStartStop() throws InterruptedException {\r\n    Assert.assertFalse(streamRegexMonitor.isRunning());\r\n    // Normal start\r\n    streamRegexMonitor.start();\r\n    Assert.assertTrue(streamRegexMonitor.isRunning());\r\n    // Start ought to be idempotent\r\n    streamRegexMonitor.start();\r\n    Assert.assertTrue(streamRegexMonitor.isRunning());\r\n    // Normal stop\r\n    streamRegexMonitor.stop();\r\n    Assert.assertTrue(streamRegexMonitor.awaitTermination(1, TimeUnit.SECONDS));\r\n    Assert.assertFalse(streamRegexMonitor.isRunning());\r\n    try {\r\n        streamRegexMonitor.start();\r\n    } catch (Exception e) {\r\n        Assert.assertTrue(e.getClass().equals(IllegalStateException.class));\r\n    }\r\n    // Stop ought to be idempotent\r\n    Assert.assertFalse(streamRegexMonitor.isRunning());\r\n    streamRegexMonitor.stop();\r\n    Assert.assertFalse(streamRegexMonitor.isRunning());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\coordinator\\TestInputRegexMonitor.java",
  "methodName" : "testSchedulingAndInputAddition",
  "sourceCode" : "@Test\r\npublic void testSchedulingAndInputAddition() throws Exception {\r\n    this.streamRegexMonitor.start();\r\n    try {\r\n        if (!callbackCount.await(1, TimeUnit.SECONDS)) {\r\n            throw new Exception(\"Did not see \" + expectedNumberOfCallbacks + \" callbacks after waiting. \" + callbackCount.toString());\r\n        }\r\n    } finally {\r\n        System.out.println(\"CallbackCount is \" + callbackCount.getCount());\r\n        this.streamRegexMonitor.stop();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\metrics\\TestBoundedList.java",
  "methodName" : "basicTest",
  "sourceCode" : "@Test\r\npublic void basicTest() {\r\n    BoundedList<String> boundedList = getBoundedListForTest();\r\n    boundedList.add(\"sampleValue\");\r\n    Assert.assertEquals(\"Names should be the same\", boundedList.getName(), \"sampleListGauge\");\r\n    Assert.assertEquals(\"List sizes should match\", boundedList.getValues().size(), 1);\r\n    Assert.assertEquals(\"BoundedList should contain sampleGauge\", boundedList.getValues().contains(\"sampleValue\"), true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\metrics\\TestBoundedList.java",
  "methodName" : "testSizeEnforcement",
  "sourceCode" : "@Test\r\npublic void testSizeEnforcement() {\r\n    BoundedList boundedList = getBoundedListForTest();\r\n    for (int i = 15; i > 0; i--) {\r\n        boundedList.add(\"v\" + i);\r\n    }\r\n    Assert.assertEquals(\"List sizes should be as configured at creation time\", boundedList.getValues().size(), 10);\r\n    int valueIndex = 10;\r\n    Collection<String> currentList = boundedList.getValues();\r\n    Iterator iterator = currentList.iterator();\r\n    while (iterator.hasNext()) {\r\n        String gaugeValue = (String) iterator.next();\r\n        Assert.assertTrue(gaugeValue.equals(\"v\" + valueIndex));\r\n        valueIndex--;\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\metrics\\TestBoundedList.java",
  "methodName" : "testThreadSafety",
  "sourceCode" : "@Test\r\npublic void testThreadSafety() throws InterruptedException {\r\n    BoundedList<Integer> boundedList = getBoundedListForTest();\r\n    Thread thread1 = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            for (int i = 1; i <= 100; i++) {\r\n                boundedList.add(i);\r\n            }\r\n        }\r\n    });\r\n    Thread thread2 = new Thread(new Runnable() {\r\n\r\n        @Override\r\n        public void run() {\r\n            for (int i = 1; i <= 100; i++) {\r\n                boundedList.add(i);\r\n            }\r\n        }\r\n    });\r\n    thread1.start();\r\n    thread2.start();\r\n    thread1.join(THREAD_TEST_TIMEOUT.toMillis());\r\n    thread2.join(THREAD_TEST_TIMEOUT.toMillis());\r\n    Assert.assertTrue(\"BoundedList should have the last 10 values\", boundedList.getValues().size() == 10);\r\n    for (Integer gaugeValue : boundedList.getValues()) {\r\n        Assert.assertTrue(\"Values should have the last 10 range\", gaugeValue <= 100 && gaugeValue > 90);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testParallelismAndMetrics",
  "sourceCode" : "@Test\r\npublic void testParallelismAndMetrics() throws InterruptedException {\r\n    this.containerStorageManager.start();\r\n    this.containerStorageManager.shutdown();\r\n    for (Gauge gauge : taskRestoreMetricGauges.values()) {\r\n        Assert.assertTrue(\"Restoration time gauge value should be invoked atleast once\", mockingDetails(gauge).getInvocations().size() >= 1);\r\n    }\r\n    Assert.assertEquals(\"Store restore count should be 2 because there are 2 tasks\", 2, this.storeRestoreCallCount);\r\n    Assert.assertEquals(\"systemConsumerCreation count should be 1 (1 consumer per system)\", 1, this.systemConsumerCreationCount);\r\n    Assert.assertEquals(\"systemConsumerStopCount count should be 1\", 1, this.systemConsumerStopCount);\r\n    Assert.assertEquals(\"systemConsumerStartCount count should be 1\", 1, this.systemConsumerStartCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testDeleteLoggedStoreOnNoCheckpoints",
  "sourceCode" : "/**\r\n * This test will attempt to verify if logged stores are deleted if the input checkpoints are empty.\r\n */\r\n@Test\r\n@SuppressWarnings(\"ResultOfMethodCallIgnored\")\r\npublic void testDeleteLoggedStoreOnNoCheckpoints() {\r\n    // reset the mock to reset the stubs in setup method\r\n    reset(this.checkpointManager);\r\n    // redo stubbing to return null checkpoints\r\n    when(this.checkpointManager.readLastCheckpoint(any())).thenReturn(null);\r\n    // create store under logged stores to demonstrate deletion\r\n    final File storeFile = new File(DEFAULT_LOGGED_STORE_BASE_DIR.getPath() + File.separator + STORE_NAME);\r\n    // add contents to store\r\n    final File storeFilePartition = new File(DEFAULT_LOGGED_STORE_BASE_DIR.getPath() + File.separator + STORE_NAME + File.separator + \"Partition_0\");\r\n    storeFilePartition.deleteOnExit();\r\n    storeFile.deleteOnExit();\r\n    try {\r\n        storeFile.mkdirs();\r\n        storeFilePartition.createNewFile();\r\n        Assert.assertTrue(\"Assert that stores are present prior to the test.\", storeFile.exists());\r\n        Assert.assertTrue(\"Assert that store files are present prior to the test.\", storeFilePartition.exists());\r\n        this.containerStorageManager.start();\r\n        this.containerStorageManager.shutdown();\r\n        Assert.assertFalse(\"Assert that stores are deleted after the test.\", storeFile.exists());\r\n        Assert.assertFalse(\"Assert that store files are deleted after the test.\", storeFilePartition.exists());\r\n    } catch (Exception e) {\r\n        System.out.printf(\"File %s could not be created.\", storeFile);\r\n        Assert.fail();\r\n    } finally {\r\n        storeFilePartition.delete();\r\n        storeFile.delete();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testNoConfiguredDurableStores",
  "sourceCode" : "@Test\r\npublic void testNoConfiguredDurableStores() throws InterruptedException {\r\n    taskRestoreMetricGauges = new HashMap<>();\r\n    this.tasks = new HashMap<>();\r\n    this.taskInstanceMetrics = new HashMap<>();\r\n    // Add two mocked tasks\r\n    addMockedTask(\"task 0\", 0);\r\n    addMockedTask(\"task 1\", 1);\r\n    // Mock container metrics\r\n    samzaContainerMetrics = mock(SamzaContainerMetrics.class);\r\n    when(samzaContainerMetrics.taskStoreRestorationMetrics()).thenReturn(taskRestoreMetricGauges);\r\n    // Create mocked configs for specifying serdes\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"serializers.registry.stringserde.class\", StringSerdeFactory.class.getName());\r\n    configMap.put(TaskConfig.TRANSACTIONAL_STATE_RETAIN_EXISTING_STATE, \"true\");\r\n    Config config = new MapConfig(configMap);\r\n    Map<String, Serde<Object>> serdes = new HashMap<>();\r\n    serdes.put(\"stringserde\", mock(Serde.class));\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    when(checkpointManager.readLastCheckpoint(any(TaskName.class))).thenReturn(new CheckpointV1(new HashMap<>()));\r\n    ContainerContext mockContainerContext = mock(ContainerContext.class);\r\n    ContainerModel mockContainerModel = new ContainerModel(\"samza-container-test\", tasks);\r\n    when(mockContainerContext.getContainerModel()).thenReturn(mockContainerModel);\r\n    // Reset the expected number of sysConsumer create, start and stop calls, and store.restore() calls\r\n    this.systemConsumerCreationCount = 0;\r\n    this.systemConsumerStartCount = 0;\r\n    this.systemConsumerStopCount = 0;\r\n    this.storeRestoreCallCount = 0;\r\n    StateBackendFactory backendFactory = mock(StateBackendFactory.class);\r\n    TaskRestoreManager restoreManager = mock(TaskRestoreManager.class);\r\n    when(backendFactory.getRestoreManager(any(), any(), any(), any(), any(), any(), any(), any(), any(), any(), any())).thenReturn(restoreManager);\r\n    doAnswer(invocation -> {\r\n        storeRestoreCallCount++;\r\n        return CompletableFuture.completedFuture(null);\r\n    }).when(restoreManager).restore();\r\n    // Create the container storage manager\r\n    ContainerStorageManager containerStorageManager = new ContainerStorageManager(checkpointManager, mockContainerModel, mock(StreamMetadataCache.class), mock(SystemAdmins.class), new HashMap<>(), new HashMap<>(), new HashMap<>(), new HashMap<>(), serdes, config, taskInstanceMetrics, samzaContainerMetrics, mock(JobContext.class), mockContainerContext, new HashMap<>(), mock(Map.class), DEFAULT_LOGGED_STORE_BASE_DIR, DEFAULT_STORE_BASE_DIR, null, SystemClock.instance());\r\n    containerStorageManager.start();\r\n    containerStorageManager.shutdown();\r\n    for (Gauge gauge : taskRestoreMetricGauges.values()) {\r\n        Assert.assertTrue(\"Restoration time gauge value should never be invoked\", mockingDetails(gauge).getInvocations().size() == 0);\r\n    }\r\n    Assert.assertEquals(\"Store restore count should be 2 because there are 0 stores\", 0, this.storeRestoreCallCount);\r\n    Assert.assertEquals(0, this.systemConsumerCreationCount);\r\n    Assert.assertEquals(0, this.systemConsumerStopCount);\r\n    Assert.assertEquals(0, this.systemConsumerStartCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testCheckpointBasedRestoreFactoryCreation",
  "sourceCode" : "@Test\r\npublic void testCheckpointBasedRestoreFactoryCreation() {\r\n    Set<String> storeNames = ImmutableSet.of(\"storeName0\", \"storeName1\", \"storeName2\");\r\n    StorageConfig mockConfig = mock(StorageConfig.class);\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName0\")).thenReturn(ImmutableList.of(\"factory0\", \"factory1\", \"factory2\"));\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName1\")).thenReturn(ImmutableList.of(\"factory2\", \"factory1\"));\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName2\")).thenReturn(Collections.emptyList());\r\n    when(mockConfig.getChangelogStream(\"storeName0\")).thenReturn(Optional.empty());\r\n    when(mockConfig.getChangelogStream(\"storeName1\")).thenReturn(Optional.of(\"changelog\"));\r\n    when(mockConfig.getChangelogStream(\"storeName2\")).thenReturn(Optional.of(\"changelog\"));\r\n    CheckpointV1 checkpointV1 = mock(CheckpointV1.class);\r\n    when(checkpointV1.getVersion()).thenReturn((short) 1);\r\n    Map<String, Set<String>> factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV1, mockConfig);\r\n    Assert.assertEquals(1, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\", \"storeName2\"), factoriesToStores.get(StorageConfig.KAFKA_STATE_BACKEND_FACTORY));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, null, mockConfig);\r\n    Assert.assertEquals(2, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\"), factoriesToStores.get(\"factory0\"));\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\"), factoriesToStores.get(\"factory2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testCheckpointV2BasedRestoreFactoryCreation",
  "sourceCode" : "@Test\r\npublic void testCheckpointV2BasedRestoreFactoryCreation() {\r\n    Set<String> storeNames = ImmutableSet.of(\"storeName0\", \"storeName1\", \"storeName2\");\r\n    StorageConfig mockConfig = mock(StorageConfig.class);\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName0\")).thenReturn(ImmutableList.of(\"factory0\", \"factory1\", \"factory2\"));\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName1\")).thenReturn(ImmutableList.of(\"factory2\", \"factory1\"));\r\n    when(mockConfig.getStoreRestoreFactories(\"storeName2\")).thenReturn(Collections.emptyList());\r\n    when(mockConfig.getChangelogStream(\"storeName0\")).thenReturn(Optional.empty());\r\n    when(mockConfig.getChangelogStream(\"storeName1\")).thenReturn(Optional.of(\"changelog\"));\r\n    when(mockConfig.getChangelogStream(\"storeName2\")).thenReturn(Optional.of(\"changelog\"));\r\n    CheckpointV2 checkpointV2 = mock(CheckpointV2.class);\r\n    when(checkpointV2.getVersion()).thenReturn((short) 2);\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory0\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\"), \"factory1\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\"), \"factory2\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\")));\r\n    Map<String, Set<String>> factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(2, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\"), factoriesToStores.get(\"factory0\"));\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\"), factoriesToStores.get(\"factory2\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory2\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\")));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(1, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\", \"storeName0\"), factoriesToStores.get(\"factory2\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory1\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\"), \"factory2\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\")));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(2, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\"), factoriesToStores.get(\"factory1\"));\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\"), factoriesToStores.get(\"factory2\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory1\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\"), \"factory2\", ImmutableMap.of(\"storeName0\", \"\", \"storeName2\", \"\")));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(1, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\", \"storeName1\"), factoriesToStores.get(\"factory1\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory1\", ImmutableMap.of(\"storeName0\", \"\", \"storeName1\", \"\", \"storeName2\", \"\")));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(1, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\", \"storeName1\"), factoriesToStores.get(\"factory1\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(Collections.emptyMap());\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(2, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\"), factoriesToStores.get(\"factory0\"));\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\"), factoriesToStores.get(\"factory2\"));\r\n    when(checkpointV2.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(\"factory0\", ImmutableMap.of(\"storeName1\", \"\", \"storeName2\", \"\"), \"factory1\", ImmutableMap.of(\"storeName1\", \"\", \"storeName2\", \"\"), \"factory2\", ImmutableMap.of(\"storeName0\", \"\", \"storeName2\", \"\")));\r\n    factoriesToStores = ContainerStorageManagerUtil.getBackendFactoryStoreNames(storeNames, checkpointV2, mockConfig);\r\n    Assert.assertEquals(2, factoriesToStores.size());\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName1\"), factoriesToStores.get(\"factory1\"));\r\n    Assert.assertEquals(ImmutableSet.of(\"storeName0\"), factoriesToStores.get(\"factory2\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testInitRecoversFromDeletedException",
  "sourceCode" : "@Test\r\npublic void testInitRecoversFromDeletedException() {\r\n    TaskName taskName = new TaskName(\"task\");\r\n    Set<String> stores = Collections.singleton(\"store\");\r\n    BlobStoreRestoreManager taskRestoreManager = mock(BlobStoreRestoreManager.class);\r\n    Throwable deletedException = new SamzaException(new CompletionException(new DeletedException(\"410 gone\")));\r\n    doThrow(deletedException).when(taskRestoreManager).init(any(Checkpoint.class));\r\n    when(taskRestoreManager.restore()).thenReturn(CompletableFuture.completedFuture(null));\r\n    when(taskRestoreManager.restore(true)).thenReturn(CompletableFuture.completedFuture(null));\r\n    // mock ReflectionUtil.getObj\r\n    PowerMockito.mockStatic(ReflectionUtil.class);\r\n    BlobStoreManagerFactory blobStoreManagerFactory = mock(BlobStoreManagerFactory.class);\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    PowerMockito.when(ReflectionUtil.getObj(anyString(), eq(BlobStoreManagerFactory.class))).thenReturn(blobStoreManagerFactory);\r\n    when(blobStoreManagerFactory.getRestoreBlobStoreManager(any(Config.class), any(ExecutorService.class))).thenReturn(blobStoreManager);\r\n    Map<String, TaskRestoreManager> storeTaskRestoreManager = ImmutableMap.of(\"store\", taskRestoreManager);\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(jobContext.getJobModel()).thenReturn(mock(JobModel.class));\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(taskModel.getTaskName()).thenReturn(new TaskName(\"test\"));\r\n    ContainerModel containerModel = mock(ContainerModel.class);\r\n    when(containerModel.getTasks()).thenReturn(ImmutableMap.of(taskName, taskModel));\r\n    Checkpoint checkpoint = mock(CheckpointV2.class);\r\n    Map<TaskName, Checkpoint> taskCheckpoints = ImmutableMap.of(taskName, checkpoint);\r\n    Map<TaskName, Map<String, Set<String>>> taskBackendFactoryToStoreNames = ImmutableMap.of(taskName, ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), stores));\r\n    Config config = new MapConfig(ImmutableMap.of(\"job.name\", \"test\"), ImmutableMap.of(\"stores.store.backup.factories\", BlobStoreStateBackendFactory.class.getName()));\r\n    ExecutorService executor = Executors.newSingleThreadExecutor();\r\n    SystemConsumer systemConsumer = mock(SystemConsumer.class);\r\n    ContainerStorageManagerRestoreUtil.initAndRestoreTaskInstances(ImmutableMap.of(taskName, storeTaskRestoreManager), samzaContainerMetrics, checkpointManager, jobContext, containerModel, taskCheckpoints, taskBackendFactoryToStoreNames, config, executor, new HashMap<>(), null, ImmutableMap.of(\"store\", systemConsumer));\r\n    // verify init() is called twice -> once without getDeleted flag, once with getDeleted flag\r\n    verify(taskRestoreManager, times(1)).init(any(Checkpoint.class));\r\n    verify(taskRestoreManager, times(1)).init(any(Checkpoint.class), anyBoolean());\r\n    // verify restore is called with getDeletedFlag only\r\n    verify(taskRestoreManager, times(0)).restore();\r\n    verify(taskRestoreManager, times(1)).restore(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testRestoreRecoversFromDeletedException",
  "sourceCode" : "@Test\r\npublic void testRestoreRecoversFromDeletedException() throws Exception {\r\n    TaskName taskName = new TaskName(\"task\");\r\n    String storeName = \"store\";\r\n    Set<String> stores = Collections.singleton(storeName);\r\n    String jobName = \"job\";\r\n    String jobId = \"jobId\";\r\n    BlobStoreManager blobStoreManager = mock(BlobStoreManager.class);\r\n    doNothing().when(blobStoreManager).init();\r\n    doNothing().when(blobStoreManager).close();\r\n    BlobStoreRestoreManager taskRestoreManager = mock(BlobStoreRestoreManager.class);\r\n    doAnswer(invocation -> {\r\n        blobStoreManager.init();\r\n        return null;\r\n    }).when(taskRestoreManager).init(any(Checkpoint.class));\r\n    doAnswer(invocation -> {\r\n        blobStoreManager.close();\r\n        return null;\r\n    }).when(taskRestoreManager).close();\r\n    CompletableFuture<Void> failedFuture = CompletableFuture.completedFuture(null).thenCompose(v -> {\r\n        throw new DeletedException(\"410 Gone\");\r\n    });\r\n    when(taskRestoreManager.restore()).thenReturn(failedFuture);\r\n    when(taskRestoreManager.restore(true)).thenReturn(CompletableFuture.completedFuture(null));\r\n    Map<String, TaskRestoreManager> factoryToTaskRestoreManager = ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), taskRestoreManager);\r\n    JobContext jobContext = mock(JobContext.class);\r\n    TaskModel taskModel = mock(TaskModel.class);\r\n    when(taskModel.getTaskName()).thenReturn(taskName);\r\n    when(taskModel.getTaskMode()).thenReturn(TaskMode.Active);\r\n    ContainerModel containerModel = mock(ContainerModel.class);\r\n    when(containerModel.getTasks()).thenReturn(ImmutableMap.of(taskName, taskModel));\r\n    CheckpointV2 checkpoint = mock(CheckpointV2.class);\r\n    when(checkpoint.getOffsets()).thenReturn(ImmutableMap.of());\r\n    when(checkpoint.getCheckpointId()).thenReturn(CheckpointId.create());\r\n    when(checkpoint.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(KafkaChangelogStateBackendFactory.class.getName(), new HashMap<>()));\r\n    CheckpointManager checkpointManager = mock(CheckpointManager.class);\r\n    when(checkpointManager.readLastCheckpoint(taskName)).thenReturn(checkpoint);\r\n    String expectedOldBlobId = \"oldBlobId\";\r\n    when(checkpoint.getStateCheckpointMarkers()).thenReturn(ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), ImmutableMap.of(storeName, expectedOldBlobId)));\r\n    Map<TaskName, Checkpoint> taskCheckpoints = ImmutableMap.of(taskName, checkpoint);\r\n    Map<TaskName, Map<String, Set<String>>> taskBackendFactoryToStoreNames = ImmutableMap.of(taskName, ImmutableMap.of(BlobStoreStateBackendFactory.class.getName(), stores));\r\n    Config config = new MapConfig(ImmutableMap.of(\"blob.store.manager.factory\", BlobStoreStateBackendFactory.class.getName(), \"job.name\", jobName));\r\n    ExecutorService executor = Executors.newFixedThreadPool(5);\r\n    SystemConsumer systemConsumer = mock(SystemConsumer.class);\r\n    // mock ReflectionUtil.getObj\r\n    PowerMockito.mockStatic(ReflectionUtil.class);\r\n    BlobStoreManagerFactory blobStoreManagerFactory = mock(BlobStoreManagerFactory.class);\r\n    PowerMockito.when(ReflectionUtil.getObj(anyString(), eq(BlobStoreManagerFactory.class))).thenReturn(blobStoreManagerFactory);\r\n    when(blobStoreManagerFactory.getRestoreBlobStoreManager(any(Config.class), any(ExecutorService.class))).thenReturn(blobStoreManager);\r\n    // To verify order of operations\r\n    InOrder inOrder = inOrder(blobStoreManager, taskRestoreManager);\r\n    // mock ContainerStorageManagerRestoreUtil.backupRecoveredStore\r\n    String expectedBlobId = \"blobId\";\r\n    PowerMockito.spy(ContainerStorageManagerRestoreUtil.class);\r\n    PowerMockito.doReturn(CompletableFuture.completedFuture(ImmutableMap.of(storeName, expectedBlobId))).when(ContainerStorageManagerRestoreUtil.class, \"backupRecoveredStore\", any(JobContext.class), any(ContainerModel.class), any(Config.class), any(TaskName.class), any(Set.class), any(Checkpoint.class), any(File.class), any(BlobStoreManager.class), any(MetricsRegistry.class), any(ExecutorService.class));\r\n    SnapshotIndex snapshotIndex = new SnapshotIndex(System.currentTimeMillis(), new SnapshotMetadata(CheckpointId.create(), jobName, jobId, taskName.getTaskName(), storeName), new DirIndex(\"test\", new ArrayList<>(), new ArrayList<>(), new ArrayList<>(), new ArrayList<>()), Optional.empty());\r\n    ArgumentCaptor<String> getBlobIdCaptor = ArgumentCaptor.forClass(String.class);\r\n    ArgumentCaptor<ByteArrayOutputStream> outputStreamCaptor = ArgumentCaptor.forClass(ByteArrayOutputStream.class);\r\n    when(blobStoreManager.get(getBlobIdCaptor.capture(), outputStreamCaptor.capture(), any(Metadata.class), any(Boolean.class))).thenAnswer(invocation -> {\r\n        ByteArrayOutputStream outputStream = outputStreamCaptor.getValue();\r\n        outputStream.write(new SnapshotIndexSerde().toBytes(snapshotIndex));\r\n        return CompletableFuture.completedFuture(null);\r\n    });\r\n    ArgumentCaptor<String> removeTTLBlobIdCaptor = ArgumentCaptor.forClass(String.class);\r\n    when(blobStoreManager.removeTTL(removeTTLBlobIdCaptor.capture(), any(Metadata.class))).thenAnswer(invocation -> CompletableFuture.completedFuture(null));\r\n    ArgumentCaptor<String> deleteBlobIdCaptor = ArgumentCaptor.forClass(String.class);\r\n    when(blobStoreManager.delete(deleteBlobIdCaptor.capture(), any(Metadata.class))).thenAnswer(invocation -> CompletableFuture.completedFuture(null));\r\n    CompletableFuture<Map<TaskName, Checkpoint>> updatedTaskCheckpointsFuture = ContainerStorageManagerRestoreUtil.initAndRestoreTaskInstances(ImmutableMap.of(taskName, factoryToTaskRestoreManager), samzaContainerMetrics, checkpointManager, jobContext, containerModel, taskCheckpoints, taskBackendFactoryToStoreNames, config, executor, new HashMap<>(), null, ImmutableMap.of(storeName, systemConsumer));\r\n    // verify close is not called until init and restore futures are complete\r\n    verify(taskRestoreManager, never()).close();\r\n    Map<TaskName, Checkpoint> updatedTaskCheckpoints = updatedTaskCheckpointsFuture.get();\r\n    // verify close is called only once after restore future is complete\r\n    verify(taskRestoreManager, times(1)).close();\r\n    // verify taskCheckpoint is updated\r\n    assertNotEquals(((CheckpointV2) taskCheckpoints.get(taskName)).getCheckpointId(), ((CheckpointV2) updatedTaskCheckpoints.get(taskName)).getCheckpointId());\r\n    // verify init is not retried with getDeleted\r\n    verify(taskRestoreManager, times(0)).init(any(Checkpoint.class), anyBoolean());\r\n    // verify restore is call twice - once without getDeleted flag, once with getDeleted flag\r\n    verify(taskRestoreManager, times(1)).restore();\r\n    verify(taskRestoreManager, times(1)).restore(true);\r\n    // verify the GET and removeTTL was called on the new SnapshotIndex\r\n    assertEquals(expectedBlobId, getBlobIdCaptor.getAllValues().get(0));\r\n    assertEquals(expectedBlobId, removeTTLBlobIdCaptor.getAllValues().get(0));\r\n    // verify that GET and delete was called on the old SnapshotIndex\r\n    assertEquals(expectedOldBlobId, getBlobIdCaptor.getAllValues().get(1));\r\n    assertEquals(expectedOldBlobId, deleteBlobIdCaptor.getValue());\r\n    // verify the order of operations in taskRestoreManager and blobStoreManager\r\n    // Verifies that close is called after restore(true)\r\n    inOrder.verify(taskRestoreManager).init(any(Checkpoint.class));\r\n    // init called on blobStoreManager passed to taskRestoreManager\r\n    inOrder.verify(blobStoreManager).init();\r\n    inOrder.verify(taskRestoreManager).restore();\r\n    // init called on blobStoreManager created in ContainerStorageManagerRestoreUtil#restoreDeletedSnapshot\r\n    inOrder.verify(blobStoreManager).init();\r\n    inOrder.verify(taskRestoreManager).restore(true);\r\n    // close called on blobStoreManager created in ContainerStorageManagerRestoreUtil#restoreDeletedSnapshot\r\n    inOrder.verify(blobStoreManager).close();\r\n    // close called on blobStoreManager passed to taskRestoreManager\r\n    inOrder.verify(blobStoreManager).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "testStoreDirectoriesInitialized",
  "sourceCode" : "@Test\r\npublic void testStoreDirectoriesInitialized() {\r\n    String sideInputStore = \"sideInputStore\";\r\n    String inMemoryStore = \"inMemoryStore\";\r\n    String regularStore = \"regularStore\";\r\n    Map<String, String> storeFactories = new HashMap<>();\r\n    storeFactories.put(String.format(\"stores.%s.side.inputs.processor.factory\", sideInputStore), \"sideinputfactory\");\r\n    storeFactories.put(String.format(\"stores.%s.factory\", regularStore), \"regularstorefactory\");\r\n    storeFactories.put(String.format(\"stores.%s.factory\", inMemoryStore), \"org.apache.samza.storage.kv.inmemory.InMemoryKeyValueStorageEngineFactory\");\r\n    Map<String, String> configMap = new HashMap<>(storeFactories);\r\n    Config config = new MapConfig(configMap);\r\n    Map<String, StorageEngineFactory<Object, Object>> storageEngineFactories = new HashMap<>();\r\n    storageEngineFactories.put(sideInputStore, (StorageEngineFactory<Object, Object>) mock(StorageEngineFactory.class));\r\n    storageEngineFactories.put(inMemoryStore, (StorageEngineFactory<Object, Object>) mock(StorageEngineFactory.class));\r\n    storageEngineFactories.put(regularStore, (StorageEngineFactory<Object, Object>) mock(StorageEngineFactory.class));\r\n    Map<String, SystemStream> activeTaskChangelogSystemStreams = new HashMap<>();\r\n    activeTaskChangelogSystemStreams.put(regularStore, new SystemStream(\"kafka\", \"changelog\"));\r\n    Set<String> sideInputStoreNames = new HashSet<>();\r\n    sideInputStoreNames.add(sideInputStore);\r\n    ContainerModel containerModel = mock(ContainerModel.class);\r\n    when(containerModel.getTasks()).thenReturn(ImmutableMap.of(new TaskName(\"task\"), new TaskModel(new TaskName(\"task\"), Collections.emptySet(), new Partition(1))));\r\n    Set<Path> storeDirPaths = ContainerStorageManagerUtil.getStoreDirPaths(config, storageEngineFactories, activeTaskChangelogSystemStreams, sideInputStoreNames, containerModel, new StorageManagerUtil(), new File(\"/tmp\"), new File(\"/tmp2\"));\r\n    assertEquals(3, storeDirPaths.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getActiveTaskChangelogSystemStreams",
  "sourceCode" : "@Test\r\npublic void getActiveTaskChangelogSystemStreams() {\r\n    Map<String, SystemStream> storeToChangelogSystemStreams = ContainerStorageManagerUtil.getActiveTaskChangelogSystemStreams(testContext.storesToSystemStreams, testContext.standbyContainerModel);\r\n    assertEquals(\"Standby container should have no active change log\", Collections.emptyMap(), storeToChangelogSystemStreams);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getActiveTaskChangelogSystemStreamsForActiveAndStandbyContainer",
  "sourceCode" : "@Test\r\npublic void getActiveTaskChangelogSystemStreamsForActiveAndStandbyContainer() {\r\n    Map<String, SystemStream> expectedStoreToChangelogSystemStreams = testContext.storesToSystemStreams;\r\n    Map<String, SystemStream> storeToChangelogSystemStreams = ContainerStorageManagerUtil.getActiveTaskChangelogSystemStreams(testContext.storesToSystemStreams, testContext.activeAndStandbyContainerModel);\r\n    assertEquals(\"Active and standby container model should have non empty store to changelog mapping\", expectedStoreToChangelogSystemStreams, storeToChangelogSystemStreams);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getActiveTaskChangelogSystemStreamsForStandbyContainer",
  "sourceCode" : "@Test\r\npublic void getActiveTaskChangelogSystemStreamsForStandbyContainer() {\r\n    Map<String, SystemStream> expectedStoreToChangelogSystemStreams = testContext.storesToSystemStreams;\r\n    Map<String, SystemStream> storeToChangelogSystemStreams = ContainerStorageManagerUtil.getActiveTaskChangelogSystemStreams(testContext.storesToSystemStreams, testContext.activeContainerModel);\r\n    assertEquals(\"Active container model should have non empty store to changelog mapping\", expectedStoreToChangelogSystemStreams, storeToChangelogSystemStreams);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getSideInputStoresForActiveContainer",
  "sourceCode" : "@Test\r\npublic void getSideInputStoresForActiveContainer() {\r\n    Set<String> expectedSideInputStores = testContext.activeStores;\r\n    Set<String> actualSideInputStores = ContainerStorageManagerUtil.getSideInputStoreNames(testContext.sideInputStoresToSystemStreams, testContext.storesToSystemStreams, testContext.activeContainerModel);\r\n    assertEquals(\"Mismatch in stores\", expectedSideInputStores, actualSideInputStores);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getSideInputStoresForStandbyContainer",
  "sourceCode" : "@Test\r\npublic void getSideInputStoresForStandbyContainer() {\r\n    final Set<String> expectedSideInputStores = testContext.standbyStores;\r\n    Set<String> actualSideInputStores = ContainerStorageManagerUtil.getSideInputStoreNames(testContext.sideInputStoresToSystemStreams, testContext.storesToSystemStreams, testContext.standbyContainerModel);\r\n    assertEquals(\"Mismatch in side input stores\", expectedSideInputStores, actualSideInputStores);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getTaskSideInputSSPsForActiveContainer",
  "sourceCode" : "@Test\r\npublic void getTaskSideInputSSPsForActiveContainer() {\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> expectedSideInputSSPs = testContext.activeSideInputSSPs;\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> actualSideInputSSPs = SideInputsManager.getTaskSideInputSSPs(Collections.emptyMap(), testContext.storesToSystemStreams, testContext.activeContainerModel);\r\n    assertEquals(\"Mismatch in task name --> store --> SSP mapping\", expectedSideInputSSPs, actualSideInputSSPs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getTaskSideInputSSPsForStandbyContainerWithSideInput",
  "sourceCode" : "@Test\r\npublic void getTaskSideInputSSPsForStandbyContainerWithSideInput() {\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> expectedSideInputSSPs = testContext.standbyWithSideInputSSPs;\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> actualSideInputSSPs = SideInputsManager.getTaskSideInputSSPs(testContext.sideInputStoresToSystemStreams, testContext.storesToSystemStreams, testContext.standbyContainerModelWithSideInputs);\r\n    assertEquals(\"Mismatch in task name --> store --> SSP mapping\", expectedSideInputSSPs, actualSideInputSSPs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-core\\src\\test\\scala\\org\\apache\\samza\\storage\\TestContainerStorageManager.java",
  "methodName" : "getTaskSideInputSSPsForStandbyContainerWithoutSideInputs",
  "sourceCode" : "@Test\r\npublic void getTaskSideInputSSPsForStandbyContainerWithoutSideInputs() {\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> expectedSideInputSSPs = testContext.standbyChangelogSSPs;\r\n    Map<TaskName, Map<String, Set<SystemStreamPartition>>> actualSideInputSSPs = SideInputsManager.getTaskSideInputSSPs(Collections.emptyMap(), testContext.storesToSystemStreams, testContext.standbyContainerModel);\r\n    assertEquals(\"Mismatch in task name --> store --> SSP mapping\", expectedSideInputSSPs, actualSideInputSSPs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetClientFactoryClassName",
  "sourceCode" : "@Test\r\npublic void testGetClientFactoryClassName() throws Exception {\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.client.factory\", \"bar\");\r\n    assertEquals(\"bar\", config.getClientFactoryClassName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetTransportHost",
  "sourceCode" : "@Test\r\npublic void testGetTransportHost() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getTransportHost().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.client.transport.host\", \"example.org\");\r\n    assertTrue(config.getTransportHost().isPresent());\r\n    assertEquals(\"example.org\", config.getTransportHost().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetTransportPort",
  "sourceCode" : "@Test\r\npublic void testGetTransportPort() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getTransportPort().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.client.transport.port\", \"9300\");\r\n    assertTrue(config.getTransportPort().isPresent());\r\n    assertEquals(Integer.valueOf(9300), config.getTransportPort().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetElasticsearchSettings",
  "sourceCode" : "@Test\r\npublic void testGetElasticsearchSettings() throws Exception {\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.client.elasticsearch.foo\", \"bar\");\r\n    assertEquals(\"bar\", config.getElasticseachSettings().get(\"foo\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetBulkFlushMaxActions",
  "sourceCode" : "@Test\r\npublic void testGetBulkFlushMaxActions() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getBulkFlushMaxActions().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.bulk.flush.max.actions\", \"10\");\r\n    assertEquals(Integer.valueOf(10), config.getBulkFlushMaxActions().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetBulkFlushMaxSizeMB",
  "sourceCode" : "@Test\r\npublic void testGetBulkFlushMaxSizeMB() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getBulkFlushMaxSizeMB().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.bulk.flush.max.size.mb\", \"10\");\r\n    assertTrue(config.getBulkFlushMaxSizeMB().isPresent());\r\n    assertEquals(Integer.valueOf(10), config.getBulkFlushMaxSizeMB().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetBulkFlushIntervalMS",
  "sourceCode" : "@Test\r\npublic void testGetBulkFlushIntervalMS() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getBulkFlushIntervalMS().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.bulk.flush.interval.ms\", \"10\");\r\n    assertTrue(config.getBulkFlushIntervalMS().isPresent());\r\n    assertEquals(Integer.valueOf(10), config.getBulkFlushIntervalMS().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\config\\ElasticsearchConfigTest.java",
  "methodName" : "testGetIndexRequestFactoryClassName",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestFactoryClassName() throws Exception {\r\n    assertFalse(EMPTY_CONFIG.getIndexRequestFactoryClassName().isPresent());\r\n    ElasticsearchConfig config = configForProperty(\"systems.es.index.request.factory\", \"foo\");\r\n    assertTrue(config.getIndexRequestFactoryClassName().isPresent());\r\n    assertEquals(\"foo\", config.getIndexRequestFactoryClassName().get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerMetricsTest.java",
  "methodName" : "testMetrics",
  "sourceCode" : "@Test\r\npublic void testMetrics() {\r\n    ReadableMetricsRegistry registry = new MetricsRegistryMap();\r\n    ElasticsearchSystemProducerMetrics metrics = new ElasticsearchSystemProducerMetrics(\"es\", registry);\r\n    metrics.bulkSendSuccess.inc(29L);\r\n    metrics.inserts.inc();\r\n    metrics.updates.inc(7L);\r\n    metrics.conflicts.inc(3L);\r\n    Set<String> groups = registry.getGroups();\r\n    assertEquals(1, groups.size());\r\n    assertEquals(GRP_NAME, groups.toArray()[0]);\r\n    Map<String, Metric> metricMap = registry.getGroup(GRP_NAME);\r\n    assertEquals(4, metricMap.size());\r\n    assertEquals(29L, ((Counter) metricMap.get(\"es-bulk-send-success\")).getCount());\r\n    assertEquals(1L, ((Counter) metricMap.get(\"es-docs-inserted\")).getCount());\r\n    assertEquals(7L, ((Counter) metricMap.get(\"es-docs-updated\")).getCount());\r\n    assertEquals(3L, ((Counter) metricMap.get(\"es-version-conflicts\")).getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testRegisterStop",
  "sourceCode" : "@Test\r\npublic void testRegisterStop() throws Exception {\r\n    producer.stop();\r\n    verify(processorOne).flush();\r\n    verify(processorTwo).flush();\r\n    verify(processorOne).close();\r\n    verify(processorTwo).close();\r\n    verify(CLIENT).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testSend",
  "sourceCode" : "@Test\r\npublic void testSend() throws Exception {\r\n    OutgoingMessageEnvelope envelope = mock(OutgoingMessageEnvelope.class);\r\n    IndexRequest indexRequest = mock(IndexRequest.class);\r\n    when(INDEX_REQUEST_FACTORY.getIndexRequest(envelope)).thenReturn(indexRequest);\r\n    producer.send(SOURCE_ONE, envelope);\r\n    verify(processorOne).add(indexRequest);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testFlushNoFailedSend",
  "sourceCode" : "@Test\r\npublic void testFlushNoFailedSend() throws Exception {\r\n    producer.flush(SOURCE_ONE);\r\n    verify(processorOne).flush();\r\n    verify(processorTwo, never()).flush();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testFlushFailedSendFromException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testFlushFailedSendFromException() throws Exception {\r\n    ArgumentCaptor<BulkProcessor.Listener> listenerCaptor = ArgumentCaptor.forClass(BulkProcessor.Listener.class);\r\n    when(BULK_PROCESSOR_FACTORY.getBulkProcessor(eq(CLIENT), listenerCaptor.capture())).thenReturn(processorOne);\r\n    producer.register(SOURCE_ONE);\r\n    listenerCaptor.getValue().afterBulk(0, null, new Throwable());\r\n    producer.flush(SOURCE_ONE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testFlushFailedSendFromFailedDocument",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testFlushFailedSendFromFailedDocument() throws Exception {\r\n    ArgumentCaptor<BulkProcessor.Listener> listenerCaptor = ArgumentCaptor.forClass(BulkProcessor.Listener.class);\r\n    when(BULK_PROCESSOR_FACTORY.getBulkProcessor(eq(CLIENT), listenerCaptor.capture())).thenReturn(processorOne);\r\n    producer.register(SOURCE_ONE);\r\n    BulkResponse response = getRespWithFailedDocument(RestStatus.BAD_REQUEST);\r\n    listenerCaptor.getValue().afterBulk(0, null, response);\r\n    producer.flush(SOURCE_ONE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\ElasticsearchSystemProducerTest.java",
  "methodName" : "testIgnoreVersionConficts",
  "sourceCode" : "@Test\r\npublic void testIgnoreVersionConficts() throws Exception {\r\n    ArgumentCaptor<BulkProcessor.Listener> listenerCaptor = ArgumentCaptor.forClass(BulkProcessor.Listener.class);\r\n    when(BULK_PROCESSOR_FACTORY.getBulkProcessor(eq(CLIENT), listenerCaptor.capture())).thenReturn(processorOne);\r\n    producer.register(SOURCE_ONE);\r\n    BulkResponse response = getRespWithFailedDocument(RestStatus.CONFLICT);\r\n    listenerCaptor.getValue().afterBulk(0, null, response);\r\n    assertEquals(1, metrics.conflicts.getCount());\r\n    producer.flush(SOURCE_ONE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestStreamName",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestStreamName() {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, EMPTY_MSG));\r\n    assertEquals(INDEX, indexRequest.index());\r\n    assertEquals(TYPE, indexRequest.type());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestInvalidStreamName",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetIndexRequestInvalidStreamName() {\r\n    when(SYSTEM.getStream()).thenReturn(INDEX);\r\n    INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, EMPTY_MSG));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestNoId",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestNoId() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, EMPTY_MSG));\r\n    assertNull(indexRequest.id());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestWithId",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestWithId() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, \"id\", EMPTY_MSG));\r\n    assertEquals(\"id\", indexRequest.id());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestNoPartitionKey",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestNoPartitionKey() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, EMPTY_MSG));\r\n    assertNull(indexRequest.routing());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestWithPartitionKey",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestWithPartitionKey() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, \"shardKey\", \"id\", EMPTY_MSG));\r\n    assertEquals(\"shardKey\", indexRequest.routing());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestMessageBytes",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestMessageBytes() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, \"{\\\"foo\\\":\\\"bar\\\"}\".getBytes(Charsets.UTF_8)));\r\n    assertEquals(Collections.singletonMap(\"foo\", \"bar\"), indexRequest.sourceAsMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestMessageMap",
  "sourceCode" : "@Test\r\npublic void testGetIndexRequestMessageMap() throws Exception {\r\n    IndexRequest indexRequest = INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, Collections.singletonMap(\"foo\", \"bar\")));\r\n    assertEquals(Collections.singletonMap(\"foo\", \"bar\"), indexRequest.sourceAsMap());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-elasticsearch\\src\\test\\java\\org\\apache\\samza\\system\\elasticsearch\\indexrequest\\DefaultIndexRequestFactoryTest.java",
  "methodName" : "testGetIndexRequestInvalidMessage",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetIndexRequestInvalidMessage() throws Exception {\r\n    INDEX_REQUEST_FACTORY.getIndexRequest(new OutgoingMessageEnvelope(SYSTEM, \"{'foo':'bar'}\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\descriptors\\TestHdfsSystemDescriptor.java",
  "methodName" : "testMajorConfigGeneration",
  "sourceCode" : "@Test\r\npublic void testMajorConfigGeneration() {\r\n    String systemName = \"hdfs\";\r\n    HdfsSystemDescriptor sd = new HdfsSystemDescriptor(systemName).withConsumerBufferCapacity(950).withConsumerWhiteList(\".*\").withReaderType(\"avro\").withOutputBaseDir(\"/home/output\").withWriterClassName(AvroDataFileHdfsWriter.class.getName());\r\n    sd.getInputDescriptor(\"input\");\r\n    Map<String, String> generatedConfig = sd.toConfig();\r\n    Assert.assertEquals(6, generatedConfig.size());\r\n    System.out.println(generatedConfig);\r\n    Assert.assertEquals(HdfsSystemFactory.class.getName(), generatedConfig.get(\"systems.hdfs.samza.factory\"));\r\n    Assert.assertEquals(\"950\", generatedConfig.get(String.format(HdfsConfig.CONSUMER_BUFFER_CAPACITY(), systemName)));\r\n    Assert.assertEquals(\".*\", generatedConfig.get(String.format(HdfsConfig.CONSUMER_PARTITIONER_WHITELIST(), systemName)));\r\n    Assert.assertEquals(\"avro\", generatedConfig.get(String.format(HdfsConfig.FILE_READER_TYPE(), systemName)));\r\n    Assert.assertEquals(\"/home/output\", generatedConfig.get(String.format(HdfsConfig.BASE_OUTPUT_DIR(), systemName)));\r\n    Assert.assertEquals(AvroDataFileHdfsWriter.class.getName(), generatedConfig.get(String.format(HdfsConfig.HDFS_WRITER_CLASS_NAME(), systemName)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testBasicWhiteListFiltering",
  "sourceCode" : "@Test\r\npublic void testBasicWhiteListFiltering() {\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 9;\r\n    String[] inputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"delta-01.avro\", \"part-005.avro\", \"delta-03.avro\", \"part-004.avro\", \"delta-02.avro\", \"part-006.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345, 313245, 234212, 413232 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \"part-.*\\\\.avro\";\r\n    String blackList = \"\";\r\n    String groupPattern = \"\";\r\n    int expectedNumPartition = 6;\r\n    int[][] expectedPartitioning = { { 0 }, { 1 }, { 2 }, { 4 }, { 6 }, { 8 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriptorMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriptorMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testBasicBlackListFiltering",
  "sourceCode" : "@Test\r\npublic void testBasicBlackListFiltering() {\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 9;\r\n    String[] inputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"delta-01.avro\", \"part-005.avro\", \"delta-03.avro\", \"part-004.avro\", \"delta-02.avro\", \"part-006.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345, 313245, 234212, 413232 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \".*\";\r\n    String blackList = \"delta-.*\\\\.avro\";\r\n    String groupPattern = \"\";\r\n    int expectedNumPartition = 6;\r\n    int[][] expectedPartitioning = { { 0 }, { 1 }, { 2 }, { 4 }, { 6 }, { 8 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriporMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testWhiteListBlackListFiltering",
  "sourceCode" : "@Test\r\npublic void testWhiteListBlackListFiltering() {\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 9;\r\n    String[] inputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"delta-01.avro\", \"part-005.avro\", \"delta-03.avro\", \"part-004.avro\", \"delta-02.avro\", \"part-006.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345, 313245, 234212, 413232 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \"part-.*\\\\.avro\";\r\n    String blackList = \"part-002.avro\";\r\n    String groupPattern = \"\";\r\n    int expectedNumPartition = 5;\r\n    int[][] expectedPartitioning = { { 0 }, { 2 }, { 4 }, { 6 }, { 8 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriporMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testBasicGrouping",
  "sourceCode" : "@Test\r\npublic void testBasicGrouping() {\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 9;\r\n    String[] inputFiles = { \"00_10-run_2016-08-15-13-04-part.0.150582.avro\", \"00_10-run_2016-08-15-13-04-part.1.138132.avro\", \"00_10-run_2016-08-15-13-04-part.2.214005.avro\", \"00_10-run_2016-08-15-13-05-part.0.205738.avro\", \"00_10-run_2016-08-15-13-05-part.1.158273.avro\", \"00_10-run_2016-08-15-13-05-part.2.982345.avro\", \"00_10-run_2016-08-15-13-06-part.0.313245.avro\", \"00_10-run_2016-08-15-13-06-part.1.234212.avro\", \"00_10-run_2016-08-15-13-06-part.2.413232.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345, 313245, 234212, 413232 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \".*\\\\.avro\";\r\n    String blackList = \"\";\r\n    // 00_10-run_2016-08-15-13-04-part.[id].138132.avro\r\n    String groupPattern = \".*part\\\\.[id]\\\\..*\\\\.avro\";\r\n    int expectedNumPartition = 3;\r\n    int[][] expectedPartitioning = { // files from index 0, 3, 6 should be grouped into one partition\r\n    { 0, 3, 6 }, // similar as above\r\n    { 1, 4, 7 }, { 2, 5, 8 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriporMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testValidDirectoryUpdating",
  "sourceCode" : "@Test\r\npublic void testValidDirectoryUpdating() {\r\n    // the update is valid when there are only new files being added to the directory\r\n    // no changes on the old files\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 6;\r\n    String[] inputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"part-005.avro\", \"part-004.avro\", \"part-006.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \".*\";\r\n    String blackList = \"\";\r\n    String groupPattern = \"\";\r\n    int expectedNumPartition = 6;\r\n    int[][] expectedPartitioning = { { 0 }, { 1 }, { 2 }, { 3 }, { 4 }, { 5 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriporMap);\r\n    numInput = 7;\r\n    String[] updatedInputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"part-005.avro\", \"part-004.avro\", // add a new file to the directory\r\n    \"part-007.avro\", \"part-006.avro\" };\r\n    long[] updatedFileLength = { 150582, 138132, 214005, 205738, 158273, 2513454, 982345 };\r\n    testList.clear();\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(updatedInputFiles[i], updatedFileLength[i]));\r\n    }\r\n    directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", descriporMap);\r\n    // still expect only 6 partitions instead of 7\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> updatedDescriptorMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, updatedDescriptorMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestDirectoryPartitioner.java",
  "methodName" : "testInvalidDirectoryUpdating",
  "sourceCode" : "@Test\r\npublic void testInvalidDirectoryUpdating() {\r\n    // the update is invalid when at least one old file is removed\r\n    List<FileMetadata> testList = new ArrayList<>();\r\n    int numInput = 6;\r\n    String[] inputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"part-005.avro\", \"part-004.avro\", \"part-006.avro\" };\r\n    long[] fileLength = { 150582, 138132, 214005, 205738, 158273, 982345 };\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(inputFiles[i], fileLength[i]));\r\n    }\r\n    String whiteList = \".*\";\r\n    String blackList = \"\";\r\n    String groupPattern = \"\";\r\n    int expectedNumPartition = 6;\r\n    int[][] expectedPartitioning = { { 0 }, { 1 }, { 2 }, { 3 }, { 4 }, { 5 } };\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    Map<Partition, SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(\"hdfs\", null);\r\n    Assert.assertEquals(expectedNumPartition, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(\"hdfs\");\r\n    verifyPartitionDescriptor(inputFiles, expectedPartitioning, expectedNumPartition, descriporMap);\r\n    String[] updatedInputFiles = { \"part-001.avro\", \"part-002.avro\", \"part-003.avro\", \"part-005.avro\", // remove part-004 and replace it with 007\r\n    \"part-007.avro\", \"part-006.avro\" };\r\n    long[] updatedFileLength = { 150582, 138132, 214005, 205738, 158273, 982345 };\r\n    testList.clear();\r\n    for (int i = 0; i < numInput; i++) {\r\n        testList.add(new FileMetadata(updatedInputFiles[i], updatedFileLength[i]));\r\n    }\r\n    directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new TestFileSystemAdapter(testList));\r\n    try {\r\n        directoryPartitioner.getPartitionMetadataMap(\"hdfs\", descriporMap);\r\n        Assert.fail(\"Expect exception thrown from getting metadata. Should not reach this point.\");\r\n    } catch (SamzaException e) {\r\n        // expect exception to be thrown\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestHdfsFileSystemAdapter.java",
  "methodName" : "testGetAllFiles",
  "sourceCode" : "@Test\r\npublic void testGetAllFiles() throws Exception {\r\n    URL url = this.getClass().getResource(\"/partitioner\");\r\n    FileSystemAdapter adapter = new HdfsFileSystemAdapter();\r\n    List<FileSystemAdapter.FileMetadata> result = adapter.getAllFiles(url.getPath());\r\n    Assert.assertEquals(3, result.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\partitioner\\TestHdfsFileSystemAdapter.java",
  "methodName" : "testIntegrationWithPartitioner",
  "sourceCode" : "@Test\r\npublic void testIntegrationWithPartitioner() throws Exception {\r\n    URL url = this.getClass().getResource(\"/partitioner\");\r\n    String whiteList = \".*\";\r\n    String blackList = \".*02\";\r\n    String groupPattern = \"\";\r\n    String streamName = String.format(url.getPath());\r\n    DirectoryPartitioner directoryPartitioner = new DirectoryPartitioner(whiteList, blackList, groupPattern, new HdfsFileSystemAdapter());\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> metadataMap = directoryPartitioner.getPartitionMetadataMap(streamName, null);\r\n    Assert.assertEquals(1, metadataMap.size());\r\n    Map<Partition, List<String>> descriporMap = directoryPartitioner.getPartitionDescriptor(streamName);\r\n    Assert.assertEquals(1, descriporMap.values().size());\r\n    Assert.assertTrue(descriporMap.get(new Partition(0)).get(0).endsWith(\"testfile01\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestAvroFileHdfsReader.java",
  "methodName" : "testSequentialRead",
  "sourceCode" : "@Test\r\npublic void testSequentialRead() throws Exception {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    SingleFileHdfsReader reader = new AvroFileHdfsReader(ssp);\r\n    reader.open(AVRO_FILE, \"0\");\r\n    int index = 0;\r\n    while (reader.hasNext()) {\r\n        GenericRecord record = (GenericRecord) reader.readNext().getMessage();\r\n        Assert.assertEquals(index, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + index, record.get(FIELD_2).toString());\r\n        index++;\r\n    }\r\n    Assert.assertEquals(NUM_EVENTS, index);\r\n    reader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestAvroFileHdfsReader.java",
  "methodName" : "testFileReopen",
  "sourceCode" : "@Test\r\npublic void testFileReopen() throws Exception {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    SingleFileHdfsReader reader = new AvroFileHdfsReader(ssp);\r\n    reader.open(AVRO_FILE, \"0\");\r\n    int index = 0;\r\n    for (; index < NUM_EVENTS / 2; index++) {\r\n        GenericRecord record = (GenericRecord) reader.readNext().getMessage();\r\n        Assert.assertEquals(index, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + index, record.get(FIELD_2).toString());\r\n    }\r\n    String offset = reader.nextOffset();\r\n    reader.close();\r\n    reader = new AvroFileHdfsReader(ssp);\r\n    reader.open(AVRO_FILE, offset);\r\n    for (; index < NUM_EVENTS; index++) {\r\n        GenericRecord record = (GenericRecord) reader.readNext().getMessage();\r\n        Assert.assertEquals(index, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + index, record.get(FIELD_2).toString());\r\n    }\r\n    Assert.assertEquals(NUM_EVENTS, index);\r\n    reader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestAvroFileHdfsReader.java",
  "methodName" : "testRandomRead",
  "sourceCode" : "@Test\r\npublic void testRandomRead() throws Exception {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    SingleFileHdfsReader reader = new AvroFileHdfsReader(ssp);\r\n    reader.open(AVRO_FILE, \"0\");\r\n    for (int i = 0; i < NUM_EVENTS / 2; i++) {\r\n        reader.readNext();\r\n    }\r\n    String offset = reader.nextOffset();\r\n    IncomingMessageEnvelope envelope = reader.readNext();\r\n    Assert.assertEquals(offset, envelope.getOffset());\r\n    GenericRecord record1 = (GenericRecord) envelope.getMessage();\r\n    for (int i = 0; i < 5; i++) reader.readNext();\r\n    // seek to the offset within the same reader\r\n    reader.seek(offset);\r\n    Assert.assertEquals(offset, reader.nextOffset());\r\n    envelope = reader.readNext();\r\n    Assert.assertEquals(offset, envelope.getOffset());\r\n    GenericRecord record2 = (GenericRecord) envelope.getMessage();\r\n    Assert.assertEquals(record1, record2);\r\n    reader.close();\r\n    // open a new reader and initialize it with the offset\r\n    reader = new AvroFileHdfsReader(ssp);\r\n    reader.open(AVRO_FILE, offset);\r\n    envelope = reader.readNext();\r\n    Assert.assertEquals(offset, envelope.getOffset());\r\n    GenericRecord record3 = (GenericRecord) envelope.getMessage();\r\n    Assert.assertEquals(record1, record3);\r\n    reader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestAvroFileHdfsReader.java",
  "methodName" : "testOffsetComparator",
  "sourceCode" : "@Test\r\npublic void testOffsetComparator() {\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"0\", \"1452\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001@3\", \"2001@4\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001@4\", \"2010@1\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001@3\", \"2011@3\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001\", \"2001@4\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001\", \"2010@1\"));\r\n    Assert.assertEquals(-1, AvroFileHdfsReader.offsetComparator(\"2001@3\", \"2010\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"1984\", \"0\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"1984@2\", \"1984@1\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"14341@2\", \"1984@2\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"14341@1\", \"1984@10\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"14341\", \"1984@10\"));\r\n    Assert.assertEquals(1, AvroFileHdfsReader.offsetComparator(\"14341@1\", \"1984\"));\r\n    Assert.assertEquals(0, AvroFileHdfsReader.offsetComparator(\"1989\", \"1989\"));\r\n    Assert.assertEquals(0, AvroFileHdfsReader.offsetComparator(\"1989@0\", \"1989\"));\r\n    Assert.assertEquals(0, AvroFileHdfsReader.offsetComparator(\"1989\", \"1989@0\"));\r\n    Assert.assertEquals(0, AvroFileHdfsReader.offsetComparator(\"0\", \"0\"));\r\n    Assert.assertEquals(0, AvroFileHdfsReader.offsetComparator(\"1989@1\", \"1989@1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestAvroFileHdfsReader.java",
  "methodName" : "testOffsetComparator_InvalidInput",
  "sourceCode" : "@Test(expected = Exception.class)\r\npublic void testOffsetComparator_InvalidInput() {\r\n    AvroFileHdfsReader.offsetComparator(\"1982,13\", \"1930,1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testSequentialRead",
  "sourceCode" : "@Test\r\npublic void testSequentialRead() throws Exception {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    MultiFileHdfsReader multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"0:0\");\r\n    int index = 0;\r\n    while (multiReader.hasNext()) {\r\n        GenericRecord record = (GenericRecord) multiReader.readNext().getMessage();\r\n        Assert.assertEquals(index % NUM_EVENTS, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + (index % NUM_EVENTS), record.get(FIELD_2).toString());\r\n        index++;\r\n    }\r\n    Assert.assertEquals(3 * NUM_EVENTS, index);\r\n    multiReader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testReaderReopen",
  "sourceCode" : "@Test\r\npublic void testReaderReopen() throws Exception {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    // read until the middle of the first file\r\n    MultiFileHdfsReader multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"0:0\");\r\n    int index = 0;\r\n    String offset = \"0:0\";\r\n    for (; index < NUM_EVENTS / 2; index++) {\r\n        IncomingMessageEnvelope envelope = multiReader.readNext();\r\n        GenericRecord record = (GenericRecord) envelope.getMessage();\r\n        Assert.assertEquals(index % NUM_EVENTS, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + (index % NUM_EVENTS), record.get(FIELD_2).toString());\r\n        offset = envelope.getOffset();\r\n    }\r\n    multiReader.close();\r\n    // read until the middle of the second file\r\n    multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), offset);\r\n    // notAValidEvent one duplicate event\r\n    multiReader.readNext();\r\n    for (; index < NUM_EVENTS + NUM_EVENTS / 2; index++) {\r\n        IncomingMessageEnvelope envelope = multiReader.readNext();\r\n        GenericRecord record = (GenericRecord) envelope.getMessage();\r\n        Assert.assertEquals(index % NUM_EVENTS, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + (index % NUM_EVENTS), record.get(FIELD_2).toString());\r\n        offset = envelope.getOffset();\r\n    }\r\n    multiReader.close();\r\n    // read the rest of all files\r\n    multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), offset);\r\n    // notAValidEvent one duplicate event\r\n    multiReader.readNext();\r\n    while (multiReader.hasNext()) {\r\n        IncomingMessageEnvelope envelope = multiReader.readNext();\r\n        GenericRecord record = (GenericRecord) envelope.getMessage();\r\n        Assert.assertEquals(index % NUM_EVENTS, record.get(FIELD_1));\r\n        Assert.assertEquals(\"string_\" + (index % NUM_EVENTS), record.get(FIELD_2).toString());\r\n        index++;\r\n        offset = envelope.getOffset();\r\n    }\r\n    Assert.assertEquals(3 * NUM_EVENTS, index);\r\n    multiReader.close();\r\n    // reopen with the offset of the last record\r\n    multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), offset);\r\n    // notAValidEvent one duplicate event\r\n    multiReader.readNext();\r\n    Assert.assertFalse(multiReader.hasNext());\r\n    multiReader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testOutOfRangeSingleFileOffset",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testOutOfRangeSingleFileOffset() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"0:1000000&0\");\r\n    Assert.fail();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testOutOfRangeFileIndex",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testOutOfRangeFileIndex() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"3:0\");\r\n    Assert.fail();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testInvalidPartitionDescriptor",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testInvalidPartitionDescriptor() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, new ArrayList<>(), \"0:0\");\r\n    Assert.fail();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testReconnect",
  "sourceCode" : "@Test\r\npublic void testReconnect() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    MultiFileHdfsReader multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"0:0\");\r\n    // first read a few events, and then reconnect\r\n    for (int i = 0; i < NUM_EVENTS / 2; i++) {\r\n        multiReader.readNext();\r\n    }\r\n    IncomingMessageEnvelope envelope = multiReader.readNext();\r\n    multiReader.reconnect();\r\n    IncomingMessageEnvelope envelopeAfterReconnect = multiReader.readNext();\r\n    Assert.assertEquals(envelope, envelopeAfterReconnect);\r\n    multiReader.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\reader\\TestMultiFileHdfsReader.java",
  "methodName" : "testReachingMaxReconnect",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testReachingMaxReconnect() {\r\n    int numMaxRetries = 3;\r\n    SystemStreamPartition ssp = new SystemStreamPartition(\"hdfs\", \"testStream\", new Partition(0));\r\n    MultiFileHdfsReader multiReader = new MultiFileHdfsReader(HdfsReaderFactory.ReaderType.AVRO, ssp, Arrays.asList(descriptors), \"0:0\", numMaxRetries);\r\n    // first read a few events, and then reconnect\r\n    for (int i = 0; i < NUM_EVENTS / 2; i++) {\r\n        multiReader.readNext();\r\n    }\r\n    for (int i = 0; i < numMaxRetries; i++) {\r\n        IncomingMessageEnvelope envelope = multiReader.readNext();\r\n        multiReader.reconnect();\r\n        IncomingMessageEnvelope envelopeAfterReconnect = multiReader.readNext();\r\n        Assert.assertEquals(envelope, envelopeAfterReconnect);\r\n    }\r\n    multiReader.readNext();\r\n    multiReader.reconnect();\r\n    Assert.fail();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestHdfsSystemConsumer.java",
  "methodName" : "testEmptyStagingDirectory",
  "sourceCode" : "/*\r\n   * Ensure that empty staging directory will not break system admin,\r\n   * but should fail system consumer\r\n   */\r\n@Test\r\npublic void testEmptyStagingDirectory() throws Exception {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(String.format(HdfsConfig.CONSUMER_PARTITIONER_WHITELIST(), SYSTEM_NAME), \".*avro\");\r\n    Config config = new MapConfig(configMap);\r\n    HdfsSystemFactory systemFactory = new HdfsSystemFactory();\r\n    // create admin and do partitioning\r\n    HdfsSystemAdmin systemAdmin = systemFactory.getAdmin(SYSTEM_NAME, config);\r\n    String stream = WORKING_DIRECTORY;\r\n    Set<String> streamNames = new HashSet<>();\r\n    streamNames.add(stream);\r\n    generateAvroDataFiles();\r\n    Map<String, SystemStreamMetadata> streamMetadataMap = systemAdmin.getSystemStreamMetadata(streamNames);\r\n    SystemStreamMetadata systemStreamMetadata = streamMetadataMap.get(stream);\r\n    Assert.assertEquals(NUM_FILES, systemStreamMetadata.getSystemStreamPartitionMetadata().size());\r\n    // create consumer and read from files\r\n    HdfsSystemConsumer systemConsumer = systemFactory.getConsumer(SYSTEM_NAME, config, new NoOpMetricsRegistry());\r\n    Partition partition = new Partition(0);\r\n    SystemStreamPartition ssp = new SystemStreamPartition(SYSTEM_NAME, stream, partition);\r\n    try {\r\n        systemConsumer.register(ssp, \"0\");\r\n        Assert.fail(\"Empty staging directory should fail system consumer\");\r\n    } catch (UncheckedExecutionException e) {\r\n        Assert.assertTrue(e.getCause() instanceof SamzaException);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestPartitionDesctiptorUtil.java",
  "methodName" : "testBasicEncodeDecode",
  "sourceCode" : "@Test\r\npublic void testBasicEncodeDecode() {\r\n    Map<Partition, List<String>> input = new HashMap<>();\r\n    input.put(new Partition(0), Collections.singletonList(\"path_0\"));\r\n    String[] array = { \"path_1\", \"path_2\" };\r\n    input.put(new Partition(1), Arrays.asList(array));\r\n    input.put(new Partition(3), Collections.singletonList(\"path_3\"));\r\n    String json = PartitionDescriptorUtil.getJsonFromDescriptorMap(input);\r\n    Map<Partition, List<String>> output = PartitionDescriptorUtil.getDescriptorMapFromJson(json);\r\n    Assert.assertEquals(3, output.entrySet().size());\r\n    Assert.assertTrue(output.containsKey(new Partition(0)));\r\n    Assert.assertEquals(\"path_0\", output.get(new Partition(0)).get(0));\r\n    Assert.assertTrue(output.containsKey(new Partition(1)));\r\n    Assert.assertEquals(\"path_1\", output.get(new Partition(1)).get(0));\r\n    Assert.assertEquals(\"path_2\", output.get(new Partition(1)).get(1));\r\n    Assert.assertTrue(output.containsKey(new Partition(3)));\r\n    Assert.assertEquals(\"path_3\", output.get(new Partition(3)).get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestPartitionDesctiptorUtil.java",
  "methodName" : "testSingleEntry",
  "sourceCode" : "@Test\r\npublic void testSingleEntry() {\r\n    Map<Partition, List<String>> input = new HashMap<>();\r\n    input.put(new Partition(1), Collections.singletonList(\"random_path_1\"));\r\n    String json = PartitionDescriptorUtil.getJsonFromDescriptorMap(input);\r\n    Map<Partition, List<String>> output = PartitionDescriptorUtil.getDescriptorMapFromJson(json);\r\n    Assert.assertEquals(1, output.entrySet().size());\r\n    Assert.assertTrue(output.containsKey(new Partition(1)));\r\n    Assert.assertEquals(\"random_path_1\", output.get(new Partition(1)).get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestPartitionDesctiptorUtil.java",
  "methodName" : "testKeyOverriding",
  "sourceCode" : "@Test\r\npublic void testKeyOverriding() {\r\n    Map<Partition, List<String>> input = new HashMap<>();\r\n    input.put(new Partition(0), Collections.singletonList(\"path_0\"));\r\n    input.put(new Partition(0), Collections.singletonList(\"new_path_0\"));\r\n    String json = PartitionDescriptorUtil.getJsonFromDescriptorMap(input);\r\n    Map<Partition, List<String>> output = PartitionDescriptorUtil.getDescriptorMapFromJson(json);\r\n    Assert.assertEquals(1, output.entrySet().size());\r\n    Assert.assertTrue(output.containsKey(new Partition(0)));\r\n    Assert.assertEquals(\"new_path_0\", output.get(new Partition(0)).get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestPartitionDesctiptorUtil.java",
  "methodName" : "testEmptyInput",
  "sourceCode" : "@Test\r\npublic void testEmptyInput() {\r\n    Map<Partition, List<String>> input = new HashMap<>();\r\n    String json = PartitionDescriptorUtil.getJsonFromDescriptorMap(input);\r\n    Assert.assertNotNull(json);\r\n    Map<Partition, List<String>> output = PartitionDescriptorUtil.getDescriptorMapFromJson(json);\r\n    Assert.assertTrue(output.isEmpty());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-hdfs\\src\\test\\java\\org\\apache\\samza\\system\\hdfs\\TestPartitionDesctiptorUtil.java",
  "methodName" : "testInvalidInput",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testInvalidInput() {\r\n    String json = \"invalidStr\";\r\n    PartitionDescriptorUtil.getDescriptorMapFromJson(json);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\main\\java\\org\\apache\\samza\\system\\kafka\\KafkaSystemAdmin.java",
  "methodName" : "assembleMetadata",
  "sourceCode" : "/**\r\n * A helper method that takes oldest, newest, and upcoming offsets for each\r\n * system stream partition, and creates a single map from stream name to\r\n * SystemStreamMetadata.\r\n *\r\n * @param newestOffsets map of SSP to newest offset\r\n * @param oldestOffsets map of SSP to oldest offset\r\n * @param upcomingOffsets map of SSP to upcoming offset\r\n * @return a {@link Map} from {@code system} to {@link SystemStreamMetadata}\r\n */\r\n@VisibleForTesting\r\nstatic Map<String, SystemStreamMetadata> assembleMetadata(Map<SystemStreamPartition, String> oldestOffsets, Map<SystemStreamPartition, String> newestOffsets, Map<SystemStreamPartition, String> upcomingOffsets) {\r\n    HashSet<SystemStreamPartition> allSSPs = new HashSet<>();\r\n    allSSPs.addAll(oldestOffsets.keySet());\r\n    allSSPs.addAll(newestOffsets.keySet());\r\n    allSSPs.addAll(upcomingOffsets.keySet());\r\n    Map<String, SystemStreamMetadata> assembledMetadata = allSSPs.stream().collect(Collectors.groupingBy(SystemStreamPartition::getStream)).entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, entry -> {\r\n        Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> partitionMetadata = entry.getValue().stream().collect(Collectors.toMap(SystemStreamPartition::getPartition, ssp -> new SystemStreamMetadata.SystemStreamPartitionMetadata(oldestOffsets.getOrDefault(ssp, null), newestOffsets.getOrDefault(ssp, null), upcomingOffsets.get(ssp))));\r\n        return new SystemStreamMetadata(entry.getKey(), partitionMetadata);\r\n    }));\r\n    return assembledMetadata;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\main\\java\\org\\apache\\samza\\system\\kafka\\KafkaSystemAdmin.java",
  "methodName" : "getIntermediateStreamProperties",
  "sourceCode" : "/**\r\n * Fetch stream properties for all intermediate streams.\r\n *\r\n * @param config kafka system config\r\n * @return a {@link Map} from {@code streamId} to stream {@link Properties}\r\n */\r\n@VisibleForTesting\r\nstatic Map<String, Properties> getIntermediateStreamProperties(Config config) {\r\n    Map<String, Properties> intermedidateStreamProperties = Collections.emptyMap();\r\n    ApplicationConfig appConfig = new ApplicationConfig(config);\r\n    if (appConfig.getAppMode() == ApplicationConfig.ApplicationMode.BATCH) {\r\n        StreamConfig streamConfig = new StreamConfig(config);\r\n        intermedidateStreamProperties = streamConfig.getStreamIds().stream().filter(streamConfig::getIsIntermediateStream).collect(Collectors.toMap(Function.identity(), streamId -> {\r\n            Properties properties = new Properties();\r\n            properties.putAll(streamConfig.getStreamProperties(streamId));\r\n            properties.putIfAbsent(TopicConfig.RETENTION_MS_CONFIG, String.valueOf(KafkaConfig.DEFAULT_RETENTION_MS_FOR_BATCH()));\r\n            return properties;\r\n        }));\r\n    }\r\n    return intermedidateStreamProperties;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointLogKeySerde.java",
  "methodName" : "testBinaryCompatibility",
  "sourceCode" : "@Test\r\npublic void testBinaryCompatibility() {\r\n    KafkaCheckpointLogKey logKey1 = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE, new TaskName(\"Partition 0\"), GroupByPartitionFactory.class.getCanonicalName());\r\n    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();\r\n    byte[] bytes = (\"{\\\"systemstreampartition-grouper-factory\\\"\" + \":\\\"org.apache.samza.container.grouper.stream.GroupByPartitionFactory\\\",\\\"taskName\\\":\\\"Partition 0\\\",\" + \"\\\"type\\\":\\\"checkpoint\\\"}\").getBytes();\r\n    // test that the checkpoints returned by the Serde are byte-wise identical to an actual checkpoint in Kafka\r\n    Assert.assertEquals(true, Arrays.equals(bytes, checkpointSerde.toBytes(logKey1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointLogKeySerde.java",
  "methodName" : "testSerde",
  "sourceCode" : "@Test\r\npublic void testSerde() {\r\n    KafkaCheckpointLogKey key = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE, new TaskName(\"Partition 0\"), GroupByPartitionFactory.class.getCanonicalName());\r\n    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();\r\n    // test that deserialize(serialize(k)) == k\r\n    Assert.assertEquals(key, checkpointSerde.fromBytes(checkpointSerde.toBytes(key)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointLogKeySerde.java",
  "methodName" : "testCheckpointTypeV2",
  "sourceCode" : "@Test\r\npublic void testCheckpointTypeV2() {\r\n    KafkaCheckpointLogKey keyV2 = new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V2_KEY_TYPE, new TaskName(\"Partition 0\"), GroupByPartitionFactory.class.getCanonicalName());\r\n    KafkaCheckpointLogKeySerde checkpointKeySerde = new KafkaCheckpointLogKeySerde();\r\n    // test that deserialize(serialize(k)) == k\r\n    Assert.assertEquals(keyV2, checkpointKeySerde.fromBytes(checkpointKeySerde.toBytes(keyV2)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointLogKeySerde.java",
  "methodName" : "testForwardsCompatibility",
  "sourceCode" : "@Test\r\npublic void testForwardsCompatibility() {\r\n    // Set the key to another value, this is for the future if we want to support multiple checkpoint keys\r\n    // we do not want to throw in the Serdes layer, but must be validated in the CheckpointManager\r\n    KafkaCheckpointLogKey key = new KafkaCheckpointLogKey(\"checkpoint-v2\", new TaskName(\"Partition 0\"), GroupByPartitionFactory.class.getCanonicalName());\r\n    KafkaCheckpointLogKeySerde checkpointSerde = new KafkaCheckpointLogKeySerde();\r\n    // test that deserialize(serialize(k)) == k\r\n    Assert.assertEquals(key, checkpointSerde.fromBytes(checkpointSerde.toBytes(key)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testCreateResourcesTopicCreationError",
  "sourceCode" : "@Test(expected = TopicAlreadyMarkedForDeletionException.class)\r\npublic void testCreateResourcesTopicCreationError() {\r\n    setupSystemFactory(config());\r\n    // throw an exception during createStream\r\n    doThrow(new TopicAlreadyMarkedForDeletionException(\"invalid stream\")).when(this.createResourcesSystemAdmin).createStream(CHECKPOINT_SPEC);\r\n    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());\r\n    // expect an exception during startup\r\n    checkpointManager.createResources();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testCreateResourcesTopicValidationError",
  "sourceCode" : "@Test(expected = StreamValidationException.class)\r\npublic void testCreateResourcesTopicValidationError() {\r\n    setupSystemFactory(config());\r\n    // throw an exception during validateStream\r\n    doThrow(new StreamValidationException(\"invalid stream\")).when(this.createResourcesSystemAdmin).validateStream(CHECKPOINT_SPEC);\r\n    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());\r\n    // expect an exception during startup\r\n    checkpointManager.createResources();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadFailsOnSerdeExceptions",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testReadFailsOnSerdeExceptions() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, \"0\"), \"0\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    // wire up an exception throwing serde with the checkpointManager\r\n    CheckpointV1Serde checkpointV1Serde = mock(CheckpointV1Serde.class);\r\n    doThrow(new RuntimeException(\"serde failed\")).when(checkpointV1Serde).fromBytes(any());\r\n    KafkaCheckpointManager checkpointManager = new KafkaCheckpointManager(CHECKPOINT_SPEC, this.systemFactory, true, config(), this.metricsRegistry, checkpointV1Serde, CHECKPOINT_V2_SERDE, KAFKA_CHECKPOINT_LOG_KEY_SERDE);\r\n    checkpointManager.register(TASK0);\r\n    // expect an exception\r\n    checkpointManager.readLastCheckpoint(TASK0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadSucceedsOnKeySerdeExceptionsWhenValidationIsDisabled",
  "sourceCode" : "@Test\r\npublic void testReadSucceedsOnKeySerdeExceptionsWhenValidationIsDisabled() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, \"0\"), \"0\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    // wire up an exception throwing serde with the checkpointManager\r\n    CheckpointV1Serde checkpointV1Serde = mock(CheckpointV1Serde.class);\r\n    doThrow(new RuntimeException(\"serde failed\")).when(checkpointV1Serde).fromBytes(any());\r\n    KafkaCheckpointManager checkpointManager = new KafkaCheckpointManager(CHECKPOINT_SPEC, this.systemFactory, false, config(), this.metricsRegistry, checkpointV1Serde, CHECKPOINT_V2_SERDE, KAFKA_CHECKPOINT_LOG_KEY_SERDE);\r\n    checkpointManager.register(TASK0);\r\n    // expect the read to succeed in spite of the exception from ExceptionThrowingSerde\r\n    assertNull(checkpointManager.readLastCheckpoint(TASK0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testStart",
  "sourceCode" : "@Test\r\npublic void testStart() {\r\n    setupSystemFactory(config());\r\n    String oldestOffset = \"1\";\r\n    String newestOffset = \"2\";\r\n    SystemStreamMetadata checkpointTopicMetadata = new SystemStreamMetadata(CHECKPOINT_TOPIC, ImmutableMap.of(new Partition(0), new SystemStreamPartitionMetadata(oldestOffset, newestOffset, Integer.toString(Integer.parseInt(newestOffset) + 1))));\r\n    when(this.systemAdmin.getSystemStreamMetadata(Collections.singleton(CHECKPOINT_TOPIC))).thenReturn(ImmutableMap.of(CHECKPOINT_TOPIC, checkpointTopicMetadata));\r\n    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());\r\n    checkpointManager.start();\r\n    verify(this.systemProducer).start();\r\n    verify(this.systemAdmin).start();\r\n    verify(this.systemConsumer).register(CHECKPOINT_SSP, oldestOffset);\r\n    verify(this.systemConsumer).start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testRegister",
  "sourceCode" : "@Test\r\npublic void testRegister() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    verify(this.systemProducer).register(TASK0.getTaskName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testStop",
  "sourceCode" : "@Test\r\npublic void testStop() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());\r\n    checkpointManager.stop();\r\n    verify(this.systemProducer).stop();\r\n    // default configuration for stopConsumerAfterFirstRead means that consumer is not stopped here\r\n    verify(this.systemConsumer, never()).stop();\r\n    verify(this.systemAdmin).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testWriteCheckpointShouldRecreateSystemProducerOnFailure",
  "sourceCode" : "@Test\r\npublic void testWriteCheckpointShouldRecreateSystemProducerOnFailure() {\r\n    setupSystemFactory(config());\r\n    SystemProducer secondKafkaProducer = mock(SystemProducer.class);\r\n    // override default mock behavior to return a second producer on the second call to create a producer\r\n    when(this.systemFactory.getProducer(CHECKPOINT_SYSTEM, config(), this.metricsRegistry, KafkaCheckpointManager.class.getSimpleName())).thenReturn(this.systemProducer, secondKafkaProducer);\r\n    // first producer throws an exception on flush\r\n    doThrow(new RuntimeException(\"flush failed\")).when(this.systemProducer).flush(TASK0.getTaskName());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    kafkaCheckpointManager.writeCheckpoint(TASK0, checkpointV1);\r\n    // first producer should be stopped\r\n    verify(this.systemProducer).stop();\r\n    // register and start the second producer\r\n    verify(secondKafkaProducer).register(TASK0.getTaskName());\r\n    verify(secondKafkaProducer).start();\r\n    // check that the second producer was given the message to send out\r\n    ArgumentCaptor<OutgoingMessageEnvelope> outgoingMessageEnvelopeArgumentCaptor = ArgumentCaptor.forClass(OutgoingMessageEnvelope.class);\r\n    verify(secondKafkaProducer).send(eq(TASK0.getTaskName()), outgoingMessageEnvelopeArgumentCaptor.capture());\r\n    assertEquals(CHECKPOINT_SSP, outgoingMessageEnvelopeArgumentCaptor.getValue().getSystemStream());\r\n    assertEquals(new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE, TASK0, GROUPER_FACTORY_CLASS), KAFKA_CHECKPOINT_LOG_KEY_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getKey()));\r\n    assertEquals(checkpointV1, CHECKPOINT_V1_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getMessage()));\r\n    verify(secondKafkaProducer).flush(TASK0.getTaskName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testCreateResources",
  "sourceCode" : "@Test\r\npublic void testCreateResources() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.createResources();\r\n    verify(this.createResourcesSystemAdmin).start();\r\n    verify(this.createResourcesSystemAdmin).createStream(CHECKPOINT_SPEC);\r\n    verify(this.createResourcesSystemAdmin).validateStream(CHECKPOINT_SPEC);\r\n    verify(this.createResourcesSystemAdmin).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testCreateResourcesSkipValidation",
  "sourceCode" : "@Test\r\npublic void testCreateResourcesSkipValidation() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(false, config());\r\n    kafkaCheckpointManager.createResources();\r\n    verify(this.createResourcesSystemAdmin).start();\r\n    verify(this.createResourcesSystemAdmin).createStream(CHECKPOINT_SPEC);\r\n    verify(this.createResourcesSystemAdmin, never()).validateStream(CHECKPOINT_SPEC);\r\n    verify(this.createResourcesSystemAdmin).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadEmpty",
  "sourceCode" : "@Test\r\npublic void testReadEmpty() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    setupConsumer(ImmutableList.of());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    assertNull(kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadCheckpointV1",
  "sourceCode" : "@Test\r\npublic void testReadCheckpointV1() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV1Envelope(TASK0, checkpointV1, \"0\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);\r\n    assertEquals(checkpointV1, actualCheckpoint);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadIgnoreCheckpointV2WhenV1Enabled",
  "sourceCode" : "@Test\r\npublic void testReadIgnoreCheckpointV2WhenV1Enabled() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV1Envelope(TASK0, checkpointV1, \"0\"), newCheckpointV2Envelope(TASK0, buildCheckpointV2(INPUT_SSP0, \"1\"), \"1\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    // default is to only read CheckpointV1\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);\r\n    assertEquals(checkpointV1, actualCheckpoint);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadCheckpointV2",
  "sourceCode" : "@Test\r\npublic void testReadCheckpointV2() throws InterruptedException {\r\n    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, \"1,2\"));\r\n    setupSystemFactory(config);\r\n    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, \"0\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV2Envelope(TASK0, checkpointV2, \"0\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);\r\n    kafkaCheckpointManager.register(TASK0);\r\n    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);\r\n    assertEquals(checkpointV2, actualCheckpoint);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadCheckpointPriority",
  "sourceCode" : "@Test\r\npublic void testReadCheckpointPriority() throws InterruptedException {\r\n    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, \"2,1\"));\r\n    setupSystemFactory(config);\r\n    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, \"1\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes = ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, \"0\"), \"0\"), newCheckpointV2Envelope(TASK0, checkpointV2, \"1\"));\r\n    setupConsumer(checkpointEnvelopes);\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);\r\n    kafkaCheckpointManager.register(TASK0);\r\n    Checkpoint actualCheckpoint = kafkaCheckpointManager.readLastCheckpoint(TASK0);\r\n    assertEquals(checkpointV2, actualCheckpoint);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadMultipleCheckpointsMultipleSSP",
  "sourceCode" : "@Test\r\npublic void testReadMultipleCheckpointsMultipleSSP() throws InterruptedException {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager checkpointManager = buildKafkaCheckpointManager(true, config());\r\n    checkpointManager.register(TASK0);\r\n    checkpointManager.register(TASK1);\r\n    // mock out a consumer that returns 5 checkpoint IMEs for each SSP\r\n    int newestOffset = 5;\r\n    int checkpointOffsetCounter = 0;\r\n    List<List<IncomingMessageEnvelope>> pollOutputs = new ArrayList<>();\r\n    for (int offset = 1; offset <= newestOffset; offset++) {\r\n        pollOutputs.add(ImmutableList.of(// use regular offset value for INPUT_SSP0\r\n        newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, Integer.toString(offset)), Integer.toString(checkpointOffsetCounter++)), // use (offset * 2) value for INPUT_SSP1 so offsets are different from INPUT_SSP0\r\n        newCheckpointV1Envelope(TASK1, buildCheckpointV1(INPUT_SSP1, Integer.toString(offset * 2)), Integer.toString(checkpointOffsetCounter++))));\r\n    }\r\n    setupConsumerMultiplePoll(pollOutputs);\r\n    assertEquals(buildCheckpointV1(INPUT_SSP0, Integer.toString(newestOffset)), checkpointManager.readLastCheckpoint(TASK0));\r\n    assertEquals(buildCheckpointV1(INPUT_SSP1, Integer.toString(newestOffset * 2)), checkpointManager.readLastCheckpoint(TASK1));\r\n    // check expected number of polls (+1 is for the final empty poll), and the checkpoint is the newest message\r\n    verify(this.systemConsumer, times(newestOffset + 1)).poll(ImmutableSet.of(CHECKPOINT_SSP), SystemConsumer.BLOCK_ON_OUTSTANDING_MESSAGES);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testReadMultipleCheckpointsUpgradeCheckpointVersion",
  "sourceCode" : "@Test\r\npublic void testReadMultipleCheckpointsUpgradeCheckpointVersion() throws InterruptedException {\r\n    Config config = config(ImmutableMap.of(TaskConfig.CHECKPOINT_READ_VERSIONS, \"2,1\"));\r\n    setupSystemFactory(config);\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);\r\n    kafkaCheckpointManager.register(TASK0);\r\n    kafkaCheckpointManager.register(TASK1);\r\n    List<IncomingMessageEnvelope> checkpointEnvelopesV1 = ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, \"0\"), \"0\"), newCheckpointV1Envelope(TASK1, buildCheckpointV1(INPUT_SSP1, \"0\"), \"1\"));\r\n    CheckpointV2 ssp0CheckpointV2 = buildCheckpointV2(INPUT_SSP0, \"10\");\r\n    CheckpointV2 ssp1CheckpointV2 = buildCheckpointV2(INPUT_SSP1, \"11\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopesV2 = ImmutableList.of(newCheckpointV2Envelope(TASK0, ssp0CheckpointV2, \"2\"), newCheckpointV2Envelope(TASK1, ssp1CheckpointV2, \"3\"));\r\n    setupConsumerMultiplePoll(ImmutableList.of(checkpointEnvelopesV1, checkpointEnvelopesV2));\r\n    assertEquals(ssp0CheckpointV2, kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n    assertEquals(ssp1CheckpointV2, kafkaCheckpointManager.readLastCheckpoint(TASK1));\r\n    // 2 polls for actual checkpoints, 1 final empty poll\r\n    verify(this.systemConsumer, times(3)).poll(ImmutableSet.of(CHECKPOINT_SSP), SystemConsumer.BLOCK_ON_OUTSTANDING_MESSAGES);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testWriteCheckpointV1",
  "sourceCode" : "@Test\r\npublic void testWriteCheckpointV1() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    kafkaCheckpointManager.writeCheckpoint(TASK0, checkpointV1);\r\n    ArgumentCaptor<OutgoingMessageEnvelope> outgoingMessageEnvelopeArgumentCaptor = ArgumentCaptor.forClass(OutgoingMessageEnvelope.class);\r\n    verify(this.systemProducer).send(eq(TASK0.getTaskName()), outgoingMessageEnvelopeArgumentCaptor.capture());\r\n    assertEquals(CHECKPOINT_SSP, outgoingMessageEnvelopeArgumentCaptor.getValue().getSystemStream());\r\n    assertEquals(new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V1_KEY_TYPE, TASK0, GROUPER_FACTORY_CLASS), KAFKA_CHECKPOINT_LOG_KEY_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getKey()));\r\n    assertEquals(checkpointV1, CHECKPOINT_V1_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getMessage()));\r\n    verify(this.systemProducer).flush(TASK0.getTaskName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testWriteCheckpointV2",
  "sourceCode" : "@Test\r\npublic void testWriteCheckpointV2() {\r\n    setupSystemFactory(config());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, \"0\");\r\n    kafkaCheckpointManager.writeCheckpoint(TASK0, checkpointV2);\r\n    ArgumentCaptor<OutgoingMessageEnvelope> outgoingMessageEnvelopeArgumentCaptor = ArgumentCaptor.forClass(OutgoingMessageEnvelope.class);\r\n    verify(this.systemProducer).send(eq(TASK0.getTaskName()), outgoingMessageEnvelopeArgumentCaptor.capture());\r\n    assertEquals(CHECKPOINT_SSP, outgoingMessageEnvelopeArgumentCaptor.getValue().getSystemStream());\r\n    assertEquals(new KafkaCheckpointLogKey(KafkaCheckpointLogKey.CHECKPOINT_V2_KEY_TYPE, TASK0, GROUPER_FACTORY_CLASS), KAFKA_CHECKPOINT_LOG_KEY_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getKey()));\r\n    assertEquals(checkpointV2, CHECKPOINT_V2_SERDE.fromBytes((byte[]) outgoingMessageEnvelopeArgumentCaptor.getValue().getMessage()));\r\n    verify(this.systemProducer).flush(TASK0.getTaskName());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testWriteCheckpointShouldRetryFiniteTimesOnFailure",
  "sourceCode" : "@Test\r\npublic void testWriteCheckpointShouldRetryFiniteTimesOnFailure() {\r\n    setupSystemFactory(config());\r\n    doThrow(new RuntimeException(\"send failed\")).when(this.systemProducer).send(any(), any());\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    // setter for scala var MaxRetryDurationInMillis\r\n    kafkaCheckpointManager.MaxRetryDurationInMillis_$eq(100);\r\n    CheckpointV2 checkpointV2 = buildCheckpointV2(INPUT_SSP0, \"0\");\r\n    try {\r\n        kafkaCheckpointManager.writeCheckpoint(TASK0, checkpointV2);\r\n        fail(\"Expected to throw SamzaException\");\r\n    } catch (SamzaException e) {\r\n        // expected to get here\r\n    }\r\n    // one call to send which fails, then writeCheckpoint gives up\r\n    verify(this.systemProducer).send(any(), any());\r\n    verify(this.systemProducer, never()).flush(any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testConsumerStopsAfterInitialRead",
  "sourceCode" : "@Test\r\npublic void testConsumerStopsAfterInitialRead() throws Exception {\r\n    setupSystemFactory(config());\r\n    CheckpointV1 checkpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    setupConsumer(ImmutableList.of(newCheckpointV1Envelope(TASK0, checkpointV1, \"0\")));\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config());\r\n    kafkaCheckpointManager.register(TASK0);\r\n    assertEquals(checkpointV1, kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n    // 1 call to get actual checkpoints, 1 call for empty poll to signal done reading\r\n    verify(this.systemConsumer, times(2)).poll(ImmutableSet.of(CHECKPOINT_SSP), SystemConsumer.BLOCK_ON_OUTSTANDING_MESSAGES);\r\n    verify(this.systemConsumer).stop();\r\n    // reading checkpoint again should not read more messages from the consumer\r\n    assertEquals(checkpointV1, kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n    verifyNoMoreInteractions(this.systemConsumer);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\checkpoint\\kafka\\TestKafkaCheckpointManager.java",
  "methodName" : "testConsumerStopsAfterInitialReadDisabled",
  "sourceCode" : "@Test\r\npublic void testConsumerStopsAfterInitialReadDisabled() throws Exception {\r\n    Config config = config(ImmutableMap.of(TaskConfig.INTERNAL_CHECKPOINT_MANAGER_CONSUMER_STOP_AFTER_FIRST_READ, \"false\"));\r\n    setupSystemFactory(config);\r\n    // 1) return checkpointV1 for INPUT_SSP\r\n    CheckpointV1 ssp0FirstCheckpointV1 = buildCheckpointV1(INPUT_SSP0, \"0\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes0 = ImmutableList.of(newCheckpointV1Envelope(TASK0, buildCheckpointV1(INPUT_SSP0, \"0\"), \"0\"));\r\n    setupConsumer(checkpointEnvelopes0);\r\n    KafkaCheckpointManager kafkaCheckpointManager = buildKafkaCheckpointManager(true, config);\r\n    kafkaCheckpointManager.register(TASK0);\r\n    assertEquals(ssp0FirstCheckpointV1, kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n    // 2) return new checkpointV1 for just INPUT_SSP\r\n    CheckpointV1 ssp0SecondCheckpointV1 = buildCheckpointV1(INPUT_SSP0, \"10\");\r\n    List<IncomingMessageEnvelope> checkpointEnvelopes1 = ImmutableList.of(newCheckpointV1Envelope(TASK0, ssp0SecondCheckpointV1, \"1\"));\r\n    setupConsumer(checkpointEnvelopes1);\r\n    assertEquals(ssp0SecondCheckpointV1, kafkaCheckpointManager.readLastCheckpoint(TASK0));\r\n    verify(this.systemConsumer, never()).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testDefaults",
  "sourceCode" : "@Test\r\npublic void testDefaults() {\r\n    Map<String, String> props = new HashMap<>();\r\n    // should be ignored\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, // should be ignored\r\n    \"Ignore\");\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, // should NOT be ignored\r\n    \"100\");\r\n    props.put(JobConfig.JOB_NAME, JOB_NAME);\r\n    // if KAFKA_CONSUMER_PROPERTY_PREFIX is set, then PRODUCER should be ignored\r\n    props.put(KAFKA_PRODUCER_PROPERTY_PREFIX + \"bootstrap.servers\", \"ignroeThis:9092\");\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + \"bootstrap.servers\", \"useThis:9092\");\r\n    Config config = new MapConfig(props);\r\n    String clientId = KafkaConsumerConfig.createClientId(CLIENT_ID_PREFIX, config);\r\n    KafkaConsumerConfig kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(config, SYSTEM_NAME, clientId);\r\n    Assert.assertEquals(\"false\", kafkaConsumerConfig.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\r\n    Assert.assertEquals(KafkaConsumerConfig.DEFAULT_KAFKA_CONSUMER_MAX_POLL_RECORDS, kafkaConsumerConfig.get(ConsumerConfig.MAX_POLL_RECORDS_CONFIG));\r\n    Assert.assertEquals(RangeAssignor.class.getName(), kafkaConsumerConfig.get(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG));\r\n    Assert.assertEquals(\"useThis:9092\", kafkaConsumerConfig.get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));\r\n    Assert.assertEquals(\"100\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG));\r\n    Assert.assertEquals(ByteArrayDeserializer.class.getName(), kafkaConsumerConfig.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG));\r\n    Assert.assertEquals(ByteArrayDeserializer.class.getName(), kafkaConsumerConfig.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG));\r\n    // validate group and client id generation\r\n    Assert.assertEquals(CLIENT_ID_PREFIX.replace(\"-\", \"_\") + \"-\" + JOB_NAME + \"-\" + \"1\", kafkaConsumerConfig.get(ConsumerConfig.CLIENT_ID_CONFIG));\r\n    Assert.assertEquals(CLIENT_ID_PREFIX.replace(\"-\", \"_\") + \"-jobName-1\", KafkaConsumerConfig.createClientId(CLIENT_ID_PREFIX, config));\r\n    Assert.assertEquals(\"jobName-1\", KafkaConsumerConfig.createConsumerGroupId(config));\r\n    // validate setting of group and client id\r\n    Assert.assertEquals(KafkaConsumerConfig.createConsumerGroupId(config), kafkaConsumerConfig.get(ConsumerConfig.GROUP_ID_CONFIG));\r\n    Assert.assertEquals(KafkaConsumerConfig.createConsumerGroupId(config), kafkaConsumerConfig.get(ConsumerConfig.GROUP_ID_CONFIG));\r\n    Assert.assertEquals(KafkaConsumerConfig.createClientId(CLIENT_ID_PREFIX, config), kafkaConsumerConfig.get(ConsumerConfig.CLIENT_ID_CONFIG));\r\n    // with non-default job id\r\n    props.put(JobConfig.JOB_ID, JOB_ID);\r\n    config = new MapConfig(props);\r\n    Assert.assertEquals(CLIENT_ID_PREFIX.replace(\"-\", \"_\") + \"-jobName-jobId\", kafkaConsumerConfig.createClientId(CLIENT_ID_PREFIX, config));\r\n    Assert.assertEquals(\"jobName-jobId\", KafkaConsumerConfig.createConsumerGroupId(config));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testNotOverride",
  "sourceCode" : "// test stuff that should not be overridden\r\n@Test\r\npublic void testNotOverride() {\r\n    Map<String, String> props = new HashMap<>();\r\n    // if KAFKA_CONSUMER_PROPERTY_PREFIX is not set, then PRODUCER should be used\r\n    props.put(KAFKA_PRODUCER_PROPERTY_PREFIX + ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"useThis:9092\");\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, TestKafkaConsumerConfig.class.getName());\r\n    props.put(KAFKA_CONSUMER_PROPERTY_PREFIX + ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, TestKafkaConsumerConfig.class.getName());\r\n    props.put(JobConfig.JOB_NAME, \"jobName\");\r\n    Config config = new MapConfig(props);\r\n    String clientId = KafkaConsumerConfig.createClientId(CLIENT_ID_PREFIX, config);\r\n    KafkaConsumerConfig kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(config, SYSTEM_NAME, clientId);\r\n    Assert.assertEquals(\"useThis:9092\", kafkaConsumerConfig.get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));\r\n    Assert.assertEquals(TestKafkaConsumerConfig.class.getName(), kafkaConsumerConfig.get(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG));\r\n    Assert.assertEquals(TestKafkaConsumerConfig.class.getName(), kafkaConsumerConfig.get(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testGetConsumerClientId",
  "sourceCode" : "@Test\r\npublic void testGetConsumerClientId() {\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(JobConfig.JOB_NAME, \"jobName\");\r\n    map.put(JobConfig.JOB_ID, \"jobId\");\r\n    String result = KafkaConsumerConfig.createClientId(\"consumer\", new MapConfig(map));\r\n    Assert.assertEquals(\"consumer-jobName-jobId\", result);\r\n    result = KafkaConsumerConfig.createClientId(\"consumer-\", new MapConfig(map));\r\n    Assert.assertEquals(\"consumer_-jobName-jobId\", result);\r\n    result = KafkaConsumerConfig.createClientId(\"super-duper-consumer\", new MapConfig(map));\r\n    Assert.assertEquals(\"super_duper_consumer-jobName-jobId\", result);\r\n    map.put(JobConfig.JOB_NAME, \" very important!job\");\r\n    result = KafkaConsumerConfig.createClientId(\"consumer\", new MapConfig(map));\r\n    Assert.assertEquals(\"consumer-_very_important_job-jobId\", result);\r\n    map.put(JobConfig.JOB_ID, \"number-#3\");\r\n    result = KafkaConsumerConfig.createClientId(\"consumer\", new MapConfig(map));\r\n    Assert.assertEquals(\"consumer-_very_important_job-number__3\", result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testNoBootstrapServers",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testNoBootstrapServers() {\r\n    Config config = new MapConfig(Collections.emptyMap());\r\n    String clientId = KafkaConsumerConfig.createClientId(\"clientId\", config);\r\n    KafkaConsumerConfig.getKafkaSystemConsumerConfig(config, SYSTEM_NAME, clientId);\r\n    Assert.fail(\"didn't get exception for the missing config:\" + ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testResetValues",
  "sourceCode" : "@Test\r\npublic void testResetValues() {\r\n    Map<String, String> props = new HashMap<>();\r\n    props.put(KAFKA_PRODUCER_PROPERTY_PREFIX + ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"locahost:9092\");\r\n    props.put(JobConfig.JOB_NAME, JOB_NAME);\r\n    // largest -> latest\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"largest\");\r\n    Config config = new MapConfig(props);\r\n    KafkaConsumerConfig kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(config, SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"latest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // smallest -> earliest\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"smallest\");\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"earliest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // earliest -> earliest\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"earliest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // none -> none\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"none\");\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"none\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // someval -> latest\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"someval\");\r\n    try {\r\n        kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n        Assert.fail(\"Should've failed for invalid value for default offset reset\");\r\n    } catch (Exception e) {\r\n        // expected\r\n    }\r\n    // no value -> latest\r\n    props.remove(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"latest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // if samza system has a reset value - use it (override kafka\r\n    // upcoming -> latest\r\n    props.put(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), \"earliest\");\r\n    props.put(String.format(\"systems.%s.samza.offset.default\", SYSTEM_NAME), \"upcoming\");\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"earliest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    // stream default should override it\r\n    props.remove(String.format(\"systems.%s.consumer.%s\", SYSTEM_NAME, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n    props.put(String.format(\"systems.%s.default.stream.samza.offset.default\", SYSTEM_NAME), \"oldest\");\r\n    kafkaConsumerConfig = KafkaConsumerConfig.getKafkaSystemConsumerConfig(new MapConfig(props), SYSTEM_NAME, \"client1\");\r\n    Assert.assertEquals(\"earliest\", kafkaConsumerConfig.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\config\\TestKafkaConsumerConfig.java",
  "methodName" : "testKafkaAutoResetValue",
  "sourceCode" : "@Test\r\npublic void testKafkaAutoResetValue() {\r\n    Assert.assertEquals(\"latest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"latest\", \"oldest\"));\r\n    try {\r\n        KafkaConsumerConfig.getAutoOffsetResetValue(\"someValue\", \"oldest\");\r\n        Assert.fail(\"Invalid value should've triggered an exception\");\r\n    } catch (Exception e) {\r\n        // expected\r\n    }\r\n    Assert.assertEquals(\"earliest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"earliest\", \"upcoming\"));\r\n    Assert.assertEquals(\"none\", KafkaConsumerConfig.getAutoOffsetResetValue(\"none\", \"oldest\"));\r\n    Assert.assertEquals(\"latest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"largest\", \"oldest\"));\r\n    Assert.assertEquals(\"earliest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"smallest\", \"upcoming\"));\r\n    Assert.assertEquals(\"earliest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"\", \"oldest\"));\r\n    Assert.assertEquals(\"latest\", KafkaConsumerConfig.getAutoOffsetResetValue(\"\", \"upcoming\"));\r\n    try {\r\n        KafkaConsumerConfig.getAutoOffsetResetValue(\"\", \"whatever\");\r\n        Assert.fail(\"Invalid value should've triggered an exception\");\r\n    } catch (Exception e) {\r\n        //expected\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testFlushOrder",
  "sourceCode" : "@Test\r\npublic void testFlushOrder() {\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    StorageEngine mockStore = mock(StorageEngine.class);\r\n    java.util.Map<String, StorageEngine> taskStores = ImmutableMap.of(\"mockStore\", mockStore);\r\n    when(csm.getAllStores(any())).thenReturn(taskStores);\r\n    when(mockStore.getStoreProperties()).thenReturn(new StoreProperties.StorePropertiesBuilder().setPersistedToDisk(true).setLoggedStore(true).build());\r\n    TaskInstanceMetrics metrics = mock(TaskInstanceMetrics.class);\r\n    Timer checkpointTimer = mock(Timer.class);\r\n    when(metrics.storeCheckpointNs()).thenReturn(checkpointTimer);\r\n    KafkaTransactionalStateTaskBackupManager tsm = spy(buildTSM(csm, mock(Partition.class), new StorageManagerUtil()));\r\n    TaskStorageCommitManager commitManager = new TaskStorageCommitManager(new TaskName(\"task\"), ImmutableMap.of(\"kafka\", tsm), csm, null, null, null, null, ForkJoinPool.commonPool(), new StorageManagerUtil(), null, metrics);\r\n    // stub actual method call\r\n    doReturn(mock(java.util.Map.class)).when(tsm).getNewestChangelogSSPOffsets(any(), any(), any(), any());\r\n    // invoke Kafka flush\r\n    commitManager.init(null);\r\n    commitManager.snapshot(CheckpointId.create());\r\n    // ensure that stores are flushed before we get newest changelog offsets\r\n    InOrder inOrder = inOrder(mockStore, tsm);\r\n    inOrder.verify(mockStore).flush();\r\n    inOrder.verify(tsm).getNewestChangelogSSPOffsets(any(), any(), any(), any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testGetNewestOffsetsReturnsCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testGetNewestOffsetsReturnsCorrectOffset() {\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    KafkaTransactionalStateTaskBackupManager tsm = buildTSM(csm, mock(Partition.class), new StorageManagerUtil());\r\n    TaskName taskName = mock(TaskName.class);\r\n    String changelogSystemName = \"systemName\";\r\n    String storeName = \"storeName\";\r\n    String changelogStreamName = \"changelogName\";\r\n    String newestChangelogSSPOffset = \"1\";\r\n    SystemStream changelogSystemStream = new SystemStream(changelogSystemName, changelogStreamName);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogs = new HashMap<>();\r\n    storeChangelogs.put(storeName, changelogSystemStream);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin systemAdmin = mock(SystemAdmin.class);\r\n    SystemStreamPartitionMetadata metadata = mock(SystemStreamPartitionMetadata.class);\r\n    when(metadata.getNewestOffset()).thenReturn(newestChangelogSSPOffset);\r\n    when(systemAdmins.getSystemAdmin(changelogSystemName)).thenReturn(systemAdmin);\r\n    when(systemAdmin.getSSPMetadata(eq(ImmutableSet.of(changelogSSP)))).thenReturn(ImmutableMap.of(changelogSSP, metadata));\r\n    // invoke the method\r\n    java.util.Map<String, String> stateCheckpointMarkerMap = tsm.getNewestChangelogSSPOffsets(taskName, storeChangelogs, changelogPartition, systemAdmins);\r\n    // verify results\r\n    assertEquals(1, stateCheckpointMarkerMap.size());\r\n    KafkaStateCheckpointMarker kscm = KafkaStateCheckpointMarker.deserialize(stateCheckpointMarkerMap.get(storeName));\r\n    assertEquals(newestChangelogSSPOffset, kscm.getChangelogOffset());\r\n    assertEquals(changelogSSP, kscm.getChangelogSSP());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testGetNewestOffsetsReturnsNoneForEmptyTopic",
  "sourceCode" : "@Test\r\npublic void testGetNewestOffsetsReturnsNoneForEmptyTopic() {\r\n    // empty topic == null newest offset\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    KafkaTransactionalStateTaskBackupManager tsm = buildTSM(csm, mock(Partition.class), new StorageManagerUtil());\r\n    TaskName taskName = mock(TaskName.class);\r\n    String changelogSystemName = \"systemName\";\r\n    String storeName = \"storeName\";\r\n    String changelogStreamName = \"changelogName\";\r\n    String newestChangelogSSPOffset = null;\r\n    SystemStream changelogSystemStream = new SystemStream(changelogSystemName, changelogStreamName);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogs = new HashMap<String, SystemStream>();\r\n    storeChangelogs.put(storeName, changelogSystemStream);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin systemAdmin = mock(SystemAdmin.class);\r\n    SystemStreamPartitionMetadata metadata = mock(SystemStreamPartitionMetadata.class);\r\n    when(metadata.getNewestOffset()).thenReturn(newestChangelogSSPOffset);\r\n    when(systemAdmins.getSystemAdmin(changelogSystemName)).thenReturn(systemAdmin);\r\n    when(systemAdmin.getSSPMetadata(eq(ImmutableSet.of(changelogSSP)))).thenReturn(ImmutableMap.of(changelogSSP, metadata));\r\n    // invoke the method\r\n    java.util.Map<String, String> stateCheckpointMarkerMap = tsm.getNewestChangelogSSPOffsets(taskName, storeChangelogs, changelogPartition, systemAdmins);\r\n    // verify results\r\n    assertEquals(1, stateCheckpointMarkerMap.size());\r\n    KafkaStateCheckpointMarker kscm = KafkaStateCheckpointMarker.deserialize(stateCheckpointMarkerMap.get(storeName));\r\n    assertEquals(changelogSSP, kscm.getChangelogSSP());\r\n    assertNull(kscm.getChangelogOffset());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testGetNewestOffsetsThrowsIfNullMetadata",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetNewestOffsetsThrowsIfNullMetadata() {\r\n    // empty topic == null newest offset\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    KafkaTransactionalStateTaskBackupManager tsm = buildTSM(csm, mock(Partition.class), new StorageManagerUtil());\r\n    TaskName taskName = mock(TaskName.class);\r\n    String changelogSystemName = \"systemName\";\r\n    String storeName = \"storeName\";\r\n    String changelogStreamName = \"changelogName\";\r\n    String newestChangelogSSPOffset = null;\r\n    SystemStream changelogSystemStream = new SystemStream(changelogSystemName, changelogStreamName);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogs = new HashMap<>();\r\n    storeChangelogs.put(storeName, changelogSystemStream);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin systemAdmin = mock(SystemAdmin.class);\r\n    SystemStreamPartitionMetadata metadata = mock(SystemStreamPartitionMetadata.class);\r\n    when(metadata.getNewestOffset()).thenReturn(newestChangelogSSPOffset);\r\n    when(systemAdmins.getSystemAdmin(changelogSystemName)).thenReturn(systemAdmin);\r\n    when(systemAdmin.getSSPMetadata(eq(ImmutableSet.of(changelogSSP)))).thenReturn(null);\r\n    // invoke the method\r\n    java.util.Map<String, String> offsets = tsm.getNewestChangelogSSPOffsets(taskName, storeChangelogs, changelogPartition, systemAdmins);\r\n    // verify results\r\n    fail(\"Should have thrown an exception if admin didn't return any metadata\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testGetNewestOffsetsThrowsIfNullSSPMetadata",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetNewestOffsetsThrowsIfNullSSPMetadata() {\r\n    // empty topic == null newest offset\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    KafkaTransactionalStateTaskBackupManager tsm = buildTSM(csm, mock(Partition.class), new StorageManagerUtil());\r\n    TaskName taskName = mock(TaskName.class);\r\n    String changelogSystemName = \"systemName\";\r\n    String storeName = \"storeName\";\r\n    String changelogStreamName = \"changelogName\";\r\n    String newestChangelogSSPOffset = null;\r\n    SystemStream changelogSystemStream = new SystemStream(changelogSystemName, changelogStreamName);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogs = new HashMap<>();\r\n    storeChangelogs.put(storeName, changelogSystemStream);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin systemAdmin = mock(SystemAdmin.class);\r\n    SystemStreamPartitionMetadata metadata = mock(SystemStreamPartitionMetadata.class);\r\n    when(metadata.getNewestOffset()).thenReturn(newestChangelogSSPOffset);\r\n    when(systemAdmins.getSystemAdmin(changelogSystemName)).thenReturn(systemAdmin);\r\n    java.util.Map metadataMap = new HashMap() {\r\n\r\n        {\r\n            put(changelogSSP, null);\r\n        }\r\n    };\r\n    when(systemAdmin.getSSPMetadata(eq(ImmutableSet.of(changelogSSP)))).thenReturn(metadataMap);\r\n    // invoke the method\r\n    java.util.Map<String, String> offsets = tsm.getNewestChangelogSSPOffsets(taskName, storeChangelogs, changelogPartition, systemAdmins);\r\n    // verify results\r\n    fail(\"Should have thrown an exception if admin returned null metadata for changelog SSP\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\storage\\TestTransactionalStateTaskBackupManager.java",
  "methodName" : "testGetNewestOffsetsThrowsIfErrorGettingMetadata",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetNewestOffsetsThrowsIfErrorGettingMetadata() {\r\n    // empty topic == null newest offset\r\n    ContainerStorageManager csm = mock(ContainerStorageManager.class);\r\n    KafkaTransactionalStateTaskBackupManager tsm = buildTSM(csm, mock(Partition.class), new StorageManagerUtil());\r\n    TaskName taskName = mock(TaskName.class);\r\n    String changelogSystemName = \"systemName\";\r\n    String storeName = \"storeName\";\r\n    String changelogStreamName = \"changelogName\";\r\n    String newestChangelogSSPOffset = null;\r\n    SystemStream changelogSystemStream = new SystemStream(changelogSystemName, changelogStreamName);\r\n    Partition changelogPartition = new Partition(0);\r\n    SystemStreamPartition changelogSSP = new SystemStreamPartition(changelogSystemStream, changelogPartition);\r\n    java.util.Map<String, SystemStream> storeChangelogs = new HashMap<>();\r\n    storeChangelogs.put(storeName, changelogSystemStream);\r\n    SystemAdmins systemAdmins = mock(SystemAdmins.class);\r\n    SystemAdmin systemAdmin = mock(SystemAdmin.class);\r\n    SystemStreamPartitionMetadata metadata = mock(SystemStreamPartitionMetadata.class);\r\n    when(metadata.getNewestOffset()).thenReturn(newestChangelogSSPOffset);\r\n    when(systemAdmins.getSystemAdmin(changelogSystemName)).thenThrow(new SamzaException(\"Error getting metadata\"));\r\n    when(systemAdmin.getSSPMetadata(eq(ImmutableSet.of(changelogSSP)))).thenReturn(null);\r\n    // invoke the method\r\n    java.util.Map<String, String> offsets = tsm.getNewestChangelogSSPOffsets(taskName, storeChangelogs, changelogPartition, systemAdmins);\r\n    // verify results\r\n    fail(\"Should have thrown an exception if admin had an error getting metadata\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\descriptors\\TestKafkaInputDescriptor.java",
  "methodName" : "testISDConfigsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testISDConfigsWithOverrides() {\r\n    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(\"kafka\");\r\n    KafkaInputDescriptor<KV<String, Integer>> isd = sd.getInputDescriptor(\"input-stream\", KVSerde.of(new StringSerde(), new IntegerSerde())).withConsumerAutoOffsetReset(\"largest\").withConsumerFetchMessageMaxBytes(1024 * 1024);\r\n    Map<String, String> generatedConfigs = isd.toConfig();\r\n    assertEquals(\"kafka\", generatedConfigs.get(\"streams.input-stream.samza.system\"));\r\n    assertEquals(\"largest\", generatedConfigs.get(\"systems.kafka.streams.input-stream.consumer.auto.offset.reset\"));\r\n    assertEquals(\"1048576\", generatedConfigs.get(\"systems.kafka.streams.input-stream.consumer.fetch.message.max.bytes\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\descriptors\\TestKafkaInputDescriptor.java",
  "methodName" : "testISDConfigsWithDefaults",
  "sourceCode" : "@Test\r\npublic void testISDConfigsWithDefaults() {\r\n    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(\"kafka\").withConsumerZkConnect(ImmutableList.of(\"localhost:123\")).withProducerBootstrapServers(ImmutableList.of(\"localhost:567\", \"localhost:890\"));\r\n    KafkaInputDescriptor<KV<String, Integer>> isd = sd.getInputDescriptor(\"input-stream\", KVSerde.of(new StringSerde(), new IntegerSerde()));\r\n    Map<String, String> generatedConfigs = isd.toConfig();\r\n    assertEquals(\"kafka\", generatedConfigs.get(\"streams.input-stream.samza.system\"));\r\n    // verify that there are no other configs\r\n    assertEquals(1, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\descriptors\\TestKafkaSystemDescriptor.java",
  "methodName" : "testSDConfigsWithOverrides",
  "sourceCode" : "@Test\r\npublic void testSDConfigsWithOverrides() {\r\n    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(\"kafka\").withConsumerZkConnect(ImmutableList.of(\"localhost:1234\")).withProducerBootstrapServers(ImmutableList.of(\"localhost:567\", \"localhost:890\")).withDefaultStreamOffsetDefault(SystemStreamMetadata.OffsetType.OLDEST).withConsumerAutoOffsetReset(\"smallest\").withConsumerFetchMessageMaxBytes(1024 * 1024).withSamzaFetchThreshold(10000).withSamzaFetchThresholdBytes(1024 * 1024).withConsumerConfigs(ImmutableMap.of(\"custom-consumer-config-key\", \"custom-consumer-config-value\")).withProducerConfigs(ImmutableMap.of(\"custom-producer-config-key\", \"custom-producer-config-value\")).withDefaultStreamConfigs(ImmutableMap.of(\"custom-stream-config-key\", \"custom-stream-config-value\"));\r\n    Map<String, String> generatedConfigs = sd.toConfig();\r\n    assertEquals(\"org.apache.samza.system.kafka.KafkaSystemFactory\", generatedConfigs.get(\"systems.kafka.samza.factory\"));\r\n    assertEquals(\"localhost:1234\", generatedConfigs.get(\"systems.kafka.consumer.zookeeper.connect\"));\r\n    assertEquals(\"localhost:567,localhost:890\", generatedConfigs.get(\"systems.kafka.producer.bootstrap.servers\"));\r\n    assertEquals(\"smallest\", generatedConfigs.get(\"systems.kafka.consumer.auto.offset.reset\"));\r\n    assertEquals(\"1048576\", generatedConfigs.get(\"systems.kafka.consumer.fetch.message.max.bytes\"));\r\n    assertEquals(\"10000\", generatedConfigs.get(\"systems.kafka.samza.fetch.threshold\"));\r\n    assertEquals(\"1048576\", generatedConfigs.get(\"systems.kafka.samza.fetch.threshold.bytes\"));\r\n    assertEquals(\"custom-consumer-config-value\", generatedConfigs.get(\"systems.kafka.consumer.custom-consumer-config-key\"));\r\n    assertEquals(\"custom-producer-config-value\", generatedConfigs.get(\"systems.kafka.producer.custom-producer-config-key\"));\r\n    assertEquals(\"custom-stream-config-value\", generatedConfigs.get(\"systems.kafka.default.stream.custom-stream-config-key\"));\r\n    assertEquals(\"oldest\", generatedConfigs.get(\"systems.kafka.default.stream.samza.offset.default\"));\r\n    assertEquals(11, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\descriptors\\TestKafkaSystemDescriptor.java",
  "methodName" : "testSDConfigsWithoutOverrides",
  "sourceCode" : "@Test\r\npublic void testSDConfigsWithoutOverrides() {\r\n    KafkaSystemDescriptor sd = new KafkaSystemDescriptor(\"kafka\");\r\n    Map<String, String> generatedConfigs = sd.toConfig();\r\n    assertEquals(\"org.apache.samza.system.kafka.KafkaSystemFactory\", generatedConfigs.get(\"systems.kafka.samza.factory\"));\r\n    // verify that there are no other configs\r\n    assertEquals(1, generatedConfigs.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaCheckpointManagerFactory.java",
  "methodName" : "testGetCheckpointTopicProperties",
  "sourceCode" : "@Test\r\npublic void testGetCheckpointTopicProperties() {\r\n    Map<String, String> config = new HashMap<>();\r\n    Properties properties = new KafkaConfig(new MapConfig(config)).getCheckpointTopicProperties();\r\n    assertEquals(properties.getProperty(\"cleanup.policy\"), \"compact\");\r\n    assertEquals(properties.getProperty(\"segment.bytes\"), String.valueOf(KafkaConfig.DEFAULT_CHECKPOINT_SEGMENT_BYTES()));\r\n    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());\r\n    properties = new KafkaConfig(new MapConfig(config)).getCheckpointTopicProperties();\r\n    assertEquals(properties.getProperty(\"cleanup.policy\"), \"compact,delete\");\r\n    assertEquals(properties.getProperty(\"segment.bytes\"), String.valueOf(KafkaConfig.DEFAULT_CHECKPOINT_SEGMENT_BYTES()));\r\n    assertEquals(properties.getProperty(\"retention.ms\"), String.valueOf(KafkaConfig.DEFAULT_RETENTION_MS_FOR_BATCH()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaStreamSpec.java",
  "methodName" : "testUnsupportedConfigStrippedFromProperties",
  "sourceCode" : "@Test\r\npublic void testUnsupportedConfigStrippedFromProperties() {\r\n    StreamSpec original = new StreamSpec(\"dummyId\", \"dummyPhysicalName\", \"dummySystemName\", ImmutableMap.of(\"segment.bytes\", \"4\", \"replication.factor\", \"7\"));\r\n    // First verify the original\r\n    assertEquals(\"7\", original.get(\"replication.factor\"));\r\n    assertEquals(\"4\", original.get(\"segment.bytes\"));\r\n    Map<String, String> config = original.getConfig();\r\n    assertEquals(\"7\", config.get(\"replication.factor\"));\r\n    assertEquals(\"4\", config.get(\"segment.bytes\"));\r\n    // Now verify the Kafka spec\r\n    KafkaStreamSpec spec = KafkaStreamSpec.fromSpec(original);\r\n    assertNull(spec.get(\"replication.factor\"));\r\n    assertEquals(\"4\", spec.get(\"segment.bytes\"));\r\n    Properties kafkaProperties = spec.getProperties();\r\n    Map<String, String> kafkaConfig = spec.getConfig();\r\n    assertNull(kafkaProperties.get(\"replication.factor\"));\r\n    assertEquals(\"4\", kafkaProperties.get(\"segment.bytes\"));\r\n    assertNull(kafkaConfig.get(\"replication.factor\"));\r\n    assertEquals(\"4\", kafkaConfig.get(\"segment.bytes\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaStreamSpec.java",
  "methodName" : "testInvalidPartitionCount",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testInvalidPartitionCount() {\r\n    new KafkaStreamSpec(\"dummyId\", \"dummyPhysicalName\", \"dummySystemName\", 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateStreamShouldCoordinatorStreamWithCorrectTopicProperties",
  "sourceCode" : "@Test\r\npublic void testCreateStreamShouldCoordinatorStreamWithCorrectTopicProperties() throws Exception {\r\n    String coordinatorTopicName = String.format(\"topic-name-%s\", RandomStringUtils.randomAlphabetic(5));\r\n    StreamSpec coordinatorStreamSpec = KafkaStreamSpec.createCoordinatorStreamSpec(coordinatorTopicName, SYSTEM());\r\n    boolean hasCreatedStream = systemAdmin().createStream(coordinatorStreamSpec);\r\n    assertTrue(hasCreatedStream);\r\n    Map<String, String> coordinatorTopicProperties = getTopicConfigFromKafkaBroker(coordinatorTopicName);\r\n    assertEquals(\"compact\", coordinatorTopicProperties.get(TopicConfig.CLEANUP_POLICY_CONFIG));\r\n    assertEquals(\"26214400\", coordinatorTopicProperties.get(TopicConfig.SEGMENT_BYTES_CONFIG));\r\n    assertEquals(\"86400000\", coordinatorTopicProperties.get(TopicConfig.DELETE_RETENTION_MS_CONFIG));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testGetOffsetsAfter",
  "sourceCode" : "@Test\r\npublic void testGetOffsetsAfter() {\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(SYSTEM, TOPIC, new Partition(0));\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(SYSTEM, TOPIC, new Partition(1));\r\n    Map<SystemStreamPartition, String> offsets = new HashMap<>();\r\n    offsets.put(ssp1, \"1\");\r\n    offsets.put(ssp2, \"2\");\r\n    offsets = systemAdmin().getOffsetsAfter(offsets);\r\n    Assert.assertEquals(\"2\", offsets.get(ssp1));\r\n    Assert.assertEquals(\"3\", offsets.get(ssp2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testToKafkaSpecForCheckpointStreamShouldReturnTheCorrectStreamSpecByPreservingTheConfig",
  "sourceCode" : "@Test\r\npublic void testToKafkaSpecForCheckpointStreamShouldReturnTheCorrectStreamSpecByPreservingTheConfig() {\r\n    String topicName = \"testStream\";\r\n    String streamId = \"samza-internal-checkpoint-stream-id\";\r\n    int partitionCount = 1;\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"cleanup.policy\", \"compact\");\r\n    map.put(\"replication.factor\", \"3\");\r\n    map.put(\"segment.bytes\", \"536870912\");\r\n    map.put(\"delete.retention.ms\", \"86400000\");\r\n    Config config = new MapConfig(map);\r\n    StreamSpec spec = new StreamSpec(streamId, topicName, SYSTEM, partitionCount, config);\r\n    KafkaSystemAdmin kafkaSystemAdmin = systemAdmin();\r\n    KafkaStreamSpec kafkaStreamSpec = kafkaSystemAdmin.toKafkaSpec(spec);\r\n    System.out.println(kafkaStreamSpec);\r\n    assertEquals(streamId, kafkaStreamSpec.getId());\r\n    assertEquals(topicName, kafkaStreamSpec.getPhysicalName());\r\n    assertEquals(partitionCount, kafkaStreamSpec.getPartitionCount());\r\n    assertEquals(3, kafkaStreamSpec.getReplicationFactor());\r\n    assertEquals(\"compact\", kafkaStreamSpec.getConfig().get(\"cleanup.policy\"));\r\n    assertEquals(\"536870912\", kafkaStreamSpec.getConfig().get(\"segment.bytes\"));\r\n    assertEquals(\"86400000\", kafkaStreamSpec.getConfig().get(\"delete.retention.ms\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testToKafkaSpec",
  "sourceCode" : "@Test\r\npublic void testToKafkaSpec() {\r\n    String topicName = \"testStream\";\r\n    int defaultPartitionCount = 2;\r\n    int changeLogPartitionFactor = 5;\r\n    Map<String, String> map = new HashMap<>();\r\n    Config config = new MapConfig(map);\r\n    StreamSpec spec = new StreamSpec(\"id\", topicName, SYSTEM, defaultPartitionCount, config);\r\n    KafkaSystemAdmin kafkaAdmin = systemAdmin();\r\n    KafkaStreamSpec kafkaSpec = kafkaAdmin.toKafkaSpec(spec);\r\n    Assert.assertEquals(\"id\", kafkaSpec.getId());\r\n    Assert.assertEquals(topicName, kafkaSpec.getPhysicalName());\r\n    Assert.assertEquals(SYSTEM, kafkaSpec.getSystemName());\r\n    Assert.assertEquals(defaultPartitionCount, kafkaSpec.getPartitionCount());\r\n    // validate that conversion is using coordination metadata\r\n    map.put(\"job.coordinator.segment.bytes\", \"123\");\r\n    map.put(\"job.coordinator.cleanup.policy\", \"superCompact\");\r\n    int coordReplicatonFactor = 4;\r\n    map.put(org.apache.samza.config.KafkaConfig.JOB_COORDINATOR_REPLICATION_FACTOR(), String.valueOf(coordReplicatonFactor));\r\n    KafkaSystemAdmin admin = Mockito.spy(createSystemAdmin(SYSTEM, map));\r\n    spec = StreamSpec.createCoordinatorStreamSpec(topicName, SYSTEM);\r\n    kafkaSpec = admin.toKafkaSpec(spec);\r\n    Assert.assertEquals(coordReplicatonFactor, kafkaSpec.getReplicationFactor());\r\n    Assert.assertEquals(\"123\", kafkaSpec.getProperties().getProperty(\"segment.bytes\"));\r\n    // cleanup policy is overridden in the KafkaAdmin\r\n    Assert.assertEquals(\"compact\", kafkaSpec.getProperties().getProperty(\"cleanup.policy\"));\r\n    // validate that conversion is using changeLog metadata\r\n    map = new HashMap<>();\r\n    map.put(JobConfig.JOB_DEFAULT_SYSTEM, SYSTEM);\r\n    map.put(String.format(\"stores.%s.changelog\", \"fakeStore\"), topicName);\r\n    int changeLogReplicationFactor = 3;\r\n    map.put(String.format(\"stores.%s.changelog.replication.factor\", \"fakeStore\"), String.valueOf(changeLogReplicationFactor));\r\n    admin = Mockito.spy(createSystemAdmin(SYSTEM, map));\r\n    spec = StreamSpec.createChangeLogStreamSpec(topicName, SYSTEM, changeLogPartitionFactor);\r\n    kafkaSpec = admin.toKafkaSpec(spec);\r\n    Assert.assertEquals(changeLogReplicationFactor, kafkaSpec.getReplicationFactor());\r\n    // same, but with missing topic info\r\n    try {\r\n        admin = Mockito.spy(createSystemAdmin(SYSTEM, map));\r\n        spec = StreamSpec.createChangeLogStreamSpec(\"anotherTopic\", SYSTEM, changeLogPartitionFactor);\r\n        kafkaSpec = admin.toKafkaSpec(spec);\r\n        Assert.fail(\"toKafkaSpec should've failed for missing topic\");\r\n    } catch (StreamValidationException e) {\r\n        // expected\r\n    }\r\n    // validate that conversion is using intermediate streams properties\r\n    String interStreamId = \"isId\";\r\n    Map<String, String> interStreamMap = new HashMap<>();\r\n    interStreamMap.put(\"app.mode\", ApplicationConfig.ApplicationMode.BATCH.toString());\r\n    interStreamMap.put(String.format(\"streams.%s.samza.intermediate\", interStreamId), \"true\");\r\n    interStreamMap.put(String.format(\"streams.%s.samza.system\", interStreamId), \"testSystem\");\r\n    interStreamMap.put(String.format(\"streams.%s.p1\", interStreamId), \"v1\");\r\n    interStreamMap.put(String.format(\"streams.%s.retention.ms\", interStreamId), \"123\");\r\n    // legacy format\r\n    interStreamMap.put(String.format(\"systems.%s.streams.%s.p2\", \"testSystem\", interStreamId), \"v2\");\r\n    admin = Mockito.spy(createSystemAdmin(SYSTEM, interStreamMap));\r\n    spec = new StreamSpec(interStreamId, topicName, SYSTEM, defaultPartitionCount, config);\r\n    kafkaSpec = admin.toKafkaSpec(spec);\r\n    Assert.assertEquals(\"v1\", kafkaSpec.getProperties().getProperty(\"p1\"));\r\n    Assert.assertEquals(\"v2\", kafkaSpec.getProperties().getProperty(\"p2\"));\r\n    Assert.assertEquals(\"123\", kafkaSpec.getProperties().getProperty(\"retention.ms\"));\r\n    Assert.assertEquals(defaultPartitionCount, kafkaSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateCoordinatorStream",
  "sourceCode" : "@Test\r\npublic void testCreateCoordinatorStream() {\r\n    SystemAdmin admin = Mockito.spy(systemAdmin());\r\n    StreamSpec spec = StreamSpec.createCoordinatorStreamSpec(\"testCoordinatorStream\", \"testSystem\");\r\n    admin.createStream(spec);\r\n    admin.validateStream(spec);\r\n    Mockito.verify(admin).createStream(Mockito.any());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateCoordinatorStreamWithSpecialCharsInTopicName",
  "sourceCode" : "@Test\r\npublic void testCreateCoordinatorStreamWithSpecialCharsInTopicName() {\r\n    final String stream = \"test.coordinator_test.Stream\";\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"job.coordinator.segment.bytes\", \"123\");\r\n    map.put(\"job.coordinator.cleanup.policy\", \"compact\");\r\n    int coordReplicatonFactor = 2;\r\n    map.put(org.apache.samza.config.KafkaConfig.JOB_COORDINATOR_REPLICATION_FACTOR(), String.valueOf(coordReplicatonFactor));\r\n    KafkaSystemAdmin admin = Mockito.spy(createSystemAdmin(SYSTEM, map));\r\n    StreamSpec spec = StreamSpec.createCoordinatorStreamSpec(stream, SYSTEM);\r\n    Mockito.doAnswer(invocationOnMock -> {\r\n        StreamSpec internalSpec = (StreamSpec) invocationOnMock.callRealMethod();\r\n        // KafkaStreamSpec is used to carry replication factor\r\n        assertTrue(internalSpec instanceof KafkaStreamSpec);\r\n        assertTrue(internalSpec.isCoordinatorStream());\r\n        assertEquals(SYSTEM, internalSpec.getSystemName());\r\n        assertEquals(stream, internalSpec.getPhysicalName());\r\n        assertEquals(1, internalSpec.getPartitionCount());\r\n        Assert.assertEquals(coordReplicatonFactor, ((KafkaStreamSpec) internalSpec).getReplicationFactor());\r\n        Assert.assertEquals(\"123\", ((KafkaStreamSpec) internalSpec).getProperties().getProperty(\"segment.bytes\"));\r\n        // cleanup policy is overridden in the KafkaAdmin\r\n        Assert.assertEquals(\"compact\", ((KafkaStreamSpec) internalSpec).getProperties().getProperty(\"cleanup.policy\"));\r\n        return internalSpec;\r\n    }).when(admin).toKafkaSpec(Mockito.any());\r\n    admin.createStream(spec);\r\n    admin.validateStream(spec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateChangelogStreamHelp",
  "sourceCode" : "@Test\r\npublic void testCreateChangelogStreamHelp() {\r\n    testCreateChangelogStreamHelp(\"testChangeLogStream\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateChangelogStreamWithSpecialCharsInTopicName",
  "sourceCode" : "@Test\r\npublic void testCreateChangelogStreamWithSpecialCharsInTopicName() {\r\n    // cannot contain period\r\n    testCreateChangelogStreamHelp(\"test-Change_Log-Stream\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testCreateStream",
  "sourceCode" : "@Test\r\npublic void testCreateStream() {\r\n    StreamSpec spec = new StreamSpec(\"testId\", \"testStream\", \"testSystem\", 8);\r\n    KafkaSystemAdmin admin = systemAdmin();\r\n    assertTrue(\"createStream should return true if the stream does not exist and then is created.\", admin.createStream(spec));\r\n    admin.validateStream(spec);\r\n    assertFalse(\"createStream should return false if the stream already exists.\", systemAdmin().createStream(spec));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testValidateStreamDoesNotExist",
  "sourceCode" : "@Test(expected = StreamValidationException.class)\r\npublic void testValidateStreamDoesNotExist() {\r\n    StreamSpec spec = new StreamSpec(\"testId\", \"testStreamNameExist\", \"testSystem\", 8);\r\n    systemAdmin().validateStream(spec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testValidateStreamWrongPartitionCount",
  "sourceCode" : "@Test(expected = StreamValidationException.class)\r\npublic void testValidateStreamWrongPartitionCount() {\r\n    StreamSpec spec1 = new StreamSpec(\"testId\", \"testStreamPartition\", \"testSystem\", 8);\r\n    StreamSpec spec2 = new StreamSpec(\"testId\", \"testStreamPartition\", \"testSystem\", 4);\r\n    assertTrue(\"createStream should return true if the stream does not exist and then is created.\", systemAdmin().createStream(spec1));\r\n    systemAdmin().validateStream(spec2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testValidateStreamWrongName",
  "sourceCode" : "@Test(expected = StreamValidationException.class)\r\npublic void testValidateStreamWrongName() {\r\n    StreamSpec spec1 = new StreamSpec(\"testId\", \"testStreamName1\", \"testSystem\", 8);\r\n    StreamSpec spec2 = new StreamSpec(\"testId\", \"testStreamName2\", \"testSystem\", 8);\r\n    assertTrue(\"createStream should return true if the stream does not exist and then is created.\", systemAdmin().createStream(spec1));\r\n    systemAdmin().validateStream(spec2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testClearStream",
  "sourceCode" : "@Test\r\npublic void testClearStream() {\r\n    StreamSpec spec = new StreamSpec(\"testId\", \"testStreamClear\", \"testSystem\", 8);\r\n    KafkaSystemAdmin admin = systemAdmin();\r\n    String topicName = spec.getPhysicalName();\r\n    assertTrue(\"createStream should return true if the stream does not exist and then is created.\", admin.createStream(spec));\r\n    // validate topic exists\r\n    assertTrue(admin.clearStream(spec));\r\n    // validate that topic was removed\r\n    DescribeTopicsResult dtr = admin.adminClient.describeTopics(ImmutableSet.of(topicName));\r\n    try {\r\n        TopicDescription td = dtr.all().get().get(topicName);\r\n        Assert.fail(\"topic \" + topicName + \" should've been removed. td=\" + td);\r\n    } catch (Exception e) {\r\n        if (!(e.getCause() instanceof org.apache.kafka.common.errors.UnknownTopicOrPartitionException)) {\r\n            Assert.fail(\"topic \" + topicName + \" should've been removed. Expected UnknownTopicOrPartitionException.\");\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testShouldAssembleMetadata",
  "sourceCode" : "@Test\r\npublic void testShouldAssembleMetadata() {\r\n    Map<SystemStreamPartition, String> oldestOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>().put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(0)), \"o1\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(0)), \"o2\").put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(1)), \"o3\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(1)), \"o4\").build();\r\n    Map<SystemStreamPartition, String> newestOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>().put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(0)), \"n1\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(0)), \"n2\").put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(1)), \"n3\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(1)), \"n4\").build();\r\n    Map<SystemStreamPartition, String> upcomingOffsets = new ImmutableMap.Builder<SystemStreamPartition, String>().put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(0)), \"u1\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(0)), \"u2\").put(new SystemStreamPartition(SYSTEM, \"stream1\", new Partition(1)), \"u3\").put(new SystemStreamPartition(SYSTEM, \"stream2\", new Partition(1)), \"u4\").build();\r\n    Map<String, SystemStreamMetadata> metadata = assembleMetadata(oldestOffsets, newestOffsets, upcomingOffsets);\r\n    assertNotNull(metadata);\r\n    assertEquals(2, metadata.size());\r\n    assertTrue(metadata.containsKey(\"stream1\"));\r\n    assertTrue(metadata.containsKey(\"stream2\"));\r\n    SystemStreamMetadata stream1Metadata = metadata.get(\"stream1\");\r\n    SystemStreamMetadata stream2Metadata = metadata.get(\"stream2\");\r\n    assertNotNull(stream1Metadata);\r\n    assertNotNull(stream2Metadata);\r\n    assertEquals(\"stream1\", stream1Metadata.getStreamName());\r\n    assertEquals(\"stream2\", stream2Metadata.getStreamName());\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream1Partition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"o1\", \"n1\", \"u1\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream1Partition1Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"o3\", \"n3\", \"u3\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream2Partition0Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"o2\", \"n2\", \"u2\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata expectedSystemStream2Partition1Metadata = new SystemStreamMetadata.SystemStreamPartitionMetadata(\"o4\", \"n4\", \"u4\");\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> stream1PartitionMetadata = stream1Metadata.getSystemStreamPartitionMetadata();\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> stream2PartitionMetadata = stream2Metadata.getSystemStreamPartitionMetadata();\r\n    assertEquals(expectedSystemStream1Partition0Metadata, stream1PartitionMetadata.get(new Partition(0)));\r\n    assertEquals(expectedSystemStream1Partition1Metadata, stream1PartitionMetadata.get(new Partition(1)));\r\n    assertEquals(expectedSystemStream2Partition0Metadata, stream2PartitionMetadata.get(new Partition(0)));\r\n    assertEquals(expectedSystemStream2Partition1Metadata, stream2PartitionMetadata.get(new Partition(1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testStartpointSpecificOffsetVisitorShouldResolveToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointSpecificOffsetVisitorShouldResolveToCorrectOffset() {\r\n    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);\r\n    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);\r\n    final StartpointSpecific testStartpointSpecific = new StartpointSpecific(TEST_OFFSET);\r\n    // Invoke the consumer with startpoint.\r\n    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);\r\n    Assert.assertEquals(TEST_OFFSET, resolvedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testStartpointTimestampVisitorShouldResolveToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointTimestampVisitorShouldResolveToCorrectOffset() {\r\n    // Define dummy variables for testing.\r\n    final Long testTimeStamp = 10L;\r\n    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);\r\n    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);\r\n    final StartpointTimestamp startpointTimestamp = new StartpointTimestamp(testTimeStamp);\r\n    final Map<TopicPartition, OffsetAndTimestamp> offsetForTimesResult = ImmutableMap.of(TEST_TOPIC_PARTITION, new OffsetAndTimestamp(Long.valueOf(TEST_OFFSET), testTimeStamp));\r\n    // Mock the consumer interactions.\r\n    Mockito.when(consumer.offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, testTimeStamp))).thenReturn(offsetForTimesResult);\r\n    Mockito.when(consumer.position(TEST_TOPIC_PARTITION)).thenReturn(Long.valueOf(TEST_OFFSET));\r\n    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, startpointTimestamp);\r\n    Assert.assertEquals(TEST_OFFSET, resolvedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testStartpointTimestampVisitorShouldResolveToCorrectOffsetWhenTimestampDoesNotExist",
  "sourceCode" : "@Test\r\npublic void testStartpointTimestampVisitorShouldResolveToCorrectOffsetWhenTimestampDoesNotExist() {\r\n    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);\r\n    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);\r\n    final StartpointTimestamp startpointTimestamp = new StartpointTimestamp(0L);\r\n    final Map<TopicPartition, OffsetAndTimestamp> offsetForTimesResult = new HashMap<>();\r\n    offsetForTimesResult.put(TEST_TOPIC_PARTITION, null);\r\n    // Mock the consumer interactions.\r\n    Mockito.when(consumer.offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, 0L))).thenReturn(offsetForTimesResult);\r\n    Mockito.when(consumer.endOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));\r\n    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, startpointTimestamp);\r\n    Assert.assertEquals(TEST_OFFSET, resolvedOffset);\r\n    // Mock verifications.\r\n    Mockito.verify(consumer).offsetsForTimes(ImmutableMap.of(TEST_TOPIC_PARTITION, 0L));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testStartpointOldestVisitorShouldResolveToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointOldestVisitorShouldResolveToCorrectOffset() {\r\n    // Define dummy variables for testing.\r\n    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);\r\n    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);\r\n    final StartpointOldest testStartpointSpecific = new StartpointOldest();\r\n    // Mock the consumer interactions.\r\n    Mockito.when(consumer.beginningOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));\r\n    // Invoke the consumer with startpoint.\r\n    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);\r\n    Assert.assertEquals(TEST_OFFSET, resolvedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminJava.java",
  "methodName" : "testStartpointUpcomingVisitorShouldResolveToCorrectOffset",
  "sourceCode" : "@Test\r\npublic void testStartpointUpcomingVisitorShouldResolveToCorrectOffset() {\r\n    // Define dummy variables for testing.\r\n    final KafkaConsumer consumer = Mockito.mock(KafkaConsumer.class);\r\n    final KafkaStartpointToOffsetResolver kafkaStartpointToOffsetResolver = new KafkaStartpointToOffsetResolver(consumer);\r\n    final StartpointUpcoming testStartpointSpecific = new StartpointUpcoming();\r\n    // Mock the consumer interactions.\r\n    Mockito.when(consumer.endOffsets(ImmutableSet.of(TEST_TOPIC_PARTITION))).thenReturn(ImmutableMap.of(TEST_TOPIC_PARTITION, 10L));\r\n    // Invoke the consumer with startpoint.\r\n    String resolvedOffset = kafkaStartpointToOffsetResolver.visit(TEST_SYSTEM_STREAM_PARTITION, testStartpointSpecific);\r\n    Assert.assertEquals(TEST_OFFSET, resolvedOffset);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetaDataWithValidTopic",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetaDataWithValidTopic() {\r\n    System.out.println(\"STARTING\");\r\n    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));\r\n    // verify metadata size\r\n    assertEquals(\"metadata should return for 1 topic\", metadataMap.size(), 1);\r\n    System.out.println(\"STARTING1\");\r\n    // verify the metadata streamName\r\n    assertEquals(\"the stream name should be \" + VALID_TOPIC, metadataMap.get(VALID_TOPIC).getStreamName(), VALID_TOPIC);\r\n    System.out.println(\"STARTING2\");\r\n    // verify the offset for each partition\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> systemStreamPartitionMetadata = metadataMap.get(VALID_TOPIC).getSystemStreamPartitionMetadata();\r\n    assertEquals(\"there are 2 partitions\", systemStreamPartitionMetadata.size(), 2);\r\n    System.out.println(\"STARTING3\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata partition0Metadata = systemStreamPartitionMetadata.get(new Partition(0));\r\n    assertEquals(\"oldest offset for partition 0\", partition0Metadata.getOldestOffset(), KAFKA_BEGINNING_OFFSET_FOR_PARTITION0.toString());\r\n    assertEquals(\"upcoming offset for partition 0\", partition0Metadata.getUpcomingOffset(), KAFKA_END_OFFSET_FOR_PARTITION0.toString());\r\n    assertEquals(\"newest offset for partition 0\", partition0Metadata.getNewestOffset(), Long.toString(KAFKA_END_OFFSET_FOR_PARTITION0 - 1));\r\n    System.out.println(\"STARTING4\");\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata partition1Metadata = systemStreamPartitionMetadata.get(new Partition(1));\r\n    assertEquals(\"oldest offset for partition 1\", partition1Metadata.getOldestOffset(), KAFKA_BEGINNING_OFFSET_FOR_PARTITION1.toString());\r\n    assertEquals(\"upcoming offset for partition 1\", partition1Metadata.getUpcomingOffset(), KAFKA_END_OFFSET_FOR_PARTITION1.toString());\r\n    assertEquals(\"newest offset for partition 1\", partition1Metadata.getNewestOffset(), Long.toString(KAFKA_END_OFFSET_FOR_PARTITION1 - 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetaDataWithInvalidTopic",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetaDataWithInvalidTopic() {\r\n    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(INVALID_TOPIC));\r\n    assertEquals(\"empty metadata for invalid topic\", metadataMap.size(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetaDataWithNoTopic",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetaDataWithNoTopic() {\r\n    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(Collections.emptySet());\r\n    assertEquals(\"empty metadata for no topic\", metadataMap.size(), 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetaDataForTopicWithNoMessage",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetaDataForTopicWithNoMessage() {\r\n    // The topic with no messages will have beginningOffset = 0 and endOffset = 0\r\n    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(testTopicPartition0, testTopicPartition1))).thenReturn(ImmutableMap.of(testTopicPartition0, 0L, testTopicPartition1, 0L));\r\n    when(mockKafkaConsumer.endOffsets(ImmutableList.of(testTopicPartition0, testTopicPartition1))).thenReturn(ImmutableMap.of(testTopicPartition0, 0L, testTopicPartition1, 0L));\r\n    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));\r\n    assertEquals(\"metadata should return for 1 topic\", metadataMap.size(), 1);\r\n    // verify the metadata streamName\r\n    assertEquals(\"the stream name should be \" + VALID_TOPIC, metadataMap.get(VALID_TOPIC).getStreamName(), VALID_TOPIC);\r\n    // verify the offset for each partition\r\n    Map<Partition, SystemStreamMetadata.SystemStreamPartitionMetadata> systemStreamPartitionMetadata = metadataMap.get(VALID_TOPIC).getSystemStreamPartitionMetadata();\r\n    assertEquals(\"there are 2 partitions\", systemStreamPartitionMetadata.size(), 2);\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata partition0Metadata = systemStreamPartitionMetadata.get(new Partition(0));\r\n    assertEquals(\"oldest offset for partition 0\", partition0Metadata.getOldestOffset(), \"0\");\r\n    assertEquals(\"upcoming offset for partition 0\", partition0Metadata.getUpcomingOffset(), \"0\");\r\n    assertEquals(\"newest offset is not set due to abnormal upcoming offset\", partition0Metadata.getNewestOffset(), null);\r\n    SystemStreamMetadata.SystemStreamPartitionMetadata partition1Metadata = systemStreamPartitionMetadata.get(new Partition(1));\r\n    assertEquals(\"oldest offset for partition 1\", partition1Metadata.getOldestOffset(), \"0\");\r\n    assertEquals(\"upcoming offset for partition 1\", partition1Metadata.getUpcomingOffset(), \"0\");\r\n    assertEquals(\"newest offset is not set due to abnormal upcoming offset\", partition1Metadata.getNewestOffset(), null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadata",
  "sourceCode" : "@Test\r\npublic void testGetSSPMetadata() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, \"otherTopic\", new Partition(1));\r\n    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);\r\n    TopicPartition otherTopicPartition = new TopicPartition(\"otherTopic\", 1);\r\n    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(ImmutableMap.of(topicPartition, 1L, otherTopicPartition, 2L));\r\n    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(ImmutableMap.of(topicPartition, 11L, otherTopicPartition, 12L));\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(\"1\", \"10\", \"11\"), otherSSP, new SystemStreamMetadata.SystemStreamPartitionMetadata(\"2\", \"11\", \"12\"));\r\n    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP)), expected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadataEmptyPartition",
  "sourceCode" : "@Test\r\npublic void testGetSSPMetadataEmptyPartition() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, \"otherTopic\", new Partition(1));\r\n    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);\r\n    TopicPartition otherTopicPartition = new TopicPartition(\"otherTopic\", 1);\r\n    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(ImmutableMap.of(topicPartition, 1L));\r\n    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition, otherTopicPartition))).thenReturn(ImmutableMap.of(topicPartition, 11L));\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(\"1\", \"10\", \"11\"), otherSSP, new SystemStreamMetadata.SystemStreamPartitionMetadata(null, null, null));\r\n    assertEquals(expected, kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp, otherSSP)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadataEmptyUpcomingOffset",
  "sourceCode" : "@Test\r\npublic void testGetSSPMetadataEmptyUpcomingOffset() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);\r\n    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition))).thenReturn(ImmutableMap.of(topicPartition, 0L));\r\n    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition))).thenReturn(ImmutableMap.of());\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", null, null));\r\n    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp)), expected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadataZeroUpcomingOffset",
  "sourceCode" : "@Test\r\npublic void testGetSSPMetadataZeroUpcomingOffset() {\r\n    SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    TopicPartition topicPartition = new TopicPartition(VALID_TOPIC, 0);\r\n    when(mockKafkaConsumer.beginningOffsets(ImmutableList.of(topicPartition))).thenReturn(ImmutableMap.of(topicPartition, -1L));\r\n    when(mockKafkaConsumer.endOffsets(ImmutableList.of(topicPartition))).thenReturn(ImmutableMap.of(topicPartition, 0L));\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> expected = ImmutableMap.of(ssp, new SystemStreamMetadata.SystemStreamPartitionMetadata(\"0\", null, \"0\"));\r\n    assertEquals(kafkaSystemAdmin.getSSPMetadata(ImmutableSet.of(ssp)), expected);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetaDataWithRetry",
  "sourceCode" : "@Test\r\npublic void testGetSystemStreamMetaDataWithRetry() {\r\n    final List<PartitionInfo> partitionInfosForTopic = ImmutableList.of(mockPartitionInfo0, mockPartitionInfo1);\r\n    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException()).thenReturn(partitionInfosForTopic);\r\n    Map<String, SystemStreamMetadata> metadataMap = kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));\r\n    assertEquals(\"metadata should return for 1 topic\", metadataMap.size(), 1);\r\n    // retried twice because the first fails and the second succeeds\r\n    Mockito.verify(mockKafkaConsumer, Mockito.times(2)).partitionsFor(VALID_TOPIC);\r\n    final List<TopicPartition> topicPartitions = Arrays.asList(new TopicPartition(mockPartitionInfo0.topic(), mockPartitionInfo0.partition()), new TopicPartition(mockPartitionInfo1.topic(), mockPartitionInfo1.partition()));\r\n    // the following methods thereafter are only called once\r\n    Mockito.verify(mockKafkaConsumer, Mockito.times(1)).beginningOffsets(topicPartitions);\r\n    Mockito.verify(mockKafkaConsumer, Mockito.times(1)).endOffsets(topicPartitions);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamMetadataShouldTerminateAfterFiniteRetriesOnException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetSystemStreamMetadataShouldTerminateAfterFiniteRetriesOnException() {\r\n    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException());\r\n    kafkaSystemAdmin.getSystemStreamMetadata(ImmutableSet.of(VALID_TOPIC));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSystemStreamPartitionCountsShouldTerminateAfterFiniteRetriesOnException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetSystemStreamPartitionCountsShouldTerminateAfterFiniteRetriesOnException() throws Exception {\r\n    final Set<String> streamNames = ImmutableSet.of(VALID_TOPIC);\r\n    final long cacheTTL = 100L;\r\n    when(mockKafkaConsumer.partitionsFor(VALID_TOPIC)).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException()).thenThrow(new RuntimeException());\r\n    kafkaSystemAdmin.getSystemStreamPartitionCounts(streamNames, cacheTTL);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadataWithRetry",
  "sourceCode" : "@Test\r\npublic void testGetSSPMetadataWithRetry() {\r\n    SystemStreamPartition oneSSP = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, \"otherTopic\", new Partition(1));\r\n    ImmutableSet<SystemStreamPartition> ssps = ImmutableSet.of(oneSSP, otherSSP);\r\n    List<TopicPartition> topicPartitions = ssps.stream().map(ssp -> new TopicPartition(ssp.getStream(), ssp.getPartition().getPartitionId())).collect(Collectors.toList());\r\n    Map<TopicPartition, Long> testBeginningOffsets = ImmutableMap.of(testTopicPartition0, KAFKA_BEGINNING_OFFSET_FOR_PARTITION0, testTopicPartition1, KAFKA_BEGINNING_OFFSET_FOR_PARTITION1);\r\n    when(mockKafkaConsumer.beginningOffsets(topicPartitions)).thenThrow(new RuntimeException()).thenReturn(testBeginningOffsets);\r\n    Map<SystemStreamPartition, SystemStreamMetadata.SystemStreamPartitionMetadata> sspMetadata = kafkaSystemAdmin.getSSPMetadata(ssps, new ExponentialSleepStrategy(2, 1, 1));\r\n    assertEquals(\"metadata should return for 2 topics\", sspMetadata.size(), 2);\r\n    // retried twice because the first fails and the second succeeds\r\n    Mockito.verify(mockKafkaConsumer, Mockito.times(2)).beginningOffsets(topicPartitions);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemAdminWithMock.java",
  "methodName" : "testGetSSPMetadataShouldTerminateAfterFiniteRetriesOnException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetSSPMetadataShouldTerminateAfterFiniteRetriesOnException() throws Exception {\r\n    SystemStreamPartition oneSSP = new SystemStreamPartition(TEST_SYSTEM, VALID_TOPIC, new Partition(0));\r\n    SystemStreamPartition otherSSP = new SystemStreamPartition(TEST_SYSTEM, \"otherTopic\", new Partition(1));\r\n    ImmutableSet<SystemStreamPartition> ssps = ImmutableSet.of(oneSSP, otherSSP);\r\n    List<TopicPartition> topicPartitions = ssps.stream().map(ssp -> new TopicPartition(ssp.getStream(), ssp.getPartition().getPartitionId())).collect(Collectors.toList());\r\n    when(mockKafkaConsumer.beginningOffsets(topicPartitions)).thenThrow(new RuntimeException()).thenThrow(new RuntimeException());\r\n    kafkaSystemAdmin.getSSPMetadata(ssps, new ExponentialSleepStrategy(2, 1, 1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testConfigValidations",
  "sourceCode" : "@Test\r\npublic void testConfigValidations() {\r\n    final KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);\r\n    consumer.start();\r\n    // should be no failures\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testFetchThresholdShouldDivideEvenlyAmongPartitions",
  "sourceCode" : "@Test\r\npublic void testFetchThresholdShouldDivideEvenlyAmongPartitions() {\r\n    final KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);\r\n    final int partitionsNum = 50;\r\n    for (int i = 0; i < partitionsNum; i++) {\r\n        consumer.register(new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(i)), \"0\");\r\n    }\r\n    consumer.start();\r\n    Assert.assertEquals(Long.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum, consumer.perPartitionFetchThreshold);\r\n    Assert.assertEquals(Long.valueOf(FETCH_THRESHOLD_BYTES) / 2 / partitionsNum, consumer.perPartitionFetchThresholdBytes);\r\n    consumer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testConsumerRegisterOlderOffsetOfTheSamzaSSP",
  "sourceCode" : "@Test\r\npublic void testConsumerRegisterOlderOffsetOfTheSamzaSSP() {\r\n    KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));\r\n    SystemStreamPartition ssp2 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(2));\r\n    consumer.register(ssp0, \"0\");\r\n    consumer.register(ssp0, \"5\");\r\n    consumer.register(ssp1, \"2\");\r\n    consumer.register(ssp1, \"3\");\r\n    consumer.register(ssp2, \"0\");\r\n    assertEquals(\"0\", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp0)));\r\n    assertEquals(\"2\", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp1)));\r\n    assertEquals(\"0\", consumer.topicPartitionsToOffset.get(KafkaSystemConsumer.toTopicPartition(ssp2)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testFetchThresholdBytes",
  "sourceCode" : "@Test\r\npublic void testFetchThresholdBytes() {\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));\r\n    int partitionsNum = 2;\r\n    // fake size\r\n    int ime0Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum;\r\n    // fake size\r\n    int ime1Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum - 1;\r\n    int ime11Size = 20;\r\n    ByteArraySerializer bytesSerde = new ByteArraySerializer();\r\n    IncomingMessageEnvelope ime0 = new IncomingMessageEnvelope(ssp0, \"0\", bytesSerde.serialize(\"\", \"key0\".getBytes()), bytesSerde.serialize(\"\", \"value0\".getBytes()), ime0Size);\r\n    IncomingMessageEnvelope ime1 = new IncomingMessageEnvelope(ssp1, \"0\", bytesSerde.serialize(\"\", \"key1\".getBytes()), bytesSerde.serialize(\"\", \"value1\".getBytes()), ime1Size);\r\n    IncomingMessageEnvelope ime11 = new IncomingMessageEnvelope(ssp1, \"0\", bytesSerde.serialize(\"\", \"key11\".getBytes()), bytesSerde.serialize(\"\", \"value11\".getBytes()), ime11Size);\r\n    KafkaSystemConsumer consumer = createConsumer(FETCH_THRESHOLD_MSGS, FETCH_THRESHOLD_BYTES);\r\n    consumer.register(ssp0, \"0\");\r\n    consumer.register(ssp1, \"0\");\r\n    consumer.start();\r\n    consumer.messageSink.addMessage(ssp0, ime0);\r\n    // queue for ssp0 should be full now, because we added message of size FETCH_THRESHOLD_MSGS/partitionsNum\r\n    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp0));\r\n    consumer.messageSink.addMessage(ssp1, ime1);\r\n    // queue for ssp1 should be less then full now, because we added message of size (FETCH_THRESHOLD_MSGS/partitionsNum - 1)\r\n    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp1));\r\n    consumer.messageSink.addMessage(ssp1, ime11);\r\n    // queue for ssp1 should full now, because we added message of size 20 on top\r\n    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp1));\r\n    Assert.assertEquals(1, consumer.getNumMessagesInQueue(ssp0));\r\n    Assert.assertEquals(2, consumer.getNumMessagesInQueue(ssp1));\r\n    Assert.assertEquals(ime0Size, consumer.getMessagesSizeInQueue(ssp0));\r\n    Assert.assertEquals(ime1Size + ime11Size, consumer.getMessagesSizeInQueue(ssp1));\r\n    consumer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testFetchThresholdBytesDiabled",
  "sourceCode" : "@Test\r\npublic void testFetchThresholdBytesDiabled() {\r\n    // Pass 0 as fetchThresholdByBytes, which disables checking for limit by size\r\n    SystemStreamPartition ssp0 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));\r\n    SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));\r\n    int partitionsNum = 2;\r\n    // fake size, upto the limit\r\n    int ime0Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum;\r\n    // fake size, below the limit\r\n    int ime1Size = Integer.valueOf(FETCH_THRESHOLD_MSGS) / partitionsNum - 100;\r\n    // event with the second message still below the size limit\r\n    int ime11Size = 20;\r\n    ByteArraySerializer bytesSerde = new ByteArraySerializer();\r\n    IncomingMessageEnvelope ime0 = new IncomingMessageEnvelope(ssp0, \"0\", bytesSerde.serialize(\"\", \"key0\".getBytes()), bytesSerde.serialize(\"\", \"value0\".getBytes()), ime0Size);\r\n    IncomingMessageEnvelope ime1 = new IncomingMessageEnvelope(ssp1, \"0\", bytesSerde.serialize(\"\", \"key1\".getBytes()), bytesSerde.serialize(\"\", \"value1\".getBytes()), ime1Size);\r\n    IncomingMessageEnvelope ime11 = new IncomingMessageEnvelope(ssp1, \"0\", bytesSerde.serialize(\"\", \"key11\".getBytes()), bytesSerde.serialize(\"\", \"value11\".getBytes()), ime11Size);\r\n    // limit by number of messages 4/2 = 2 per partition\r\n    // limit by number of bytes - disabled\r\n    // should disable\r\n    KafkaSystemConsumer consumer = createConsumer(\"4\", \"0\");\r\n    consumer.register(ssp0, \"0\");\r\n    consumer.register(ssp1, \"0\");\r\n    consumer.start();\r\n    consumer.messageSink.addMessage(ssp0, ime0);\r\n    // should be full by size, but not full by number of messages (1 of 2)\r\n    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp0));\r\n    consumer.messageSink.addMessage(ssp1, ime1);\r\n    // not full neither by size nor by messages\r\n    Assert.assertTrue(consumer.messageSink.needsMoreMessages(ssp1));\r\n    consumer.messageSink.addMessage(ssp1, ime11);\r\n    // not full by size, but should be full by messages\r\n    Assert.assertFalse(consumer.messageSink.needsMoreMessages(ssp1));\r\n    Assert.assertEquals(1, consumer.getNumMessagesInQueue(ssp0));\r\n    Assert.assertEquals(2, consumer.getNumMessagesInQueue(ssp1));\r\n    Assert.assertEquals(ime0Size, consumer.getMessagesSizeInQueue(ssp0));\r\n    Assert.assertEquals(ime1Size + ime11Size, consumer.getMessagesSizeInQueue(ssp1));\r\n    consumer.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumer.java",
  "methodName" : "testStartConsumer",
  "sourceCode" : "@Test\r\npublic void testStartConsumer() {\r\n    final Consumer consumer = Mockito.mock(Consumer.class);\r\n    final KafkaConsumerProxyFactory kafkaConsumerProxyFactory = Mockito.mock(KafkaConsumerProxyFactory.class);\r\n    final KafkaSystemConsumerMetrics kafkaSystemConsumerMetrics = new KafkaSystemConsumerMetrics(TEST_SYSTEM, new NoOpMetricsRegistry());\r\n    final SystemStreamPartition testSystemStreamPartition1 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(0));\r\n    final SystemStreamPartition testSystemStreamPartition2 = new SystemStreamPartition(TEST_SYSTEM, TEST_STREAM, new Partition(1));\r\n    final String testOffset = \"1\";\r\n    final KafkaConsumerProxy kafkaConsumerProxy = Mockito.mock(KafkaConsumerProxy.class);\r\n    Mockito.when(kafkaConsumerProxyFactory.create(Mockito.anyObject())).thenReturn(kafkaConsumerProxy);\r\n    Mockito.doNothing().when(consumer).seek(new TopicPartition(TEST_STREAM, 0), 1);\r\n    Mockito.doNothing().when(consumer).seek(new TopicPartition(TEST_STREAM, 1), 1);\r\n    KafkaSystemConsumer kafkaSystemConsumer = new KafkaSystemConsumer(consumer, TEST_SYSTEM, new MapConfig(), TEST_CLIENT_ID, kafkaConsumerProxyFactory, kafkaSystemConsumerMetrics, SystemClock.instance());\r\n    kafkaSystemConsumer.register(testSystemStreamPartition1, testOffset);\r\n    kafkaSystemConsumer.register(testSystemStreamPartition2, testOffset);\r\n    kafkaSystemConsumer.startConsumer();\r\n    Mockito.verify(consumer).seek(new TopicPartition(TEST_STREAM, 0), 1);\r\n    Mockito.verify(consumer).seek(new TopicPartition(TEST_STREAM, 1), 1);\r\n    Mockito.verify(kafkaConsumerProxy).start();\r\n    Mockito.verify(kafkaConsumerProxy).addTopicPartition(testSystemStreamPartition1, Long.valueOf(testOffset));\r\n    Mockito.verify(kafkaConsumerProxy).addTopicPartition(testSystemStreamPartition2, Long.valueOf(testOffset));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemConsumerMetrics.java",
  "methodName" : "testKafkaSystemConsumerMetrics",
  "sourceCode" : "@Test\r\npublic void testKafkaSystemConsumerMetrics() {\r\n    String systemName = \"system\";\r\n    TopicPartition tp1 = new TopicPartition(\"topic1\", 1);\r\n    TopicPartition tp2 = new TopicPartition(\"topic2\", 2);\r\n    String clientName = \"clientName\";\r\n    // record expected values for further comparison\r\n    Map<String, String> expectedValues = new HashMap<>();\r\n    ReadableMetricsRegistry registry = new MetricsRegistryMap();\r\n    KafkaSystemConsumerMetrics metrics = new KafkaSystemConsumerMetrics(systemName, registry);\r\n    // initialize the metrics for the partitions\r\n    metrics.registerTopicPartition(tp1);\r\n    metrics.registerTopicPartition(tp2);\r\n    // initialize the metrics for the host:port\r\n    metrics.registerClientProxy(clientName);\r\n    metrics.setOffsets(tp1, 1001);\r\n    metrics.setOffsets(tp2, 1002);\r\n    expectedValues.put(metrics.offsets().get(tp1).getName(), \"1001\");\r\n    expectedValues.put(metrics.offsets().get(tp2).getName(), \"1002\");\r\n    metrics.incBytesReads(tp1, 10);\r\n    // total 15\r\n    metrics.incBytesReads(tp1, 5);\r\n    expectedValues.put(metrics.bytesRead().get(tp1).getName(), \"15\");\r\n    metrics.incReads(tp1);\r\n    // total 2\r\n    metrics.incReads(tp1);\r\n    expectedValues.put(metrics.reads().get(tp1).getName(), \"2\");\r\n    metrics.setHighWatermarkValue(tp2, 1000);\r\n    // final value 1001\r\n    metrics.setHighWatermarkValue(tp2, 1001);\r\n    expectedValues.put(metrics.highWatermark().get(tp2).getName(), \"1001\");\r\n    metrics.setLagValue(tp1, 200);\r\n    // final value 201\r\n    metrics.setLagValue(tp1, 201);\r\n    expectedValues.put(metrics.lag().get(tp1).getName(), \"201\");\r\n    // broker-bytes-read\r\n    metrics.incClientBytesReads(clientName, 100);\r\n    // total 210\r\n    metrics.incClientBytesReads(clientName, 110);\r\n    expectedValues.put(metrics.clientBytesRead().get(clientName).getName(), \"210\");\r\n    // messages-read\r\n    metrics.incClientReads(clientName);\r\n    // total 2\r\n    metrics.incClientReads(clientName);\r\n    expectedValues.put(metrics.clientReads().get(clientName).getName(), \"2\");\r\n    // \"topic-partitions\"\r\n    metrics.setNumTopicPartitions(clientName, 2);\r\n    // final value 3\r\n    metrics.setNumTopicPartitions(clientName, 3);\r\n    expectedValues.put(metrics.topicPartitions().get(clientName).getName(), \"3\");\r\n    String groupName = metrics.group();\r\n    Assert.assertEquals(groupName, KafkaSystemConsumerMetrics.class.getName());\r\n    Assert.assertEquals(metrics.systemName(), systemName);\r\n    Map<String, Metric> metricMap = registry.getGroup(groupName);\r\n    validate(metricMap, expectedValues);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemFactoryJava.java",
  "methodName" : "testGetIntermediateStreamProperties",
  "sourceCode" : "@Test\r\npublic void testGetIntermediateStreamProperties() {\r\n    Map<String, String> config = new HashMap<>();\r\n    KafkaSystemFactory factory = new KafkaSystemFactory();\r\n    Map<String, Properties> properties = JavaConversions.mapAsJavaMap(factory.getIntermediateStreamProperties(new MapConfig(config)));\r\n    assertTrue(properties.isEmpty());\r\n    // no properties for stream\r\n    config.put(\"streams.test.samza.intermediate\", \"true\");\r\n    //some random config\r\n    config.put(\"streams.test.compression.type\", \"lz4\");\r\n    properties = JavaConversions.mapAsJavaMap(factory.getIntermediateStreamProperties(new MapConfig(config)));\r\n    assertTrue(properties.isEmpty());\r\n    config.put(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name());\r\n    KafkaSystemAdmin admin = createSystemAdmin(SYSTEM(), config);\r\n    StreamSpec spec = new StreamSpec(\"test\", \"test\", SYSTEM(), Collections.singletonMap(\"replication.factor\", \"1\"));\r\n    KafkaStreamSpec kspec = admin.toKafkaSpec(spec);\r\n    Properties prop = kspec.getProperties();\r\n    assertEquals(prop.getProperty(\"retention.ms\"), String.valueOf(KafkaConfig.DEFAULT_RETENTION_MS_FOR_BATCH()));\r\n    assertEquals(prop.getProperty(\"compression.type\"), \"lz4\");\r\n    // replication.factor should be removed from the properties and set on the spec directly\r\n    assertEquals(kspec.getReplicationFactor(), 1);\r\n    assertNull(prop.getProperty(\"replication.factor\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\system\\kafka\\TestKafkaSystemProducerJava.java",
  "methodName" : "testInstantiateProducer",
  "sourceCode" : "@Test\r\npublic void testInstantiateProducer() {\r\n    KafkaSystemProducer ksp = new KafkaSystemProducer(\"SysName\", new ExponentialSleepStrategy(2.0, 200, 10000), new AbstractFunction0<Producer<byte[], byte[]>>() {\r\n\r\n        @Override\r\n        public Producer<byte[], byte[]> apply() {\r\n            return new KafkaProducer<>(new HashMap<String, Object>());\r\n        }\r\n    }, new KafkaSystemProducerMetrics(\"SysName\", new MetricsRegistryMap()), new AbstractFunction0<Object>() {\r\n\r\n        @Override\r\n        public Object apply() {\r\n            return System.currentTimeMillis();\r\n        }\r\n    }, false);\r\n    long now = System.currentTimeMillis();\r\n    assertTrue((Long) ksp.clock().apply() >= now);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\util\\TestKafkaUtil.java",
  "methodName" : "testGetIntegerPartitionKey",
  "sourceCode" : "@Test\r\npublic void testGetIntegerPartitionKey() {\r\n    SystemStream systemStream = new SystemStream(\"system\", \"stream\");\r\n    int partitionKeyInt = 10;\r\n    OutgoingMessageEnvelope outgoingMessageEnvelope = new OutgoingMessageEnvelope(systemStream, partitionKeyInt, \"key\", \"message\");\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(1)).intValue());\r\n    assertEquals(Integer.hashCode(partitionKeyInt) % 3, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(3)).intValue());\r\n    assertEquals(Integer.hashCode(partitionKeyInt) % 8, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(8)).intValue());\r\n    partitionKeyInt = -10;\r\n    outgoingMessageEnvelope = new OutgoingMessageEnvelope(systemStream, partitionKeyInt, \"key\", \"message\");\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(1)).intValue());\r\n    assertEquals(Integer.hashCode(Math.abs(partitionKeyInt)) % 3, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(3)).intValue());\r\n    assertEquals(Integer.hashCode(Math.abs(partitionKeyInt)) % 8, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(8)).intValue());\r\n    // KafkaUtil uses 0 when partition key is MIN_VALUE\r\n    partitionKeyInt = Integer.MIN_VALUE;\r\n    outgoingMessageEnvelope = new OutgoingMessageEnvelope(systemStream, partitionKeyInt, \"key\", \"message\");\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(1)).intValue());\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(3)).intValue());\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(8)).intValue());\r\n    // hash code is 96353\r\n    String partitionKeyString = \"abc\";\r\n    outgoingMessageEnvelope = new OutgoingMessageEnvelope(systemStream, partitionKeyString, \"key\", \"message\");\r\n    assertEquals(0, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(1)).intValue());\r\n    assertEquals(Math.abs(partitionKeyString.hashCode()) % 3, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(3)).intValue());\r\n    assertEquals(Math.abs(partitionKeyString.hashCode()) % 8, KafkaUtil.getIntegerPartitionKey(outgoingMessageEnvelope, partitionInfoList(8)).intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\util\\TestKafkaUtil.java",
  "methodName" : "testGetCheckpointTopic",
  "sourceCode" : "@Test\r\npublic void testGetCheckpointTopic() {\r\n    assertEquals(\"__samza_checkpoint_ver_1_for_myJob_myId\", KafkaUtil.getCheckpointTopic(\"myJob\", \"myId\", new MapConfig()));\r\n    assertEquals(\"__samza_checkpoint_ver_1_for_my-job-with-underscore_my-id-with-underscore\", KafkaUtil.getCheckpointTopic(\"my_job-with_underscore\", \"my_id-with_underscore\", new MapConfig()));\r\n    Config configWithBatchMode = new MapConfig(ImmutableMap.of(ApplicationConfig.APP_MODE, ApplicationConfig.ApplicationMode.BATCH.name(), ApplicationConfig.APP_RUN_ID, \"myRunId\"));\r\n    assertEquals(\"__samza_checkpoint_ver_1_for_myJob_myId-myRunId\", KafkaUtil.getCheckpointTopic(\"myJob\", \"myId\", configWithBatchMode));\r\n    assertEquals(\"__samza_checkpoint_ver_1_for_my-job-with-underscore_my-id-with-underscore-myRunId\", KafkaUtil.getCheckpointTopic(\"my_job-with_underscore\", \"my_id-with_underscore\", configWithBatchMode));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kafka\\src\\test\\java\\org\\apache\\samza\\util\\TestKafkaUtil.java",
  "methodName" : "testToSystemStreamPartition",
  "sourceCode" : "@Test\r\npublic void testToSystemStreamPartition() {\r\n    String system = \"mySystem\", stream = \"myStream\";\r\n    int partition = 3;\r\n    TopicPartition topicPartition = new TopicPartition(stream, partition);\r\n    assertEquals(new SystemStreamPartition(system, stream, new Partition(partition)), KafkaUtil.toSystemStreamPartition(system, topicPartition));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\main\\java\\org\\apache\\samza\\storage\\kv\\LargeMessageSafeStore.java",
  "methodName" : "getStore",
  "sourceCode" : "@VisibleForTesting\r\nKeyValueStore<byte[], byte[]> getStore() {\r\n    return this.store;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testMissingStoreFactory",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testMissingStoreFactory() {\r\n    Config config = new MapConfig();\r\n    callGetStorageEngine(config, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testInvalidCacheSize",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testInvalidCacheSize() {\r\n    Config config = new MapConfig(BASE_CONFIG, ImmutableMap.of(String.format(\"stores.%s.write.cache.batch\", STORE_NAME), \"100\", String.format(\"stores.%s.object.cache.size\", STORE_NAME), \"50\"));\r\n    callGetStorageEngine(config, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testMissingKeySerde",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testMissingKeySerde() {\r\n    Config config = new MapConfig(BASE_CONFIG);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    new MockKeyValueStorageEngineFactory(this.rawKeyValueStore).getStorageEngine(STORE_NAME, this.storeDir, null, this.msgSerde, this.changelogCollector, this.metricsRegistry, null, this.jobContext, this.containerContext, STORE_MODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testMissingValueSerde",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testMissingValueSerde() {\r\n    Config config = new MapConfig(BASE_CONFIG);\r\n    when(this.jobContext.getConfig()).thenReturn(config);\r\n    new MockKeyValueStorageEngineFactory(this.rawKeyValueStore).getStorageEngine(STORE_NAME, this.storeDir, this.keySerde, null, this.changelogCollector, this.metricsRegistry, null, this.jobContext, this.containerContext, STORE_MODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testInMemoryKeyValueStore",
  "sourceCode" : "@Test\r\npublic void testInMemoryKeyValueStore() {\r\n    Config config = new MapConfig(DISABLE_CACHE, ImmutableMap.of(String.format(StorageConfig.FACTORY, STORE_NAME), \"org.apache.samza.storage.kv.inmemory.InMemoryKeyValueStorageEngineFactory\"));\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), false, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    // config has the in-memory key-value factory, but still calling the test factory, so store will be the test store\r\n    assertEquals(this.rawKeyValueStore, serializedKeyValueStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testDurableKeyValueStore",
  "sourceCode" : "@Test\r\npublic void testDurableKeyValueStore() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE, ImmutableMap.of(String.format(StorageConfig.STORE_BACKUP_FACTORIES, STORE_NAME), \"backendFactory,backendFactory2\"));\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, true);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    // config has the in-memory key-value factory, but still calling the test factory, so store will be the test store\r\n    assertEquals(this.rawKeyValueStore, serializedKeyValueStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testRawStoreOnly",
  "sourceCode" : "@Test\r\npublic void testRawStoreOnly() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    assertEquals(this.rawKeyValueStore, serializedKeyValueStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testWithLoggedStore",
  "sourceCode" : "@Test\r\npublic void testWithLoggedStore() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, CHANGELOG_SSP);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, true, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    LoggedStore<?, ?> loggedStore = assertAndCast(serializedKeyValueStore.getStore(), LoggedStore.class);\r\n    // type generics don't match due to wildcard type, but checking reference equality, so type generics don't matter\r\n    // noinspection AssertEqualsBetweenInconvertibleTypes\r\n    assertEquals(this.rawKeyValueStore, loggedStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testWithLoggedStoreAndCachedStore",
  "sourceCode" : "@Test\r\npublic void testWithLoggedStoreAndCachedStore() {\r\n    Config config = new MapConfig(BASE_CONFIG);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, CHANGELOG_SSP);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, true, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    CachedStore<?, ?> cachedStore = assertAndCast(nullSafeKeyValueStore.getStore(), CachedStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(cachedStore.getStore(), SerializedKeyValueStore.class);\r\n    LoggedStore<?, ?> loggedStore = assertAndCast(serializedKeyValueStore.getStore(), LoggedStore.class);\r\n    // type generics don't match due to wildcard type, but checking reference equality, so type generics don't matter\r\n    // noinspection AssertEqualsBetweenInconvertibleTypes\r\n    assertEquals(this.rawKeyValueStore, loggedStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testDisallowLargeMessages",
  "sourceCode" : "@Test\r\npublic void testDisallowLargeMessages() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE, DISALLOW_LARGE_MESSAGES);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    LargeMessageSafeStore largeMessageSafeStore = assertAndCast(serializedKeyValueStore.getStore(), LargeMessageSafeStore.class);\r\n    assertEquals(this.rawKeyValueStore, largeMessageSafeStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testDisallowLargeMessagesWithCache",
  "sourceCode" : "@Test\r\npublic void testDisallowLargeMessagesWithCache() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISALLOW_LARGE_MESSAGES);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    LargeMessageSafeStore largeMessageSafeStore = assertAndCast(serializedKeyValueStore.getStore(), LargeMessageSafeStore.class);\r\n    CachedStore<?, ?> cachedStore = assertAndCast(largeMessageSafeStore.getStore(), CachedStore.class);\r\n    // type generics don't match due to wildcard type, but checking reference equality, so type generics don't matter\r\n    // noinspection AssertEqualsBetweenInconvertibleTypes\r\n    assertEquals(this.rawKeyValueStore, cachedStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testDropLargeMessages",
  "sourceCode" : "@Test\r\npublic void testDropLargeMessages() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE, DROP_LARGE_MESSAGES);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(nullSafeKeyValueStore.getStore(), SerializedKeyValueStore.class);\r\n    LargeMessageSafeStore largeMessageSafeStore = assertAndCast(serializedKeyValueStore.getStore(), LargeMessageSafeStore.class);\r\n    assertEquals(this.rawKeyValueStore, largeMessageSafeStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testDropLargeMessagesWithCache",
  "sourceCode" : "@Test\r\npublic void testDropLargeMessagesWithCache() {\r\n    Config config = new MapConfig(BASE_CONFIG, DROP_LARGE_MESSAGES);\r\n    StorageEngine storageEngine = callGetStorageEngine(config, null);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, false, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    CachedStore<?, ?> cachedStore = assertAndCast(nullSafeKeyValueStore.getStore(), CachedStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(cachedStore.getStore(), SerializedKeyValueStore.class);\r\n    LargeMessageSafeStore largeMessageSafeStore = assertAndCast(serializedKeyValueStore.getStore(), LargeMessageSafeStore.class);\r\n    assertEquals(this.rawKeyValueStore, largeMessageSafeStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestBaseKeyValueStorageEngineFactory.java",
  "methodName" : "testAccessLogStore",
  "sourceCode" : "@Test\r\npublic void testAccessLogStore() {\r\n    Config config = new MapConfig(BASE_CONFIG, DISABLE_CACHE, ACCESS_LOG_ENABLED);\r\n    // AccessLoggedStore requires a changelog SSP\r\n    StorageEngine storageEngine = callGetStorageEngine(config, CHANGELOG_SSP);\r\n    KeyValueStorageEngine<?, ?> keyValueStorageEngine = baseStorageEngineValidation(storageEngine);\r\n    assertStoreProperties(keyValueStorageEngine.getStoreProperties(), true, true, false);\r\n    NullSafeKeyValueStore<?, ?> nullSafeKeyValueStore = assertAndCast(keyValueStorageEngine.getWrapperStore(), NullSafeKeyValueStore.class);\r\n    AccessLoggedStore<?, ?> accessLoggedStore = assertAndCast(nullSafeKeyValueStore.getStore(), AccessLoggedStore.class);\r\n    SerializedKeyValueStore<?, ?> serializedKeyValueStore = assertAndCast(accessLoggedStore.getStore(), SerializedKeyValueStore.class);\r\n    LoggedStore<?, ?> loggedStore = assertAndCast(serializedKeyValueStore.getStore(), LoggedStore.class);\r\n    // type generics don't match due to wildcard type, but checking reference equality, so type generics don't matter\r\n    // noinspection AssertEqualsBetweenInconvertibleTypes\r\n    assertEquals(this.rawKeyValueStore, loggedStore.getStore());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testSmallMessagePut",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePut() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] smallMessage = new byte[32];\r\n    largeMessageSafeKeyValueStore.put(key, smallMessage);\r\n    Mockito.verify(store).put(Matchers.eq(key), Matchers.eq(smallMessage));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testLargeMessagePutWithDropLargeMessageDisabled",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePutWithDropLargeMessageDisabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] largeMessage = new byte[maxMessageSize + 1];\r\n    try {\r\n        largeMessageSafeKeyValueStore.put(key, largeMessage);\r\n        Assert.fail(\"The test case should have failed due to a large message being passed to the changelog, but it didn't.\");\r\n    } catch (RecordTooLargeException e) {\r\n        Mockito.verifyZeroInteractions(store);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testSmallMessagePutAllSuccessWithDropLargeMessageDisabled",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePutAllSuccessWithDropLargeMessageDisabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] smallMessage = new byte[32];\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, smallMessage));\r\n    largeMessageSafeKeyValueStore.putAll(entries);\r\n    Mockito.verify(store).putAll(Matchers.eq(entries));\r\n    Mockito.verify(store, Mockito.never()).put(Matchers.any(byte[].class), Matchers.any(byte[].class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testLargeMessagePutAllFailureWithDropLargeMessageDisabled",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePutAllFailureWithDropLargeMessageDisabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] largeMessage = new byte[maxMessageSize + 1];\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, largeMessage));\r\n    try {\r\n        largeMessageSafeKeyValueStore.putAll(entries);\r\n        Assert.fail(\"The test case should have failed due to a large message being passed to the changelog, but it didn't.\");\r\n    } catch (RecordTooLargeException e) {\r\n        Mockito.verifyZeroInteractions(store);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testSmallMessagePutWithSerdeAndDropLargeMessageDisabled",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePutWithSerdeAndDropLargeMessageDisabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    Serde<Long> longSerde = new LongSerde();\r\n    long longObj = 1000L;\r\n    byte[] key = longSerde.toBytes(longObj);\r\n    JsonSerdeV2<Map<String, Object>> jsonSerde = new JsonSerdeV2<>();\r\n    Map<String, Object> obj = new HashMap<>();\r\n    obj.put(\"jack\", \"jill\");\r\n    obj.put(\"john\", 2);\r\n    byte[] smallMessage = jsonSerde.toBytes(obj);\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, smallMessage));\r\n    largeMessageSafeKeyValueStore.putAll(entries);\r\n    Mockito.verify(store).putAll(Matchers.eq(entries));\r\n    Mockito.verify(store, Mockito.never()).put(Matchers.any(byte[].class), Matchers.any(byte[].class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testLargeMessagePutWithSerdeAndDropLargeMessageDisabled",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePutWithSerdeAndDropLargeMessageDisabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, false, maxMessageSize);\r\n    Serde<Long> longSerde = new LongSerde();\r\n    long longObj = 1000L;\r\n    byte[] key = longSerde.toBytes(longObj);\r\n    Serde<String> stringSerde = new StringSerde();\r\n    String largeString = StringUtils.repeat(\"a\", maxMessageSize + 1);\r\n    byte[] largeMessage = stringSerde.toBytes(largeString);\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, largeMessage));\r\n    try {\r\n        largeMessageSafeKeyValueStore.putAll(entries);\r\n    } catch (RecordTooLargeException e) {\r\n        Mockito.verifyZeroInteractions(store);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testSmallMessagePutSuccessWithDropLargeMessageEnabled",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePutSuccessWithDropLargeMessageEnabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, true, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] smallMessage = new byte[32];\r\n    largeMessageSafeKeyValueStore.put(key, smallMessage);\r\n    Mockito.verify(store).put(Matchers.eq(key), Matchers.eq(smallMessage));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testLargeMessagePutSuccessWithDropLargeMessageEnabled",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePutSuccessWithDropLargeMessageEnabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, true, maxMessageSize);\r\n    byte[] key = new byte[16];\r\n    byte[] largeMessage = new byte[maxMessageSize + 1];\r\n    largeMessageSafeKeyValueStore.put(key, largeMessage);\r\n    Mockito.verifyZeroInteractions(store);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeStore.java",
  "methodName" : "testPutAllSuccessWithDropLargeMessageEnabled",
  "sourceCode" : "@Test\r\npublic void testPutAllSuccessWithDropLargeMessageEnabled() {\r\n    LargeMessageSafeStore largeMessageSafeKeyValueStore = new LargeMessageSafeStore(store, storeName, true, maxMessageSize);\r\n    byte[] key1 = new byte[16];\r\n    byte[] largeMessage = new byte[maxMessageSize + 1];\r\n    byte[] key2 = new byte[8];\r\n    byte[] smallMessage = new byte[1];\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    Entry<byte[], byte[]> largeMessageEntry = new Entry<>(key1, largeMessage);\r\n    Entry<byte[], byte[]> smallMessageEntry = new Entry<>(key2, smallMessage);\r\n    entries.add(largeMessageEntry);\r\n    entries.add(smallMessageEntry);\r\n    largeMessageSafeKeyValueStore.putAll(entries);\r\n    entries.remove(largeMessageEntry);\r\n    Mockito.verify(store).putAll(Matchers.eq(entries));\r\n    Mockito.verify(store, Mockito.never()).put(Matchers.any(byte[].class), Matchers.any(byte[].class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableProvider.java",
  "methodName" : "testInit",
  "sourceCode" : "@Test\r\npublic void testInit() {\r\n    Context context = mock(Context.class);\r\n    JobContext jobContext = mock(JobContext.class);\r\n    when(context.getJobContext()).thenReturn(jobContext);\r\n    when(jobContext.getConfig()).thenReturn(new MapConfig());\r\n    ContainerContext containerContext = mock(ContainerContext.class);\r\n    when(context.getContainerContext()).thenReturn(containerContext);\r\n    when(containerContext.getContainerMetricsRegistry()).thenReturn(new NoOpMetricsRegistry());\r\n    TaskContext taskContext = mock(TaskContext.class);\r\n    when(context.getTaskContext()).thenReturn(taskContext);\r\n    when(taskContext.getStore(any())).thenReturn(mock(KeyValueStore.class));\r\n    TableProvider tableProvider = createTableProvider(\"t1\");\r\n    tableProvider.init(context);\r\n    Assert.assertNotNull(tableProvider.getTable());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableProvider.java",
  "methodName" : "testInitFail",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testInitFail() {\r\n    TableProvider tableProvider = createTableProvider(\"t1\");\r\n    Assert.assertNotNull(tableProvider.getTable());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableRead.java",
  "methodName" : "testGet",
  "sourceCode" : "@Test\r\npublic void testGet() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    Assert.assertEquals(\"v1\", table.get(\"k1\"));\r\n    Assert.assertEquals(\"v2\", table.getAsync(\"k2\").get());\r\n    Assert.assertNull(table.get(\"k3\"));\r\n    verify(kvStore, times(3)).get(any());\r\n    Assert.assertEquals(3, numGets.getCount());\r\n    Assert.assertEquals(1, numMissedLookups.getCount());\r\n    Assert.assertTrue(getNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, numGetAlls.getCount());\r\n    Assert.assertEquals(0, getAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, getCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableRead.java",
  "methodName" : "testGetAll",
  "sourceCode" : "@Test\r\npublic void testGetAll() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    Assert.assertEquals(values, table.getAll(keys));\r\n    Assert.assertEquals(values, table.getAllAsync(keys).get());\r\n    verify(kvStore, times(2)).getAll(any());\r\n    Assert.assertEquals(Collections.emptyMap(), table.getAll(Collections.emptyList()));\r\n    Assert.assertEquals(2, numMissedLookups.getCount());\r\n    Assert.assertEquals(3, numGetAlls.getCount());\r\n    Assert.assertTrue(getAllNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, numGets.getCount());\r\n    Assert.assertEquals(0, getNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, getCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableRead.java",
  "methodName" : "testTimerDisabled",
  "sourceCode" : "@Test\r\npublic void testTimerDisabled() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(true);\r\n    table.get(\"\");\r\n    table.getAsync(\"\").get();\r\n    table.getAll(keys);\r\n    table.getAllAsync(keys).get();\r\n    Assert.assertEquals(2, numGets.getCount());\r\n    Assert.assertEquals(4, numMissedLookups.getCount());\r\n    Assert.assertEquals(2, numGetAlls.getCount());\r\n    Assert.assertEquals(0, getNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, getAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, getCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testPut",
  "sourceCode" : "@Test\r\npublic void testPut() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    table.put(\"k1\", \"v1\");\r\n    table.putAsync(\"k2\", \"v2\").get();\r\n    table.putAsync(\"k3\", null).get();\r\n    verify(kvStore, times(2)).put(any(), any());\r\n    verify(kvStore, times(1)).delete(any());\r\n    Assert.assertEquals(2, numPuts.getCount());\r\n    Assert.assertEquals(1, numDeletes.getCount());\r\n    Assert.assertTrue(putNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertTrue(deleteNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, putAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, flushNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, numPutAlls.getCount());\r\n    Assert.assertEquals(0, numDeleteAlls.getCount());\r\n    Assert.assertEquals(0, numFlushes.getCount());\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testPutAll",
  "sourceCode" : "@Test\r\npublic void testPutAll() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    List<Entry> entries = Arrays.asList(new Entry(\"k1\", \"v1\"), new Entry(\"k2\", null));\r\n    table.putAll(entries);\r\n    table.putAllAsync(entries).get();\r\n    verify(kvStore, times(2)).putAll(any());\r\n    verify(kvStore, times(2)).deleteAll(any());\r\n    Assert.assertEquals(2, numPutAlls.getCount());\r\n    Assert.assertEquals(2, numDeleteAlls.getCount());\r\n    Assert.assertTrue(putAllNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertTrue(deleteAllNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, putNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, flushNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, numPuts.getCount());\r\n    Assert.assertEquals(0, numDeletes.getCount());\r\n    Assert.assertEquals(0, numFlushes.getCount());\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testDelete",
  "sourceCode" : "@Test\r\npublic void testDelete() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    table.delete(\"\");\r\n    table.deleteAsync(\"\").get();\r\n    verify(kvStore, times(2)).delete(any());\r\n    Assert.assertEquals(2, numDeletes.getCount());\r\n    Assert.assertTrue(deleteNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, putNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, flushNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, numPuts.getCount());\r\n    Assert.assertEquals(0, numPutAlls.getCount());\r\n    Assert.assertEquals(0, numDeleteAlls.getCount());\r\n    Assert.assertEquals(0, numFlushes.getCount());\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testDeleteAll",
  "sourceCode" : "@Test\r\npublic void testDeleteAll() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    table.deleteAll(Collections.emptyList());\r\n    table.deleteAllAsync(Collections.emptyList()).get();\r\n    verify(kvStore, times(2)).deleteAll(any());\r\n    Assert.assertEquals(2, numDeleteAlls.getCount());\r\n    Assert.assertTrue(deleteAllNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, putNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, flushNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, numPuts.getCount());\r\n    Assert.assertEquals(0, numPutAlls.getCount());\r\n    Assert.assertEquals(0, numDeletes.getCount());\r\n    Assert.assertEquals(0, numFlushes.getCount());\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    ReadWriteUpdateTable table = createTable(false);\r\n    table.flush();\r\n    table.flush();\r\n    // Note: store.flush() is NOT called here\r\n    verify(kvStore, times(0)).flush();\r\n    Assert.assertEquals(2, numFlushes.getCount());\r\n    Assert.assertTrue(flushNs.getSnapshot().getAverage() > 0);\r\n    Assert.assertEquals(0, putNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, numPuts.getCount());\r\n    Assert.assertEquals(0, numPutAlls.getCount());\r\n    Assert.assertEquals(0, numDeletes.getCount());\r\n    Assert.assertEquals(0, numDeleteAlls.getCount());\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLocalTableWrite.java",
  "methodName" : "testTimerDisabled",
  "sourceCode" : "@Test\r\npublic void testTimerDisabled() throws Exception {\r\n    ReadWriteUpdateTable table = createTable(true);\r\n    table.put(\"\", \"\");\r\n    table.putAsync(\"\", \"\").get();\r\n    table.putAll(Collections.emptyList());\r\n    table.putAllAsync(Collections.emptyList()).get();\r\n    table.delete(\"\");\r\n    table.deleteAsync(\"\").get();\r\n    table.deleteAll(Collections.emptyList());\r\n    table.deleteAllAsync(Collections.emptyList()).get();\r\n    table.flush();\r\n    Assert.assertEquals(1, numFlushes.getCount());\r\n    Assert.assertEquals(2, numPuts.getCount());\r\n    Assert.assertEquals(0, numPutAlls.getCount());\r\n    Assert.assertEquals(2, numDeletes.getCount());\r\n    Assert.assertEquals(2, numDeleteAlls.getCount());\r\n    Assert.assertEquals(0, flushNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteAllNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, putCallbackNs.getSnapshot().getAverage(), 0.001);\r\n    Assert.assertEquals(0, deleteCallbackNs.getSnapshot().getAverage(), 0.001);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseBucketRegistry.java",
  "methodName" : "testOpenBuckets",
  "sourceCode" : "/**\r\n * This unit test uses CouchbaseBucketRegistry to register two mocked buckets. It tests:\r\n * 1. Calling registry.getBucket with same bucketName and clusterNodes should return same Bucket instance\r\n * 2. Calling registry.getBucket with different bucketNames should return different Bucket instances\r\n */\r\n@Test\r\npublic void testOpenBuckets() {\r\n    String bucketName1 = \"bucket1\";\r\n    String bucketName2 = \"bucket2\";\r\n    List<String> clusterNodes = Arrays.asList(\"cluster\");\r\n    CouchbaseEnvironmentConfigs configs = new CouchbaseEnvironmentConfigs();\r\n    CouchbaseCluster cluster = mock(CouchbaseCluster.class);\r\n    when(cluster.openBucket(bucketName1)).thenReturn(mock(Bucket.class));\r\n    when(cluster.openBucket(bucketName2)).thenReturn(mock(Bucket.class));\r\n    mockStatic(CouchbaseCluster.class);\r\n    when(CouchbaseCluster.create(any(CouchbaseEnvironment.class), anyListOf(String.class))).thenReturn(cluster);\r\n    CouchbaseBucketRegistry registry = new CouchbaseBucketRegistry();\r\n    Bucket bucket1 = registry.getBucket(bucketName1, clusterNodes, configs);\r\n    Bucket bucket1Copy = registry.getBucket(bucketName1, clusterNodes, configs);\r\n    Bucket bucket2 = registry.getBucket(bucketName2, clusterNodes, configs);\r\n    assertEquals(bucket1, bucket1Copy);\r\n    assertNotEquals(bucket1, bucket2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseBucketRegistry.java",
  "methodName" : "testOpenSameBucketNameFromDifferentClusters",
  "sourceCode" : "/**\r\n * This unit test uses CouchbaseBucketRegistry to register two mocked buckets with same name but in different clusters.\r\n * Calling registry.getBucket with same bucketName but different clusterNodes should return different Bucket instances.\r\n */\r\n@Test\r\npublic void testOpenSameBucketNameFromDifferentClusters() {\r\n    String bucketName = \"bucket\";\r\n    List<String> clusterNodes1 = Arrays.asList(\"cluster1\");\r\n    List<String> clusterNodes2 = Arrays.asList(\"cluster2\");\r\n    CouchbaseEnvironmentConfigs configs = new CouchbaseEnvironmentConfigs();\r\n    CouchbaseCluster cluster1 = mock(CouchbaseCluster.class);\r\n    CouchbaseCluster cluster2 = mock(CouchbaseCluster.class);\r\n    when(cluster1.openBucket(bucketName)).thenReturn(mock(Bucket.class));\r\n    when(cluster2.openBucket(bucketName)).thenReturn(mock(Bucket.class));\r\n    mockStatic(CouchbaseCluster.class);\r\n    when(CouchbaseCluster.create(any(CouchbaseEnvironment.class), eq(clusterNodes1))).thenReturn(cluster1);\r\n    when(CouchbaseCluster.create(any(CouchbaseEnvironment.class), eq(clusterNodes2))).thenReturn(cluster2);\r\n    CouchbaseBucketRegistry registry = new CouchbaseBucketRegistry();\r\n    Bucket bucketInCluster1 = registry.getBucket(bucketName, clusterNodes1, configs);\r\n    Bucket bucketInCluster2 = registry.getBucket(bucketName, clusterNodes2, configs);\r\n    assertNotEquals(bucketInCluster1, bucketInCluster2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseBucketRegistry.java",
  "methodName" : "testCloseBucket",
  "sourceCode" : "/**\r\n * This unit test simulates 10 tasks using the same bucket. Each task will call registry.getBucket once. Then\r\n * each task will also call registry.closeBucket once. After that, registry.closeBucket should return false if we\r\n * close the bucket one more time. And the bucket should have already been closed.\r\n */\r\n@Test\r\npublic void testCloseBucket() {\r\n    String bucketName = \"bucket\";\r\n    List<String> clusterNodes = Arrays.asList(\"cluster\");\r\n    CouchbaseEnvironmentConfigs configs = new CouchbaseEnvironmentConfigs();\r\n    CouchbaseCluster cluster = mock(CouchbaseCluster.class);\r\n    Bucket bucket = mock(Bucket.class);\r\n    when(bucket.close()).thenReturn(true).thenReturn(false);\r\n    when(cluster.openBucket(bucketName)).thenReturn(bucket);\r\n    when(cluster.disconnect()).thenReturn(true).thenReturn(false);\r\n    mockStatic(CouchbaseCluster.class);\r\n    when(CouchbaseCluster.create(any(CouchbaseEnvironment.class), eq(clusterNodes))).thenReturn(cluster);\r\n    CouchbaseBucketRegistry registry = new CouchbaseBucketRegistry();\r\n    int numOfThreads = 10;\r\n    for (int i = 0; i < numOfThreads; i++) {\r\n        registry.getBucket(bucketName, clusterNodes, configs);\r\n    }\r\n    for (int i = 0; i < numOfThreads; i++) {\r\n        assertTrue(registry.closeBucket(bucketName, clusterNodes));\r\n    }\r\n    // Close one more time. Should return false.\r\n    assertFalse(registry.closeBucket(bucketName, clusterNodes));\r\n    // Bucket should has been closed\r\n    assertFalse(bucket.close());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseBucketRegistry.java",
  "methodName" : "testCloseTwoBucketsInSameCluster",
  "sourceCode" : "/**\r\n * This unit test simulates closing two buckets within one cluster. The cluster should only be disconnected when all\r\n * buckets has been closed.\r\n */\r\n@Test\r\npublic void testCloseTwoBucketsInSameCluster() {\r\n    String bucketName1 = \"bucket1\";\r\n    String bucketName2 = \"bucket2\";\r\n    List<String> clusterNodes = Arrays.asList(\"cluster\");\r\n    CouchbaseEnvironmentConfigs configs = new CouchbaseEnvironmentConfigs();\r\n    CouchbaseCluster cluster = mock(CouchbaseCluster.class);\r\n    Bucket bucket1 = mock(Bucket.class);\r\n    Bucket bucket2 = mock(Bucket.class);\r\n    when(bucket1.close()).thenReturn(true).thenReturn(false);\r\n    when(bucket2.close()).thenReturn(true).thenReturn(false);\r\n    when(cluster.openBucket(bucketName1)).thenReturn(bucket1);\r\n    when(cluster.openBucket(bucketName2)).thenReturn(bucket2);\r\n    when(cluster.disconnect()).thenReturn(true).thenReturn(false);\r\n    mockStatic(CouchbaseCluster.class);\r\n    when(CouchbaseCluster.create(any(CouchbaseEnvironment.class), eq(clusterNodes))).thenReturn(cluster);\r\n    CouchbaseBucketRegistry registry = new CouchbaseBucketRegistry();\r\n    registry.getBucket(bucketName1, clusterNodes, configs);\r\n    registry.getBucket(bucketName2, clusterNodes, configs);\r\n    assertTrue(registry.closeBucket(bucketName1, clusterNodes));\r\n    assertTrue(registry.closeBucket(bucketName2, clusterNodes));\r\n    // Cluster should have been disconnected. Should return false.\r\n    assertFalse(cluster.disconnect());\r\n    // Buckets should have been closed. Should return false.\r\n    assertFalse(cluster.disconnect());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testConstructorInvalidBucketName",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidBucketName() {\r\n    new CouchbaseTableReadFunction<>(\"\", String.class, DEFAULT_CLUSTER_NODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testConstructorInvalidClusterNodes",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidClusterNodes() {\r\n    new CouchbaseTableReadFunction<>(DEFAULT_BUCKET_NAME, String.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testConstructorInvalidValueClass",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidValueClass() {\r\n    new CouchbaseTableReadFunction<>(DEFAULT_BUCKET_NAME, null, DEFAULT_CLUSTER_NODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testIsNotRetriable",
  "sourceCode" : "@Test\r\npublic void testIsNotRetriable() {\r\n    CouchbaseTableReadFunction readFunction = createAndInit();\r\n    assertFalse(readFunction.isRetriable(null));\r\n    assertFalse(readFunction.isRetriable(new SamzaException()));\r\n    assertFalse(readFunction.isRetriable(new SamzaException(\"\", new IllegalArgumentException())));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testIsRetriable",
  "sourceCode" : "@Test\r\npublic void testIsRetriable() {\r\n    CouchbaseTableReadFunction readFunction = createAndInit();\r\n    assertTrue(readFunction.isRetriable(new TemporaryFailureException()));\r\n    assertTrue(readFunction.isRetriable(new TemporaryLockFailureException()));\r\n    assertTrue(readFunction.isRetriable(new SamzaException(new TemporaryLockFailureException())));\r\n    assertTrue(readFunction.isRetriable(new SamzaException(new TemporaryFailureException())));\r\n    assertTrue(readFunction.isRetriable(new RuntimeException(new SamzaException(new RuntimeException(new TemporaryFailureException())))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncNullKey",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testGetAsyncNullKey() {\r\n    CouchbaseTableReadFunction readFunction = createAndInit();\r\n    readFunction.getAsync(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncException",
  "sourceCode" : "@Test\r\npublic void testGetAsyncException() {\r\n    String key = \"throwExceptionKey\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.error(new CouchbaseException()));\r\n    assertTrue(readFunction.getAsync(key).isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncFailedToDeserialize",
  "sourceCode" : "@Test\r\npublic void testGetAsyncFailedToDeserialize() {\r\n    String key = \"key\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(String.class, new StringSerde(), bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(BinaryDocument.create(key, null)));\r\n    assertTrue(readFunction.getAsync(key).isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncNullValue",
  "sourceCode" : "@Test\r\npublic void testGetAsyncNullValue() throws Exception {\r\n    String key = \"NonExistingKey\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(String.class, new StringSerde(), bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.empty());\r\n    assertNull(readFunction.getAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncNonExistingKey",
  "sourceCode" : "@Test\r\npublic void testGetAsyncNonExistingKey() throws Exception {\r\n    String key = \"NonExistingKey\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(String.class, new StringSerde(), bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.empty());\r\n    assertNull(readFunction.getAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncStringValue",
  "sourceCode" : "@Test\r\npublic void testGetAsyncStringValue() throws Exception {\r\n    String key = \"key\";\r\n    String value = \"value\";\r\n    StringSerde stringSerde = new StringSerde();\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(String.class, stringSerde, bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(BinaryDocument.create(key, Unpooled.wrappedBuffer(stringSerde.toBytes(value)))));\r\n    assertEquals(value, readFunction.getAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableReadFunction.java",
  "methodName" : "testGetAsyncJsonObjectValue",
  "sourceCode" : "@Test\r\npublic void testGetAsyncJsonObjectValue() throws Exception {\r\n    String key = \"key\";\r\n    JsonObject value = JsonObject.fromJson(\"{\\\"id\\\": 1}\");\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableReadFunction readFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.get(eq(key), anyObject(), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(JsonDocument.create(key, value)));\r\n    assertEquals(value, readFunction.getAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testConstructorInvalidBucketName",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidBucketName() {\r\n    new CouchbaseTableWriteFunction<>(\"\", String.class, DEFAULT_CLUSTER_NODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testConstructorInvalidClusterNodes",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidClusterNodes() {\r\n    new CouchbaseTableWriteFunction<>(DEFAULT_BUCKET_NAME, String.class);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testConstructorInvalidValueClass",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testConstructorInvalidValueClass() {\r\n    new CouchbaseTableWriteFunction<>(DEFAULT_BUCKET_NAME, null, DEFAULT_CLUSTER_NODE);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testIsNotRetriable",
  "sourceCode" : "@Test\r\npublic void testIsNotRetriable() {\r\n    CouchbaseTableWriteFunction writeFunction = createAndInit();\r\n    assertFalse(writeFunction.isRetriable(null));\r\n    assertFalse(writeFunction.isRetriable(new SamzaException()));\r\n    assertFalse(writeFunction.isRetriable(new SamzaException(\"\", new IllegalArgumentException())));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testIsRetriable",
  "sourceCode" : "@Test\r\npublic void testIsRetriable() {\r\n    CouchbaseTableWriteFunction writeFunction = createAndInit();\r\n    assertTrue(writeFunction.isRetriable(new TemporaryFailureException()));\r\n    assertTrue(writeFunction.isRetriable(new TemporaryLockFailureException()));\r\n    assertTrue(writeFunction.isRetriable(new SamzaException(new TemporaryLockFailureException())));\r\n    assertTrue(writeFunction.isRetriable(new SamzaException(new TemporaryFailureException())));\r\n    assertTrue(writeFunction.isRetriable(new RuntimeException(new SamzaException(new RuntimeException(new TemporaryFailureException())))));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncNullKey",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testPutAsyncNullKey() {\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit();\r\n    writeFunction.putAsync(null, JsonObject.create());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncKeyWithSpaces",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testPutAsyncKeyWithSpaces() {\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit();\r\n    writeFunction.putAsync(\"k e y\", JsonObject.create());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncNullValue",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testPutAsyncNullValue() {\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit();\r\n    writeFunction.putAsync(\"key\", null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncException",
  "sourceCode" : "@Test\r\npublic void testPutAsyncException() {\r\n    String key = \"throwExceptionKey\";\r\n    JsonObject value = JsonObject.create();\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.upsert(any(Document.class), anyLong(), any(TimeUnit.class))).thenReturn(Observable.error(new CouchbaseException()));\r\n    assertTrue(writeFunction.putAsync(key, value).isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncJsonObjectValue",
  "sourceCode" : "@Test\r\npublic void testPutAsyncJsonObjectValue() throws Exception {\r\n    String key = \"key\";\r\n    JsonObject value = JsonObject.fromJson(\"{\\\"id\\\": 1}\");\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.upsert(any(Document.class), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(null));\r\n    assertNull(writeFunction.putAsync(key, value).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testPutAsyncStringValue",
  "sourceCode" : "@Test\r\npublic void testPutAsyncStringValue() throws Exception {\r\n    String key = \"key\";\r\n    String value = \"value\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<String> writeFunction = createAndInit(String.class, new StringSerde(), bucket, asyncBucket);\r\n    when(asyncBucket.upsert(any(Document.class), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(null));\r\n    assertNull(writeFunction.putAsync(key, value).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testDeleteAsyncNullKey",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testDeleteAsyncNullKey() {\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit();\r\n    writeFunction.deleteAsync(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testDeleteAsyncException",
  "sourceCode" : "@Test\r\npublic void testDeleteAsyncException() {\r\n    String key = \"throwExceptionKey\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.remove(eq(key), anyLong(), any(TimeUnit.class))).thenReturn(Observable.error(new CouchbaseException()));\r\n    assertTrue(writeFunction.deleteAsync(key).isCompletedExceptionally());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testDeleteAsyncJsonObjectValue",
  "sourceCode" : "@Test\r\npublic void testDeleteAsyncJsonObjectValue() throws Exception {\r\n    String key = \"key\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<JsonObject> writeFunction = createAndInit(bucket, asyncBucket);\r\n    when(asyncBucket.remove(eq(key), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(null));\r\n    assertNull(writeFunction.deleteAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-couchbase\\src\\test\\java\\org\\apache\\samza\\table\\remote\\couchbase\\TestCouchbaseTableWriteFunction.java",
  "methodName" : "testDeleteAsyncStringValue",
  "sourceCode" : "@Test\r\npublic void testDeleteAsyncStringValue() throws Exception {\r\n    String key = \"key\";\r\n    Bucket bucket = mock(Bucket.class);\r\n    AsyncBucket asyncBucket = mock(AsyncBucket.class);\r\n    CouchbaseTableWriteFunction<String> writeFunction = createAndInit(String.class, new StringSerde(), bucket, asyncBucket);\r\n    when(asyncBucket.remove(eq(key), anyLong(), any(TimeUnit.class))).thenReturn(Observable.just(null));\r\n    assertNull(writeFunction.deleteAsync(key).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testGet",
  "sourceCode" : "@Test\r\npublic void testGet() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, 1), value(OTHER_VALUE_PREFIX, 1));\r\n    assertArrayEquals(value(0), this.inMemoryKeyValueStore.get(key(0)));\r\n    assertArrayEquals(value(OTHER_VALUE_PREFIX, 1), this.inMemoryKeyValueStore.get(key(OTHER_KEY_PREFIX, 1)));\r\n    verify(this.getsCounter, times(2)).inc();\r\n    verify(this.bytesReadCounter).inc(DEFAULT_VALUE_LENGTH);\r\n    verify(this.bytesReadCounter).inc(value(OTHER_VALUE_PREFIX, 1).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testGetEmpty",
  "sourceCode" : "@Test\r\npublic void testGetEmpty() {\r\n    assertNull(this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.getsCounter).inc();\r\n    verifyZeroInteractions(this.bytesReadCounter);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testGetAfterDelete",
  "sourceCode" : "@Test\r\npublic void testGetAfterDelete() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.delete(key(0));\r\n    assertNull(this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.getsCounter).inc();\r\n    verifyZeroInteractions(this.bytesReadCounter);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPut",
  "sourceCode" : "@Test\r\npublic void testPut() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, 1), value(OTHER_VALUE_PREFIX, 1));\r\n    assertArrayEquals(value(0), this.inMemoryKeyValueStore.get(key(0)));\r\n    assertArrayEquals(value(OTHER_VALUE_PREFIX, 1), this.inMemoryKeyValueStore.get(key(OTHER_KEY_PREFIX, 1)));\r\n    verify(this.putsCounter, times(2)).inc();\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n    verify(this.bytesWrittenCounter).inc(key(OTHER_KEY_PREFIX, 1).length + value(OTHER_VALUE_PREFIX, 1).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutExistingEntry",
  "sourceCode" : "@Test\r\npublic void testPutExistingEntry() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.put(key(0), value(OTHER_VALUE_PREFIX, 1));\r\n    assertArrayEquals(value(OTHER_VALUE_PREFIX, 1), this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.putsCounter, times(2)).inc();\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH + value(OTHER_VALUE_PREFIX, 1).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutEmptyValue",
  "sourceCode" : "@Test\r\npublic void testPutEmptyValue() {\r\n    byte[] emptyValue = new byte[0];\r\n    this.inMemoryKeyValueStore.put(key(0), emptyValue);\r\n    assertEquals(0, this.inMemoryKeyValueStore.get(key(0)).length);\r\n    verify(this.putsCounter).inc();\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutNull",
  "sourceCode" : "@Test\r\npublic void testPutNull() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.put(key(0), null);\r\n    assertNull(this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.putsCounter, times(2)).inc();\r\n    verify(this.deletesCounter).inc();\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutAll",
  "sourceCode" : "@Test\r\npublic void testPutAll() {\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        entries.add(new Entry<>(key(i), value(i)));\r\n    }\r\n    this.inMemoryKeyValueStore.putAll(entries);\r\n    for (int i = 0; i < 10; i++) {\r\n        assertArrayEquals(value(i), this.inMemoryKeyValueStore.get(key(i)));\r\n    }\r\n    verify(this.putsCounter, times(10)).inc();\r\n    // when using key(i) and value(i), the byte[] lengths will be the same\r\n    verify(this.bytesWrittenCounter, times(10)).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutAllUpdate",
  "sourceCode" : "@Test\r\npublic void testPutAllUpdate() {\r\n    // check that an existing value is overridden\r\n    this.inMemoryKeyValueStore.put(key(0), value(OTHER_VALUE_PREFIX, 0));\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        entries.add(new Entry<>(key(i), value(i)));\r\n    }\r\n    this.inMemoryKeyValueStore.putAll(entries);\r\n    for (int i = 0; i < 10; i++) {\r\n        assertArrayEquals(value(i), this.inMemoryKeyValueStore.get(key(i)));\r\n    }\r\n    // 1 time for initial value to be overridden, 10 times for \"regular\" puts\r\n    verify(this.putsCounter, times(11)).inc();\r\n    // for initial value which is overridden\r\n    verify(this.bytesWrittenCounter).inc(DEFAULT_KEY_LENGTH + value(OTHER_VALUE_PREFIX, 0).length);\r\n    // when using key(i) and value(i), the byte[] lengths will be the same\r\n    verify(this.bytesWrittenCounter, times(10)).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testPutAllWithNull",
  "sourceCode" : "@Test\r\npublic void testPutAllWithNull() {\r\n    List<Entry<byte[], byte[]>> entries = new ArrayList<>();\r\n    for (int i = 0; i < 10; i++) {\r\n        entries.add(new Entry<>(key(i), value(i)));\r\n    }\r\n    this.inMemoryKeyValueStore.putAll(entries);\r\n    List<Entry<byte[], byte[]>> deleteEntries = new ArrayList<>();\r\n    for (int i = 0; i < 3; i++) {\r\n        deleteEntries.add(new Entry<>(key(i), null));\r\n    }\r\n    this.inMemoryKeyValueStore.putAll(deleteEntries);\r\n    for (int i = 0; i < 10; i++) {\r\n        if (i < 3) {\r\n            assertNull(this.inMemoryKeyValueStore.get(key(i)));\r\n        } else {\r\n            assertArrayEquals(value(i), this.inMemoryKeyValueStore.get(key(i)));\r\n        }\r\n    }\r\n    // 10 times for \"regular\" puts, 3 times for deletion puts\r\n    verify(this.putsCounter, times(13)).inc();\r\n    // 10 \"regular\" puts all have same size for key/value\r\n    verify(this.bytesWrittenCounter, times(10)).inc(DEFAULT_KEY_LENGTH + DEFAULT_VALUE_LENGTH);\r\n    verifyNoMoreInteractions(this.bytesWrittenCounter);\r\n    verify(this.deletesCounter, times(3)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testDelete",
  "sourceCode" : "@Test\r\npublic void testDelete() {\r\n    this.inMemoryKeyValueStore.put(key(0), value(0));\r\n    this.inMemoryKeyValueStore.delete(key(0));\r\n    assertNull(this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.deletesCounter, times(1)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testDeleteNonExistentEntry",
  "sourceCode" : "@Test\r\npublic void testDeleteNonExistentEntry() {\r\n    this.inMemoryKeyValueStore.delete(key(0));\r\n    assertNull(this.inMemoryKeyValueStore.get(key(0)));\r\n    verify(this.deletesCounter, times(1)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testRange",
  "sourceCode" : "@Test\r\npublic void testRange() {\r\n    Counter rangesCounter = mock(Counter.class);\r\n    when(this.keyValueStoreMetrics.ranges()).thenReturn(rangesCounter);\r\n    for (int i = 0; i < 10; i++) {\r\n        this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, i), value(OTHER_VALUE_PREFIX, i));\r\n        this.inMemoryKeyValueStore.put(key(i), value(i));\r\n    }\r\n    KeyValueIterator<byte[], byte[]> range = this.inMemoryKeyValueStore.range(key(0), key(5));\r\n    for (int i = 0; i < 5; i++) {\r\n        assertEntryEquals(key(i), value(i), range.next());\r\n    }\r\n    assertFalse(range.hasNext());\r\n    verify(rangesCounter).inc();\r\n    // key size increments: key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5)).inc(DEFAULT_KEY_LENGTH);\r\n    // value size increments: value(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5)).inc(DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testRangeWithUpdate",
  "sourceCode" : "@Test\r\npublic void testRangeWithUpdate() {\r\n    Counter rangesCounter = mock(Counter.class);\r\n    when(this.keyValueStoreMetrics.ranges()).thenReturn(rangesCounter);\r\n    // exclude the index 1 entry so it can be added later\r\n    for (int i = 0; i < 10; i++) {\r\n        if (i != 1) {\r\n            this.inMemoryKeyValueStore.put(key(i), value(i));\r\n        }\r\n    }\r\n    KeyValueIterator<byte[], byte[]> range = this.inMemoryKeyValueStore.range(key(0), key(5));\r\n    assertEquals(4, Iterators.size(range));\r\n    // delete an entry from the range\r\n    this.inMemoryKeyValueStore.delete(key(2));\r\n    // update an entry\r\n    this.inMemoryKeyValueStore.put(key(3), value(OTHER_VALUE_PREFIX, 3));\r\n    // add a new entry\r\n    this.inMemoryKeyValueStore.put(key(1), value(DEFAULT_VALUE_PREFIX, 1));\r\n    range = this.inMemoryKeyValueStore.range(key(0), key(5));\r\n    for (int i = 0; i < 5; i++) {\r\n        if (i != 2) {\r\n            // index 2 was deleted\r\n            if (i == 3) {\r\n                // index 3 has an updated value\r\n                assertEntryEquals(key(i), value(OTHER_VALUE_PREFIX, 3), range.next());\r\n            } else {\r\n                // all other entries (including index 1 have the \"normal\" entries)\r\n                assertEntryEquals(key(i), value(DEFAULT_VALUE_PREFIX, i), range.next());\r\n            }\r\n        }\r\n    }\r\n    assertFalse(range.hasNext());\r\n    verify(rangesCounter, times(2)).inc();\r\n    // key increments: 4 for iterator size for first range, 4 for second range; key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(8)).inc(DEFAULT_KEY_LENGTH);\r\n    /*\r\n     * value increments: 4 for iterator size for first range, 3 for second range (updated entry is different); value(i)\r\n     * produces byte[] of same length\r\n     */\r\n    verify(this.bytesReadCounter, times(7)).inc(DEFAULT_VALUE_LENGTH);\r\n    // 1 call for updated entry\r\n    verify(this.bytesReadCounter).inc(value(OTHER_VALUE_PREFIX, 0).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testSnapshot",
  "sourceCode" : "@Test\r\npublic void testSnapshot() {\r\n    for (int i = 0; i < 10; i++) {\r\n        this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, i), value(OTHER_VALUE_PREFIX, i));\r\n        this.inMemoryKeyValueStore.put(key(i), value(i));\r\n    }\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = this.inMemoryKeyValueStore.snapshot(key(0), key(5));\r\n    KeyValueIterator<byte[], byte[]> iterator = snapshot.iterator();\r\n    for (int i = 0; i < 5; i++) {\r\n        assertEntryEquals(key(i), value(i), iterator.next());\r\n    }\r\n    assertFalse(iterator.hasNext());\r\n    // key size increments: key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5)).inc(DEFAULT_KEY_LENGTH);\r\n    // value size increments: value(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5)).inc(DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testSnapshotMultipleIterators",
  "sourceCode" : "@Test\r\npublic void testSnapshotMultipleIterators() {\r\n    for (int i = 0; i < 10; i++) {\r\n        this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, i), value(OTHER_VALUE_PREFIX, i));\r\n        this.inMemoryKeyValueStore.put(key(i), value(i));\r\n    }\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = this.inMemoryKeyValueStore.snapshot(key(0), key(5));\r\n    // Iterators.size exhausts the iterator\r\n    assertEquals(5, Iterators.size(snapshot.iterator()));\r\n    assertEquals(5, Iterators.size(snapshot.iterator()));\r\n    // key size increments: calling two separate iterators; key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5 * 2)).inc(DEFAULT_KEY_LENGTH);\r\n    // value size increments: calling two separate iterators; value(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5 * 2)).inc(DEFAULT_VALUE_LENGTH);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testSnapshotImmutable",
  "sourceCode" : "@Test\r\npublic void testSnapshotImmutable() {\r\n    for (int i = 0; i < 10; i++) {\r\n        this.inMemoryKeyValueStore.put(key(i), value(i));\r\n    }\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = this.inMemoryKeyValueStore.snapshot(key(0), key(5));\r\n    // make sure the entries in the snapshot don't change when something is added\r\n    this.inMemoryKeyValueStore.put(key(1), value(OTHER_VALUE_PREFIX, 1));\r\n    KeyValueIterator<byte[], byte[]> iterator = snapshot.iterator();\r\n    for (int i = 0; i < 5; i++) {\r\n        if (i == 1) {\r\n            /*\r\n         * There is a bug in which the snapshot is impacted by writes after calling snapshot.\r\n         * When the bug is fixed, the value for key(1) should be the original value from when snapshot was called.\r\n         */\r\n            assertEntryEquals(key(i), value(OTHER_VALUE_PREFIX, 1), iterator.next());\r\n        } else {\r\n            assertEntryEquals(key(i), value(i), iterator.next());\r\n        }\r\n    }\r\n    assertFalse(iterator.hasNext());\r\n    // key size increments; key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(5)).inc(DEFAULT_KEY_LENGTH);\r\n    // value size increments; value(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(4)).inc(DEFAULT_VALUE_LENGTH);\r\n    // when snapshot immutability bug is fixed, this should be merged into the bytesRead check above\r\n    verify(this.bytesReadCounter).inc(value(OTHER_VALUE_PREFIX, 1).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testSnapshotWithUpdate",
  "sourceCode" : "@Test\r\npublic void testSnapshotWithUpdate() {\r\n    // exclude the index 1 entry so it can be added later\r\n    for (int i = 0; i < 10; i++) {\r\n        if (i != 1) {\r\n            this.inMemoryKeyValueStore.put(key(i), value(i));\r\n        }\r\n    }\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = this.inMemoryKeyValueStore.snapshot(key(0), key(5));\r\n    assertEquals(4, Iterators.size(snapshot.iterator()));\r\n    // delete an entry fro the snapshot range\r\n    this.inMemoryKeyValueStore.delete(key(2));\r\n    // update an entry\r\n    this.inMemoryKeyValueStore.put(key(3), value(OTHER_VALUE_PREFIX, 3));\r\n    // add a new entry\r\n    this.inMemoryKeyValueStore.put(key(1), value(DEFAULT_VALUE_PREFIX, 1));\r\n    snapshot = this.inMemoryKeyValueStore.snapshot(key(0), key(5));\r\n    KeyValueIterator<byte[], byte[]> iterator = snapshot.iterator();\r\n    for (int i = 0; i < 5; i++) {\r\n        if (i != 2) {\r\n            // index 2 was deleted\r\n            if (i == 3) {\r\n                // index 3 has an updated value\r\n                assertEntryEquals(key(i), value(OTHER_VALUE_PREFIX, 3), iterator.next());\r\n            } else {\r\n                // all other entries (including index 1 have the \"normal\" entries)\r\n                assertEntryEquals(key(i), value(DEFAULT_VALUE_PREFIX, i), iterator.next());\r\n            }\r\n        }\r\n    }\r\n    assertFalse(iterator.hasNext());\r\n    // key increments: 4 for iterator size for first range, 4 for second range; key(i) produces byte[] of same length\r\n    verify(this.bytesReadCounter, times(8)).inc(DEFAULT_KEY_LENGTH);\r\n    /*\r\n     * value increments: 4 for iterator size for first range, 3 for second range (updated entry is different); value(i)\r\n     * produces byte[] of same length\r\n     */\r\n    verify(this.bytesReadCounter, times(7)).inc(DEFAULT_VALUE_LENGTH);\r\n    // 1 call for updated entry\r\n    verify(this.bytesReadCounter).inc(value(OTHER_VALUE_PREFIX, 0).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testAll",
  "sourceCode" : "@Test\r\npublic void testAll() {\r\n    Counter allsCounter = mock(Counter.class);\r\n    when(this.keyValueStoreMetrics.alls()).thenReturn(allsCounter);\r\n    for (int i = 0; i < 10; i++) {\r\n        this.inMemoryKeyValueStore.put(key(OTHER_KEY_PREFIX, i), value(OTHER_VALUE_PREFIX, i));\r\n        this.inMemoryKeyValueStore.put(key(i), value(i));\r\n    }\r\n    KeyValueIterator<byte[], byte[]> all = this.inMemoryKeyValueStore.all();\r\n    // all entries of one prefix comes first due to ordering\r\n    for (int i = 0; i < 10; i++) {\r\n        assertEntryEquals(key(i), value(i), all.next());\r\n    }\r\n    for (int i = 0; i < 10; i++) {\r\n        assertEntryEquals(key(OTHER_KEY_PREFIX, i), value(OTHER_VALUE_PREFIX, i), all.next());\r\n    }\r\n    assertFalse(all.hasNext());\r\n    verify(allsCounter).inc();\r\n    // key size increments: 10 calls for each prefix\r\n    verify(this.bytesReadCounter, times(10)).inc(DEFAULT_KEY_LENGTH);\r\n    // all keys using OTHER_KEY_PREFIX have the same length\r\n    int otherKeyLength = key(OTHER_KEY_PREFIX, 0).length;\r\n    verify(this.bytesReadCounter, times(10)).inc(otherKeyLength);\r\n    // value size increments: 10 calls for each prefix\r\n    verify(this.bytesReadCounter, times(10)).inc(DEFAULT_VALUE_LENGTH);\r\n    // all values using OTHER_VALUE_PREFIX have the same length\r\n    int otherValueLength = value(OTHER_VALUE_PREFIX, 0).length;\r\n    verify(this.bytesReadCounter, times(10)).inc(otherValueLength);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testAllWithUpdate",
  "sourceCode" : "@Test\r\npublic void testAllWithUpdate() {\r\n    Counter allsCounter = mock(Counter.class);\r\n    when(this.keyValueStoreMetrics.alls()).thenReturn(allsCounter);\r\n    // fill in a range values for two different prefixes, but leave out index 1 so it can be added later\r\n    for (int i = 0; i < 10; i++) {\r\n        if (i != 1) {\r\n            this.inMemoryKeyValueStore.put(key(i), value(i));\r\n        }\r\n    }\r\n    KeyValueIterator<byte[], byte[]> all = this.inMemoryKeyValueStore.all();\r\n    assertEquals(9, Iterators.size(all));\r\n    // delete an entry\r\n    this.inMemoryKeyValueStore.delete(key(2));\r\n    // update an entry\r\n    this.inMemoryKeyValueStore.put(key(3), value(OTHER_VALUE_PREFIX, 3));\r\n    // add a new entry\r\n    this.inMemoryKeyValueStore.put(key(1), value(1));\r\n    all = this.inMemoryKeyValueStore.all();\r\n    for (int i = 0; i < 10; i++) {\r\n        if (i != 2) {\r\n            // index 2 was deleted\r\n            if (i == 3) {\r\n                // index 3 has an updated value\r\n                assertEntryEquals(key(i), value(OTHER_VALUE_PREFIX, 3), all.next());\r\n            } else {\r\n                // all other entries (including index 1 have the \"normal\" entries)\r\n                assertEntryEquals(key(i), value(i), all.next());\r\n            }\r\n        }\r\n    }\r\n    assertFalse(all.hasNext());\r\n    verify(allsCounter, times(2)).inc();\r\n    // key size increments: 9 calls for iterator size check of first \"all\", 9 calls for second \"all\"\r\n    verify(this.bytesReadCounter, times(18)).inc(DEFAULT_KEY_LENGTH);\r\n    /*\r\n     * value size increments: 9 calls for iterator size check of first \"all\", 8 calls for second \"all\" (updated entry is\r\n     * different)\r\n     */\r\n    verify(this.bytesReadCounter, times(17)).inc(DEFAULT_VALUE_LENGTH);\r\n    // 1 call for \"updatedValue\"\r\n    verify(this.bytesReadCounter).inc(value(OTHER_VALUE_PREFIX, 3).length);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryKeyValueStore.java",
  "methodName" : "testFlush",
  "sourceCode" : "@Test\r\npublic void testFlush() {\r\n    Counter flushesCounter = mock(Counter.class);\r\n    when(this.keyValueStoreMetrics.flushes()).thenReturn(flushesCounter);\r\n    this.inMemoryKeyValueStore.flush();\r\n    verify(flushesCounter).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryTableDescriptor.java",
  "methodName" : "testMinimal",
  "sourceCode" : "@Test\r\npublic void testMinimal() {\r\n    Map tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertNotNull(tableConfig);\r\n    Assert.assertEquals(2, tableConfig.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-inmemory\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\inmemory\\TestInMemoryTableDescriptor.java",
  "methodName" : "testTableProviderFactoryConfig",
  "sourceCode" : "@Test\r\npublic void testTableProviderFactoryConfig() {\r\n    Map tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertEquals(2, tableConfig.size());\r\n    Assert.assertEquals(LocalTableProviderFactory.class.getName(), tableConfig.get(String.format(JavaTableConfig.TABLE_PROVIDER_FACTORY, TABLE_ID)));\r\n    Assert.assertEquals(InMemoryKeyValueStorageEngineFactory.class.getName(), tableConfig.get(String.format(StorageConfig.FACTORY, TABLE_ID)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\descriptors\\TestRocksDbTableDescriptor.java",
  "methodName" : "testMinimal",
  "sourceCode" : "@Test\r\npublic void testMinimal() {\r\n    Map tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertNotNull(tableConfig);\r\n    Assert.assertEquals(2, tableConfig.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\descriptors\\TestRocksDbTableDescriptor.java",
  "methodName" : "testTableProviderFactoryConfig",
  "sourceCode" : "@Test\r\npublic void testTableProviderFactoryConfig() {\r\n    Map tableConfig = createTableDescriptor().toConfig(createJobConfig());\r\n    Assert.assertEquals(2, tableConfig.size());\r\n    Assert.assertEquals(LocalTableProviderFactory.class.getName(), tableConfig.get(String.format(JavaTableConfig.TABLE_PROVIDER_FACTORY, TABLE_ID)));\r\n    Assert.assertEquals(RocksDbKeyValueStorageEngineFactory.class.getName(), tableConfig.get(String.format(StorageConfig.FACTORY, TABLE_ID)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\descriptors\\TestRocksDbTableDescriptor.java",
  "methodName" : "testRocksDbConfig",
  "sourceCode" : "@Test\r\npublic void testRocksDbConfig() {\r\n    Map tableConfig = new RocksDbTableDescriptor<Integer, String>(TABLE_ID, KVSerde.of(new IntegerSerde(), new StringSerde())).withBlockSize(1).withCacheSize(2).withCompactionStyle(\"fifo\").withCompressionType(\"snappy\").withMaxLogFileSize(3).withNumLogFilesToKeep(4).withNumWriteBuffers(5).withObjectCacheSize(6).withTtl(7).withWriteBatchSize(8).withWriteBufferSize(9).withMaxOpenFiles(10).withMaxFileOpeningThreads(11).withConfig(\"abc\", \"xyz\").toConfig(createJobConfig());\r\n    Assert.assertEquals(16, tableConfig.size());\r\n    assertEquals(\"1\", RocksDbTableDescriptor.ROCKSDB_BLOCK_SIZE_BYTES, tableConfig);\r\n    assertEquals(\"2\", RocksDbTableDescriptor.CONTAINER_CACHE_SIZE_BYTES, tableConfig);\r\n    assertEquals(\"3\", RocksDbTableDescriptor.ROCKSDB_MAX_LOG_FILE_SIZE_BYTES, tableConfig);\r\n    assertEquals(\"4\", RocksDbTableDescriptor.ROCKSDB_KEEP_LOG_FILE_NUM, tableConfig);\r\n    assertEquals(\"5\", RocksDbTableDescriptor.ROCKSDB_NUM_WRITE_BUFFERS, tableConfig);\r\n    assertEquals(\"6\", RocksDbTableDescriptor.OBJECT_CACHE_SIZE, tableConfig);\r\n    assertEquals(\"7\", RocksDbTableDescriptor.ROCKSDB_TTL_MS, tableConfig);\r\n    assertEquals(\"8\", RocksDbTableDescriptor.WRITE_BATCH_SIZE, tableConfig);\r\n    assertEquals(\"9\", RocksDbTableDescriptor.CONTAINER_WRITE_BUFFER_SIZE_BYTES, tableConfig);\r\n    assertEquals(\"10\", RocksDbTableDescriptor.ROCKSDB_MAX_OPEN_FILES, tableConfig);\r\n    assertEquals(\"11\", RocksDbTableDescriptor.ROCKSDB_MAX_FILE_OPENING_THREADS, tableConfig);\r\n    assertEquals(\"snappy\", RocksDbTableDescriptor.ROCKSDB_COMPRESSION, tableConfig);\r\n    assertEquals(\"fifo\", RocksDbTableDescriptor.ROCKSDB_COMPACTION_STYLE, tableConfig);\r\n    Assert.assertFalse(tableConfig.containsKey(String.format(StorageConfig.CHANGELOG_STREAM, TABLE_ID)));\r\n    Assert.assertFalse(tableConfig.containsKey(String.format(StorageConfig.CHANGELOG_REPLICATION_FACTOR, TABLE_ID)));\r\n    Assert.assertEquals(\"xyz\", tableConfig.get(\"abc\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestRocksDbKeyValueReader.java",
  "methodName" : "testReadCorrectDbValue",
  "sourceCode" : "@Test\r\npublic void testReadCorrectDbValue() throws RocksDBException {\r\n    HashMap<String, String> map = new HashMap<String, String>();\r\n    map.put(\"stores.\" + DB_NAME + \".factory\", \"mockFactory\");\r\n    map.put(\"stores.\" + DB_NAME + \".key.serde\", \"string\");\r\n    map.put(\"stores.\" + DB_NAME + \".msg.serde\", \"string\");\r\n    Config config = new MapConfig(map);\r\n    RocksDbKeyValueReader reader = new RocksDbKeyValueReader(DB_NAME, dirPath.toString(), config);\r\n    assertEquals(\"this is string\", reader.get(\"testString\"));\r\n    // should throw exception if the input is in other type\r\n    boolean throwClassCastException = false;\r\n    try {\r\n        reader.get(123);\r\n    } catch (Exception e) {\r\n        if (e instanceof ClassCastException) {\r\n            throwClassCastException = true;\r\n        }\r\n    }\r\n    assertTrue(throwClassCastException);\r\n    reader.stop();\r\n    // test with customized serde\r\n    map.put(\"serializers.registry.mock.class\", IntegerSerdeFactory.class.getCanonicalName());\r\n    map.put(\"stores.\" + DB_NAME + \".key.serde\", \"mock\");\r\n    map.put(\"stores.\" + DB_NAME + \".msg.serde\", \"mock\");\r\n    config = new MapConfig(map);\r\n    reader = new RocksDbKeyValueReader(DB_NAME, dirPath.toString(), config);\r\n    assertEquals(456, reader.get(123));\r\n    assertNull(reader.get(789));\r\n    reader.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestRocksDbKeyValueStoreJava.java",
  "methodName" : "testIterate",
  "sourceCode" : "@Test\r\npublic void testIterate() throws Exception {\r\n    Config config = new MapConfig();\r\n    Options options = new Options();\r\n    options.setCreateIfMissing(true);\r\n    File dbDir = new File(System.getProperty(\"java.io.tmpdir\") + \"/dbStore\" + System.currentTimeMillis());\r\n    RocksDbKeyValueStore store = new RocksDbKeyValueStore(dbDir, options, config, false, \"dbStore\", new WriteOptions(), new FlushOptions(), new KeyValueStoreMetrics(\"dbStore\", new MetricsRegistryMap()));\r\n    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\r\n    String prefix = \"prefix\";\r\n    for (int i = 0; i < 100; i++) {\r\n        store.put(genKey(outputStream, prefix, i), genValue());\r\n    }\r\n    byte[] firstKey = genKey(outputStream, prefix, 0);\r\n    byte[] lastKey = genKey(outputStream, prefix, 1000);\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = store.snapshot(firstKey, lastKey);\r\n    // Make sure the cached Iterable won't change when new elements are added\r\n    store.put(genKey(outputStream, prefix, 200), genValue());\r\n    KeyValueIterator<byte[], byte[]> iterator = snapshot.iterator();\r\n    assertTrue(Iterators.size(iterator) == 100);\r\n    iterator.close();\r\n    List<Integer> keys = new ArrayList<>();\r\n    KeyValueIterator<byte[], byte[]> iterator2 = snapshot.iterator();\r\n    while (iterator2.hasNext()) {\r\n        Entry<byte[], byte[]> entry = iterator2.next();\r\n        int key = Ints.fromByteArray(Arrays.copyOfRange(entry.getKey(), prefix.getBytes().length, entry.getKey().length));\r\n        keys.add(key);\r\n    }\r\n    assertEquals(keys, IntStream.rangeClosed(0, 99).boxed().collect(Collectors.toList()));\r\n    iterator2.close();\r\n    outputStream.close();\r\n    snapshot.close();\r\n    store.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-kv-rocksdb\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestRocksDbKeyValueStoreJava.java",
  "methodName" : "testPerf",
  "sourceCode" : "@Test\r\npublic void testPerf() throws Exception {\r\n    Config config = new MapConfig();\r\n    Options options = new Options();\r\n    options.setCreateIfMissing(true);\r\n    File dbDir = new File(System.getProperty(\"java.io.tmpdir\") + \"/dbStore\" + System.currentTimeMillis());\r\n    RocksDbKeyValueStore store = new RocksDbKeyValueStore(dbDir, options, config, false, \"dbStore\", new WriteOptions(), new FlushOptions(), new KeyValueStoreMetrics(\"dbStore\", new MetricsRegistryMap()));\r\n    ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\r\n    String prefix = \"this is the key prefix\";\r\n    Random r = new Random();\r\n    for (int i = 0; i < 100000; i++) {\r\n        store.put(genKey(outputStream, prefix, r.nextInt()), genValue());\r\n    }\r\n    byte[] firstKey = genKey(outputStream, prefix, 0);\r\n    byte[] lastKey = genKey(outputStream, prefix, Integer.MAX_VALUE);\r\n    long start;\r\n    start = System.currentTimeMillis();\r\n    KeyValueIterator<byte[], byte[]> iterator1 = store.range(firstKey, lastKey);\r\n    long rangeTime = System.currentTimeMillis() - start;\r\n    start = System.currentTimeMillis();\r\n    Iterators.size(iterator1);\r\n    long rangeIterTime = System.currentTimeMillis() - start;\r\n    System.out.println(\"range iter create time: \" + rangeTime + \", iterate time: \" + rangeIterTime);\r\n    iterator1.close();\r\n    // Please comment out range query part in order to do an accurate perf test for snapshot\r\n    start = System.currentTimeMillis();\r\n    KeyValueSnapshot<byte[], byte[]> snapshot = store.snapshot(firstKey, lastKey);\r\n    KeyValueIterator<byte[], byte[]> iterator2 = snapshot.iterator();\r\n    long snapshotTime = System.currentTimeMillis() - start;\r\n    start = System.currentTimeMillis();\r\n    Iterators.size(iterator2);\r\n    long snapshotIterTime = System.currentTimeMillis() - start;\r\n    System.out.println(\"snapshot iter create time: \" + snapshotTime + \", iterate time: \" + snapshotIterTime);\r\n    iterator2.close();\r\n    snapshot.close();\r\n    store.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\main\\java\\org\\apache\\samza\\logging\\log4j\\StreamAppender.java",
  "methodName" : "readyToInitialize",
  "sourceCode" : "@VisibleForTesting\r\nboolean readyToInitialize() {\r\n    return LoggingContextHolder.INSTANCE.getConfig() != null;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\config\\TestLog4jSystemConfig.java",
  "methodName" : "testGetSystemNames",
  "sourceCode" : "@Test\r\npublic void testGetSystemNames() {\r\n    Map<String, String> map = new HashMap<String, String>();\r\n    map.put(\"systems.system1.samza.factory\", \"1\");\r\n    map.put(\"systems.system2.samza.factory\", \"2\");\r\n    Log4jSystemConfig log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    assertEquals(2, log4jSystemConfig.getSystemNames().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\config\\TestLog4jSystemConfig.java",
  "methodName" : "testGetLog4jSystemName",
  "sourceCode" : "@Test\r\npublic void testGetLog4jSystemName() {\r\n    Map<String, String> map = new HashMap<String, String>();\r\n    map.put(\"task.log4j.system\", \"log4j-system\");\r\n    map.put(\"systems.system1.samza.factory\", \"1\");\r\n    Log4jSystemConfig log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    assertEquals(\"log4j-system\", log4jSystemConfig.getSystemName());\r\n    // throw ConfigException\r\n    map.remove(\"task.log4j.system\");\r\n    log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    exception.expect(ConfigException.class);\r\n    log4jSystemConfig.getSystemName();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\config\\TestLog4jSystemConfig.java",
  "methodName" : "testGetSerdeClass",
  "sourceCode" : "@Test\r\npublic void testGetSerdeClass() {\r\n    Map<String, String> map = new HashMap<String, String>();\r\n    Log4jSystemConfig log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    // get null\r\n    assertNull(log4jSystemConfig.getSerdeClass(\"otherName\"));\r\n    // get serde\r\n    map.put(\"serializers.registry.log4j.class\", \"someClass\");\r\n    log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    assertEquals(\"someClass\", log4jSystemConfig.getSerdeClass(\"log4j\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\config\\TestLog4jSystemConfig.java",
  "methodName" : "testGetSerdeName",
  "sourceCode" : "@Test\r\npublic void testGetSerdeName() {\r\n    Map<String, String> map = new HashMap<String, String>();\r\n    map.put(\"systems.mockSystem.samza.msg.serde\", \"systemSerde\");\r\n    Log4jSystemConfig log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    // no stream serde\r\n    assertNull(log4jSystemConfig.getStreamSerdeName(\"mockSystem\", \"mockStream\"));\r\n    // stream serde\r\n    map.put(\"systems.mockSystem.streams.mockStream.samza.msg.serde\", \"streamSerde\");\r\n    log4jSystemConfig = new Log4jSystemConfig(new MapConfig(map));\r\n    assertEquals(\"streamSerde\", log4jSystemConfig.getStreamSerdeName(\"mockSystem\", \"mockStream\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\serializers\\TestLoggingEventStringSerde.java",
  "methodName" : "test",
  "sourceCode" : "@Test\r\npublic void test() {\r\n    String testLog = \"testing\";\r\n    Logger logger = Logger.getLogger(TestLoggingEventStringSerde.class);\r\n    LoggingEvent log = new LoggingEvent(logger.getName(), logger, logger.getLevel(), testLog, null);\r\n    LoggingEventStringSerde loggingEventStringSerde = new LoggingEventStringSerde();\r\n    assertNull(loggingEventStringSerde.fromBytes(null));\r\n    assertNull(loggingEventStringSerde.toBytes(null));\r\n    assertArrayEquals(testLog.getBytes(), loggingEventStringSerde.toBytes(log));\r\n    // only the log messages are guaranteed to be equivalent\r\n    assertEquals(log.getMessage().toString(), loggingEventStringSerde.fromBytes(testLog.getBytes()).getMessage().toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestJmxAppender.java",
  "methodName" : "testJmxAppender",
  "sourceCode" : "@Test\r\npublic void testJmxAppender() throws Exception {\r\n    MBeanServerConnection mbserver = JMXConnectorFactory.connect(URL).getMBeanServerConnection();\r\n    ObjectName objectName = new ObjectName(JmxAppender.JMX_OBJECT_DOMAIN + \":type=\" + JmxAppender.JMX_OBJECT_TYPE + \",name=\" + JmxAppender.JMX_OBJECT_NAME);\r\n    String level = null;\r\n    MockAppender mockAppender = new MockAppender();\r\n    Logger.getRootLogger().addAppender(mockAppender);\r\n    // Check INFO is set (from log4j.xml).\r\n    level = (String) mbserver.getAttribute(objectName, \"Level\");\r\n    assertEquals(\"INFO\", level);\r\n    log.info(\"info1\");\r\n    log.debug(\"debug1\");\r\n    // Set to debug.\r\n    mbserver.setAttribute(objectName, new Attribute(\"Level\", \"debug\"));\r\n    // Check DEBUG is set.\r\n    level = (String) mbserver.getAttribute(objectName, \"Level\");\r\n    assertEquals(\"DEBUG\", level);\r\n    log.info(\"info2\");\r\n    log.debug(\"debug2\");\r\n    List<LoggingEvent> logLines = mockAppender.getLogLines();\r\n    // Should not have debug1 because log level is info at first.\r\n    Iterator<LoggingEvent> logLineIterator = logLines.iterator();\r\n    assertEquals(3, logLines.size());\r\n    assertEquals(\"info1\", logLineIterator.next().getMessage());\r\n    assertEquals(\"info2\", logLineIterator.next().getMessage());\r\n    assertEquals(\"debug2\", logLineIterator.next().getMessage());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testDefaultSerde",
  "sourceCode" : "@Test\r\npublic void testDefaultSerde() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender();\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    assertNotNull(systemProducerAppender.getSerde());\r\n    assertEquals(LoggingEventJsonSerde.class, systemProducerAppender.getSerde().getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testNonDefaultSerde",
  "sourceCode" : "@Test\r\npublic void testNonDefaultSerde() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    String streamName = StreamAppender.getStreamName(\"log4jTest\", \"1\");\r\n    Map<String, String> map = new HashMap<>();\r\n    map.put(\"job.name\", \"log4jTest\");\r\n    map.put(\"job.id\", \"1\");\r\n    map.put(\"serializers.registry.log4j-string.class\", LoggingEventStringSerdeFactory.class.getCanonicalName());\r\n    map.put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName());\r\n    map.put(\"systems.mock.streams.\" + streamName + \".samza.msg.serde\", \"log4j-string\");\r\n    map.put(\"task.log4j.system\", \"mock\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender(new MapConfig(map));\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    assertNotNull(systemProducerAppender.getSerde());\r\n    assertEquals(LoggingEventStringSerde.class, systemProducerAppender.getSerde().getClass());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testSystemProducerAppenderInContainer",
  "sourceCode" : "@Test\r\npublic void testSystemProducerAppenderInContainer() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender();\r\n    PatternLayout layout = new PatternLayout();\r\n    layout.setConversionPattern(\"%m\");\r\n    systemProducerAppender.setLayout(layout);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing1\", \"testing2\");\r\n    logAndVerifyMessages(messages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testSystemProducerAppenderNotInitialized",
  "sourceCode" : "@Test\r\npublic void testSystemProducerAppenderNotInitialized() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-job-coordinator\");\r\n    // add a counter to make sure that the initial message doesn't get produced\r\n    AtomicInteger numMessagesProduced = new AtomicInteger(0);\r\n    MockSystemProducer.listeners.add((source, envelope) -> numMessagesProduced.incrementAndGet());\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender(baseConfig(), false);\r\n    PatternLayout layout = new PatternLayout();\r\n    layout.setConversionPattern(\"%m\");\r\n    systemProducerAppender.setLayout(layout);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // System isn't initialized yet, so this message should be dropped\r\n    LOG.info(\"no-received\");\r\n    // explicitly trigger initialization to test that new messages do get sent to the stream\r\n    systemProducerAppender.setupSystem();\r\n    systemProducerAppender.systemInitialized = true;\r\n    List<String> messages = Lists.newArrayList(\"testing3\", \"testing4\");\r\n    logAndVerifyMessages(messages);\r\n    assertEquals(messages.size(), numMessagesProduced.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testNoStreamCreationUponSetupByDefault",
  "sourceCode" : "@Test\r\npublic void testNoStreamCreationUponSetupByDefault() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender();\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    Assert.assertNull(MockSystemAdmin.createdStreamSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testStreamCreationUpSetupWhenEnabled",
  "sourceCode" : "@Test\r\npublic void testStreamCreationUpSetupWhenEnabled() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(// Enable explicit stream creation\r\n    \"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\", \"job.name\", \"log4jTest\", \"job.id\", \"1\", \"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName(), \"task.log4j.system\", \"mock\"));\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender(mapConfig);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    Assert.assertEquals(\"__samza_log4jTest_1_logs\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    // job.container.count defaults to 1\r\n    Assert.assertEquals(1, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testStreamCreationUpSetupWithJobContainerCountConfigured",
  "sourceCode" : "@Test\r\npublic void testStreamCreationUpSetupWithJobContainerCountConfigured() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(new ImmutableMap.Builder<String, String>().put(\"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\").put(\"job.name\", \"log4jTest\").put(\"job.id\", \"1\").put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName()).put(\"task.log4j.system\", \"mock\").put(\"job.container.count\", \"4\").build());\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender(mapConfig);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    Assert.assertEquals(\"__samza_log4jTest_1_logs\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    Assert.assertEquals(4, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testStreamCreationUpSetupWithPartitionCountConfigured",
  "sourceCode" : "@Test\r\npublic void testStreamCreationUpSetupWithPartitionCountConfigured() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(// Enable explicit stream creation\r\n    \"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\", \"job.name\", \"log4jTest\", \"job.id\", \"1\", \"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName(), \"task.log4j.system\", \"mock\"));\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender(mapConfig);\r\n    systemProducerAppender.setPartitionCount(8);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    Assert.assertEquals(\"__samza_log4jTest_1_logs\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    Assert.assertEquals(8, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testExceptionsDoNotKillTransferThread",
  "sourceCode" : "@Test\r\npublic void testExceptionsDoNotKillTransferThread() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender();\r\n    PatternLayout layout = new PatternLayout();\r\n    layout.setConversionPattern(\"%m\");\r\n    systemProducerAppender.setLayout(layout);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing5\", \"testing6\", \"testing7\");\r\n    // Set up latch\r\n    final CountDownLatch allMessagesSent = new CountDownLatch(messages.size());\r\n    MockSystemProducer.listeners.add((source, envelope) -> {\r\n        allMessagesSent.countDown();\r\n        if (allMessagesSent.getCount() == messages.size() - 1) {\r\n            // Throw on the first message\r\n            throw new RuntimeException();\r\n        }\r\n    });\r\n    // Log the messages\r\n    messages.forEach(LOG::info);\r\n    // Wait for messages\r\n    assertTrue(\"Thread did not send all messages. Count: \" + allMessagesSent.getCount(), allMessagesSent.await(60, TimeUnit.SECONDS));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j\\src\\test\\java\\org\\apache\\samza\\logging\\log4j\\TestStreamAppender.java",
  "methodName" : "testQueueTimeout",
  "sourceCode" : "@Test\r\npublic void testQueueTimeout() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MockSystemProducerAppender systemProducerAppender = new MockSystemProducerAppender();\r\n    systemProducerAppender.queueTimeoutS = 1;\r\n    PatternLayout layout = new PatternLayout();\r\n    layout.setConversionPattern(\"%m\");\r\n    systemProducerAppender.setLayout(layout);\r\n    systemProducerAppender.activateOptions();\r\n    LOG.addAppender(systemProducerAppender);\r\n    int extraMessageCount = 5;\r\n    // -1 because when the queue is drained there is one additional message that couldn't be added\r\n    int expectedMessagesSent = extraMessageCount - 1;\r\n    List<String> messages = new ArrayList<>(StreamAppender.DEFAULT_QUEUE_SIZE + extraMessageCount);\r\n    for (int i = 0; i < StreamAppender.DEFAULT_QUEUE_SIZE + extraMessageCount; i++) {\r\n        messages.add(String.valueOf(i));\r\n    }\r\n    // Set up latch\r\n    // We expect to drop all but the extra messages\r\n    final CountDownLatch allMessagesSent = new CountDownLatch(expectedMessagesSent);\r\n    final CountDownLatch waitForTimeout = new CountDownLatch(1);\r\n    MockSystemProducer.listeners.add((source, envelope) -> {\r\n        allMessagesSent.countDown();\r\n        try {\r\n            waitForTimeout.await();\r\n        } catch (InterruptedException e) {\r\n            fail(\"Test could not run properly because of a thread interrupt.\");\r\n        }\r\n    });\r\n    // Log the messages. This is where the timeout will happen!\r\n    messages.forEach(LOG::info);\r\n    assertEquals(messages.size() - expectedMessagesSent, systemProducerAppender.metrics.logMessagesDropped.getCount());\r\n    // Allow all the rest of the messages to send.\r\n    waitForTimeout.countDown();\r\n    // Wait for messages\r\n    assertTrue(\"Thread did not send all messages. Count: \" + allMessagesSent.getCount(), allMessagesSent.await(60, TimeUnit.SECONDS));\r\n    assertEquals(expectedMessagesSent, MockSystemProducer.messagesReceived.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\serializers\\TestLoggingEventStringSerde.java",
  "methodName" : "test",
  "sourceCode" : "@Test\r\npublic void test() {\r\n    String testLog = \"testing\";\r\n    Logger logger = (Logger) LogManager.getLogger(TestLoggingEventStringSerde.class);\r\n    LogEvent log = Log4jLogEvent.newBuilder().setLevel(logger.getLevel()).setLoggerName(logger.getName()).setMessage(new SimpleMessage(testLog)).setThrown(null).build();\r\n    LoggingEventStringSerde loggingEventStringSerde = new LoggingEventStringSerde();\r\n    assertNull(loggingEventStringSerde.fromBytes(null));\r\n    assertNull(loggingEventStringSerde.toBytes(null));\r\n    assertArrayEquals(testLog.getBytes(), loggingEventStringSerde.toBytes(log));\r\n    // only the log messages are guaranteed to be equivalent\r\n    assertEquals(log.getMessage().toString(), loggingEventStringSerde.fromBytes(testLog.getBytes()).getMessage().toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testDefaultSerde",
  "sourceCode" : "@Test\r\npublic void testDefaultSerde() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    assertEquals(LoggingEventJsonSerde.class, streamAppender.getSerde().getClass());\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testNonDefaultSerde",
  "sourceCode" : "@Test\r\npublic void testNonDefaultSerde() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"job.name\", \"log4jTest\");\r\n    configMap.put(\"job.id\", \"1\");\r\n    configMap.put(\"serializers.registry.log4j-string.class\", LoggingEventStringSerdeFactory.class.getCanonicalName());\r\n    configMap.put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName());\r\n    configMap.put(\"systems.mock.streams.__samza_log4jTest_1_logs.samza.msg.serde\", \"log4j-string\");\r\n    configMap.put(\"task.log4j.system\", \"mock\");\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(new MapConfig(configMap));\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    assertEquals(LoggingEventStringSerde.class, streamAppender.getSerde().getClass());\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSystemProducerAppenderAppend",
  "sourceCode" : "@Test\r\npublic void testSystemProducerAppenderAppend() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing1\", \"testing2\");\r\n    logAndVerifyMessages(messages);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSystemProducerAppenderInContainerWithAsyncLogger",
  "sourceCode" : "@Test\r\npublic void testSystemProducerAppenderInContainerWithAsyncLogger() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    // Enabling async logger on log4j2 programmatically\r\n    ConfigurationFactory.setConfigurationFactory(new AsyncLoggerConfigurationFactory());\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing1\", \"testing2\");\r\n    logAndVerifyMessages(messages);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSystemProducerAppenderNotInitialized",
  "sourceCode" : "@Test\r\npublic void testSystemProducerAppenderNotInitialized() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-job-coordinator\");\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(null);\r\n    // add a counter to make sure that the initial message doesn't get produced\r\n    AtomicInteger numMessagesProduced = new AtomicInteger(0);\r\n    MockSystemProducer.listeners.add((source, envelope) -> numMessagesProduced.incrementAndGet());\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // System isn't initialized yet, so this message should be dropped\r\n    LOG.info(\"no-received\");\r\n    // make config available so messages now get sent to the stream\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(baseConfig());\r\n    List<String> messages = Lists.newArrayList(\"testing3\", \"testing4\");\r\n    logAndVerifyMessages(messages);\r\n    streamAppender.stop();\r\n    assertEquals(messages.size(), numMessagesProduced.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testNoStreamCreationUponSetupByDefault",
  "sourceCode" : "@Test\r\npublic void testNoStreamCreationUponSetupByDefault() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    streamAppender.stop();\r\n    Assert.assertNull(MockSystemAdmin.createdStreamSpec);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testStreamCreationDefaultStreamName",
  "sourceCode" : "@Test\r\npublic void testStreamCreationDefaultStreamName() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(// Enable explicit stream creation\r\n    \"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\", \"job.name\", \"log4jTest\", \"job.id\", \"1\", \"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName(), \"task.log4j.system\", \"mock\"));\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    streamAppender.stop();\r\n    Assert.assertEquals(\"__samza_log4jTest_1_logs\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    // job.container.count defaults to 1\r\n    Assert.assertEquals(1, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testStreamCreationCustomStreamName",
  "sourceCode" : "@Test\r\npublic void testStreamCreationCustomStreamName() {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(ImmutableMap.of(// Enable explicit stream creation\r\n    \"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\", \"job.name\", \"log4jTest\", \"job.id\", \"1\", \"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName(), \"task.log4j.system\", \"mock\"));\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, \"test-stream-name\", this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    streamAppender.stop();\r\n    Assert.assertEquals(\"test-stream-name\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    // job.container.count defaults to 1\r\n    Assert.assertEquals(1, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testStreamCreationUpSetupWithJobContainerCountConfigured",
  "sourceCode" : "@Test\r\npublic void testStreamCreationUpSetupWithJobContainerCountConfigured() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(new ImmutableMap.Builder<String, String>().put(\"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\").put(\"job.name\", \"log4jTest\").put(\"job.id\", \"1\").put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName()).put(\"task.log4j.system\", \"mock\").put(\"job.container.count\", \"4\").build());\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    // trigger system set up by sending a log\r\n    LOG.info(\"log message\");\r\n    streamAppender.stop();\r\n    Assert.assertEquals(\"__samza_log4jTest_1_logs\", MockSystemAdmin.createdStreamSpec.getPhysicalName());\r\n    Assert.assertEquals(4, MockSystemAdmin.createdStreamSpec.getPartitionCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testExceptionsDoNotKillTransferThread",
  "sourceCode" : "@Test\r\npublic void testExceptionsDoNotKillTransferThread() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing5\", \"testing6\", \"testing7\");\r\n    // Set up latch\r\n    final CountDownLatch allMessagesSent = new CountDownLatch(messages.size());\r\n    MockSystemProducer.listeners.add((source, envelope) -> {\r\n        allMessagesSent.countDown();\r\n        if (allMessagesSent.getCount() == messages.size() - 1) {\r\n            // Throw on the first message\r\n            throw new RuntimeException();\r\n        }\r\n    });\r\n    // Log the messages\r\n    messages.forEach(LOG::info);\r\n    // Wait for messages\r\n    assertTrue(\"Thread did not send all messages. Count: \" + allMessagesSent.getCount(), allMessagesSent.await(60, TimeUnit.SECONDS));\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testQueueTimeout",
  "sourceCode" : "@Test\r\npublic void testQueueTimeout() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, null, false, false, null, this.loggingContextHolder);\r\n    streamAppender.queueTimeoutS = 1;\r\n    startAndAttachAppender(streamAppender);\r\n    int extraMessageCount = 5;\r\n    // -1 because when the queue is drained there is one additional message that couldn't be added\r\n    int expectedMessagesSent = extraMessageCount - 1;\r\n    List<String> messages = new ArrayList<>(StreamAppender.DEFAULT_QUEUE_SIZE + extraMessageCount);\r\n    for (int i = 0; i < StreamAppender.DEFAULT_QUEUE_SIZE + extraMessageCount; i++) {\r\n        messages.add(String.valueOf(i));\r\n    }\r\n    // Set up latch\r\n    // We expect to drop all but the extra messages\r\n    final CountDownLatch allMessagesSent = new CountDownLatch(expectedMessagesSent);\r\n    final CountDownLatch waitForTimeout = new CountDownLatch(1);\r\n    MockSystemProducer.listeners.add((source, envelope) -> {\r\n        allMessagesSent.countDown();\r\n        try {\r\n            waitForTimeout.await();\r\n        } catch (InterruptedException e) {\r\n            fail(\"Test could not run properly because of a thread interrupt.\");\r\n        }\r\n    });\r\n    // Log the messages. This is where the timeout will happen!\r\n    messages.forEach(LOG::info);\r\n    assertEquals(messages.size() - expectedMessagesSent, streamAppender.metrics.logMessagesDropped.getCount());\r\n    // Allow all the rest of the messages to send.\r\n    waitForTimeout.countDown();\r\n    // Wait for messages\r\n    assertTrue(\"Thread did not send all messages. Count: \" + allMessagesSent.getCount(), allMessagesSent.await(60, TimeUnit.SECONDS));\r\n    assertEquals(expectedMessagesSent, MockSystemProducer.messagesReceived.size());\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testLogConcurrently",
  "sourceCode" : "@Test\r\npublic void testLogConcurrently() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    List<String> messages = new ArrayList<>();\r\n    for (int i = 0; i < 100; i++) {\r\n        messages.add(\"testing\" + i);\r\n    }\r\n    logConcurrentlyAndVerifyMessages(messages);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testLogRecursively",
  "sourceCode" : "@Test\r\npublic void testLogRecursively() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    List<String> messages = Lists.newArrayList(\"testing1\", \"testing2\");\r\n    logRecursivelyAndVerifyMessages(messages);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSetupStreamTimeout",
  "sourceCode" : "@Test\r\npublic void testSetupStreamTimeout() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-1\");\r\n    MapConfig mapConfig = new MapConfig(new ImmutableMap.Builder<String, String>().put(\"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\").put(\"job.name\", \"log4jTest\").put(\"job.id\", \"1\").put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName()).put(\"task.log4j.system\", \"mock\").put(\"job.container.count\", \"4\").build());\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    setupSystemTimeoutAndVerifyMessages(SET_UP_SYSTEM_TIMEOUT_MILLI_SECONDS * 2);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSetupStreamNoTimeout",
  "sourceCode" : "@Test\r\npublic void testSetupStreamNoTimeout() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-2\");\r\n    MapConfig mapConfig = new MapConfig(new ImmutableMap.Builder<String, String>().put(\"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\").put(\"job.name\", \"log4jTest\").put(\"job.id\", \"1\").put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName()).put(\"task.log4j.system\", \"mock\").put(\"job.container.count\", \"4\").build());\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    setupSystemTimeoutAndVerifyMessages(SET_UP_SYSTEM_TIMEOUT_MILLI_SECONDS / 2);\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-log4j2\\src\\test\\java\\org\\apache\\samza\\logging\\log4j2\\TestStreamAppender.java",
  "methodName" : "testSetupStreamException",
  "sourceCode" : "@Test\r\npublic void testSetupStreamException() throws InterruptedException {\r\n    System.setProperty(\"samza.container.name\", \"samza-container-2\");\r\n    MapConfig mapConfig = new MapConfig(new ImmutableMap.Builder<String, String>().put(\"task.log4j.create.stream.enabled\", // Enable explicit stream creation\r\n    \"true\").put(\"job.name\", \"log4jTest\").put(\"job.id\", \"1\").put(\"systems.mock.samza.factory\", MockSystemFactory.class.getCanonicalName()).put(\"task.log4j.system\", \"mock\").put(\"job.container.count\", \"4\").build());\r\n    when(this.loggingContextHolder.getConfig()).thenReturn(mapConfig);\r\n    PatternLayout layout = PatternLayout.newBuilder().withPattern(\"%m\").build();\r\n    StreamAppender streamAppender = new StreamAppender(\"testName\", null, layout, false, true, null, this.loggingContextHolder);\r\n    startAndAttachAppender(streamAppender);\r\n    setupSystemExceptionAndVerifyMessages();\r\n    streamAppender.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\main\\java\\org\\apache\\samza\\monitor\\SamzaMonitorService.java",
  "methodName" : "createSchedulerAndScheduleMonitor",
  "sourceCode" : "/**\r\n * Creates a ScheduledThreadPoolExecutor with core pool size 1 and schedules the monitor to run every schedulingIntervalInMs\r\n */\r\n@VisibleForTesting\r\npublic void createSchedulerAndScheduleMonitor(String monitorName, MonitorConfig monitorConfig, long schedulingIntervalInMs) throws InstantiationException {\r\n    ThreadFactory threadFactory = new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"MonitorThread-%d\").build();\r\n    ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1, threadFactory);\r\n    scheduledExecutors.add(scheduledExecutorService);\r\n    scheduledExecutorService.scheduleAtFixedRate(getRunnable(instantiateMonitor(monitorName, monitorConfig, metricsRegistry)), 0, schedulingIntervalInMs, TimeUnit.MILLISECONDS);\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDeleteLocalTaskStoreWhenItHasNoOffsetFile",
  "sourceCode" : "@Test\r\npublic void shouldDeleteLocalTaskStoreWhenItHasNoOffsetFile() throws Exception {\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Task store directory should not exist.\", !taskStoreDir.exists());\r\n    assertEquals(taskStoreSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n    assertEquals(1, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDeleteLocalStoreWhenLastModifiedTimeOfOffsetFileIsGreaterThanOffsetTTL",
  "sourceCode" : "@Test\r\npublic void shouldDeleteLocalStoreWhenLastModifiedTimeOfOffsetFileIsGreaterThanOffsetTTL() throws Exception {\r\n    File offsetFile = createOffsetFile(taskStoreDir);\r\n    offsetFile.setLastModified(0);\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Offset file should not exist.\", !offsetFile.exists());\r\n    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDeleteInActiveLocalStoresOfTheJob",
  "sourceCode" : "@Test\r\npublic void shouldDeleteInActiveLocalStoresOfTheJob() throws Exception {\r\n    File inActiveStoreDir = new File(jobDir, \"inActiveStore\");\r\n    FileUtils.forceMkdir(inActiveStoreDir);\r\n    File inActiveTaskDir = new File(inActiveStoreDir, \"test-task\");\r\n    FileUtils.forceMkdir(inActiveTaskDir);\r\n    long inActiveTaskDirSize = inActiveTaskDir.getTotalSpace();\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Inactive task store directory should not exist.\", !inActiveTaskDir.exists());\r\n    assertEquals(taskStoreSize + inActiveTaskDirSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n    assertEquals(2, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());\r\n    if (inActiveStoreDir.exists() && inActiveStoreDir.isDirectory()) {\r\n        PathUtils.deleteDirectory(inActiveStoreDir.toPath());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDoNothingWhenLastModifiedTimeOfOffsetFileIsLessThanOffsetTTL",
  "sourceCode" : "@Test\r\npublic void shouldDoNothingWhenLastModifiedTimeOfOffsetFileIsLessThanOffsetTTL() throws Exception {\r\n    File offsetFile = createOffsetFile(taskStoreDir);\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Offset file should exist.\", offsetFile.exists());\r\n    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDoNothingWhenTheJobIsRunning",
  "sourceCode" : "@Test\r\npublic void shouldDoNothingWhenTheJobIsRunning() throws Exception {\r\n    Mockito.when(jobsClientMock.getJobStatus(Mockito.any())).thenReturn(JobStatus.STARTED);\r\n    File offsetFile = createOffsetFile(taskStoreDir);\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Offset file should exist.\", offsetFile.exists());\r\n    assertEquals(0, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldDeleteTaskStoreWhenTaskPreferredStoreIsNotLocalHost",
  "sourceCode" : "@Test\r\npublic void shouldDeleteTaskStoreWhenTaskPreferredStoreIsNotLocalHost() throws Exception {\r\n    Task task = new Task(\"notLocalHost\", \"test-task\", \"0\", new ArrayList<>(), ImmutableList.of(\"test-store\"));\r\n    Mockito.when(jobsClientMock.getTasks(Mockito.any())).thenReturn(ImmutableList.of(task));\r\n    localStoreMonitor.monitor();\r\n    assertTrue(\"Task store directory should not exist.\", !taskStoreDir.exists());\r\n    assertEquals(taskStoreSize, localStoreMonitorMetrics.diskSpaceFreedInBytes.getCount());\r\n    assertEquals(1, localStoreMonitorMetrics.noOfDeletedTaskPartitionStores.getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestLocalStoreMonitor.java",
  "methodName" : "shouldContinueLocalStoreCleanUpAfterFailureToCleanUpStoreOfAJob",
  "sourceCode" : "@Test\r\npublic void shouldContinueLocalStoreCleanUpAfterFailureToCleanUpStoreOfAJob() throws Exception {\r\n    File testFailingJobDir = new File(localStoreDir, \"test-jobName-jobId-1\");\r\n    File testFailingTaskStoreDir = new File(new File(testFailingJobDir, \"test-store\"), \"test-task\");\r\n    FileUtils.forceMkdir(testFailingTaskStoreDir);\r\n    // For job: test-jobName-jobId-1, throw up in getTasks call and\r\n    // expect the cleanup to succeed for other job: test-jobName-jobId.\r\n    Mockito.doThrow(new RuntimeException(\"Dummy exception message.\")).when(jobsClientMock).getTasks(new JobInstance(\"test-jobName\", \"jobId-1\"));\r\n    Task task = new Task(\"notLocalHost\", \"test-task\", \"0\", new ArrayList<>(), ImmutableList.of(\"test-store\"));\r\n    Mockito.when(jobsClientMock.getTasks(new JobInstance(\"test-jobName\", \"jobId\"))).thenReturn(ImmutableList.of(task));\r\n    Map<String, String> configMap = new HashMap<>(config);\r\n    configMap.put(LocalStoreMonitorConfig.CONFIG_IGNORE_FAILURES, \"true\");\r\n    LocalStoreMonitor localStoreMonitor = new LocalStoreMonitor(new LocalStoreMonitorConfig(new MapConfig(configMap)), localStoreMonitorMetrics, jobsClientMock);\r\n    localStoreMonitor.monitor();\r\n    // Non failing job directory should be cleaned up.\r\n    assertTrue(\"Task store directory should not exist.\", !taskStoreDir.exists());\r\n    if (testFailingJobDir.exists() && testFailingJobDir.isDirectory()) {\r\n        PathUtils.deleteDirectory(testFailingJobDir.toPath());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testMonitorsShouldBeInstantiatedProperly",
  "sourceCode" : "@Test\r\npublic void testMonitorsShouldBeInstantiatedProperly() {\r\n    // Test that a monitor should be instantiated properly by invoking\r\n    // the appropriate factory method.\r\n    Map<String, String> configMap = ImmutableMap.of(CONFIG_MONITOR_FACTORY_CLASS, DummyMonitorFactory.class.getCanonicalName());\r\n    Monitor monitor = null;\r\n    try {\r\n        monitor = MonitorLoader.instantiateMonitor(\"testMonitor\", new MonitorConfig(new MapConfig(configMap)), METRICS_REGISTRY);\r\n    } catch (InstantiationException e) {\r\n        fail();\r\n    }\r\n    assertNotNull(monitor);\r\n    // Object should implement the monitor().\r\n    try {\r\n        monitor.monitor();\r\n    } catch (Exception e) {\r\n        fail();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testShouldGroupRelevantMonitorConfigTogether",
  "sourceCode" : "@Test\r\npublic void testShouldGroupRelevantMonitorConfigTogether() {\r\n    // Test that Monitor Loader groups relevant config together.\r\n    Map<String, String> firstMonitorConfig = ImmutableMap.of(\"monitor.monitor1.factory.class\", \"org.apache.samza.monitor.DummyMonitor\", \"monitor.monitor1.scheduling.interval.ms\", \"100\");\r\n    Map<String, String> secondMonitorConfig = ImmutableMap.of(\"monitor.monitor2.factory.class\", \"org.apache.samza.monitor.DummyMonitor\", \"monitor.monitor2.scheduling.interval.ms\", \"200\");\r\n    MapConfig mapConfig = new MapConfig(ImmutableList.of(firstMonitorConfig, secondMonitorConfig));\r\n    MonitorConfig expectedFirstConfig = new MonitorConfig(new MapConfig(firstMonitorConfig).subset(\"monitor.monitor1.\"));\r\n    MonitorConfig expectedSecondConfig = new MonitorConfig(new MapConfig(secondMonitorConfig).subset(\"monitor.monitor2.\"));\r\n    Map<String, MonitorConfig> expected = ImmutableMap.of(\"monitor1\", expectedFirstConfig, \"monitor2\", expectedSecondConfig);\r\n    assertEquals(expected, MonitorConfig.getMonitorConfigs(mapConfig));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testMonitorExceptionIsolation",
  "sourceCode" : "@Test\r\npublic void testMonitorExceptionIsolation() {\r\n    // Test that an exception from a monitor doesn't bubble up out of the scheduler.\r\n    Map<String, String> configMap = ImmutableMap.of(String.format(\"monitor.name.%s\", CONFIG_MONITOR_FACTORY_CLASS), ExceptionThrowingMonitorFactory.class.getCanonicalName());\r\n    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));\r\n    SamzaMonitorService monitorService = new SamzaMonitorService(config, METRICS_REGISTRY);\r\n    // This will throw if the exception isn't caught within the provider.\r\n    monitorService.start();\r\n    monitorService.stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testShouldNotFailWhenTheMonitorFactoryClassIsNotDefined",
  "sourceCode" : "@Test\r\npublic void testShouldNotFailWhenTheMonitorFactoryClassIsNotDefined() throws Exception {\r\n    // Test that when MonitorFactoryClass is not defined in the config, monitor service\r\n    // should not fail.\r\n    Map<String, String> configMap = ImmutableMap.of(\"monitor.monitor1.config.key1\", \"configValue1\", \"monitor.monitor1.config.key2\", \"configValue2\", String.format(\"monitor.MOCK_MONITOR.%s\", CONFIG_MONITOR_FACTORY_CLASS), MockMonitorFactory.class.getCanonicalName());\r\n    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));\r\n    class SamzaMonitorServiceTest extends SamzaMonitorService {\r\n\r\n        MetricsRegistry metricsRegistry;\r\n\r\n        public SamzaMonitorServiceTest(SamzaRestConfig config, MetricsRegistry metricsRegistry) {\r\n            super(config, metricsRegistry);\r\n            this.metricsRegistry = metricsRegistry;\r\n        }\r\n\r\n        @Override\r\n        public void createSchedulerAndScheduleMonitor(String monitorName, MonitorConfig monitorConfig, long schedulingIntervalInMs) {\r\n            try {\r\n                // immediately run monitor, without scheduling\r\n                instantiateMonitor(monitorName, monitorConfig, metricsRegistry).monitor();\r\n            } catch (Exception e) {\r\n                fail();\r\n            }\r\n        }\r\n    }\r\n    SamzaMonitorService monitorService = new SamzaMonitorServiceTest(config, METRICS_REGISTRY);\r\n    try {\r\n        monitorService.start();\r\n    } catch (Exception e) {\r\n        fail();\r\n    }\r\n    Mockito.verify(MockMonitorFactory.MOCK_MONITOR, Mockito.times(1)).monitor();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testShouldFailWhenTheMonitorFactoryClassIsInvalid",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testShouldFailWhenTheMonitorFactoryClassIsInvalid() {\r\n    // Test that when MonitorFactoryClass is defined in the config and is invalid,\r\n    // monitor service should fail. Should throw back SamzaException.\r\n    Map<String, String> configMap = ImmutableMap.of(String.format(\"monitor.name.%s\", CONFIG_MONITOR_FACTORY_CLASS), \"RandomClassName\");\r\n    SamzaRestConfig config = new SamzaRestConfig(new MapConfig(configMap));\r\n    SamzaMonitorService monitorService = new SamzaMonitorService(config, METRICS_REGISTRY);\r\n    monitorService.start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\monitor\\TestMonitorService.java",
  "methodName" : "testScheduledExecutorSchedulingProvider",
  "sourceCode" : "@Test\r\npublic void testScheduledExecutorSchedulingProvider() {\r\n    // Test that the monitor is scheduled by the ScheduledExecutorSchedulingProvider\r\n    ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1);\r\n    // notifyingMonitor.monitor() should be called repeatedly.\r\n    final CountDownLatch wasCalledLatch = new CountDownLatch(3);\r\n    final Monitor notifyingMonitor = new Monitor() {\r\n\r\n        @Override\r\n        public void monitor() {\r\n            wasCalledLatch.countDown();\r\n        }\r\n    };\r\n    Runnable runnableMonitor = new Runnable() {\r\n\r\n        public void run() {\r\n            try {\r\n                notifyingMonitor.monitor();\r\n            } catch (Exception e) {\r\n                // Must be caught because they are checked in monitor()\r\n                fail();\r\n            }\r\n        }\r\n    };\r\n    // monitor should get called every 1ms, so if await() misses the first call, there will be more.\r\n    executorService.scheduleAtFixedRate(runnableMonitor, 0, 1, TimeUnit.MILLISECONDS);\r\n    try {\r\n        assertTrue(wasCalledLatch.await(5L, TimeUnit.SECONDS));\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n    } finally {\r\n        executorService.shutdownNow();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\proxy\\job\\TestYarnRestJobStatusProvider.java",
  "methodName" : "testGetJobStatuses",
  "sourceCode" : "@Test\r\npublic void testGetJobStatuses() throws IOException, InterruptedException {\r\n    doReturn(APPS_RESPONSE.getBytes()).when(provider).httpGet(anyString());\r\n    List<Job> jobs = Lists.newArrayList(// Job with multiple applications, 1 RUNNING\r\n    new Job(\"job1\", \"1\"), // Job with 1 KILLED application\r\n    new Job(\"job2\", \"1\"), // Job with 1 RUNNING application\r\n    new Job(\"job3\", \"1\"), // Job not found in YARN\r\n    new Job(\"job4\", \"1\"));\r\n    provider.getJobStatuses(jobs);\r\n    Collections.sort(jobs, (o1, o2) -> o1.getJobName().compareTo(o2.getJobName()));\r\n    assertEquals(4, jobs.size());\r\n    verifyJobStatus(jobs.get(0), \"job1\", JobStatus.STARTED, \"RUNNING\");\r\n    verifyJobStatus(jobs.get(1), \"job2\", JobStatus.STOPPED, \"KILLED\");\r\n    verifyJobStatus(jobs.get(2), \"job3\", JobStatus.STARTED, \"RUNNING\");\r\n    verifyJobStatus(jobs.get(3), \"job4\", JobStatus.UNKNOWN, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testGetJobs",
  "sourceCode" : "@Test\r\npublic void testGetJobs() throws IOException {\r\n    Response resp = target(\"v1/jobs\").request().get();\r\n    assertEquals(200, resp.getStatus());\r\n    final Job[] jobs = objectMapper.readValue(resp.readEntity(String.class), Job[].class);\r\n    assertEquals(4, jobs.length);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_1_NAME, jobs[0].getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_1_ID, jobs[0].getJobId());\r\n    assertStatusNotDefault(jobs[0]);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, jobs[1].getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, jobs[1].getJobId());\r\n    assertStatusNotDefault(jobs[1]);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_3_NAME, jobs[2].getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_3_ID, jobs[2].getJobId());\r\n    assertStatusNotDefault(jobs[2]);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_4_NAME, jobs[3].getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_4_ID, jobs[3].getJobId());\r\n    assertStatusNotDefault(jobs[3]);\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testPostJobs",
  "sourceCode" : "@Test\r\npublic void testPostJobs() throws IOException {\r\n    Response resp = target(\"v1/jobs\").request().post(Entity.text(\"\"));\r\n    assertEquals(405, resp.getStatus());\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testPutJobs",
  "sourceCode" : "@Test\r\npublic void testPutJobs() throws IOException {\r\n    Response resp = target(\"v1/jobs\").request().put(Entity.text(\"\"));\r\n    assertEquals(405, resp.getStatus());\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testGetJob",
  "sourceCode" : "@Test\r\npublic void testGetJob() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request().get();\r\n    assertEquals(200, resp.getStatus());\r\n    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());\r\n    assertStatusNotDefault(job2);\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testPostJob",
  "sourceCode" : "@Test\r\npublic void testPostJob() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request().post(Entity.text(\"\"));\r\n    assertEquals(405, resp.getStatus());\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testGetJobNameNotFound",
  "sourceCode" : "@Test\r\npublic void testGetJobNameNotFound() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", \"BadJobName\", MockJobProxy.JOB_INSTANCE_2_ID)).request().get();\r\n    assertEquals(404, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\"), errorMessage.get(\"message\").contains(\"does not exist\"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testGetJobIdNotFound",
  "sourceCode" : "@Test\r\npublic void testGetJobIdNotFound() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, \"BadJobId\")).request().get();\r\n    assertEquals(404, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\"), errorMessage.get(\"message\").contains(\"does not exist\"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testGetJobNameWithoutId",
  "sourceCode" : "@Test\r\npublic void testGetJobNameWithoutId() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s\", MockJobProxy.JOB_INSTANCE_2_NAME)).request().get();\r\n    assertEquals(404, resp.getStatus());\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testStartJob",
  "sourceCode" : "@Test\r\npublic void testStartJob() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).queryParam(\"status\", \"started\").request().put(Entity.form(new Form()));\r\n    assertEquals(202, resp.getStatus());\r\n    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());\r\n    assertStatusNotDefault(job2);\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testStopJob",
  "sourceCode" : "@Test\r\npublic void testStopJob() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).queryParam(\"status\", \"stopped\").request().put(Entity.form(new Form()));\r\n    assertEquals(202, resp.getStatus());\r\n    final Job job2 = objectMapper.readValue(resp.readEntity(String.class), Job.class);\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_NAME, job2.getJobName());\r\n    assertEquals(MockJobProxy.JOB_INSTANCE_2_ID, job2.getJobId());\r\n    assertStatusNotDefault(job2);\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testPutBadJobStatus",
  "sourceCode" : "@Test\r\npublic void testPutBadJobStatus() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).queryParam(\"status\", \"BADSTATUS\").request().put(Entity.form(new Form()));\r\n    assertEquals(400, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\").contains(\"BADSTATUS\"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestJobsResource.java",
  "methodName" : "testPutMissingStatus",
  "sourceCode" : "@Test\r\npublic void testPutMissingStatus() throws IOException {\r\n    Response resp = target(String.format(\"v1/jobs/%s/%s\", MockJobProxy.JOB_INSTANCE_2_NAME, MockJobProxy.JOB_INSTANCE_2_ID)).request().put(Entity.form(new Form()));\r\n    assertEquals(400, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\").contains(\"status\"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestTasksResource.java",
  "methodName" : "testGetTasks",
  "sourceCode" : "@Test\r\npublic void testGetTasks() throws IOException {\r\n    String requestUrl = String.format(\"v1/jobs/%s/%s/tasks\", \"testJobName\", \"testJobId\");\r\n    Response response = target(requestUrl).request().get();\r\n    assertEquals(200, response.getStatus());\r\n    Task[] tasks = objectMapper.readValue(response.readEntity(String.class), Task[].class);\r\n    assertEquals(2, tasks.length);\r\n    assertEquals(MockTaskProxy.TASK_1_PREFERRED_HOST, tasks[0].getPreferredHost());\r\n    assertEquals(MockTaskProxy.TASK_1_CONTAINER_ID, tasks[0].getContainerId());\r\n    assertEquals(MockTaskProxy.TASK_1_NAME, tasks[0].getTaskName());\r\n    assertEquals(MockTaskProxy.PARTITIONS, tasks[0].getPartitions());\r\n    assertEquals(MockTaskProxy.TASK_2_PREFERRED_HOST, tasks[1].getPreferredHost());\r\n    assertEquals(MockTaskProxy.TASK_2_CONTAINER_ID, tasks[1].getContainerId());\r\n    assertEquals(MockTaskProxy.TASK_2_NAME, tasks[1].getTaskName());\r\n    assertEquals(MockTaskProxy.PARTITIONS, tasks[1].getPartitions());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestTasksResource.java",
  "methodName" : "testGetTasksWithInvalidJobName",
  "sourceCode" : "@Test\r\npublic void testGetTasksWithInvalidJobName() throws IOException {\r\n    String requestUrl = String.format(\"v1/jobs/%s/%s/tasks\", \"BadJobName\", MockJobProxy.JOB_INSTANCE_4_ID);\r\n    Response resp = target(requestUrl).request().get();\r\n    assertEquals(400, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\"), errorMessage.get(\"message\").contains(\"Invalid arguments for getTasks. \"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\resources\\TestTasksResource.java",
  "methodName" : "testGetTasksWithInvalidJobId",
  "sourceCode" : "@Test\r\npublic void testGetTasksWithInvalidJobId() throws IOException {\r\n    String requestUrl = String.format(\"v1/jobs/%s/%s/tasks\", MockJobProxy.JOB_INSTANCE_1_NAME, \"BadJobId\");\r\n    Response resp = target(requestUrl).request().get();\r\n    assertEquals(400, resp.getStatus());\r\n    final Map<String, String> errorMessage = objectMapper.readValue(resp.readEntity(String.class), new TypeReference<Map<String, String>>() {\r\n    });\r\n    assertTrue(errorMessage.get(\"message\"), errorMessage.get(\"message\").contains(\"Invalid arguments for getTasks. \"));\r\n    resp.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\TestSamzaRestService.java",
  "methodName" : "testStartShouldStartTheMetricsReportersAndServer",
  "sourceCode" : "@Test\r\npublic void testStartShouldStartTheMetricsReportersAndServer() throws Exception {\r\n    NetworkConnector connector = Mockito.mock(NetworkConnector.class);\r\n    int testServerPort = 100;\r\n    Mockito.doReturn(testServerPort).when(connector).getPort();\r\n    Mockito.when(server.getConnectors()).thenReturn(new NetworkConnector[] { connector });\r\n    Mockito.doNothing().when(server).start();\r\n    samzaRestService.start();\r\n    Mockito.verify(metricsReporter).start();\r\n    Mockito.verify(metricsReporter).register(\"SamzaRest\", metricsRegistry);\r\n    Mockito.verify(server).start();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-rest\\src\\test\\java\\org\\apache\\samza\\rest\\TestSamzaRestService.java",
  "methodName" : "testStopShouldStopTheMetricsReportersAndStopTheServer",
  "sourceCode" : "@Test\r\npublic void testStopShouldStopTheMetricsReportersAndStopTheServer() throws Exception {\r\n    samzaRestService.stop();\r\n    Mockito.verify(metricsReporter).stop();\r\n    Mockito.verify(server).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\planner\\Checker.java",
  "methodName" : "hasOneUdfMethod",
  "sourceCode" : "/**\r\n * Checks if there is only one UdfMethod in the input {@link UdfMetadata}.\r\n * @param udfMetadata the metadata for a UDF.\r\n * @return true if there is only one udf method defined in the UdfMetadata.\r\n *         false otherwise.\r\n */\r\n@VisibleForTesting\r\nboolean hasOneUdfMethod(UdfMetadata udfMetadata) {\r\n    Class<?> udfClass = udfMetadata.getUdfMethod().getDeclaringClass();\r\n    int numAnnotatedUdfMethods = 0;\r\n    for (Method method : udfClass.getMethods()) {\r\n        if (method.isAnnotationPresent(SamzaSqlUdfMethod.class)) {\r\n            numAnnotatedUdfMethods += 1;\r\n        }\r\n    }\r\n    return numAnnotatedUdfMethods == 1;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\planner\\Checker.java",
  "methodName" : "toCalciteSqlType",
  "sourceCode" : "/**\r\n * Converts the {@link SamzaSqlFieldType} to the calcite {@link SqlTypeName}.\r\n * @param samzaSqlFieldType the samza sql field type.\r\n * @return the converted calcite SqlTypeName.\r\n */\r\n@VisibleForTesting\r\nstatic SqlTypeName toCalciteSqlType(SamzaSqlFieldType samzaSqlFieldType) {\r\n    switch(samzaSqlFieldType) {\r\n        case ANY:\r\n        case ROW:\r\n            return SqlTypeName.ANY;\r\n        case MAP:\r\n            return SqlTypeName.MAP;\r\n        case ARRAY:\r\n            return SqlTypeName.ARRAY;\r\n        case REAL:\r\n            return SqlTypeName.REAL;\r\n        case DOUBLE:\r\n            return SqlTypeName.DOUBLE;\r\n        case STRING:\r\n            return SqlTypeName.VARCHAR;\r\n        case INT16:\r\n        case INT32:\r\n            return SqlTypeName.INTEGER;\r\n        case FLOAT:\r\n            return SqlTypeName.FLOAT;\r\n        case INT64:\r\n            return SqlTypeName.BIGINT;\r\n        case BOOLEAN:\r\n            return SqlTypeName.BOOLEAN;\r\n        case BYTES:\r\n            return SqlTypeName.VARBINARY;\r\n        default:\r\n            String msg = String.format(\"Field Type %s is not supported\", samzaSqlFieldType);\r\n            LOG.error(msg);\r\n            throw new SamzaException(msg);\r\n    }\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\translator\\JoinTranslator.java",
  "methodName" : "getInputMetricsMF",
  "sourceCode" : "@VisibleForTesting\r\npublic TranslatorInputMetricsMapFunction getInputMetricsMF() {\r\n    return this.inputMetricsMF;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\translator\\JoinTranslator.java",
  "methodName" : "getOutputMetricsMF",
  "sourceCode" : "@VisibleForTesting\r\npublic TranslatorOutputMetricsMapFunction getOutputMetricsMF() {\r\n    return this.outputMetricsMF;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\translator\\QueryTranslator.java",
  "methodName" : "translate",
  "sourceCode" : "/**\r\n * For unit testing only\r\n */\r\n@VisibleForTesting\r\nvoid translate(SamzaSqlQueryParser.QueryInfo queryInfo, StreamApplicationDescriptor appDesc, int queryId) {\r\n    QueryPlanner planner = new QueryPlanner(sqlConfig.getRelSchemaProviders(), sqlConfig.getInputSystemStreamConfigBySource(), sqlConfig.getUdfMetadata(), sqlConfig.isQueryPlanOptimizerEnabled());\r\n    final RelRoot relRoot = planner.plan(queryInfo.getSelectQuery());\r\n    SamzaSqlExecutionContext executionContext = new SamzaSqlExecutionContext(sqlConfig);\r\n    TranslatorContext translatorContext = new TranslatorContext(appDesc, relRoot, executionContext);\r\n    translate(relRoot, sqlConfig.getOutputSystemStreams().get(queryId), translatorContext, queryId);\r\n    Map<Integer, TranslatorContext> translatorContexts = new HashMap<>();\r\n    translatorContexts.put(queryId, translatorContext.clone());\r\n    appDesc.withApplicationTaskContextFactory(new ApplicationTaskContextFactory<SamzaSqlApplicationContext>() {\r\n\r\n        @Override\r\n        public SamzaSqlApplicationContext create(ExternalContext externalContext, JobContext jobContext, ContainerContext containerContext, TaskContext taskContext, ApplicationContainerContext applicationContainerContext) {\r\n            return new SamzaSqlApplicationContext(translatorContexts);\r\n        }\r\n    });\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\main\\java\\org\\apache\\samza\\sql\\udf\\ReflectionBasedUdfResolver.java",
  "methodName" : "getUrls",
  "sourceCode" : "@VisibleForTesting\r\nList<URL> getUrls(Config udfConfig) {\r\n    String urls = udfConfig.getOrDefault(CONFIG_RESOURCE_URLS, \"\");\r\n    List<URL> urlList = new ArrayList<>();\r\n    if (!urls.isEmpty()) {\r\n        for (String url : urls.split(\",\")) {\r\n            try {\r\n                urlList.add(new URL(url));\r\n            } catch (MalformedURLException e) {\r\n                LOG.error(\"Exception occurred when loading url: {}\", url, e);\r\n            }\r\n        }\r\n    }\r\n    return urlList;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testSimpleSchemaConversion",
  "sourceCode" : "@Test\r\npublic void testSimpleSchemaConversion() {\r\n    String streamName = \"stream\";\r\n    SqlSchema sqlSchema = simpleRecordSchemaProvider.getSqlSchema();\r\n    RelDataType dataType = relSchemaConverter.convertToRelSchema(sqlSchema);\r\n    junit.framework.Assert.assertTrue(dataType instanceof RelRecordType);\r\n    RelRecordType recordType = (RelRecordType) dataType;\r\n    junit.framework.Assert.assertEquals(recordType.getFieldCount(), SimpleRecord.SCHEMA$.getFields().size());\r\n    junit.framework.Assert.assertTrue(recordType.getField(\"id\", true, false).getType().getSqlTypeName() == SqlTypeName.INTEGER);\r\n    junit.framework.Assert.assertTrue(recordType.getField(\"name\", true, false).getType().getSqlTypeName() == SqlTypeName.VARCHAR);\r\n    LOG.info(\"Relational schema \" + dataType);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testComplexSchemaConversion",
  "sourceCode" : "@Test\r\npublic void testComplexSchemaConversion() {\r\n    RelDataType relSchema = relSchemaConverter.convertToRelSchema(complexRecordSchemaProvider.getSqlSchema());\r\n    LOG.info(\"Relational schema \" + relSchema);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testNestedSchemaConversion",
  "sourceCode" : "@Test\r\npublic void testNestedSchemaConversion() {\r\n    RelDataType relSchema = relSchemaConverter.convertToRelSchema(nestedRecordSchemaProvider.getSqlSchema());\r\n    LOG.info(\"Relational schema \" + relSchema);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testSimpleRecordConversion",
  "sourceCode" : "@Test\r\npublic void testSimpleRecordConversion() {\r\n    GenericData.Record record = new GenericData.Record(SimpleRecord.SCHEMA$);\r\n    record.put(\"id\", 1);\r\n    record.put(\"name\", \"name1\");\r\n    SamzaSqlRelMessage message = simpleRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", record));\r\n    LOG.info(message.toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testEmptyRecordConversion",
  "sourceCode" : "@Test\r\npublic void testEmptyRecordConversion() {\r\n    GenericData.Record record = new GenericData.Record(SimpleRecord.SCHEMA$);\r\n    SamzaSqlRelMessage message = simpleRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", record));\r\n    Assert.assertEquals(message.getSamzaSqlRelRecord().getFieldNames().size(), message.getSamzaSqlRelRecord().getFieldValues().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testNullRecordConversion",
  "sourceCode" : "@Test\r\npublic void testNullRecordConversion() {\r\n    SamzaSqlRelMessage message = simpleRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", null));\r\n    Assert.assertEquals(message.getSamzaSqlRelRecord().getFieldNames().size(), message.getSamzaSqlRelRecord().getFieldValues().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testComplexRecordConversion",
  "sourceCode" : "@Test\r\npublic void testComplexRecordConversion() throws IOException {\r\n    GenericData.Record record = new GenericData.Record(ComplexRecord.SCHEMA$);\r\n    record.put(\"id\", id);\r\n    record.put(\"bool_value\", boolValue);\r\n    record.put(\"double_value\", doubleValue);\r\n    record.put(\"float_value0\", floatValue);\r\n    record.put(\"string_value\", testStrValue);\r\n    record.put(\"bytes_value\", testBytes);\r\n    record.put(\"fixed_value\", fixedBytes);\r\n    record.put(\"long_value\", longValue);\r\n    record.put(\"array_values\", arrayValue);\r\n    record.put(\"map_values\", mapValue);\r\n    record.put(\"union_value\", testStrValue);\r\n    ComplexRecord complexRecord = new ComplexRecord();\r\n    complexRecord.id = id;\r\n    complexRecord.bool_value = boolValue;\r\n    complexRecord.double_value = doubleValue;\r\n    complexRecord.float_value0 = floatValue;\r\n    complexRecord.string_value = testStrValue;\r\n    complexRecord.bytes_value = testBytes;\r\n    complexRecord.fixed_value = fixedBytes;\r\n    complexRecord.long_value = longValue;\r\n    complexRecord.array_values = new ArrayList<>();\r\n    complexRecord.array_values.addAll(arrayValue);\r\n    complexRecord.map_values = new HashMap<>();\r\n    complexRecord.map_values.putAll(mapValue);\r\n    complexRecord.union_value = testStrValue;\r\n    byte[] serializedData = bytesFromGenericRecord(record);\r\n    validateAvroSerializedData(serializedData, testStrValue);\r\n    serializedData = encodeAvroSpecificRecord(ComplexRecord.class, complexRecord);\r\n    validateAvroSerializedData(serializedData, testStrValue);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testComplexUnionConversionShouldWorkWithBothStringAndIntTypes",
  "sourceCode" : "@Test\r\npublic void testComplexUnionConversionShouldWorkWithBothStringAndIntTypes() throws Exception {\r\n    // ComplexUnion is a nested avro non-nullable union-type with both String and Integer type\r\n    // Test the complex-union conversion for String type.\r\n    GenericData.Record record = new GenericData.Record(ComplexUnion.SCHEMA$);\r\n    record.put(\"non_nullable_union_value\", testStrValue);\r\n    ComplexUnion complexUnion = new ComplexUnion();\r\n    complexUnion.non_nullable_union_value = testStrValue;\r\n    byte[] serializedData = bytesFromGenericRecord(record);\r\n    GenericRecord genericRecord = genericRecordFromBytes(serializedData, ComplexUnion.SCHEMA$);\r\n    SamzaSqlRelMessage message = complexUnionAvroRelConverter.convertToRelMessage(new KV<>(\"key\", genericRecord));\r\n    Assert.assertEquals(testStrValue, message.getSamzaSqlRelRecord().getField(\"non_nullable_union_value\").get().toString());\r\n    serializedData = encodeAvroSpecificRecord(ComplexUnion.class, complexUnion);\r\n    genericRecord = genericRecordFromBytes(serializedData, ComplexUnion.SCHEMA$);\r\n    Assert.assertEquals(testStrValue, genericRecord.get(\"non_nullable_union_value\").toString());\r\n    // Testing the complex-union conversion for Integer type\r\n    record.put(\"non_nullable_union_value\", Integer.valueOf(123));\r\n    complexUnion.non_nullable_union_value = Integer.valueOf(123);\r\n    serializedData = bytesFromGenericRecord(record);\r\n    genericRecord = genericRecordFromBytes(serializedData, ComplexUnion.SCHEMA$);\r\n    message = complexUnionAvroRelConverter.convertToRelMessage(new KV<>(\"key\", genericRecord));\r\n    Assert.assertEquals(Integer.valueOf(123), message.getSamzaSqlRelRecord().getField(\"non_nullable_union_value\").get());\r\n    serializedData = encodeAvroSpecificRecord(ComplexUnion.class, complexUnion);\r\n    genericRecord = genericRecordFromBytes(serializedData, ComplexUnion.SCHEMA$);\r\n    Assert.assertEquals(Integer.valueOf(123), genericRecord.get(\"non_nullable_union_value\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testNestedRecordConversion",
  "sourceCode" : "@Test\r\npublic void testNestedRecordConversion() throws IOException {\r\n    GenericData.Record record = new GenericData.Record(Profile.SCHEMA$);\r\n    record.put(\"id\", 1);\r\n    record.put(\"name\", \"name1\");\r\n    record.put(\"companyId\", 0);\r\n    GenericData.Record addressRecord = new GenericData.Record(AddressRecord.SCHEMA$);\r\n    addressRecord.put(\"zip\", 90000);\r\n    GenericData.Record streetNumRecord = new GenericData.Record(StreetNumRecord.SCHEMA$);\r\n    streetNumRecord.put(\"number\", 1200);\r\n    addressRecord.put(\"streetnum\", streetNumRecord);\r\n    record.put(\"address\", addressRecord);\r\n    record.put(\"selfEmployed\", \"True\");\r\n    GenericData.Record phoneNumberRecordH = new GenericData.Record(PhoneNumber.SCHEMA$);\r\n    phoneNumberRecordH.put(\"kind\", Kind.Home);\r\n    phoneNumberRecordH.put(\"number\", \"111-111-1111\");\r\n    GenericData.Record phoneNumberRecordC = new GenericData.Record(PhoneNumber.SCHEMA$);\r\n    phoneNumberRecordC.put(\"kind\", Kind.Cell);\r\n    phoneNumberRecordC.put(\"number\", \"111-111-1112\");\r\n    List<GenericData.Record> phoneNumbers = new ArrayList<>();\r\n    phoneNumbers.add(phoneNumberRecordH);\r\n    phoneNumbers.add(phoneNumberRecordC);\r\n    record.put(\"phoneNumbers\", phoneNumbers);\r\n    GenericData.Record simpleRecord1 = new GenericData.Record(SimpleRecord.SCHEMA$);\r\n    simpleRecord1.put(\"id\", 1);\r\n    simpleRecord1.put(\"name\", \"name1\");\r\n    GenericData.Record simpleRecord2 = new GenericData.Record(SimpleRecord.SCHEMA$);\r\n    simpleRecord2.put(\"id\", 2);\r\n    simpleRecord2.put(\"name\", \"name2\");\r\n    HashMap<String, IndexedRecord> mapValues = new HashMap<>();\r\n    mapValues.put(\"key1\", simpleRecord1);\r\n    mapValues.put(\"key2\", simpleRecord2);\r\n    record.put(\"mapValues\", mapValues);\r\n    SamzaSqlRelMessage relMessage = nestedRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", record));\r\n    LOG.info(relMessage.toString());\r\n    KV<Object, Object> samzaMessage = nestedRecordAvroRelConverter.convertToSamzaMessage(relMessage);\r\n    GenericRecord recordPostConversion = (GenericRecord) samzaMessage.getValue();\r\n    for (Schema.Field field : Profile.SCHEMA$.getFields()) {\r\n        // equals() on GenericRecord does the nested record equality check as well.\r\n        Assert.assertEquals(record.get(field.name()), recordPostConversion.get(field.name()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testRecordConversionWithNullPayload",
  "sourceCode" : "// SAMZA-2110 We need to enable this when we have a true support for Null records\r\n@Ignore\r\n@Test\r\npublic void testRecordConversionWithNullPayload() throws IOException {\r\n    GenericData.Record record = null;\r\n    SamzaSqlRelMessage relMessage = nestedRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", record));\r\n    LOG.info(relMessage.toString());\r\n    KV<Object, Object> samzaMessage = nestedRecordAvroRelConverter.convertToSamzaMessage(relMessage);\r\n    GenericRecord recordPostConversion = (GenericRecord) samzaMessage.getValue();\r\n    Assert.assertTrue(recordPostConversion == null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\avro\\TestAvroRelConversion.java",
  "methodName" : "testNestedRecordConversionWithSubRecordsBeingNull",
  "sourceCode" : "@Test\r\npublic void testNestedRecordConversionWithSubRecordsBeingNull() throws IOException {\r\n    GenericData.Record record = new GenericData.Record(Profile.SCHEMA$);\r\n    record.put(\"id\", 1);\r\n    record.put(\"name\", \"name1\");\r\n    record.put(\"companyId\", 0);\r\n    GenericData.Record addressRecord = null;\r\n    record.put(\"address\", addressRecord);\r\n    record.put(\"selfEmployed\", \"True\");\r\n    List<GenericData.Record> phoneNumbers = null;\r\n    record.put(\"phoneNumbers\", phoneNumbers);\r\n    HashMap<String, IndexedRecord> mapValues = null;\r\n    record.put(\"mapValues\", mapValues);\r\n    SamzaSqlRelMessage relMessage = nestedRecordAvroRelConverter.convertToRelMessage(new KV<>(\"key\", record));\r\n    LOG.info(relMessage.toString());\r\n    KV<Object, Object> samzaMessage = nestedRecordAvroRelConverter.convertToSamzaMessage(relMessage);\r\n    GenericRecord recordPostConversion = (GenericRecord) samzaMessage.getValue();\r\n    for (Schema.Field field : Profile.SCHEMA$.getFields()) {\r\n        // equals() on GenericRecord does the nested record equality check as well.\r\n        Assert.assertEquals(record.get(field.name()), recordPostConversion.get(field.name()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testGetField",
  "sourceCode" : "@Test\r\npublic void testGetField() {\r\n    SamzaSqlRelMessage message = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    Assert.assertEquals(values.get(0), message.getSamzaSqlRelRecord().getField(names.get(0)).get());\r\n    Assert.assertEquals(values.get(1), message.getSamzaSqlRelRecord().getField(names.get(1)).get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testGetNonExistentField",
  "sourceCode" : "@Test\r\npublic void testGetNonExistentField() {\r\n    SamzaSqlRelMessage message = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    Assert.assertFalse(message.getSamzaSqlRelRecord().getField(\"field3\").isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testEquality",
  "sourceCode" : "@Test\r\npublic void testEquality() {\r\n    SamzaSqlRelMessage message1 = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessage message2 = new SamzaSqlRelMessage(Arrays.asList(\"field1\", \"field2\"), Arrays.asList(\"value1\", \"value2\"), new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    Assert.assertEquals(message1, message2);\r\n    Assert.assertEquals(message1.hashCode(), message2.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testInEquality",
  "sourceCode" : "@Test\r\npublic void testInEquality() {\r\n    SamzaSqlRelMessage message1 = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessage message2 = new SamzaSqlRelMessage(Arrays.asList(\"field1\", \"field2\"), Arrays.asList(\"value2\", \"value2\"), new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    Assert.assertNotEquals(message1, message2);\r\n    Assert.assertNotEquals(message1.hashCode(), message2.hashCode());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testCompositeKeyCreation",
  "sourceCode" : "@Test\r\npublic void testCompositeKeyCreation() {\r\n    List<String> keyPartNames = Arrays.asList(\"kfield1\", \"kfield2\");\r\n    SamzaSqlRelMessage message = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelRecord relRecord1 = SamzaSqlRelMessage.createSamzaSqlCompositeKey(message, Collections.singletonList(0));\r\n    Assert.assertEquals(relRecord1.getFieldNames().size(), 1);\r\n    Assert.assertEquals(relRecord1.getFieldNames().get(0), \"field1\");\r\n    Assert.assertEquals(relRecord1.getFieldValues().get(0), \"value1\");\r\n    SamzaSqlRelRecord relRecord2 = SamzaSqlRelMessage.createSamzaSqlCompositeKey(message, Arrays.asList(1, 0), SamzaSqlRelMessage.getSamzaSqlCompositeKeyFieldNames(keyPartNames, Arrays.asList(1, 0)));\r\n    Assert.assertEquals(relRecord2.getFieldNames().size(), 2);\r\n    Assert.assertEquals(relRecord2.getFieldNames().get(0), \"kfield2\");\r\n    Assert.assertEquals(relRecord2.getFieldValues().get(0), \"value2\");\r\n    Assert.assertEquals(relRecord2.getFieldNames().get(1), \"kfield1\");\r\n    Assert.assertEquals(relRecord2.getFieldValues().get(1), \"value1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\data\\TestSamzaSqlRelMessage.java",
  "methodName" : "testCompositeKeyCreationWithInEqualKeyNameValues",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testCompositeKeyCreationWithInEqualKeyNameValues() {\r\n    List<String> keyPartNames = Arrays.asList(\"kfield1\", \"kfield2\");\r\n    SamzaSqlRelMessage message = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelRecord relRecord1 = SamzaSqlRelMessage.createSamzaSqlCompositeKey(message, Arrays.asList(1, 0), SamzaSqlRelMessage.getSamzaSqlCompositeKeyFieldNames(keyPartNames, Arrays.asList(1)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testNoArgs",
  "sourceCode" : "@Test\r\npublic void testNoArgs() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    SamzaSqlRelRecord actualRecord = buildOutputRecordUdf.execute();\r\n    SamzaSqlRelRecord expectedRecord = new SamzaSqlRelRecord(new ArrayList<>(), new ArrayList<>());\r\n    Assert.assertEquals(actualRecord.getFieldNames(), expectedRecord.getFieldNames());\r\n    Assert.assertEquals(actualRecord.getFieldValues(), expectedRecord.getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testSinglePair",
  "sourceCode" : "@Test\r\npublic void testSinglePair() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    SamzaSqlRelRecord actualRecord = buildOutputRecordUdf.execute(\"key\", \"value\");\r\n    SamzaSqlRelRecord expectedRecord = new SamzaSqlRelRecord(Arrays.asList(\"key\"), Arrays.asList(\"value\"));\r\n    Assert.assertEquals(actualRecord.getFieldNames(), expectedRecord.getFieldNames());\r\n    Assert.assertEquals(actualRecord.getFieldValues(), expectedRecord.getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testMultiPairs",
  "sourceCode" : "@Test\r\npublic void testMultiPairs() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    SamzaSqlRelRecord actualRecord = buildOutputRecordUdf.execute(\"k1\", \"v1\", \"k2\", \"v2\");\r\n    SamzaSqlRelRecord expectedRecord = new SamzaSqlRelRecord(Arrays.asList(\"k1\", \"k2\"), Arrays.asList(\"v1\", \"v2\"));\r\n    Assert.assertEquals(actualRecord.getFieldNames(), expectedRecord.getFieldNames());\r\n    Assert.assertEquals(actualRecord.getFieldValues(), expectedRecord.getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testNestedRecord",
  "sourceCode" : "@Test\r\npublic void testNestedRecord() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    SamzaSqlRelRecord nestedSamzaSqlRelRecord = new SamzaSqlRelRecord(Arrays.asList(\"k3\"), Arrays.asList(\"v3\"));\r\n    SamzaSqlRelRecord actualRecord = buildOutputRecordUdf.execute(\"k1\", \"v1\", \"k2\", nestedSamzaSqlRelRecord);\r\n    SamzaSqlRelRecord expectedRecord = new SamzaSqlRelRecord(Arrays.asList(\"k1\", \"k2\"), Arrays.asList(\"v1\", nestedSamzaSqlRelRecord));\r\n    Assert.assertEquals(actualRecord.getFieldNames(), expectedRecord.getFieldNames());\r\n    Assert.assertEquals(actualRecord.getFieldValues(), expectedRecord.getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testNullArgs",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNullArgs() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    buildOutputRecordUdf.execute(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestBuildOutputRecordUdf.java",
  "methodName" : "testOddNumOfArgs",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testOddNumOfArgs() {\r\n    BuildOutputRecordUdf buildOutputRecordUdf = new BuildOutputRecordUdf();\r\n    buildOutputRecordUdf.execute(\"k1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestConvertToStringUdf.java",
  "methodName" : "testConvertIntegerToString",
  "sourceCode" : "@Test\r\npublic void testConvertIntegerToString() {\r\n    ConvertToStringUdf convertToStringUdf = new ConvertToStringUdf();\r\n    Assert.assertEquals(convertToStringUdf.execute(10), \"10\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestConvertToStringUdf.java",
  "methodName" : "testConvertLongToString",
  "sourceCode" : "@Test\r\npublic void testConvertLongToString() {\r\n    ConvertToStringUdf convertToStringUdf = new ConvertToStringUdf();\r\n    Assert.assertEquals(convertToStringUdf.execute(10000000000L), \"10000000000\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestConvertToStringUdf.java",
  "methodName" : "testConvertDoubleToString",
  "sourceCode" : "@Test\r\npublic void testConvertDoubleToString() {\r\n    ConvertToStringUdf convertToStringUdf = new ConvertToStringUdf();\r\n    Assert.assertEquals(convertToStringUdf.execute(10.0000345), \"10.0000345\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestConvertToStringUdf.java",
  "methodName" : "testConvertBooleanToString",
  "sourceCode" : "@Test\r\npublic void testConvertBooleanToString() {\r\n    ConvertToStringUdf convertToStringUdf = new ConvertToStringUdf();\r\n    Assert.assertEquals(convertToStringUdf.execute(true), \"true\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestConvertToStringUdf.java",
  "methodName" : "testConvertEnumToString",
  "sourceCode" : "@Test\r\npublic void testConvertEnumToString() {\r\n    ConvertToStringUdf convertToStringUdf = new ConvertToStringUdf();\r\n    Assert.assertEquals(convertToStringUdf.execute(LightSwitch.On), \"On\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testSingleLevel",
  "sourceCode" : "@Test\r\npublic void testSingleLevel() {\r\n    SamzaSqlRelRecord record = createRecord(\"foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMultiLevel",
  "sourceCode" : "@Test\r\npublic void testMultiLevel() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testNullRecord",
  "sourceCode" : "@Test\r\npublic void testNullRecord() {\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(null, \"bar.baz.baf.foo\"), null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testNullFields",
  "sourceCode" : "@Test(expected = NullPointerException.class)\r\npublic void testNullFields() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    getSqlFieldUdf.execute(record, null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testSingleLevelInvalidField",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testSingleLevelInvalidField() {\r\n    SamzaSqlRelRecord record = createRecord(\"foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    getSqlFieldUdf.execute(record, \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMultiLevelInvalidIntermediateField",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testMultiLevelInvalidIntermediateField() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    getSqlFieldUdf.execute(record, \"bar.baz.bacon\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMultiLevelInvalidFinalField",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testMultiLevelInvalidFinalField() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    getSqlFieldUdf.execute(record, \"bar.baz.baf.funny\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testPathTooDeep",
  "sourceCode" : "@Test(expected = IllegalArgumentException.class)\r\npublic void testPathTooDeep() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    getSqlFieldUdf.execute(record, \"bar.baz.baf.funny\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMapAtLastField",
  "sourceCode" : "@Test\r\npublic void testMapAtLastField() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.map:foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMapAtIntermediateFields",
  "sourceCode" : "@Test\r\npublic void testMapAtIntermediateFields() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.map:baz.map:baf.foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testMapAtAllIntermediateFields",
  "sourceCode" : "@Test\r\npublic void testMapAtAllIntermediateFields() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.map:baz.map:baf.map:foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testArrayAtLastField",
  "sourceCode" : "@Test\r\npublic void testArrayAtLastField() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz.baf.foo[3]\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo[3]\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testArrayAtIntermediateFields",
  "sourceCode" : "@Test\r\npublic void testArrayAtIntermediateFields() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz[3].baf[2].foo\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz[3].baf[2].foo\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testArrayAtAllIntermediateFields",
  "sourceCode" : "@Test\r\npublic void testArrayAtAllIntermediateFields() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.baz[2].baf[3].foo[5]\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz[2].baf[3].foo[5]\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\fn\\TestGetSqlFieldUdf.java",
  "methodName" : "testAllFieldTypes",
  "sourceCode" : "@Test\r\npublic void testAllFieldTypes() {\r\n    SamzaSqlRelRecord record = createRecord(\"bar.map:baz.baf.foo[3].fun\");\r\n    GetSqlFieldUdf getSqlFieldUdf = new GetSqlFieldUdf();\r\n    Assert.assertEquals(getSqlFieldUdf.execute(record, \"bar.baz.baf.foo[3].fun\"), \"bar\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\CheckerTest.java",
  "methodName" : "testCheckOperandTypesShouldFailOnTypeMisMatch",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testCheckOperandTypesShouldFailOnTypeMisMatch() throws NoSuchMethodException {\r\n    Method udfMethod = TestUdfWithWrongTypes.class.getMethod(\"execute\", String.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"TestUdfWithWrongTypes\", \"TestUDFClass\", udfMethod, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.INT32), SamzaSqlFieldType.INT64, false);\r\n    Checker operandTypeChecker = Checker.getChecker(1, 3, udfMetadata);\r\n    SqlCallBinding callBinding = Mockito.mock(SqlCallBinding.class);\r\n    Mockito.when(callBinding.getOperandCount()).thenReturn(1);\r\n    Mockito.when(callBinding.getOperandType(0)).thenReturn(new BasicSqlType(RelDataTypeSystem.DEFAULT, SqlTypeName.VARCHAR, 12));\r\n    operandTypeChecker.checkOperandTypes(callBinding, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\CheckerTest.java",
  "methodName" : "testCheckOperandTypesShouldReturnTrueOnTypeMatch",
  "sourceCode" : "@Test\r\npublic void testCheckOperandTypesShouldReturnTrueOnTypeMatch() throws NoSuchMethodException {\r\n    Method udfMethod = MyTestPolyUdf.class.getMethod(\"execute\", String.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"MyTestPoly\", \"Test Polymorphism UDF.\", udfMethod, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.STRING), SamzaSqlFieldType.INT32, false);\r\n    Checker operandTypeChecker = Checker.getChecker(1, 3, udfMetadata);\r\n    SqlCallBinding callBinding = Mockito.mock(SqlCallBinding.class);\r\n    Mockito.when(callBinding.getOperandCount()).thenReturn(1);\r\n    Mockito.when(callBinding.getOperandType(0)).thenReturn(new BasicSqlType(RelDataTypeSystem.DEFAULT, SqlTypeName.VARCHAR, 12));\r\n    assertTrue(operandTypeChecker.checkOperandTypes(callBinding, true));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\CheckerTest.java",
  "methodName" : "testCheckOperandTypesShouldReturnTrueOnAnyTypeInArg",
  "sourceCode" : "@Test\r\npublic void testCheckOperandTypesShouldReturnTrueOnAnyTypeInArg() throws NoSuchMethodException {\r\n    Method udfMethod = TestUdfWithAnyType.class.getMethod(\"execute\", Object.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"TestUdfWithAnyType\", \"TestUDFClass\", udfMethod, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.ANY), SamzaSqlFieldType.INT64, false);\r\n    Checker operandTypeChecker = Checker.getChecker(1, 3, udfMetadata);\r\n    SqlCallBinding callBinding = Mockito.mock(SqlCallBinding.class);\r\n    Mockito.when(callBinding.getOperandCount()).thenReturn(1);\r\n    Mockito.when(callBinding.getOperandType(0)).thenReturn(new BasicSqlType(RelDataTypeSystem.DEFAULT, SqlTypeName.ARRAY));\r\n    assertTrue(operandTypeChecker.checkOperandTypes(callBinding, true));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\CheckerTest.java",
  "methodName" : "testCheckOperandTypesShouldReturnTrueWhenArgumentCheckIsDisabled",
  "sourceCode" : "@Test\r\npublic void testCheckOperandTypesShouldReturnTrueWhenArgumentCheckIsDisabled() throws NoSuchMethodException {\r\n    Method udfMethod = TestUdfWithWrongTypes.class.getMethod(\"execute\", String.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"TestUdfWithWrongTypes\", \"TestUDFClass\", udfMethod, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.INT32), SamzaSqlFieldType.INT64, true);\r\n    Checker operandTypeChecker = Checker.getChecker(1, 3, udfMetadata);\r\n    SqlCallBinding callBinding = Mockito.mock(SqlCallBinding.class);\r\n    Mockito.when(callBinding.getOperandCount()).thenReturn(1);\r\n    Mockito.when(callBinding.getOperandType(0)).thenReturn(new BasicSqlType(RelDataTypeSystem.DEFAULT, SqlTypeName.VARCHAR, 12));\r\n    assertTrue(operandTypeChecker.checkOperandTypes(callBinding, true));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\CheckerTest.java",
  "methodName" : "testCheckOperandTypesShouldReturnFalseWhenThrowOnFailureIsFalse",
  "sourceCode" : "@Test\r\npublic void testCheckOperandTypesShouldReturnFalseWhenThrowOnFailureIsFalse() throws NoSuchMethodException {\r\n    Method udfMethod = MyTestPolyUdf.class.getMethod(\"execute\", String.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"MyTestPoly\", \"Test Polymorphism UDF.\", udfMethod, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.STRING), SamzaSqlFieldType.INT32, false);\r\n    Checker operandTypeChecker = Checker.getChecker(1, 3, udfMetadata);\r\n    SqlCallBinding callBinding = Mockito.mock(SqlCallBinding.class);\r\n    Mockito.when(callBinding.getOperandCount()).thenReturn(1);\r\n    Mockito.when(callBinding.getOperandType(0)).thenReturn(new BasicSqlType(RelDataTypeSystem.DEFAULT, SqlTypeName.VARCHAR, 12));\r\n    assertTrue(operandTypeChecker.checkOperandTypes(callBinding, false));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testTranslate",
  "sourceCode" : "@Test\r\npublic void testTranslate() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    String sql = \"Insert into testavro.outputTopic(id) select MyTest(id) from testavro.level1.level2.SIMPLE1 as s where s.id = 10\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    DslConverter dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRoots = dslConverter.convertDsl(sql);\r\n    assertEquals(1, relRoots.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinWithFilter",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinWithFilter() throws SamzaSqlValidatorException {\r\n    testRemoteJoinWithFilterHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinWithUdfAndFilter",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinWithUdfAndFilter() throws SamzaSqlValidatorException {\r\n    testRemoteJoinWithUdfAndFilterHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinWithFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinWithFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    testRemoteJoinWithFilterHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinWithUdfAndFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinWithUdfAndFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    testRemoteJoinWithUdfAndFilterHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testLocalStreamTableInnerJoinFilterOptimization",
  "sourceCode" : "@Test\r\npublic void testLocalStreamTableInnerJoinFilterOptimization() throws Exception {\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId \" + \"where p.name = 'Mike'\";\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_ENABLE_PLAN_OPTIMIZER, Boolean.toString(true));\r\n    Config samzaConfig = new MapConfig(staticConfigs);\r\n    DslConverter dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRootsWithOptimization = dslConverter.convertDsl(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_ENABLE_PLAN_OPTIMIZER, Boolean.toString(false));\r\n    samzaConfig = new MapConfig(staticConfigs);\r\n    dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRootsWithoutOptimization = dslConverter.convertDsl(sql);\r\n    // We do not yet have any join filter optimizations for local joins. Hence the plans with and without optimization\r\n    // should be the same.\r\n    assertEquals(RelOptUtil.toString(relRootsWithOptimization.iterator().next().rel, SqlExplainLevel.EXPPLAN_ATTRIBUTES), RelOptUtil.toString(relRootsWithoutOptimization.iterator().next().rel, SqlExplainLevel.EXPPLAN_ATTRIBUTES));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinFilterPushDownWithUdfInFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinFilterPushDownWithUdfInFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\" + \" where p.name = pv.pageKey AND p.name = 'Mike' AND pv.profileId = MyTest(pv.profileId)\";\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_ENABLE_PLAN_OPTIMIZER, Boolean.toString(true));\r\n    Config samzaConfig = new MapConfig(staticConfigs);\r\n    DslConverter dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRoots = dslConverter.convertDsl(sql);\r\n    /*\r\n      Query plan without optimization:\r\n      LogicalProject(__key__=[$9], pageKey=[$9], companyName=['N/A'], profileName=[$2], profileAddress=[$4])\r\n        LogicalFilter(condition=[AND(=($2, $9), =($2, 'Mike'), =($10, CAST(MyTest($10)):INTEGER))])\r\n          LogicalJoin(condition=[=($0, $10)], joinType=[inner])\r\n            LogicalTableScan(table=[[testRemoteStore, Profile, $table]])\r\n            LogicalTableScan(table=[[testavro, PAGEVIEW]])\r\n\r\n      Query plan with optimization:\r\n      LogicalProject(__key__=[$9], pageKey=[$9], companyName=['N/A'], profileName=[$2], profileAddress=[$4])\r\n        LogicalFilter(condition=[AND(=($2, $9), =($2, 'Mike'))])\r\n          LogicalJoin(condition=[=($0, $10)], joinType=[inner])\r\n            LogicalTableScan(table=[[testRemoteStore, Profile, $table]])\r\n            LogicalFilter(condition=[=($2, CAST(MyTest($2)):INTEGER)])\r\n              LogicalTableScan(table=[[testavro, PAGEVIEW]])\r\n     */\r\n    assertEquals(1, relRoots.size());\r\n    RelRoot relRoot = relRoots.iterator().next();\r\n    RelNode relNode = relRoot.rel;\r\n    assertTrue(relNode instanceof LogicalProject);\r\n    relNode = relNode.getInput(0);\r\n    assertTrue(relNode instanceof LogicalFilter);\r\n    assertEquals(\"AND(=($2, $9), =($2, 'Mike'))\", ((LogicalFilter) relNode).getCondition().toString());\r\n    relNode = relNode.getInput(0);\r\n    assertTrue(relNode instanceof LogicalJoin);\r\n    assertEquals(2, relNode.getInputs().size());\r\n    LogicalJoin join = (LogicalJoin) relNode;\r\n    RelNode left = join.getLeft();\r\n    RelNode right = join.getRight();\r\n    assertTrue(left instanceof LogicalTableScan);\r\n    assertTrue(right instanceof LogicalFilter);\r\n    assertEquals(\"=($2, CAST(MyTest($2)):INTEGER)\", ((LogicalFilter) right).getCondition().toString());\r\n    assertTrue(right.getInput(0) instanceof LogicalTableScan);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestQueryPlanner.java",
  "methodName" : "testRemoteJoinNoFilterPushDownWithUdfInFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testRemoteJoinNoFilterPushDownWithUdfInFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\" + \" where p.name = pv.pageKey AND p.name = 'Mike' AND pv.profileId = MyTestPoly(p.name)\";\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_ENABLE_PLAN_OPTIMIZER, Boolean.toString(true));\r\n    Config samzaConfig = new MapConfig(staticConfigs);\r\n    DslConverter dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRootsWithOptimization = dslConverter.convertDsl(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_ENABLE_PLAN_OPTIMIZER, Boolean.toString(false));\r\n    samzaConfig = new MapConfig(staticConfigs);\r\n    dslConverter = new SamzaSqlDslConverterFactory().create(samzaConfig);\r\n    Collection<RelRoot> relRootsWithoutOptimization = dslConverter.convertDsl(sql);\r\n    /*\r\n      LogicalProject(__key__=[$9], pageKey=[$9], companyName=['N/A'], profileName=[$2], profileAddress=[$4])\r\n        LogicalFilter(condition=[AND(=($2, $9), =($2, 'Mike'), =($10, CAST(MyTestPoly($10)):INTEGER))])\r\n          LogicalJoin(condition=[=($0, $10)], joinType=[inner])\r\n            LogicalTableScan(table=[[testRemoteStore, Profile, $table]])\r\n            LogicalTableScan(table=[[testavro, PAGEVIEW]])\r\n     */\r\n    // None of the conditions in the filter could be pushed down as they all require a remote call. Hence the plans\r\n    // with and without optimization should be the same.\r\n    assertEquals(RelOptUtil.toString(relRootsWithOptimization.iterator().next().rel, SqlExplainLevel.EXPPLAN_ATTRIBUTES), RelOptUtil.toString(relRootsWithoutOptimization.iterator().next().rel, SqlExplainLevel.EXPPLAN_ATTRIBUTES));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testBasicValidation",
  "sourceCode" : "@Test\r\npublic void testBasicValidation() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic select id, true as bool_value, name as string_value\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testRepeatedTwiceFieldsValidation",
  "sourceCode" : "// Samza Sql allows users to replace a field in the input stream. For eg: To always set bool_value to false\r\n// while keeping the values of other fields the same, it could be written the below way.\r\n// SELECT false AS bool_value, c.* FROM testavro.COMPLEX1 AS c\r\n@Test\r\npublic void testRepeatedTwiceFieldsValidation() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic select false as bool_value, c.* from testavro.COMPLEX1 as c\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testRepeatedThriceFieldsValidation",
  "sourceCode" : "// Samza Sql allows a field to be replaced only once and validation will fail if the field is replaced more than\r\n// once. We disallow it to keep things simple.\r\n@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testRepeatedThriceFieldsValidation() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic select id, bool_value, true as bool_value, c.* from testavro.COMPLEX1 as c\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testIllegitFieldEndingInZeroValidation",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testIllegitFieldEndingInZeroValidation() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic select id, true as bool_value, false as non_existing_name0\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testLegitFieldEndingInZeroValidation",
  "sourceCode" : "@Test\r\npublic void testLegitFieldEndingInZeroValidation() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic\" + \" select id, bool_value, float_value0 from testavro.COMPLEX1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testNonExistingOutputField",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testNonExistingOutputField() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select id, name as strings_value\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testCalciteErrorString",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testCalciteErrorString() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select non_existing_field, name as string_value\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    try {\r\n        SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    } catch (SamzaException e) {\r\n        Assert.assertTrue(e.getMessage().contains(\"line 1, column 8 to line 1, column 27: Column 'non_existing_field' not found\"));\r\n        throw new SamzaSqlValidatorException(\"Calcite planning for sql failed.\", e);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testNonExistingUdf",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testNonExistingUdf() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select NonExistingUdf(name) as string_value\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testSelectAndOutputValidationFailure",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testSelectAndOutputValidationFailure() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select name as long_value\" + \" from testavro.level1.level2.SIMPLE1 as s where s.id = 1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testValidationStreamTableLeftJoinWithWhere",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testValidationStreamTableLeftJoinWithWhere() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey) select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv left join testavro.PROFILE.`$table` as p where p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testUnsupportedOperator",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testUnsupportedOperator() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.pageViewCountTopic(jobName, pageKey, `count`)\" + \" select 'SampleJob' as jobName, pv.pageKey, count(*) as `count`\" + \" from testavro.PAGEVIEW as pv\" + \" where pv.pageKey = 'job' or pv.pageKey = 'inbox'\" + \" group bys (pv.pageKey)\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testNonDefaultButNullableField",
  "sourceCode" : "@Test\r\npublic void testNonDefaultButNullableField() throws SamzaSqlValidatorException {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(1);\r\n    // double_value is missing\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select id, NOT(id = 5) as bool_value from testavro.SIMPLE1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testDefaultWithNonNullableField",
  "sourceCode" : "@Test\r\npublic void testDefaultWithNonNullableField() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    // bool_value is missing which has default value but is non-nullable\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select id from testavro.SIMPLE1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    try {\r\n        new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n    } catch (SamzaSqlValidatorException e) {\r\n        Assert.assertTrue(e.getMessage().contains(\"Non-optional field 'bool_value' in output schema is missing\"));\r\n        return;\r\n    }\r\n    Assert.fail(\"Validation test has failed.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testNonDefaultOutputField",
  "sourceCode" : "@Test\r\npublic void testNonDefaultOutputField() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    // id is non-default field.\r\n    String sql = \"Insert into testavro.outputTopic \" + \" select NOT(id = 5) as bool_value, CASE WHEN id IN (5, 6, 7) THEN CAST('foo' AS VARCHAR) WHEN id < 5 THEN CAST('bars' AS VARCHAR) ELSE NULL END as string_value from testavro.SIMPLE1\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    try {\r\n        new SamzaSqlValidator(samzaConfig).validate(sqlStmts);\r\n    } catch (SamzaSqlValidatorException e) {\r\n        Assert.assertTrue(e.getMessage().contains(\"Non-optional field 'id' in output schema is missing\"));\r\n        return;\r\n    }\r\n    Assert.fail(\"Validation test has failed.\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testFormatErrorString",
  "sourceCode" : "@Test\r\npublic void testFormatErrorString() {\r\n    String sql = \"select 'SampleJob' as jobName, pv.pageKey, count(*) as `count`\\n\" + \"from testavro.PAGEVIEW as pv\\n\" + \"where pv.pageKey = 'job' or pv.pageKey = 'inbox'\\n\" + \"group bys (pv.pageKey)\";\r\n    String errorStr = \"org.apache.calcite.tools.ValidationException: org.apache.calcite.runtime.CalciteContextException: \" + \"From line 3, column 7 to line 3, column 16: Column 'pv.pageKey' not found in any table\";\r\n    String formattedErrStr = SamzaSqlValidator.formatErrorString(sql, new Exception(errorStr));\r\n    LOG.info(formattedErrStr);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\planner\\TestSamzaSqlValidator.java",
  "methodName" : "testExceptionInFormatErrorString",
  "sourceCode" : "@Test\r\npublic void testExceptionInFormatErrorString() {\r\n    String sql = \"select 'SampleJob' as jobName, pv.pageKey, count(*) as `count`\\n\" + \"from testavro.PAGEVIEW as pv\\n\" + \"where pv.pageKey = 'job' or pv.pageKey = 'inbox'\\n\" + \"group bys (pv.pageKey)\";\r\n    String errorStr = \"org.apache.calcite.tools.ValidationException: org.apache.calcite.runtime.CalciteContextException: \" + \"From line 3, column 7 to line 3, column 16: Column 'pv.pageKey' not found in any table\";\r\n    String formattedErrStr = SamzaSqlValidator.formatErrorString(sql, new Exception(errorStr));\r\n    LOG.info(formattedErrStr);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\runner\\TestSamzaSqlApplicationConfig.java",
  "methodName" : "testConfigInit",
  "sourceCode" : "@Test\r\npublic void testConfigInit() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.COMPLEX1 select * from testavro.SIMPLE1\");\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    Assert.assertEquals(1, samzaSqlApplicationConfig.getInputSystemStreamConfigBySource().size());\r\n    Assert.assertEquals(1, samzaSqlApplicationConfig.getOutputSystemStreamConfigsBySource().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\runner\\TestSamzaSqlApplicationConfig.java",
  "methodName" : "testWrongConfigs",
  "sourceCode" : "@Test\r\npublic void testWrongConfigs() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    try {\r\n        // Fail because no SQL config\r\n        fetchSqlFromConfig(config);\r\n        Assert.fail();\r\n    } catch (SamzaException e) {\r\n    }\r\n    // Pass\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.COMPLEX1 select * from testavro.SIMPLE1\");\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    testWithoutConfigShouldFail(config, SamzaSqlApplicationConfig.CFG_IO_RESOLVER);\r\n    testWithoutConfigShouldFail(config, SamzaSqlApplicationConfig.CFG_UDF_RESOLVER);\r\n    String configIOResolverDomain = String.format(SamzaSqlApplicationConfig.CFG_FMT_SOURCE_RESOLVER_DOMAIN, \"config\");\r\n    String avroSamzaSqlConfigPrefix = configIOResolverDomain + String.format(\"%s.\", \"testavro\");\r\n    testWithoutConfigShouldFail(config, avroSamzaSqlConfigPrefix + SqlIOConfig.CFG_SAMZA_REL_CONVERTER);\r\n    // Configs for the unused system \"log\" is not mandatory.\r\n    String logSamzaSqlConfigPrefix = configIOResolverDomain + String.format(\"%s.\", \"log\");\r\n    testWithoutConfigShouldPass(config, logSamzaSqlConfigPrefix + SqlIOConfig.CFG_SAMZA_REL_CONVERTER);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\runner\\TestSamzaSqlApplicationConfig.java",
  "methodName" : "testGetInputAndOutputStreamConfigsFanOut",
  "sourceCode" : "@Test\r\npublic void testGetInputAndOutputStreamConfigsFanOut() {\r\n    List<String> sqlStmts = Arrays.asList(\"Insert into testavro.COMPLEX1 select * from testavro.SIMPLE1\", \"insert into testavro.Profile select * from testavro.SIMPLE1\");\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    Set<String> inputKeys = samzaSqlApplicationConfig.getInputSystemStreamConfigBySource().keySet();\r\n    Set<String> outputKeys = samzaSqlApplicationConfig.getOutputSystemStreamConfigsBySource().keySet();\r\n    List<String> outputStreamList = samzaSqlApplicationConfig.getOutputSystemStreams();\r\n    Assert.assertEquals(1, inputKeys.size());\r\n    Assert.assertTrue(inputKeys.contains(\"testavro.SIMPLE1\"));\r\n    Assert.assertEquals(2, outputKeys.size());\r\n    Assert.assertTrue(outputKeys.contains(\"testavro.COMPLEX1\"));\r\n    Assert.assertTrue(outputKeys.contains(\"testavro.Profile\"));\r\n    Assert.assertEquals(2, outputStreamList.size());\r\n    Assert.assertEquals(\"testavro.COMPLEX1\", outputStreamList.get(0));\r\n    Assert.assertEquals(\"testavro.Profile\", outputStreamList.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\runner\\TestSamzaSqlApplicationConfig.java",
  "methodName" : "testGetInputAndOutputStreamConfigsFanIn",
  "sourceCode" : "@Test\r\npublic void testGetInputAndOutputStreamConfigsFanIn() {\r\n    List<String> sqlStmts = Arrays.asList(\"Insert into testavro.COMPLEX1 select * from testavro.SIMPLE1\", \"insert into testavro.COMPLEX1 select * from testavro.SIMPLE2\");\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    Set<String> inputKeys = samzaSqlApplicationConfig.getInputSystemStreamConfigBySource().keySet();\r\n    Set<String> outputKeys = samzaSqlApplicationConfig.getOutputSystemStreamConfigsBySource().keySet();\r\n    List<String> outputStreamList = samzaSqlApplicationConfig.getOutputSystemStreams();\r\n    Assert.assertEquals(2, inputKeys.size());\r\n    Assert.assertTrue(inputKeys.contains(\"testavro.SIMPLE1\"));\r\n    Assert.assertTrue(inputKeys.contains(\"testavro.SIMPLE2\"));\r\n    Assert.assertEquals(1, outputKeys.size());\r\n    Assert.assertTrue(outputKeys.contains(\"testavro.COMPLEX1\"));\r\n    Assert.assertEquals(2, outputStreamList.size());\r\n    Assert.assertEquals(\"testavro.COMPLEX1\", outputStreamList.get(0));\r\n    Assert.assertEquals(\"testavro.COMPLEX1\", outputStreamList.get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\runner\\TestSamzaSqlApplicationRunner.java",
  "methodName" : "testComputeSamzaConfigs",
  "sourceCode" : "@Test\r\npublic void testComputeSamzaConfigs() {\r\n    Map<String, String> configs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    String sql1 = \"Insert into testavro.outputTopic(id,long_value) select id, MyTest(id) as long_value from testavro.SIMPLE1\";\r\n    configs.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql1);\r\n    configs.put(ApplicationConfig.APP_RUNNER_CLASS, SamzaSqlApplicationRunner.class.getName());\r\n    MapConfig samzaConfig = new MapConfig(configs);\r\n    Config newConfigs = SamzaSqlApplicationRunner.computeSamzaConfigs(true, samzaConfig);\r\n    Assert.assertEquals(newConfigs.get(ApplicationConfig.APP_RUNNER_CLASS), LocalApplicationRunner.class.getName());\r\n    // Check whether five new configs added.\r\n    Assert.assertEquals(newConfigs.size(), configs.size() + 5);\r\n    newConfigs = SamzaSqlApplicationRunner.computeSamzaConfigs(false, samzaConfig);\r\n    Assert.assertEquals(newConfigs.get(ApplicationConfig.APP_RUNNER_CLASS), RemoteApplicationRunner.class.getName());\r\n    // Check whether five new configs added.\r\n    Assert.assertEquals(newConfigs.size(), configs.size() + 5);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\serializers\\TestSamzaSqlRelMessageSerde.java",
  "methodName" : "testWithDifferentFields",
  "sourceCode" : "@Test\r\npublic void testWithDifferentFields() {\r\n    SamzaSqlRelMessage message = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessageSerde serde = (SamzaSqlRelMessageSerde) new SamzaSqlRelMessageSerdeFactory().getSerde(null, null);\r\n    SamzaSqlRelMessage resultMsg = serde.fromBytes(serde.toBytes(message));\r\n    Assert.assertEquals(names, resultMsg.getSamzaSqlRelRecord().getFieldNames());\r\n    Assert.assertEquals(values, resultMsg.getSamzaSqlRelRecord().getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\serializers\\TestSamzaSqlRelMessageSerde.java",
  "methodName" : "testNestedRecordConversion",
  "sourceCode" : "@Test\r\npublic void testNestedRecordConversion() {\r\n    Map<String, String> props = new HashMap<>();\r\n    SystemStream ss1 = new SystemStream(\"test\", \"nestedRecord\");\r\n    props.put(String.format(ConfigBasedAvroRelSchemaProviderFactory.CFG_SOURCE_SCHEMA, ss1.getSystem(), ss1.getStream()), Profile.SCHEMA$.toString());\r\n    ConfigBasedAvroRelSchemaProviderFactory factory = new ConfigBasedAvroRelSchemaProviderFactory();\r\n    AvroRelSchemaProvider nestedRecordSchemaProvider = (AvroRelSchemaProvider) factory.create(ss1, new MapConfig(props));\r\n    AvroRelConverter nestedRecordAvroRelConverter = new AvroRelConverter(ss1, nestedRecordSchemaProvider, new MapConfig());\r\n    Pair<SamzaSqlRelMessage, GenericData.Record> messageRecordPair = createNestedSamzaSqlRelMessage(nestedRecordAvroRelConverter);\r\n    SamzaSqlRelMessageSerde serde = (SamzaSqlRelMessageSerde) new SamzaSqlRelMessageSerdeFactory().getSerde(null, null);\r\n    SamzaSqlRelMessage resultMsg = serde.fromBytes(serde.toBytes(messageRecordPair.getKey()));\r\n    KV<Object, Object> samzaMessage = nestedRecordAvroRelConverter.convertToSamzaMessage(resultMsg);\r\n    GenericRecord recordPostConversion = (GenericRecord) samzaMessage.getValue();\r\n    for (Schema.Field field : Profile.SCHEMA$.getFields()) {\r\n        // equals() on GenericRecord does the nested record equality check as well.\r\n        Assert.assertEquals(messageRecordPair.getValue().get(field.name()), recordPostConversion.get(field.name()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\serializers\\TestSamzaSqlRelRecordSerde.java",
  "methodName" : "testWithDifferentFields",
  "sourceCode" : "@Test\r\npublic void testWithDifferentFields() {\r\n    SamzaSqlRelRecord record = new SamzaSqlRelMessage(names, values, new SamzaSqlRelMsgMetadata(0L, 0L)).getSamzaSqlRelRecord();\r\n    SamzaSqlRelRecordSerde serde = (SamzaSqlRelRecordSerde) new SamzaSqlRelRecordSerdeFactory().getSerde(null, null);\r\n    SamzaSqlRelRecord resultRecord = serde.fromBytes(serde.toBytes(record));\r\n    Assert.assertEquals(names, resultRecord.getFieldNames());\r\n    Assert.assertEquals(values, resultRecord.getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\serializers\\TestSamzaSqlRelRecordSerde.java",
  "methodName" : "testNestedRecordConversion",
  "sourceCode" : "@Test\r\npublic void testNestedRecordConversion() {\r\n    Map<String, String> props = new HashMap<>();\r\n    SystemStream ss1 = new SystemStream(\"test\", \"nestedRecord\");\r\n    props.put(String.format(ConfigBasedAvroRelSchemaProviderFactory.CFG_SOURCE_SCHEMA, ss1.getSystem(), ss1.getStream()), Profile.SCHEMA$.toString());\r\n    ConfigBasedAvroRelSchemaProviderFactory factory = new ConfigBasedAvroRelSchemaProviderFactory();\r\n    AvroRelSchemaProvider nestedRecordSchemaProvider = (AvroRelSchemaProvider) factory.create(ss1, new MapConfig(props));\r\n    AvroRelConverter nestedRecordAvroRelConverter = new AvroRelConverter(ss1, nestedRecordSchemaProvider, new MapConfig());\r\n    Pair<SamzaSqlRelMessage, GenericData.Record> messageRecordPair = TestSamzaSqlRelMessageSerde.createNestedSamzaSqlRelMessage(nestedRecordAvroRelConverter);\r\n    SamzaSqlRelRecordSerdeFactory.SamzaSqlRelRecordSerde serde = (SamzaSqlRelRecordSerdeFactory.SamzaSqlRelRecordSerde) new SamzaSqlRelRecordSerdeFactory().getSerde(null, null);\r\n    SamzaSqlRelRecord resultRecord = serde.fromBytes(serde.toBytes(messageRecordPair.getKey().getSamzaSqlRelRecord()));\r\n    GenericData.Record recordPostConversion = (GenericData.Record) nestedRecordAvroRelConverter.convertToAvroObject(resultRecord, Profile.SCHEMA$);\r\n    for (Schema.Field field : Profile.SCHEMA$.getFields()) {\r\n        // equals() on GenericRecord does the nested record equality check as well.\r\n        Assert.assertEquals(messageRecordPair.getValue().get(field.name()), recordPostConversion.get(field.name()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestFilterTranslator.java",
  "methodName" : "testTranslate",
  "sourceCode" : "@Test\r\npublic void testTranslate() throws IOException, ClassNotFoundException {\r\n    // setup mock values to the constructor of FilterTranslator\r\n    LogicalFilter mockFilter = PowerMockito.mock(LogicalFilter.class);\r\n    Context mockContext = mock(Context.class);\r\n    ContainerContext mockContainerContext = mock(ContainerContext.class);\r\n    TranslatorContext mockTranslatorContext = mock(TranslatorContext.class);\r\n    TestMetricsRegistryImpl metricsRegistry = new TestMetricsRegistryImpl();\r\n    RelNode mockInput = mock(RelNode.class);\r\n    when(mockFilter.getInput()).thenReturn(mockInput);\r\n    when(mockInput.getId()).thenReturn(1);\r\n    when(mockFilter.getId()).thenReturn(2);\r\n    StreamApplicationDescriptorImpl mockGraph = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec<Object, SamzaSqlRelMessage> mockInputOp = mock(OperatorSpec.class);\r\n    MessageStream<SamzaSqlRelMessage> mockStream = new MessageStreamImpl<>(mockGraph, mockInputOp);\r\n    when(mockTranslatorContext.getMessageStream(eq(1))).thenReturn(mockStream);\r\n    doAnswer(this.getRegisterMessageStreamAnswer()).when(mockTranslatorContext).registerMessageStream(eq(2), any(MessageStream.class));\r\n    RexToJavaCompiler mockCompiler = mock(RexToJavaCompiler.class);\r\n    when(mockTranslatorContext.getExpressionCompiler()).thenReturn(mockCompiler);\r\n    Expression mockExpr = mock(Expression.class);\r\n    when(mockCompiler.compile(any(), any())).thenReturn(mockExpr);\r\n    when(mockContext.getContainerContext()).thenReturn(mockContainerContext);\r\n    when(mockContainerContext.getContainerMetricsRegistry()).thenReturn(metricsRegistry);\r\n    // Apply translate() method to verify that we are getting the correct filter operator constructed\r\n    FilterTranslator filterTranslator = new FilterTranslator(1);\r\n    filterTranslator.translate(mockFilter, LOGICAL_OP_ID, mockTranslatorContext);\r\n    // make sure that context has been registered with LogicFilter and output message streams\r\n    verify(mockTranslatorContext, times(1)).registerRelNode(2, mockFilter);\r\n    verify(mockTranslatorContext, times(1)).registerMessageStream(2, this.getRegisteredMessageStream(2));\r\n    when(mockTranslatorContext.getRelNode(2)).thenReturn(mockFilter);\r\n    when(mockTranslatorContext.getMessageStream(2)).thenReturn(this.getRegisteredMessageStream(2));\r\n    StreamOperatorSpec filterSpec = (StreamOperatorSpec) Whitebox.getInternalState(this.getRegisteredMessageStream(2), \"operatorSpec\");\r\n    assertNotNull(filterSpec);\r\n    assertEquals(filterSpec.getOpCode(), OperatorSpec.OpCode.FILTER);\r\n    // Verify that the describe() method will establish the context for the filter function\r\n    Map<Integer, TranslatorContext> mockContexts = new HashMap<>();\r\n    mockContexts.put(1, mockTranslatorContext);\r\n    when(mockContext.getApplicationTaskContext()).thenReturn(new SamzaSqlApplicationContext(mockContexts));\r\n    filterSpec.getTransformFn().init(mockContext);\r\n    FilterFunction filterFn = (FilterFunction) Whitebox.getInternalState(filterSpec, \"filterFn\");\r\n    assertNotNull(filterFn);\r\n    assertEquals(mockTranslatorContext, Whitebox.getInternalState(filterFn, \"translatorContext\"));\r\n    assertEquals(mockFilter, Whitebox.getInternalState(filterFn, \"filter\"));\r\n    assertEquals(mockExpr, Whitebox.getInternalState(filterFn, \"expr\"));\r\n    // Verify MetricsRegistry works with Project\r\n    assertEquals(1, metricsRegistry.getGauges().size());\r\n    assertTrue(metricsRegistry.getGauges().get(LOGICAL_OP_ID).size() > 0);\r\n    assertEquals(1, metricsRegistry.getCounters().size());\r\n    assertEquals(3, metricsRegistry.getCounters().get(LOGICAL_OP_ID).size());\r\n    assertEquals(0, metricsRegistry.getCounters().get(LOGICAL_OP_ID).get(0).getCount());\r\n    assertEquals(0, metricsRegistry.getCounters().get(LOGICAL_OP_ID).get(1).getCount());\r\n    // Calling filterFn.apply() to verify the filter function is correctly applied to the input message\r\n    SamzaSqlRelMessage mockInputMsg = new SamzaSqlRelMessage(new ArrayList<>(), new ArrayList<>(), new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlExecutionContext executionContext = mock(SamzaSqlExecutionContext.class);\r\n    DataContext dataContext = mock(DataContext.class);\r\n    when(mockTranslatorContext.getExecutionContext()).thenReturn(executionContext);\r\n    when(mockTranslatorContext.getDataContext()).thenReturn(dataContext);\r\n    Object[] result = new Object[1];\r\n    doAnswer(invocation -> {\r\n        Object[] retValue = invocation.getArgumentAt(4, Object[].class);\r\n        retValue[0] = new Boolean(true);\r\n        return null;\r\n    }).when(mockExpr).execute(eq(executionContext), eq(mockContext), eq(dataContext), eq(mockInputMsg.getSamzaSqlRelRecord().getFieldValues().toArray()), eq(result));\r\n    assertTrue(filterFn.apply(mockInputMsg));\r\n    doAnswer(invocation -> {\r\n        Object[] retValue = invocation.getArgumentAt(4, Object[].class);\r\n        retValue[0] = new Boolean(false);\r\n        return null;\r\n    }).when(mockExpr).execute(eq(executionContext), eq(mockContext), eq(dataContext), eq(mockInputMsg.getSamzaSqlRelRecord().getFieldValues().toArray()), eq(result));\r\n    assertFalse(filterFn.apply(mockInputMsg));\r\n    // Verify filterFn.apply() updates the MetricsRegistry metrics\r\n    assertEquals(2, metricsRegistry.getCounters().get(LOGICAL_OP_ID).get(0).getCount());\r\n    assertEquals(1, metricsRegistry.getCounters().get(LOGICAL_OP_ID).get(1).getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestJoinTranslator.java",
  "methodName" : "testTranslateStreamToLocalTableJoin",
  "sourceCode" : "@Test\r\npublic void testTranslateStreamToLocalTableJoin() throws IOException, ClassNotFoundException {\r\n    testTranslateStreamToTableJoin(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestJoinTranslator.java",
  "methodName" : "testTranslateStreamToRemoteTableJoin",
  "sourceCode" : "@Test\r\npublic void testTranslateStreamToRemoteTableJoin() throws IOException, ClassNotFoundException {\r\n    testTranslateStreamToTableJoin(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestProjectTranslator.java",
  "methodName" : "testTranslate",
  "sourceCode" : "@Test\r\npublic void testTranslate() throws IOException, ClassNotFoundException {\r\n    // setup mock values to the constructor of FilterTranslator\r\n    LogicalProject mockProject = PowerMockito.mock(LogicalProject.class);\r\n    Context mockContext = mock(Context.class);\r\n    ContainerContext mockContainerContext = mock(ContainerContext.class);\r\n    TranslatorContext mockTranslatorContext = mock(TranslatorContext.class);\r\n    TestMetricsRegistryImpl testMetricsRegistryImpl = new TestMetricsRegistryImpl();\r\n    RelNode mockInput = mock(RelNode.class);\r\n    List<RelNode> inputs = new ArrayList<>();\r\n    inputs.add(mockInput);\r\n    when(mockInput.getId()).thenReturn(1);\r\n    when(mockProject.getId()).thenReturn(2);\r\n    when(mockProject.getInputs()).thenReturn(inputs);\r\n    when(mockProject.getInput()).thenReturn(mockInput);\r\n    RelDataType mockRowType = mock(RelDataType.class);\r\n    when(mockRowType.getFieldCount()).thenReturn(1);\r\n    when(mockProject.getRowType()).thenReturn(mockRowType);\r\n    RexNode mockRexField = mock(RexNode.class);\r\n    List<Pair<RexNode, String>> namedProjects = new ArrayList<>();\r\n    namedProjects.add(Pair.of(mockRexField, \"test_field\"));\r\n    when(mockProject.getNamedProjects()).thenReturn(namedProjects);\r\n    StreamApplicationDescriptorImpl mockAppDesc = mock(StreamApplicationDescriptorImpl.class);\r\n    OperatorSpec<Object, SamzaSqlRelMessage> mockInputOp = mock(OperatorSpec.class);\r\n    MessageStream<SamzaSqlRelMessage> mockStream = new MessageStreamImpl<>(mockAppDesc, mockInputOp);\r\n    when(mockTranslatorContext.getMessageStream(eq(1))).thenReturn(mockStream);\r\n    doAnswer(this.getRegisterMessageStreamAnswer()).when(mockTranslatorContext).registerMessageStream(eq(2), any(MessageStream.class));\r\n    RexToJavaCompiler mockCompiler = mock(RexToJavaCompiler.class);\r\n    when(mockTranslatorContext.getExpressionCompiler()).thenReturn(mockCompiler);\r\n    Expression mockExpr = mock(Expression.class);\r\n    when(mockCompiler.compile(any(), any())).thenReturn(mockExpr);\r\n    when(mockContext.getContainerContext()).thenReturn(mockContainerContext);\r\n    when(mockContainerContext.getContainerMetricsRegistry()).thenReturn(testMetricsRegistryImpl);\r\n    // Apply translate() method to verify that we are getting the correct map operator constructed\r\n    ProjectTranslator projectTranslator = new ProjectTranslator(1);\r\n    projectTranslator.translate(mockProject, LOGICAL_OP_ID, mockTranslatorContext);\r\n    // make sure that context has been registered with LogicFilter and output message streams\r\n    verify(mockTranslatorContext, times(1)).registerRelNode(2, mockProject);\r\n    verify(mockTranslatorContext, times(1)).registerMessageStream(2, this.getRegisteredMessageStream(2));\r\n    when(mockTranslatorContext.getRelNode(2)).thenReturn(mockProject);\r\n    when(mockTranslatorContext.getMessageStream(2)).thenReturn(this.getRegisteredMessageStream(2));\r\n    StreamOperatorSpec projectSpec = (StreamOperatorSpec) Whitebox.getInternalState(this.getRegisteredMessageStream(2), \"operatorSpec\");\r\n    assertNotNull(projectSpec);\r\n    assertEquals(projectSpec.getOpCode(), OperatorSpec.OpCode.MAP);\r\n    // Verify that the bootstrap() method will establish the context for the map function\r\n    Map<Integer, TranslatorContext> mockContexts = new HashMap<>();\r\n    mockContexts.put(1, mockTranslatorContext);\r\n    when(mockContext.getApplicationTaskContext()).thenReturn(new SamzaSqlApplicationContext(mockContexts));\r\n    projectSpec.getTransformFn().init(mockContext);\r\n    MapFunction mapFn = (MapFunction) Whitebox.getInternalState(projectSpec, \"mapFn\");\r\n    assertNotNull(mapFn);\r\n    assertEquals(mockTranslatorContext, Whitebox.getInternalState(mapFn, \"translatorContext\"));\r\n    assertEquals(mockProject, Whitebox.getInternalState(mapFn, \"project\"));\r\n    assertEquals(mockExpr, Whitebox.getInternalState(mapFn, \"expr\"));\r\n    // Verify TestMetricsRegistryImpl works with Project\r\n    assertEquals(1, testMetricsRegistryImpl.getGauges().size());\r\n    assertEquals(2, testMetricsRegistryImpl.getGauges().get(LOGICAL_OP_ID).size());\r\n    assertEquals(1, testMetricsRegistryImpl.getCounters().size());\r\n    assertEquals(2, testMetricsRegistryImpl.getCounters().get(LOGICAL_OP_ID).size());\r\n    assertEquals(0, testMetricsRegistryImpl.getCounters().get(LOGICAL_OP_ID).get(0).getCount());\r\n    assertEquals(0, testMetricsRegistryImpl.getCounters().get(LOGICAL_OP_ID).get(1).getCount());\r\n    // Calling mapFn.apply() to verify the filter function is correctly applied to the input message\r\n    SamzaSqlRelMessage mockInputMsg = new SamzaSqlRelMessage(new ArrayList<>(), new ArrayList<>(), new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlExecutionContext executionContext = mock(SamzaSqlExecutionContext.class);\r\n    DataContext dataContext = mock(DataContext.class);\r\n    when(mockTranslatorContext.getExecutionContext()).thenReturn(executionContext);\r\n    when(mockTranslatorContext.getDataContext()).thenReturn(dataContext);\r\n    Object[] result = new Object[1];\r\n    final Object mockFieldObj = new Object();\r\n    doAnswer(invocation -> {\r\n        Object[] retValue = invocation.getArgumentAt(4, Object[].class);\r\n        retValue[0] = mockFieldObj;\r\n        return null;\r\n    }).when(mockExpr).execute(eq(executionContext), eq(mockContext), eq(dataContext), eq(mockInputMsg.getSamzaSqlRelRecord().getFieldValues().toArray()), eq(result));\r\n    SamzaSqlRelMessage retMsg = (SamzaSqlRelMessage) mapFn.apply(mockInputMsg);\r\n    assertEquals(retMsg.getSamzaSqlRelRecord().getFieldNames(), Collections.singletonList(\"test_field\"));\r\n    assertEquals(retMsg.getSamzaSqlRelRecord().getFieldValues(), Collections.singletonList(mockFieldObj));\r\n    // Verify mapFn.apply() updates the TestMetricsRegistryImpl metrics\r\n    assertEquals(1, testMetricsRegistryImpl.getCounters().get(LOGICAL_OP_ID).get(0).getCount());\r\n    assertEquals(1, testMetricsRegistryImpl.getCounters().get(LOGICAL_OP_ID).get(1).getCount());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslate",
  "sourceCode" : "@Test\r\npublic void testTranslate() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(id) select MyTest(id) from testavro.level1.level2.SIMPLE1 as s where s.id = 10\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(appDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), appDesc, 0);\r\n    OperatorSpecGraph specGraph = appDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem = streamConfig.getSystem(inputStreamId);\r\n    String inputPhysicalName = streamConfig.getPhysicalName(inputStreamId);\r\n    String outputStreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem = streamConfig.getSystem(outputStreamId);\r\n    String outputPhysicalName = streamConfig.getPhysicalName(outputStreamId);\r\n    Assert.assertEquals(1, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem);\r\n    Assert.assertEquals(\"outputTopic\", outputPhysicalName);\r\n    Assert.assertEquals(1, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem);\r\n    Assert.assertEquals(\"SIMPLE1\", inputPhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateFanIn",
  "sourceCode" : "@Test\r\npublic void testTranslateFanIn() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    String sql1 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE2\";\r\n    String sql2 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(appDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), appDesc, 0);\r\n    translator.translate(queryInfo.get(1), appDesc, 1);\r\n    OperatorSpecGraph specGraph = appDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId1 = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem1 = streamConfig.getSystem(inputStreamId1);\r\n    String inputPhysicalName1 = streamConfig.getPhysicalName(inputStreamId1);\r\n    String inputStreamId2 = specGraph.getInputOperators().keySet().stream().skip(1).findFirst().get();\r\n    String inputSystem2 = streamConfig.getSystem(inputStreamId2);\r\n    String inputPhysicalName2 = streamConfig.getPhysicalName(inputStreamId2);\r\n    String outputStreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem = streamConfig.getSystem(outputStreamId);\r\n    String outputPhysicalName = streamConfig.getPhysicalName(outputStreamId);\r\n    Assert.assertEquals(1, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem);\r\n    Assert.assertEquals(\"simpleOutputTopic\", outputPhysicalName);\r\n    Assert.assertEquals(2, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem1);\r\n    Assert.assertEquals(\"SIMPLE2\", inputPhysicalName1);\r\n    Assert.assertEquals(\"testavro\", inputSystem2);\r\n    Assert.assertEquals(\"SIMPLE1\", inputPhysicalName2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateFanOut",
  "sourceCode" : "@Test\r\npublic void testTranslateFanOut() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    String sql1 = \"Insert into testavro.SIMPLE2 select * from testavro.SIMPLE1\";\r\n    String sql2 = \"Insert into testavro.SIMPLE3 select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(appDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), appDesc, 0);\r\n    translator.translate(queryInfo.get(1), appDesc, 1);\r\n    OperatorSpecGraph specGraph = appDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem = streamConfig.getSystem(inputStreamId);\r\n    String inputPhysicalName = streamConfig.getPhysicalName(inputStreamId);\r\n    String outputStreamId1 = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem1 = streamConfig.getSystem(outputStreamId1);\r\n    String outputPhysicalName1 = streamConfig.getPhysicalName(outputStreamId1);\r\n    String outputStreamId2 = specGraph.getOutputStreams().keySet().stream().skip(1).findFirst().get();\r\n    String outputSystem2 = streamConfig.getSystem(outputStreamId2);\r\n    String outputPhysicalName2 = streamConfig.getPhysicalName(outputStreamId2);\r\n    Assert.assertEquals(2, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem1);\r\n    Assert.assertEquals(\"SIMPLE2\", outputPhysicalName1);\r\n    Assert.assertEquals(\"testavro\", outputSystem2);\r\n    Assert.assertEquals(\"SIMPLE3\", outputPhysicalName2);\r\n    Assert.assertEquals(1, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem);\r\n    Assert.assertEquals(\"SIMPLE1\", inputPhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateMultiSql",
  "sourceCode" : "@Test\r\npublic void testTranslateMultiSql() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    String sql1 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    String sql2 = \"Insert into testavro.SIMPLE3 select * from testavro.SIMPLE2\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl appDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(appDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), appDesc, 0);\r\n    translator.translate(queryInfo.get(1), appDesc, 1);\r\n    OperatorSpecGraph specGraph = appDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId1 = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem1 = streamConfig.getSystem(inputStreamId1);\r\n    String inputPhysicalName1 = streamConfig.getPhysicalName(inputStreamId1);\r\n    String inputStreamId2 = specGraph.getInputOperators().keySet().stream().skip(1).findFirst().get();\r\n    String inputSystem2 = streamConfig.getSystem(inputStreamId2);\r\n    String inputPhysicalName2 = streamConfig.getPhysicalName(inputStreamId2);\r\n    String outputStreamId1 = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem1 = streamConfig.getSystem(outputStreamId1);\r\n    String outputPhysicalName1 = streamConfig.getPhysicalName(outputStreamId1);\r\n    String outputStreamId2 = specGraph.getOutputStreams().keySet().stream().skip(1).findFirst().get();\r\n    String outputSystem2 = streamConfig.getSystem(outputStreamId2);\r\n    String outputPhysicalName2 = streamConfig.getPhysicalName(outputStreamId2);\r\n    Assert.assertEquals(2, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem1);\r\n    Assert.assertEquals(\"simpleOutputTopic\", outputPhysicalName1);\r\n    Assert.assertEquals(\"testavro\", outputSystem2);\r\n    Assert.assertEquals(\"SIMPLE3\", outputPhysicalName2);\r\n    Assert.assertEquals(2, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem1);\r\n    Assert.assertEquals(\"SIMPLE1\", inputPhysicalName1);\r\n    Assert.assertEquals(\"testavro\", inputSystem2);\r\n    Assert.assertEquals(\"SIMPLE2\", inputPhysicalName2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateComplex",
  "sourceCode" : "@Test\r\npublic void testTranslateComplex() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(string_value) select Flatten(array_values) from testavro.COMPLEX1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem = streamConfig.getSystem(inputStreamId);\r\n    String inputPhysicalName = streamConfig.getPhysicalName(inputStreamId);\r\n    String outputStreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem = streamConfig.getSystem(outputStreamId);\r\n    String outputPhysicalName = streamConfig.getPhysicalName(outputStreamId);\r\n    Assert.assertEquals(1, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem);\r\n    Assert.assertEquals(\"outputTopic\", outputPhysicalName);\r\n    Assert.assertEquals(1, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem);\r\n    Assert.assertEquals(\"COMPLEX1\", inputPhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateSubQuery",
  "sourceCode" : "@Test\r\npublic void testTranslateSubQuery() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(10);\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, \"Insert into testavro.outputTopic(string_value, id) select Flatten(a), id \" + \" from (select id, array_values a, string_value s from testavro.COMPLEX1)\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String inputStreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String inputSystem = streamConfig.getSystem(inputStreamId);\r\n    String inputPhysicalName = streamConfig.getPhysicalName(inputStreamId);\r\n    String outputStreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String outputSystem = streamConfig.getSystem(outputStreamId);\r\n    String outputPhysicalName = streamConfig.getPhysicalName(outputStreamId);\r\n    Assert.assertEquals(1, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"testavro\", outputSystem);\r\n    Assert.assertEquals(\"outputTopic\", outputPhysicalName);\r\n    Assert.assertEquals(1, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", inputSystem);\r\n    Assert.assertEquals(\"COMPLEX1\", inputPhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithoutJoinOperator",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithoutJoinOperator() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv, testavro.PROFILE.`$table` as p\" + \" where p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithFullJoinOperator",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithFullJoinOperator() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" full join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithSelfJoinOperator",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithSelfJoinOperator() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName)\" + \" select p1.name as profileName\" + \" from testavro.PROFILE.`$table` as p1\" + \" join testavro.PROFILE.`$table` as p2\" + \" on p1.id = p2.id\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithThetaCondition",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithThetaCondition() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id <> pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableCrossJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableCrossJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv, testavro.PROFILE.`$table` as p\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithAndLiteralCondition",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithAndLiteralCondition() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId and p.name = 'John'\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableJoinWithSubQuery",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableJoinWithSubQuery() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" where exists \" + \" (select p.id from testavro.PROFILE.`$table` as p\" + \" where p.id = pv.profileId)\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateTableTableJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateTableTableJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW.`$table` as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamStreamJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamStreamJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.PROFILE as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateJoinWithIncorrectLeftJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateJoinWithIncorrectLeftJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW.`$table` as pv\" + \" left join testavro.PROFILE as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateJoinWithIncorrectRightJoin",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateJoinWithIncorrectRightJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 1);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" right join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableInnerJoinWithMissingStream",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateStreamTableInnerJoinWithMissingStream() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String configIOResolverDomain = String.format(SamzaSqlApplicationConfig.CFG_FMT_SOURCE_RESOLVER_DOMAIN, \"config\");\r\n    config.put(configIOResolverDomain + SamzaSqlApplicationConfig.CFG_FACTORY, ConfigBasedIOResolverFactory.class.getName());\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableInnerJoin",
  "sourceCode" : "@Test\r\npublic void testTranslateStreamTableInnerJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    config.put(SamzaSqlApplicationConfig.CFG_METADATA_TOPIC_PREFIX, \"sampleAppv1\");\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String input1StreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String input1System = streamConfig.getSystem(input1StreamId);\r\n    String input1PhysicalName = streamConfig.getPhysicalName(input1StreamId);\r\n    String input2StreamId = specGraph.getInputOperators().keySet().stream().skip(1).findFirst().get();\r\n    String input2System = streamConfig.getSystem(input2StreamId);\r\n    String input2PhysicalName = streamConfig.getPhysicalName(input2StreamId);\r\n    String input3StreamId = specGraph.getInputOperators().keySet().stream().skip(2).findFirst().get();\r\n    String input3System = streamConfig.getSystem(input3StreamId);\r\n    String input3PhysicalName = streamConfig.getPhysicalName(input3StreamId);\r\n    String input4StreamId = specGraph.getInputOperators().keySet().stream().skip(3).findFirst().get();\r\n    String input4System = streamConfig.getSystem(input4StreamId);\r\n    String input4PhysicalName = streamConfig.getPhysicalName(input4StreamId);\r\n    String output1StreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String output1System = streamConfig.getSystem(output1StreamId);\r\n    String output1PhysicalName = streamConfig.getPhysicalName(output1StreamId);\r\n    String output2StreamId = specGraph.getOutputStreams().keySet().stream().skip(1).findFirst().get();\r\n    String output2System = streamConfig.getSystem(output2StreamId);\r\n    String output2PhysicalName = streamConfig.getPhysicalName(output2StreamId);\r\n    String output3StreamId = specGraph.getOutputStreams().keySet().stream().skip(2).findFirst().get();\r\n    String output3System = streamConfig.getSystem(output3StreamId);\r\n    String output3PhysicalName = streamConfig.getPhysicalName(output3StreamId);\r\n    Assert.assertEquals(3, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"kafka\", output1System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-sampleAppv1_table_sql_0_join_2\", output1PhysicalName);\r\n    Assert.assertEquals(\"kafka\", output2System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-sampleAppv1_stream_sql_0_join_2\", output2PhysicalName);\r\n    Assert.assertEquals(\"testavro\", output3System);\r\n    Assert.assertEquals(\"enrichedPageViewTopic\", output3PhysicalName);\r\n    Assert.assertEquals(4, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", input1System);\r\n    Assert.assertEquals(\"PAGEVIEW\", input1PhysicalName);\r\n    Assert.assertEquals(\"testavro\", input2System);\r\n    Assert.assertEquals(\"PROFILE\", input2PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input3System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-sampleAppv1_table_sql_0_join_2\", input3PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input4System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-sampleAppv1_stream_sql_0_join_2\", input4PhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableLeftJoin",
  "sourceCode" : "@Test\r\npublic void testTranslateStreamTableLeftJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" left join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String input1StreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String input1System = streamConfig.getSystem(input1StreamId);\r\n    String input1PhysicalName = streamConfig.getPhysicalName(input1StreamId);\r\n    String input2StreamId = specGraph.getInputOperators().keySet().stream().skip(1).findFirst().get();\r\n    String input2System = streamConfig.getSystem(input2StreamId);\r\n    String input2PhysicalName = streamConfig.getPhysicalName(input2StreamId);\r\n    String input3StreamId = specGraph.getInputOperators().keySet().stream().skip(2).findFirst().get();\r\n    String input3System = streamConfig.getSystem(input3StreamId);\r\n    String input3PhysicalName = streamConfig.getPhysicalName(input3StreamId);\r\n    String input4StreamId = specGraph.getInputOperators().keySet().stream().skip(3).findFirst().get();\r\n    String input4System = streamConfig.getSystem(input4StreamId);\r\n    String input4PhysicalName = streamConfig.getPhysicalName(input4StreamId);\r\n    String output1StreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String output1System = streamConfig.getSystem(output1StreamId);\r\n    String output1PhysicalName = streamConfig.getPhysicalName(output1StreamId);\r\n    String output2StreamId = specGraph.getOutputStreams().keySet().stream().skip(1).findFirst().get();\r\n    String output2System = streamConfig.getSystem(output2StreamId);\r\n    String output2PhysicalName = streamConfig.getPhysicalName(output2StreamId);\r\n    String output3StreamId = specGraph.getOutputStreams().keySet().stream().skip(2).findFirst().get();\r\n    String output3System = streamConfig.getSystem(output3StreamId);\r\n    String output3PhysicalName = streamConfig.getPhysicalName(output3StreamId);\r\n    Assert.assertEquals(3, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"kafka\", output1System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-table_sql_0_join_2\", output1PhysicalName);\r\n    Assert.assertEquals(\"kafka\", output2System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-stream_sql_0_join_2\", output2PhysicalName);\r\n    Assert.assertEquals(\"testavro\", output3System);\r\n    Assert.assertEquals(\"enrichedPageViewTopic\", output3PhysicalName);\r\n    Assert.assertEquals(4, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", input1System);\r\n    Assert.assertEquals(\"PAGEVIEW\", input1PhysicalName);\r\n    Assert.assertEquals(\"testavro\", input2System);\r\n    Assert.assertEquals(\"PROFILE\", input2PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input3System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-table_sql_0_join_2\", input3PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input4System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-stream_sql_0_join_2\", input4PhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateStreamTableRightJoin",
  "sourceCode" : "@Test\r\npublic void testTranslateStreamTableRightJoin() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic(profileName, pageKey)\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PROFILE.`$table` as p\" + \" right join testavro.PAGEVIEW as pv\" + \" on p.id = pv.profileId\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    StreamConfig streamConfig = new StreamConfig(samzaConfig);\r\n    String input1StreamId = specGraph.getInputOperators().keySet().stream().findFirst().get();\r\n    String input1System = streamConfig.getSystem(input1StreamId);\r\n    String input1PhysicalName = streamConfig.getPhysicalName(input1StreamId);\r\n    String input2StreamId = specGraph.getInputOperators().keySet().stream().skip(1).findFirst().get();\r\n    String input2System = streamConfig.getSystem(input2StreamId);\r\n    String input2PhysicalName = streamConfig.getPhysicalName(input2StreamId);\r\n    String input3StreamId = specGraph.getInputOperators().keySet().stream().skip(2).findFirst().get();\r\n    String input3System = streamConfig.getSystem(input3StreamId);\r\n    String input3PhysicalName = streamConfig.getPhysicalName(input3StreamId);\r\n    String input4StreamId = specGraph.getInputOperators().keySet().stream().skip(3).findFirst().get();\r\n    String input4System = streamConfig.getSystem(input4StreamId);\r\n    String input4PhysicalName = streamConfig.getPhysicalName(input4StreamId);\r\n    String output1StreamId = specGraph.getOutputStreams().keySet().stream().findFirst().get();\r\n    String output1System = streamConfig.getSystem(output1StreamId);\r\n    String output1PhysicalName = streamConfig.getPhysicalName(output1StreamId);\r\n    String output2StreamId = specGraph.getOutputStreams().keySet().stream().skip(1).findFirst().get();\r\n    String output2System = streamConfig.getSystem(output2StreamId);\r\n    String output2PhysicalName = streamConfig.getPhysicalName(output2StreamId);\r\n    String output3StreamId = specGraph.getOutputStreams().keySet().stream().skip(2).findFirst().get();\r\n    String output3System = streamConfig.getSystem(output3StreamId);\r\n    String output3PhysicalName = streamConfig.getPhysicalName(output3StreamId);\r\n    Assert.assertEquals(3, specGraph.getOutputStreams().size());\r\n    Assert.assertEquals(\"kafka\", output1System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-table_sql_0_join_2\", output1PhysicalName);\r\n    Assert.assertEquals(\"kafka\", output2System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-stream_sql_0_join_2\", output2PhysicalName);\r\n    Assert.assertEquals(\"testavro\", output3System);\r\n    Assert.assertEquals(\"enrichedPageViewTopic\", output3PhysicalName);\r\n    Assert.assertEquals(4, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(\"testavro\", input1System);\r\n    Assert.assertEquals(\"PROFILE\", input1PhysicalName);\r\n    Assert.assertEquals(\"testavro\", input2System);\r\n    Assert.assertEquals(\"PAGEVIEW\", input2PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input3System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-table_sql_0_join_2\", input3PhysicalName);\r\n    Assert.assertEquals(\"kafka\", input4System);\r\n    Assert.assertEquals(\"sql-job-1-partition_by-stream_sql_0_join_2\", input4PhysicalName);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateGroupBy",
  "sourceCode" : "@Test\r\npublic void testTranslateGroupBy() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String sql = \"Insert into testavro.pageViewCountTopic(jobName, pageKey, `count`)\" + \" select 'SampleJob' as jobName, pv.pageKey, count(*) as `count`\" + \" from testavro.PAGEVIEW as pv\" + \" where pv.pageKey = 'job' or pv.pageKey = 'inbox'\" + \" group by (pv.pageKey)\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n    OperatorSpecGraph specGraph = streamAppDesc.getOperatorSpecGraph();\r\n    Assert.assertEquals(1, specGraph.getInputOperators().size());\r\n    Assert.assertEquals(1, specGraph.getOutputStreams().size());\r\n    assertTrue(specGraph.hasWindowOrJoins());\r\n    Collection<OperatorSpec> operatorSpecs = specGraph.getAllOperatorSpecs();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestQueryTranslator.java",
  "methodName" : "testTranslateGroupByWithSumAggregator",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testTranslateGroupByWithSumAggregator() {\r\n    Map<String, String> config = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(configs, 10);\r\n    String sql = \"Insert into testavro.pageViewCountTopic(jobName, pageKey, `sum`)\" + \" select 'SampleJob' as jobName, pv.pageKey, sum(pv.profileId) as `sum`\" + \" from testavro.PAGEVIEW as pv\" + \" where pv.pageKey = 'job' or pv.pageKey = 'inbox'\" + \" group by (pv.pageKey)\";\r\n    config.put(SamzaSqlApplicationConfig.CFG_SQL_STMT, sql);\r\n    Config samzaConfig = SamzaSqlApplicationRunner.computeSamzaConfigs(true, new MapConfig(config));\r\n    List<String> sqlStmts = fetchSqlFromConfig(config);\r\n    List<SamzaSqlQueryParser.QueryInfo> queryInfo = fetchQueryInfo(sqlStmts);\r\n    SamzaSqlApplicationConfig samzaSqlApplicationConfig = new SamzaSqlApplicationConfig(new MapConfig(config), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSources).flatMap(Collection::stream).collect(Collectors.toList()), queryInfo.stream().map(SamzaSqlQueryParser.QueryInfo::getSink).collect(Collectors.toList()));\r\n    StreamApplicationDescriptorImpl streamAppDesc = new StreamApplicationDescriptorImpl(streamApp -> {\r\n    }, samzaConfig);\r\n    QueryTranslator translator = new QueryTranslator(streamAppDesc, samzaSqlApplicationConfig);\r\n    translator.translate(queryInfo.get(0), streamAppDesc, 0);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestSamzaSqlLocalTableJoinFunction.java",
  "methodName" : "testWithInnerJoinWithTableOnRight",
  "sourceCode" : "@Test\r\npublic void testWithInnerJoinWithTableOnRight() {\r\n    SamzaSqlRelMessage streamMsg = new SamzaSqlRelMessage(streamFieldNames, streamFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessage tableMsg = new SamzaSqlRelMessage(tableFieldNames, tableFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    JoinRelType joinRelType = JoinRelType.INNER;\r\n    List<Integer> streamKeyIds = Arrays.asList(0, 1);\r\n    List<Integer> tableKeyIds = Arrays.asList(0, 1);\r\n    SamzaSqlRelRecord compositeKey = SamzaSqlRelMessage.createSamzaSqlCompositeKey(tableMsg, tableKeyIds);\r\n    KV<SamzaSqlRelRecord, SamzaSqlRelMessage> record = KV.of(compositeKey, tableMsg);\r\n    JoinInputNode mockTableInputNode = mock(JoinInputNode.class);\r\n    when(mockTableInputNode.getKeyIds()).thenReturn(tableKeyIds);\r\n    when(mockTableInputNode.isPosOnRight()).thenReturn(true);\r\n    when(mockTableInputNode.getFieldNames()).thenReturn(tableFieldNames);\r\n    JoinInputNode mockStreamInputNode = mock(JoinInputNode.class);\r\n    when(mockStreamInputNode.getKeyIds()).thenReturn(streamKeyIds);\r\n    when(mockStreamInputNode.isPosOnRight()).thenReturn(false);\r\n    when(mockStreamInputNode.getFieldNames()).thenReturn(streamFieldNames);\r\n    SamzaSqlLocalTableJoinFunction joinFn = new SamzaSqlLocalTableJoinFunction(mockStreamInputNode, mockTableInputNode, joinRelType);\r\n    SamzaSqlRelMessage outMsg = joinFn.apply(streamMsg, record);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues().size(), outMsg.getSamzaSqlRelRecord().getFieldNames().size());\r\n    List<String> expectedFieldNames = new ArrayList<>(streamFieldNames);\r\n    expectedFieldNames.addAll(tableFieldNames);\r\n    List<Object> expectedFieldValues = new ArrayList<>(streamFieldValues);\r\n    expectedFieldValues.addAll(tableFieldValues);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues(), expectedFieldValues);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestSamzaSqlLocalTableJoinFunction.java",
  "methodName" : "testWithInnerJoinWithTableOnLeft",
  "sourceCode" : "@Test\r\npublic void testWithInnerJoinWithTableOnLeft() {\r\n    SamzaSqlRelMessage streamMsg = new SamzaSqlRelMessage(streamFieldNames, streamFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessage tableMsg = new SamzaSqlRelMessage(tableFieldNames, tableFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    JoinRelType joinRelType = JoinRelType.INNER;\r\n    List<Integer> streamKeyIds = Arrays.asList(0, 2);\r\n    List<Integer> tableKeyIds = Arrays.asList(0, 2);\r\n    SamzaSqlRelRecord compositeKey = SamzaSqlRelMessage.createSamzaSqlCompositeKey(tableMsg, tableKeyIds);\r\n    KV<SamzaSqlRelRecord, SamzaSqlRelMessage> record = KV.of(compositeKey, tableMsg);\r\n    JoinInputNode mockTableInputNode = mock(JoinInputNode.class);\r\n    when(mockTableInputNode.getKeyIds()).thenReturn(tableKeyIds);\r\n    when(mockTableInputNode.isPosOnRight()).thenReturn(false);\r\n    when(mockTableInputNode.getFieldNames()).thenReturn(tableFieldNames);\r\n    JoinInputNode mockStreamInputNode = mock(JoinInputNode.class);\r\n    when(mockStreamInputNode.getKeyIds()).thenReturn(streamKeyIds);\r\n    when(mockStreamInputNode.isPosOnRight()).thenReturn(true);\r\n    when(mockStreamInputNode.getFieldNames()).thenReturn(streamFieldNames);\r\n    SamzaSqlLocalTableJoinFunction joinFn = new SamzaSqlLocalTableJoinFunction(mockStreamInputNode, mockTableInputNode, joinRelType);\r\n    SamzaSqlRelMessage outMsg = joinFn.apply(streamMsg, record);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues().size(), outMsg.getSamzaSqlRelRecord().getFieldNames().size());\r\n    List<String> expectedFieldNames = new ArrayList<>(tableFieldNames);\r\n    expectedFieldNames.addAll(streamFieldNames);\r\n    List<Object> expectedFieldValues = new ArrayList<>(tableFieldValues);\r\n    expectedFieldValues.addAll(streamFieldValues);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues(), expectedFieldValues);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestSamzaSqlLocalTableJoinFunction.java",
  "methodName" : "testNullRecordWithInnerJoin",
  "sourceCode" : "@Test\r\npublic void testNullRecordWithInnerJoin() {\r\n    SamzaSqlRelMessage streamMsg = new SamzaSqlRelMessage(streamFieldNames, streamFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    JoinRelType joinRelType = JoinRelType.INNER;\r\n    List<Integer> streamKeyIds = Arrays.asList(0, 1);\r\n    List<Integer> tableKeyIds = Arrays.asList(2, 3);\r\n    JoinInputNode mockTableInputNode = mock(JoinInputNode.class);\r\n    when(mockTableInputNode.getKeyIds()).thenReturn(tableKeyIds);\r\n    when(mockTableInputNode.isPosOnRight()).thenReturn(true);\r\n    when(mockTableInputNode.getFieldNames()).thenReturn(tableFieldNames);\r\n    JoinInputNode mockStreamInputNode = mock(JoinInputNode.class);\r\n    when(mockStreamInputNode.getKeyIds()).thenReturn(streamKeyIds);\r\n    when(mockStreamInputNode.isPosOnRight()).thenReturn(false);\r\n    when(mockStreamInputNode.getFieldNames()).thenReturn(streamFieldNames);\r\n    SamzaSqlLocalTableJoinFunction joinFn = new SamzaSqlLocalTableJoinFunction(mockStreamInputNode, mockTableInputNode, joinRelType);\r\n    SamzaSqlRelMessage outMsg = joinFn.apply(streamMsg, null);\r\n    Assert.assertNull(outMsg);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestSamzaSqlLocalTableJoinFunction.java",
  "methodName" : "testNullRecordWithLeftOuterJoin",
  "sourceCode" : "@Test\r\npublic void testNullRecordWithLeftOuterJoin() {\r\n    SamzaSqlRelMessage streamMsg = new SamzaSqlRelMessage(streamFieldNames, streamFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    JoinRelType joinRelType = JoinRelType.LEFT;\r\n    List<Integer> streamKeyIds = Arrays.asList(0, 1);\r\n    List<Integer> tableKeyIds = Arrays.asList(2, 3);\r\n    JoinInputNode mockTableInputNode = mock(JoinInputNode.class);\r\n    when(mockTableInputNode.getKeyIds()).thenReturn(tableKeyIds);\r\n    when(mockTableInputNode.isPosOnRight()).thenReturn(true);\r\n    when(mockTableInputNode.getFieldNames()).thenReturn(tableFieldNames);\r\n    JoinInputNode mockStreamInputNode = mock(JoinInputNode.class);\r\n    when(mockStreamInputNode.getKeyIds()).thenReturn(streamKeyIds);\r\n    when(mockStreamInputNode.isPosOnRight()).thenReturn(false);\r\n    when(mockStreamInputNode.getFieldNames()).thenReturn(streamFieldNames);\r\n    SamzaSqlLocalTableJoinFunction joinFn = new SamzaSqlLocalTableJoinFunction(mockStreamInputNode, mockTableInputNode, joinRelType);\r\n    SamzaSqlRelMessage outMsg = joinFn.apply(streamMsg, null);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues().size(), outMsg.getSamzaSqlRelRecord().getFieldNames().size());\r\n    List<String> expectedFieldNames = new ArrayList<>(streamFieldNames);\r\n    expectedFieldNames.addAll(tableFieldNames);\r\n    List<Object> expectedFieldValues = new ArrayList<>(streamFieldValues);\r\n    expectedFieldValues.addAll(tableFieldNames.stream().map(name -> null).collect(Collectors.toList()));\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues(), expectedFieldValues);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\translator\\TestSamzaSqlRemoteTableJoinFunction.java",
  "methodName" : "testWithInnerJoinWithTableOnRight",
  "sourceCode" : "@Test\r\npublic void testWithInnerJoinWithTableOnRight() {\r\n    Map<String, String> props = new HashMap<>();\r\n    SystemStream ss = new SystemStream(\"test\", \"nestedRecord\");\r\n    props.put(String.format(ConfigBasedAvroRelSchemaProviderFactory.CFG_SOURCE_SCHEMA, ss.getSystem(), ss.getStream()), SimpleRecord.SCHEMA$.toString());\r\n    ConfigBasedAvroRelSchemaProviderFactory factory = new ConfigBasedAvroRelSchemaProviderFactory();\r\n    AvroRelSchemaProvider schemaProvider = (AvroRelSchemaProvider) factory.create(ss, new MapConfig(props));\r\n    AvroRelConverter relConverter = new AvroRelConverter(ss, schemaProvider, new MapConfig());\r\n    SamzaRelTableKeyConverter relTableKeyConverter = new SampleRelTableKeyConverter();\r\n    String remoteTableName = \"testDb.testTable.$table\";\r\n    GenericData.Record tableRecord = new GenericData.Record(SimpleRecord.SCHEMA$);\r\n    tableRecord.put(\"id\", 1);\r\n    tableRecord.put(\"name\", \"name1\");\r\n    SamzaSqlRelMessage streamMsg = new SamzaSqlRelMessage(streamFieldNames, streamFieldValues, new SamzaSqlRelMsgMetadata(0L, 0L));\r\n    SamzaSqlRelMessage tableMsg = relConverter.convertToRelMessage(new KV(tableRecord.get(\"id\"), tableRecord));\r\n    JoinRelType joinRelType = JoinRelType.INNER;\r\n    List<Integer> streamKeyIds = Arrays.asList(1);\r\n    List<Integer> tableKeyIds = Arrays.asList(0);\r\n    KV<Object, GenericRecord> record = KV.of(tableRecord.get(\"id\"), tableRecord);\r\n    JoinInputNode mockTableInputNode = mock(JoinInputNode.class);\r\n    when(mockTableInputNode.getKeyIds()).thenReturn(tableKeyIds);\r\n    when(mockTableInputNode.isPosOnRight()).thenReturn(true);\r\n    when(mockTableInputNode.getFieldNames()).thenReturn(tableMsg.getSamzaSqlRelRecord().getFieldNames());\r\n    when(mockTableInputNode.getSourceName()).thenReturn(remoteTableName);\r\n    JoinInputNode mockStreamInputNode = mock(JoinInputNode.class);\r\n    when(mockStreamInputNode.getKeyIds()).thenReturn(streamKeyIds);\r\n    when(mockStreamInputNode.isPosOnRight()).thenReturn(false);\r\n    when(mockStreamInputNode.getFieldNames()).thenReturn(streamFieldNames);\r\n    SamzaSqlRemoteTableJoinFunction joinFn = new SamzaSqlRemoteTableJoinFunction(relConverter, relTableKeyConverter, mockStreamInputNode, mockTableInputNode, joinRelType, 0);\r\n    SamzaSqlRelMessage outMsg = joinFn.apply(streamMsg, record);\r\n    Assert.assertEquals(outMsg.getSamzaSqlRelRecord().getFieldValues().size(), outMsg.getSamzaSqlRelRecord().getFieldNames().size());\r\n    List<String> expectedFieldNames = new ArrayList<>(streamFieldNames);\r\n    expectedFieldNames.addAll(tableMsg.getSamzaSqlRelRecord().getFieldNames());\r\n    List<Object> expectedFieldValues = new ArrayList<>(streamFieldValues);\r\n    expectedFieldValues.addAll(tableMsg.getSamzaSqlRelRecord().getFieldValues());\r\n    Assert.assertEquals(expectedFieldNames, outMsg.getSamzaSqlRelRecord().getFieldNames());\r\n    Assert.assertEquals(expectedFieldValues, outMsg.getSamzaSqlRelRecord().getFieldValues());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\udf\\impl\\TestReflectionBasedUdfResolver.java",
  "methodName" : "testShouldReturnNothingWhenNoUDFIsInPackagePrefix",
  "sourceCode" : "@Test\r\npublic void testShouldReturnNothingWhenNoUDFIsInPackagePrefix() {\r\n    Config config = new MapConfig(ImmutableMap.of(\"samza.sql.udf.resolver.package.prefix\", \"org.apache.samza.udf.blah.blah\"));\r\n    ReflectionBasedUdfResolver reflectionBasedUdfResolver = new ReflectionBasedUdfResolver(config);\r\n    Collection<UdfMetadata> udfMetadataList = reflectionBasedUdfResolver.getUdfs();\r\n    Assert.assertEquals(0, udfMetadataList.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\udf\\impl\\TestReflectionBasedUdfResolver.java",
  "methodName" : "testUDfResolverShouldReturnAllUDFInClassPath",
  "sourceCode" : "@Test\r\npublic void testUDfResolverShouldReturnAllUDFInClassPath() throws NoSuchMethodException {\r\n    Config config = new MapConfig(ImmutableMap.of(\"samza.sql.udf.resolver.package.filter\", \"org.apache.samza.sql.udf.impl\"));\r\n    ReflectionBasedUdfResolver reflectionBasedUdfResolver = new ReflectionBasedUdfResolver(config);\r\n    Collection<UdfMetadata> udfMetadataList = reflectionBasedUdfResolver.getUdfs();\r\n    Method method = TestSamzaSqlUdf.class.getMethod(\"execute\", String.class);\r\n    UdfMetadata udfMetadata = new UdfMetadata(\"TestSamzaSqlUdf\", \"Test samza sql udf implementation\", method, new MapConfig(), ImmutableList.of(SamzaSqlFieldType.STRING), SamzaSqlFieldType.STRING, true);\r\n    Assert.assertArrayEquals(new UdfMetadata[] { udfMetadata }, udfMetadataList.toArray(new UdfMetadata[0]));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\SampleRelTableKeyConverterTest.java",
  "methodName" : "testNullValue",
  "sourceCode" : "@Test\r\npublic void testNullValue() {\r\n    SampleRelTableKeyConverter sampleRelTableKeyConverter = new SampleRelTableKeyConverter();\r\n    List<Object> values = new ArrayList<>();\r\n    values.add(null);\r\n    SamzaSqlRelRecord samzaSqlRelRecord = new SamzaSqlRelRecord(ImmutableList.of(\"c1\"), values);\r\n    Object key = sampleRelTableKeyConverter.convertToTableKeyFormat(samzaSqlRelRecord);\r\n    Assert.assertNull(key);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlFileParser.java",
  "methodName" : "testParseSqlFile",
  "sourceCode" : "@Test\r\npublic void testParseSqlFile() throws IOException {\r\n    File tempFile = File.createTempFile(\"testparser\", \"\");\r\n    PrintWriter fileWriter = new PrintWriter(tempFile.getCanonicalPath());\r\n    fileWriter.println(TEST_SQL);\r\n    fileWriter.close();\r\n    List<String> sqlStmts = SqlFileParser.parseSqlFile(tempFile.getAbsolutePath());\r\n    Assert.assertEquals(3, sqlStmts.size());\r\n    Assert.assertEquals(\"insert into log.outputStream select * from brooklin.elasticsearchEnterpriseAccounts\", sqlStmts.get(0));\r\n    Assert.assertEquals(\"insert into log.outputstream select sfdcAccountId as key, organizationUrn as name2, description as name3 from brooklin.elasticsearchEnterpriseAccounts\", sqlStmts.get(1));\r\n    Assert.assertEquals(\"insert into log.outputstream select id, MyTest(id) as id2 from tracking.SamzaSqlTestTopic1_p8\", sqlStmts.get(2));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseQuery",
  "sourceCode" : "@Test\r\npublic void testParseQuery() {\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(\"insert into log.foo select * from tracking.bar\");\r\n    Assert.assertEquals(\"log.foo\", queryInfo.getSink());\r\n    Assert.assertEquals(1, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"tracking.bar\", queryInfo.getSources().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseGroupyByQuery",
  "sourceCode" : "@Test\r\npublic void testParseGroupyByQuery() {\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(\"insert into log.foo select b.pageKey, count(*) from tracking.bar as b group by b.pageKey\");\r\n    Assert.assertEquals(\"log.foo\", queryInfo.getSink());\r\n    Assert.assertEquals(1, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"tracking.bar\", queryInfo.getSources().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseUnNestSubQuery",
  "sourceCode" : "@Test\r\npublic void testParseUnNestSubQuery() {\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(\"insert into log.foo SELECT * FROM unnest(SELECT int_array_field1 FROM tracking.bar) \");\r\n    Assert.assertEquals(\"log.foo\", queryInfo.getSink());\r\n    Assert.assertEquals(1, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"tracking.bar\", queryInfo.getSources().get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseJoinSubQuery",
  "sourceCode" : "@Test\r\npublic void testParseJoinSubQuery() {\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic\" + \" select p.name as profileName, pv.pageKey\" + \" from (SELECT * FROM testavro.PAGEVIEW pv1 where pv1.field1='foo') as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(sql);\r\n    Assert.assertEquals(\"testavro.enrichedPageViewTopic\", queryInfo.getSink());\r\n    Assert.assertEquals(2, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"testavro.PAGEVIEW\", queryInfo.getSources().get(0));\r\n    Assert.assertEquals(\"testavro.PROFILE.$table\", queryInfo.getSources().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseJoinUnNestQuery",
  "sourceCode" : "@Test\r\npublic void testParseJoinUnNestQuery() {\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic\" + \" select p.name as profileName, pv.pageKey\" + \" from unnest(SELECT int_array_field1 FROM testavro.PAGEVIEW) as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(sql);\r\n    Assert.assertEquals(\"testavro.enrichedPageViewTopic\", queryInfo.getSink());\r\n    Assert.assertEquals(2, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"testavro.PAGEVIEW\", queryInfo.getSources().get(0));\r\n    Assert.assertEquals(\"testavro.PROFILE.$table\", queryInfo.getSources().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseJoinQuery",
  "sourceCode" : "@Test\r\npublic void testParseJoinQuery() {\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic\" + \" select p.name as profileName, pv.pageKey\" + \" from testavro.PAGEVIEW as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    QueryInfo queryInfo = SamzaSqlQueryParser.parseQuery(sql);\r\n    Assert.assertEquals(\"testavro.enrichedPageViewTopic\", queryInfo.getSink());\r\n    Assert.assertEquals(2, queryInfo.getSources().size());\r\n    Assert.assertEquals(\"testavro.PAGEVIEW\", queryInfo.getSources().get(0));\r\n    Assert.assertEquals(\"testavro.PROFILE.$table\", queryInfo.getSources().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql\\src\\test\\java\\org\\apache\\samza\\sql\\util\\TestSamzaSqlQueryParser.java",
  "methodName" : "testParseInvalidQuery",
  "sourceCode" : "@Test\r\npublic void testParseInvalidQuery() {\r\n    try {\r\n        SamzaSqlQueryParser.parseQuery(\"select * from tracking.bar\");\r\n        Assert.fail(\"Expected a samzaException\");\r\n    } catch (SamzaException e) {\r\n    }\r\n    try {\r\n        SamzaSqlQueryParser.parseQuery(\"insert into select * from tracking.bar\");\r\n        Assert.fail(\"Expected a samzaException\");\r\n    } catch (SamzaException e) {\r\n    }\r\n    try {\r\n        SamzaSqlQueryParser.parseQuery(\"insert into log.off select from tracking.bar\");\r\n        Assert.fail(\"Expected a samzaException\");\r\n    } catch (SamzaException e) {\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql-shell\\src\\test\\java\\org\\apache\\samza\\sql\\client\\impl\\SamzaExecutorTest.java",
  "methodName" : "testGetTableSchema",
  "sourceCode" : "@Test\r\npublic void testGetTableSchema() throws ExecutorException {\r\n    prepareEnvironmentVariable();\r\n    SqlSchema ts = mExecutor.getTableSchema(new ExecutionContext(), \"kafka.ProfileChangeStream\");\r\n    List<SqlSchema.SqlField> fields = ts.getFields();\r\n    Assert.assertEquals(\"Name\", fields.get(0).getFieldName());\r\n    Assert.assertEquals(\"NewCompany\", fields.get(1).getFieldName());\r\n    Assert.assertEquals(\"OldCompany\", fields.get(2).getFieldName());\r\n    Assert.assertEquals(\"ProfileChangeTimestamp\", fields.get(3).getFieldName());\r\n    Assert.assertEquals(\"STRING\", fields.get(0).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"STRING\", fields.get(1).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"STRING\", fields.get(2).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"INT64\", fields.get(3).getFieldSchema().getFieldType().toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql-shell\\src\\test\\java\\org\\apache\\samza\\sql\\client\\impl\\SamzaExecutorTest.java",
  "methodName" : "testGenerateResultSchema",
  "sourceCode" : "// Generate result schema needs to be fixed. SAMZA-2079\r\n@Ignore\r\n@Test\r\npublic void testGenerateResultSchema() {\r\n    prepareEnvironmentVariable();\r\n    Map<String, String> mapConf = mExecutor.fetchSamzaSqlConfig(1);\r\n    SqlSchema ts = mExecutor.generateResultSchema(new MapConfig(mapConf));\r\n    List<SqlSchema.SqlField> fields = ts.getFields();\r\n    Assert.assertEquals(\"__key__\", fields.get(0).getFieldName());\r\n    Assert.assertEquals(\"Name\", fields.get(1).getFieldName());\r\n    Assert.assertEquals(\"NewCompany\", fields.get(2).getFieldName());\r\n    Assert.assertEquals(\"OldCompany\", fields.get(3).getFieldName());\r\n    Assert.assertEquals(\"ProfileChangeTimestamp\", fields.get(4).getFieldName());\r\n    Assert.assertEquals(\"ANY\", fields.get(0).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"VARCHAR\", fields.get(1).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"VARCHAR\", fields.get(2).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"VARCHAR\", fields.get(3).getFieldSchema().getFieldType().toString());\r\n    Assert.assertEquals(\"BIGINT\", fields.get(4).getFieldSchema().getFieldType().toString());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql-shell\\src\\test\\java\\org\\apache\\samza\\sql\\client\\util\\RandomAccessQueueTest.java",
  "methodName" : "testAddAndGetElement",
  "sourceCode" : "@Test\r\npublic void testAddAndGetElement() {\r\n    mQueue.clear();\r\n    for (int i = 0; i < 4; i++) {\r\n        mQueue.add(i);\r\n    }\r\n    Assert.assertEquals(0, mQueue.getHead());\r\n    Assert.assertEquals(4, mQueue.getSize());\r\n    Assert.assertEquals(0, mQueue.get(0));\r\n    Assert.assertEquals(3, mQueue.get(3));\r\n    for (int i = 0; i < 3; i++) {\r\n        mQueue.add(4 + i);\r\n    }\r\n    int head = mQueue.getHead();\r\n    Assert.assertEquals(2, head);\r\n    Assert.assertEquals(5, mQueue.getSize());\r\n    Assert.assertEquals(2, mQueue.get(0));\r\n    Assert.assertEquals(3, mQueue.get(1));\r\n    Assert.assertEquals(4, mQueue.get(2));\r\n    Assert.assertEquals(5, mQueue.get(3));\r\n    Assert.assertEquals(6, mQueue.get(4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql-shell\\src\\test\\java\\org\\apache\\samza\\sql\\client\\util\\RandomAccessQueueTest.java",
  "methodName" : "testGetRange",
  "sourceCode" : "@Test\r\npublic void testGetRange() {\r\n    mQueue.clear();\r\n    for (int i = 0; i < 4; i++) {\r\n        // 0, 1, 2, 3\r\n        mQueue.add(i);\r\n    }\r\n    List<Integer> rets = mQueue.get(-1, 9);\r\n    Assert.assertEquals(4, rets.size());\r\n    Assert.assertEquals(0, mQueue.get(0));\r\n    Assert.assertEquals(3, mQueue.get(3));\r\n    for (int i = 0; i <= 2; i++) {\r\n        mQueue.add(4 + i);\r\n    }\r\n    rets = mQueue.get(0, 4);\r\n    Assert.assertEquals(2, rets.get(0).intValue());\r\n    Assert.assertEquals(3, rets.get(1).intValue());\r\n    Assert.assertEquals(4, rets.get(2).intValue());\r\n    Assert.assertEquals(5, rets.get(3).intValue());\r\n    Assert.assertEquals(6, rets.get(4).intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-sql-shell\\src\\test\\java\\org\\apache\\samza\\sql\\client\\util\\RandomAccessQueueTest.java",
  "methodName" : "testConsume",
  "sourceCode" : "@Test\r\npublic void testConsume() {\r\n    mQueue.clear();\r\n    for (int i = 0; i < 4; i++) {\r\n        // 0, 1, 2, 3\r\n        mQueue.add(i);\r\n    }\r\n    List<Integer> rets = mQueue.consume(1, 2);\r\n    Assert.assertEquals(1, mQueue.getSize());\r\n    Assert.assertEquals(3, mQueue.getHead());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\checkpoint\\CheckpointVersionIntegrationTest.java",
  "methodName" : "testStopCheckpointV1V2AndRestartCheckpointV2",
  "sourceCode" : "@Test\r\npublic void testStopCheckpointV1V2AndRestartCheckpointV2() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    // double check collectors.flush\r\n    List<String> expectedChangelogMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", null, \"98\", \"99\");\r\n    runStatefulApp(inputMessagesOnInitialRun, inputMessagesOnInitialRun, expectedChangelogMessagesOnInitialRun, CONFIGS);\r\n    // first two are reverts for uncommitted messages from last run for keys 98 and 99\r\n    List<String> expectedChangelogMessagesAfterSecondRun = Arrays.asList(null, null, \"98\", \"99\", \"4\", \"5\", \"5\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    Map<String, String> configOverrides = new HashMap<>(CONFIGS);\r\n    configOverrides.put(TaskConfig.CHECKPOINT_READ_VERSIONS, \"2\");\r\n    finalRun(CHANGELOG_TOPIC, expectedChangelogMessagesAfterSecondRun, expectedInitialStoreContentsOnSecondRun, Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\"), configOverrides);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\checkpoint\\CheckpointVersionIntegrationTest.java",
  "methodName" : "testStopCheckpointV1V2AndRestartStaleCheckpointV2",
  "sourceCode" : "@Test\r\npublic void testStopCheckpointV1V2AndRestartStaleCheckpointV2() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    // double check collectors.flush\r\n    List<String> expectedChangelogMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", null, \"98\", \"99\");\r\n    runStatefulApp(inputMessagesOnInitialRun, inputMessagesOnInitialRun, expectedChangelogMessagesOnInitialRun, CONFIGS);\r\n    Map<String, String> secondConfigRunOverrides = new HashMap<>(CONFIGS);\r\n    // only write checkpoint v1, making checkpoint v2 stale\r\n    secondConfigRunOverrides.put(TaskConfig.CHECKPOINT_WRITE_VERSIONS, \"1\");\r\n    secondConfigRunOverrides.put(TaskConfig.CHECKPOINT_READ_VERSIONS, \"2, 1\");\r\n    List<String> inputMessagesOnSecondRun = Arrays.asList(\"77\", \"78\", \"79\", \":shutdown\");\r\n    // first two are reverts for uncommitted messages from last run for keys 98 and 99\r\n    expectedChangelogMessagesOnInitialRun = Arrays.asList(null, null, \"98\", \"99\", \"77\", \"78\", \"79\");\r\n    runStatefulApp(inputMessagesOnSecondRun, inputMessagesOnSecondRun, expectedChangelogMessagesOnInitialRun, secondConfigRunOverrides);\r\n    // takes the latest written checkpoint v1 from run 2 since v2 checkpoints are stale\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\", \"77\", \"78\", \"79\", \"98\", \"99\");\r\n    Map<String, String> configOverrides = new HashMap<>(CONFIGS);\r\n    configOverrides.put(TaskConfig.CHECKPOINT_READ_VERSIONS, \"2, 1\");\r\n    // Does not have to rewind to the last written v2 checkpoints (1, 2, 3) despite the v2 priority\r\n    // use the latest checkpoint\r\n    configOverrides.put(TaskConfig.LIVE_CHECKPOINT_MAX_AGE_MS, \"0\");\r\n    finalRun(CHANGELOG_TOPIC, Collections.emptyList(), expectedInitialStoreContentsOnSecondRun, Collections.emptyList(), configOverrides);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessor.java",
  "methodName" : "testSingleStreamProcessor",
  "sourceCode" : "@Test\r\npublic void testSingleStreamProcessor() {\r\n    testStreamProcessor(new String[] { \"1\" });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessor.java",
  "methodName" : "testTwoStreamProcessors",
  "sourceCode" : "@Test\r\npublic void testTwoStreamProcessors() {\r\n    testStreamProcessor(new String[] { \"2\", \"3\" });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessor.java",
  "methodName" : "testFiveStreamProcessors",
  "sourceCode" : "@Test\r\npublic void testFiveStreamProcessors() {\r\n    testStreamProcessor(new String[] { \"4\", \"5\", \"6\", \"7\", \"8\" });\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessor.java",
  "methodName" : "testStreamProcessorWithAdd",
  "sourceCode" : "@Test\r\npublic /**\r\n * Similar to the previous tests, but add another processor in the middle\r\n */\r\nvoid testStreamProcessorWithAdd() {\r\n    // set number of events we expect to read by both processes in total:\r\n    // p1 - reads 'messageCount' at first\r\n    // p1 and p2 read all messageCount together, since they start from the beginning.\r\n    // so we expect total 3 x messageCounts\r\n    int totalEventsToGenerate = 3 * messageCount;\r\n    TestStreamTask.endLatch = new CountDownLatch(totalEventsToGenerate);\r\n    // create first processor\r\n    CountDownLatch startWait1 = new CountDownLatch(1);\r\n    CountDownLatch stopWait1 = new CountDownLatch(1);\r\n    StreamProcessor sp1 = createStreamProcessor(\"20\", map, startWait1, stopWait1);\r\n    // produce first batch of messages starting with 0\r\n    produceMessages(0, inputTopic, messageCount);\r\n    // start the first processor\r\n    CountDownLatch stopLatch1 = new CountDownLatch(1);\r\n    Thread t1 = runInThread(sp1, stopLatch1);\r\n    t1.start();\r\n    // wait until the processor reports that it has started\r\n    waitForProcessorToStartStop(startWait1);\r\n    // make sure it consumes all the messages from the first batch\r\n    waitUntilMessagesLeftN(totalEventsToGenerate - messageCount);\r\n    CountDownLatch containerStopped1 = sp1.containerShutdownLatch;\r\n    // start the second processor\r\n    CountDownLatch startWait2 = new CountDownLatch(1);\r\n    StreamProcessor sp2 = createStreamProcessor(\"21\", map, startWait2, null);\r\n    CountDownLatch stopLatch2 = new CountDownLatch(1);\r\n    Thread t2 = runInThread(sp2, stopLatch2);\r\n    t2.start();\r\n    // wait until 2nd processor reports that it has started\r\n    waitForProcessorToStartStop(startWait2);\r\n    // wait until the 1st processor reports that it has stopped its container\r\n    LOG.info(\"containerStopped latch = \" + containerStopped1);\r\n    waitForProcessorToStartStop(containerStopped1);\r\n    // read again the first batch\r\n    waitUntilMessagesLeftN(totalEventsToGenerate - 2 * messageCount);\r\n    // produce the second batch of the messages, starting with 'messageCount'\r\n    produceMessages(messageCount, inputTopic, messageCount);\r\n    // wait until all the events are consumed\r\n    waitUntilMessagesLeftN(0);\r\n    // shutdown both\r\n    try {\r\n        stopProcessor(stopLatch1);\r\n        stopProcessor(stopLatch2);\r\n        t1.join(1000);\r\n        t2.join(1000);\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Failed to join finished threads:\" + e.getLocalizedMessage());\r\n    }\r\n    // p1 will read messageCount events, and then p1 and p2 will read 2xmessageCount events together,\r\n    // but the expected values are the same 0-79, they will appear in the output more then once, but we should mark then only one time.\r\n    // total number of events we gonna get is 80+40=120\r\n    verifyNumMessages(outputTopic, 2 * messageCount, totalEventsToGenerate);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessor.java",
  "methodName" : "testStreamProcessorWithRemove",
  "sourceCode" : "@Test\r\npublic /**\r\n * same as other happy path messages, but with one processor removed in the middle\r\n */\r\nvoid testStreamProcessorWithRemove() {\r\n    // set number of events we expect to read by both processes in total:\r\n    // p1 and p2 - both read messageCount at first and p1 is shutdown, new batch of events is generated\r\n    // and p2 will read all of them from the beginning (+ 2 x messageCounts, total 3 x)\r\n    int totalEventsToGenerate = 3 * messageCount;\r\n    TestStreamTask.endLatch = new CountDownLatch(totalEventsToGenerate);\r\n    // create first processor\r\n    CountDownLatch waitStart1 = new CountDownLatch(1);\r\n    CountDownLatch waitStop1 = new CountDownLatch(1);\r\n    StreamProcessor sp1 = createStreamProcessor(\"30\", map, waitStart1, waitStop1);\r\n    // start the first processor\r\n    CountDownLatch stopLatch1 = new CountDownLatch(1);\r\n    Thread t1 = runInThread(sp1, stopLatch1);\r\n    t1.start();\r\n    // start the second processor\r\n    CountDownLatch waitStart2 = new CountDownLatch(1);\r\n    CountDownLatch waitStop2 = new CountDownLatch(1);\r\n    StreamProcessor sp2 = createStreamProcessor(\"31\", map, waitStart2, waitStop2);\r\n    CountDownLatch stopLatch2 = new CountDownLatch(1);\r\n    Thread t2 = runInThread(sp2, stopLatch2);\r\n    t2.start();\r\n    // wait until the processor reports that it has started\r\n    waitForProcessorToStartStop(waitStart1);\r\n    // wait until the processor reports that it has started\r\n    waitForProcessorToStartStop(waitStart2);\r\n    // produce first batch of messages starting with 0\r\n    produceMessages(0, inputTopic, messageCount);\r\n    // make sure they consume all the messages from the first batch\r\n    waitUntilMessagesLeftN(totalEventsToGenerate - messageCount);\r\n    CountDownLatch containerStopped2 = sp2.containerShutdownLatch;\r\n    // stop the first processor\r\n    stopProcessor(stopLatch1);\r\n    // wait until it's really down\r\n    waitForProcessorToStartStop(waitStop1);\r\n    // processor2 will stop it container and start again.\r\n    // We wait for the container's stop to make sure we can count EXACTLY how many messages it reads.\r\n    LOG.info(\"containerStopped latch = \" + containerStopped2);\r\n    waitForProcessorToStartStop(containerStopped2);\r\n    // read again the first batch\r\n    waitUntilMessagesLeftN(totalEventsToGenerate - 2 * messageCount);\r\n    // produce the second batch of the messages, starting with 'messageCount'\r\n    produceMessages(messageCount, inputTopic, messageCount);\r\n    // wait until p2 consumes all the message by itself;\r\n    waitUntilMessagesLeftN(0);\r\n    // shutdown p2\r\n    try {\r\n        stopProcessor(stopLatch2);\r\n        t2.join(1000);\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Failed to join finished thread:\" + e.getLocalizedMessage());\r\n    }\r\n    // processor1 and 2 will both read 20 events (total 40), and then processor2 read 80 events by itself,\r\n    // but the expected values are the same 0-79 - we should get each value one time.\r\n    // Meanwhile the number of events we gonna get is 40 + 80\r\n    verifyNumMessages(outputTopic, 2 * messageCount, totalEventsToGenerate);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessorFailures.java",
  "methodName" : "testZkUnavailable",
  "sourceCode" : "@Test(expected = org.apache.samza.SamzaException.class)\r\npublic void testZkUnavailable() {\r\n    // non-existing zk\r\n    map.put(ZkConfig.ZK_CONNECT, \"localhost:2222\");\r\n    // shorter timeout\r\n    map.put(ZkConfig.ZK_CONNECTION_TIMEOUT_MS, \"3000\");\r\n    CountDownLatch startLatch = new CountDownLatch(1);\r\n    // this should fail with timeout exception\r\n    createStreamProcessor(\"1\", map, startLatch, null);\r\n    Assert.fail(\"should've thrown an exception\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\processor\\TestZkStreamProcessorFailures.java",
  "methodName" : "testFailStreamProcessor",
  "sourceCode" : "@Test\r\npublic // throw an exception.\r\nvoid testFailStreamProcessor() {\r\n    // either of these bad messages will cause p1 to throw and exception\r\n    final int numBadMessages = 4;\r\n    map.put(JobConfig.JOB_DEBOUNCE_TIME_MS, \"100\");\r\n    map.put(\"processor.id.to.fail\", \"101\");\r\n    // set number of events we expect to read by both processes in total:\r\n    // p1 will read messageCount/2 messages\r\n    // p2 will read messageCount/2 messages\r\n    // numBadMessages \"bad\" messages will be generated\r\n    // p2 will read 2 of the \"bad\" messages\r\n    // p1 will fail on the first of the \"bad\" messages\r\n    // a new job model will be generated\r\n    // and p2 will read all 2 * messageCount messages again, + numBadMessages (all of them this time)\r\n    // total 2 x messageCount / 2 + numBadMessages/2 + 2 * messageCount + numBadMessages\r\n    int totalEventsToBeConsumed = 3 * messageCount;\r\n    TestStreamTask.endLatch = new CountDownLatch(totalEventsToBeConsumed);\r\n    // create first processor\r\n    CountDownLatch waitStart1 = new CountDownLatch(1);\r\n    CountDownLatch waitStop1 = new CountDownLatch(1);\r\n    StreamProcessor sp1 = createStreamProcessor(\"101\", map, waitStart1, waitStop1);\r\n    // start the first processor\r\n    CountDownLatch stopLatch1 = new CountDownLatch(1);\r\n    Thread t1 = runInThread(sp1, stopLatch1);\r\n    t1.start();\r\n    // start the second processor\r\n    CountDownLatch waitStart2 = new CountDownLatch(1);\r\n    CountDownLatch waitStop2 = new CountDownLatch(1);\r\n    StreamProcessor sp2 = createStreamProcessor(\"102\", map, waitStart2, waitStop2);\r\n    CountDownLatch stopLatch2 = new CountDownLatch(1);\r\n    Thread t2 = runInThread(sp2, stopLatch2);\r\n    t2.start();\r\n    // wait until the 1st processor reports that it has started\r\n    waitForProcessorToStartStop(waitStart1);\r\n    // wait until the 2nd processor reports that it has started\r\n    waitForProcessorToStartStop(waitStart2);\r\n    // produce first batch of messages starting with 0\r\n    produceMessages(0, inputTopic, messageCount);\r\n    // make sure they consume all the messages\r\n    waitUntilMessagesLeftN(totalEventsToBeConsumed - messageCount);\r\n    CountDownLatch containerStopped1 = sp1.containerShutdownLatch;\r\n    CountDownLatch containerStopped2 = sp2.containerShutdownLatch;\r\n    // produce the bad messages\r\n    produceMessages(BAD_MESSAGE_KEY, inputTopic, 4);\r\n    waitForProcessorToStartStop(// TODO: after container failure propagates to StreamProcessor change back\r\n    containerStopped1);\r\n    // wait until the 2nd processor reports that it has stopped its container\r\n    waitForProcessorToStartStop(containerStopped2);\r\n    // give some extra time to let the system to publish and distribute the new job model\r\n    TestZkUtils.sleepMs(300);\r\n    // produce the second batch of the messages, starting with 'messageCount'\r\n    produceMessages(messageCount, inputTopic, messageCount);\r\n    // wait until p2 consumes all the message by itself\r\n    waitUntilMessagesLeftN(0);\r\n    // shutdown p2\r\n    try {\r\n        stopProcessor(stopLatch2);\r\n        t2.join(1000);\r\n    } catch (InterruptedException e) {\r\n        Assert.fail(\"Failed to join finished thread:\" + e.getLocalizedMessage());\r\n    }\r\n    // number of unique values we gonna read is from 0 to (2*messageCount - 1)\r\n    Map<Integer, Boolean> expectedValues = new HashMap<>(2 * messageCount);\r\n    for (int i = 0; i < 2 * messageCount; i++) {\r\n        expectedValues.put(i, false);\r\n    }\r\n    for (int i = BAD_MESSAGE_KEY; i < numBadMessages + BAD_MESSAGE_KEY; i++) {\r\n        //expectedValues.put(i, false);\r\n    }\r\n    verifyNumMessages(outputTopic, expectedValues, totalEventsToBeConsumed);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\BlobStoreStateBackendIntegrationTest.java",
  "methodName" : "testStopAndRestart",
  "sourceCode" : "@Test\r\npublic void testStopAndRestart() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> sideInputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, sideInputMessagesOnInitialRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), CONFIGS);\r\n    Pair<String, SnapshotIndex> lastRegularSnapshot = verifyLedger(REGULAR_STORE_NAME, Optional.empty(), hostAffinity, false, false);\r\n    Pair<String, SnapshotIndex> lastSideInputSnapshot = verifyLedger(SIDE_INPUT_STORE_NAME, Optional.empty(), hostAffinity, true, false);\r\n    // verifies transactional state too\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> sideInputMessagesBeforeSecondRun = Arrays.asList(\"7\", \"8\", \"9\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    // verifies that in-memory stores backed by changelogs work correctly\r\n    // (requires overriding store level state backends explicitly)\r\n    List<String> expectedInitialInMemoryStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    List<String> expectedInitialSideInputStoreContentsOnSecondRun = new ArrayList<>(sideInputMessagesOnInitialRun);\r\n    expectedInitialSideInputStoreContentsOnSecondRun.addAll(sideInputMessagesBeforeSecondRun);\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, sideInputMessagesBeforeSecondRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), expectedInitialStoreContentsOnSecondRun, expectedInitialInMemoryStoreContentsOnSecondRun, expectedInitialSideInputStoreContentsOnSecondRun, CONFIGS);\r\n    verifyLedger(REGULAR_STORE_NAME, Optional.of(lastRegularSnapshot), hostAffinity, false, false);\r\n    verifyLedger(SIDE_INPUT_STORE_NAME, Optional.of(lastSideInputSnapshot), hostAffinity, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\BlobStoreStateBackendIntegrationTest.java",
  "methodName" : "stopDeleteBlobRun",
  "sourceCode" : "@Test\r\npublic void stopDeleteBlobRun() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> sideInputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, sideInputMessagesOnInitialRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), CONFIGS);\r\n    Pair<String, SnapshotIndex> lastRegularSnapshot = verifyLedger(REGULAR_STORE_NAME, Optional.empty(), hostAffinity, false, false);\r\n    Pair<String, SnapshotIndex> lastSideInputSnapshot = verifyLedger(SIDE_INPUT_STORE_NAME, Optional.empty(), hostAffinity, true, false);\r\n    // verifies transactional state too\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> sideInputMessagesBeforeSecondRun = Arrays.asList(\"7\", \"8\", \"9\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    // verifies that in-memory stores backed by changelogs work correctly\r\n    // (requires overriding store level state backends explicitly)\r\n    List<String> expectedInitialInMemoryStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    List<String> expectedInitialSideInputStoreContentsOnSecondRun = new ArrayList<>(sideInputMessagesOnInitialRun);\r\n    expectedInitialSideInputStoreContentsOnSecondRun.addAll(sideInputMessagesBeforeSecondRun);\r\n    deleteBlobFromLastCheckpoint(lastRegularSnapshot.getRight());\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, sideInputMessagesBeforeSecondRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), expectedInitialStoreContentsOnSecondRun, expectedInitialInMemoryStoreContentsOnSecondRun, expectedInitialSideInputStoreContentsOnSecondRun, CONFIGS);\r\n    verifyLedger(REGULAR_STORE_NAME, Optional.of(lastRegularSnapshot), hostAffinity, false, false);\r\n    verifyLedger(SIDE_INPUT_STORE_NAME, Optional.of(lastSideInputSnapshot), hostAffinity, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\BlobStoreStateBackendIntegrationTest.java",
  "methodName" : "stopDeleteSnapshotIndexBlobRun",
  "sourceCode" : "@Test\r\npublic void stopDeleteSnapshotIndexBlobRun() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> sideInputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, sideInputMessagesOnInitialRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), CONFIGS);\r\n    Pair<String, SnapshotIndex> lastRegularSnapshot = verifyLedger(REGULAR_STORE_NAME, Optional.empty(), hostAffinity, false, false);\r\n    Pair<String, SnapshotIndex> lastSideInputSnapshot = verifyLedger(SIDE_INPUT_STORE_NAME, Optional.empty(), hostAffinity, true, false);\r\n    // verifies transactional state too\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> sideInputMessagesBeforeSecondRun = Arrays.asList(\"7\", \"8\", \"9\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    // verifies that in-memory stores backed by changelogs work correctly\r\n    // (requires overriding store level state backends explicitly)\r\n    List<String> expectedInitialInMemoryStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    List<String> expectedInitialSideInputStoreContentsOnSecondRun = new ArrayList<>(sideInputMessagesOnInitialRun);\r\n    expectedInitialSideInputStoreContentsOnSecondRun.addAll(sideInputMessagesBeforeSecondRun);\r\n    deleteLastSnapshotIndex(lastRegularSnapshot);\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, sideInputMessagesBeforeSecondRun, ImmutableSet.of(REGULAR_STORE_NAME), Collections.emptyMap(), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), expectedInitialStoreContentsOnSecondRun, expectedInitialInMemoryStoreContentsOnSecondRun, expectedInitialSideInputStoreContentsOnSecondRun, CONFIGS);\r\n    verifyLedger(REGULAR_STORE_NAME, Optional.of(lastRegularSnapshot), hostAffinity, false, false);\r\n    verifyLedger(SIDE_INPUT_STORE_NAME, Optional.of(lastSideInputSnapshot), hostAffinity, true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\KafkaNonTransactionalStateIntegrationTest.java",
  "methodName" : "testStopAndRestart",
  "sourceCode" : "@Test\r\npublic void testStopAndRestart() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> sideInputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\");\r\n    List<String> expectedChangelogMessagesAfterInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", null, \"98\", \"99\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, sideInputMessagesOnInitialRun, ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterInitialRun, CONFIGS);\r\n    // first two are reverts for uncommitted messages from last run for keys 98 and 99\r\n    List<String> expectedChangelogMessagesAfterSecondRun = Arrays.asList(\"98\", \"99\", \"4\", \"5\", \"5\");\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> sideInputMessagesBeforeSecondRun = Arrays.asList(\"7\", \"8\", \"9\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\", \"98\", \"99\");\r\n    List<String> expectedInitialInMemoryStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\", \"98\", \"99\");\r\n    List<String> expectedInitialSideInputStoreContentsOnSecondRun = new ArrayList<>(sideInputMessagesOnInitialRun);\r\n    expectedInitialSideInputStoreContentsOnSecondRun.addAll(sideInputMessagesBeforeSecondRun);\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, sideInputMessagesBeforeSecondRun, ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterSecondRun, expectedInitialStoreContentsOnSecondRun, expectedInitialInMemoryStoreContentsOnSecondRun, expectedInitialSideInputStoreContentsOnSecondRun, CONFIGS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\KafkaTransactionalStateIntegrationTest.java",
  "methodName" : "testStopAndRestart",
  "sourceCode" : "@Test\r\npublic void testStopAndRestart() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> sideInputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\");\r\n    List<String> expectedChangelogMessagesAfterInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", null, \"98\", \"99\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, sideInputMessagesOnInitialRun, ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterInitialRun, CONFIGS);\r\n    // first two are reverts for uncommitted messages from last run for keys 98 and 99\r\n    List<String> expectedChangelogMessagesAfterSecondRun = Arrays.asList(null, null, \"98\", \"99\", \"4\", \"5\", \"5\");\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> sideInputMessagesBeforeSecondRun = Arrays.asList(\"7\", \"8\", \"9\");\r\n    List<String> expectedInitialStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    List<String> expectedInitialInMemoryStoreContentsOnSecondRun = Arrays.asList(\"1\", \"2\", \"3\");\r\n    List<String> expectedInitialSideInputStoreContentsOnSecondRun = new ArrayList<>(sideInputMessagesOnInitialRun);\r\n    expectedInitialSideInputStoreContentsOnSecondRun.addAll(sideInputMessagesBeforeSecondRun);\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, sideInputMessagesBeforeSecondRun, ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterSecondRun, expectedInitialStoreContentsOnSecondRun, expectedInitialInMemoryStoreContentsOnSecondRun, expectedInitialSideInputStoreContentsOnSecondRun, CONFIGS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\KafkaTransactionalStateIntegrationTest.java",
  "methodName" : "testWithEmptyChangelogFromInitialRun",
  "sourceCode" : "@Test\r\npublic void testWithEmptyChangelogFromInitialRun() {\r\n    // expected changelog messages will always match since we'll read 0 messages\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, ImmutableList.of(\"crash_once\"), Collections.emptyList(), ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, Collections.emptyList(), CONFIGS);\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> expectedChangelogMessagesAfterSecondRun = Arrays.asList(\"4\", \"5\", \"5\");\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, Collections.emptyList(), ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterSecondRun, Collections.emptyList(), Collections.emptyList(), Collections.emptyList(), CONFIGS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\KafkaTransactionalStateIntegrationTest.java",
  "methodName" : "testWithNewChangelogAfterInitialRun",
  "sourceCode" : "@Test\r\npublic void testWithNewChangelogAfterInitialRun() {\r\n    List<String> inputMessagesOnInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", \"-97\", \":98\", \":99\", \":crash_once\");\r\n    List<String> expectedChangelogMessagesAfterInitialRun = Arrays.asList(\"1\", \"2\", \"3\", \"2\", \"97\", null, \"98\", \"99\");\r\n    initialRun(INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesOnInitialRun, Collections.emptyList(), ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, REGULAR_STORE_CHANGELOG_TOPIC), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, IN_MEMORY_STORE_CHANGELOG_TOPIC), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterInitialRun, CONFIGS);\r\n    // admin client delete topic doesn't seem to work, times out up to 60 seconds.\r\n    // simulate delete topic by changing the changelog topic instead.\r\n    String newChangelogTopic = \"changelog2\";\r\n    String newInMemoryStoreChangelogTopic = \"inMemChangelog2\";\r\n    List<String> inputMessagesBeforeSecondRun = Arrays.asList(\"4\", \"5\", \"5\", \":shutdown\");\r\n    List<String> expectedChangelogMessagesAfterSecondRun = Arrays.asList(\"98\", \"99\", \"4\", \"5\", \"5\");\r\n    secondRun(hostAffinity, LOGGED_STORE_BASE_DIR, INPUT_SYSTEM, INPUT_TOPIC, SIDE_INPUT_TOPIC, inputMessagesBeforeSecondRun, Collections.emptyList(), ImmutableSet.of(REGULAR_STORE_NAME), ImmutableMap.of(REGULAR_STORE_NAME, newChangelogTopic), ImmutableSet.of(IN_MEMORY_STORE_NAME), ImmutableMap.of(IN_MEMORY_STORE_NAME, newInMemoryStoreChangelogTopic), SIDE_INPUT_STORE_NAME, expectedChangelogMessagesAfterSecondRun, Collections.emptyList(), Collections.emptyList(), Collections.emptyList(), CONFIGS);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestKeyValueSizeHistogramMetric.java",
  "methodName" : "testHistogramMetric",
  "sourceCode" : "/**\r\n * Make sure that the histograms can record the key value size and we can use a\r\n * {@link MetricsVisitor} to get access to the value store in the histograms\r\n */\r\n@Test\r\npublic void testHistogramMetric() {\r\n    List<String> keys = new ArrayList<>();\r\n    List<String> values = new ArrayList<>();\r\n    for (int i = 0; i < 300; i++) {\r\n        keys.add(getRandomString());\r\n        values.add(getRandomString());\r\n    }\r\n    for (int i = 0; i < keys.size(); i++) {\r\n        store.put(keys.get(i), values.get(i));\r\n    }\r\n    Set<String> names = new HashSet<>();\r\n    for (Double p : serializedKeyValueStoreMetrics.record_key_size_percentiles()) {\r\n        names.add(storeName + \"-\" + keyPrefix + \"_\" + p);\r\n    }\r\n    for (Double p : serializedKeyValueStoreMetrics.record_value_size_percentiles()) {\r\n        names.add(storeName + \"-\" + valuePrefix + \"_\" + p);\r\n    }\r\n    metricsRegistry.getGroups().forEach(group -> metricsRegistry.getGroup(group.toString()).forEach((name, metric) -> {\r\n        if (names.contains(name)) {\r\n            metric.visit(new MetricsVisitor() {\r\n\r\n                @Override\r\n                public void counter(Counter counter) {\r\n                }\r\n\r\n                @Override\r\n                public <T> void gauge(Gauge<T> gauge) {\r\n                    Double num = (Double) gauge.getValue();\r\n                    Assert.assertNotEquals(0D, (Double) gauge.getValue(), 0.0001);\r\n                }\r\n\r\n                @Override\r\n                public void timer(Timer timer) {\r\n                }\r\n            });\r\n        }\r\n    }));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeKeyValueStores.java",
  "methodName" : "testLargeMessagePut",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePut() {\r\n    String key = \"test\";\r\n    String largeMessage = StringUtils.repeat(\"a\", maxMessageSize + 1);\r\n    if (dropLargeMessage) {\r\n        store.put(key, largeMessage);\r\n        Assert.assertNull(\"The large message was stored while it shouldn't have been.\", loggedStore.get(stringSerde.toBytes(key)));\r\n    } else {\r\n        try {\r\n            store.put(key, largeMessage);\r\n            Assert.fail(\"Failure since put() method invocation incorrectly completed.\");\r\n        } catch (SamzaException e) {\r\n            Assert.assertNull(\"The large message was stored while it shouldn't have been.\", loggedStore.get(stringSerde.toBytes(key)));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeKeyValueStores.java",
  "methodName" : "testLargeMessagePutAll",
  "sourceCode" : "@Test\r\npublic void testLargeMessagePutAll() {\r\n    String key = \"test\";\r\n    String largeMessage = StringUtils.repeat(\"a\", maxMessageSize + 1);\r\n    List<Entry<String, String>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, largeMessage));\r\n    if (dropLargeMessage) {\r\n        store.putAll(entries);\r\n        Assert.assertNull(\"The large message was stored while it shouldn't have been.\", loggedStore.get(stringSerde.toBytes(key)));\r\n    } else {\r\n        try {\r\n            store.putAll(entries);\r\n            Assert.fail(\"Failure since putAll() method invocation incorrectly completed.\");\r\n        } catch (SamzaException e) {\r\n            Assert.assertNull(\"The large message was stored while it shouldn't have been.\", loggedStore.get(stringSerde.toBytes(key)));\r\n        }\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeKeyValueStores.java",
  "methodName" : "testSmallMessagePut",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePut() {\r\n    String key = \"test\";\r\n    String smallMessage = StringUtils.repeat(\"a\", maxMessageSize - 1);\r\n    store.put(key, smallMessage);\r\n    Assert.assertEquals(store.get(key), smallMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\storage\\kv\\TestLargeMessageSafeKeyValueStores.java",
  "methodName" : "testSmallMessagePutAll",
  "sourceCode" : "@Test\r\npublic void testSmallMessagePutAll() {\r\n    String key = \"test\";\r\n    String smallMessage = StringUtils.repeat(\"a\", maxMessageSize - 1);\r\n    List<Entry<String, String>> entries = new ArrayList<>();\r\n    entries.add(new Entry<>(key, smallMessage));\r\n    store.putAll(entries);\r\n    Assert.assertEquals(store.get(key), smallMessage);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\controlmessages\\EndOfStreamIntegrationTest.java",
  "methodName" : "testPipeline",
  "sourceCode" : "@Test\r\npublic void testPipeline() {\r\n    class PipelineApplication implements StreamApplication {\r\n\r\n        @Override\r\n        public void describe(StreamApplicationDescriptor appDescriptor) {\r\n            DelegatingSystemDescriptor sd = new DelegatingSystemDescriptor(\"test\");\r\n            GenericInputDescriptor<KV<String, PageView>> isd = sd.getInputDescriptor(\"PageView\", KVSerde.of(new NoOpSerde<>(), new NoOpSerde<>()));\r\n            appDescriptor.getInputStream(isd).map(KV::getValue).partitionBy(PageView::getMemberId, pv -> pv, KVSerde.of(new IntegerSerde(), new TestTableData.PageViewJsonSerde()), \"p1\").sink((m, collector, coordinator) -> {\r\n                RECEIVED.add(m.getValue());\r\n            });\r\n        }\r\n    }\r\n    int numPageViews = 40;\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    TestRunner.of(new PipelineApplication()).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, 4)).run(Duration.ofSeconds(10));\r\n    assertEquals(RECEIVED.size(), numPageViews);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\controlmessages\\WatermarkIntegrationTest.java",
  "methodName" : "testWatermark",
  "sourceCode" : "@Test\r\npublic void testWatermark() throws Exception {\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(ApplicationConfig.APP_RUNNER_CLASS, MockLocalApplicationRunner.class.getName());\r\n    configs.put(\"systems.test.samza.factory\", TestSystemFactory.class.getName());\r\n    configs.put(\"streams.PageView.samza.system\", \"test\");\r\n    configs.put(\"streams.PageView.partitionCount\", String.valueOf(PARTITION_COUNT));\r\n    configs.put(JobConfig.JOB_NAME, \"test-watermark-job\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"1\");\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, PassthroughJobCoordinatorFactory.class.getName());\r\n    configs.put(TaskConfig.GROUPER_FACTORY, SingleContainerGrouperFactory.class.getName());\r\n    configs.put(\"systems.kafka.samza.factory\", \"org.apache.samza.system.kafka.KafkaSystemFactory\");\r\n    configs.put(\"systems.kafka.producer.bootstrap.servers\", bootstrapUrl());\r\n    configs.put(\"systems.kafka.consumer.zookeeper.connect\", zkConnect());\r\n    configs.put(\"systems.kafka.samza.key.serde\", \"int\");\r\n    configs.put(\"systems.kafka.samza.msg.serde\", \"json\");\r\n    configs.put(\"systems.kafka.default.stream.replication.factor\", \"1\");\r\n    configs.put(\"job.default.system\", \"kafka\");\r\n    configs.put(\"serializers.registry.int.class\", IntegerSerdeFactory.class.getName());\r\n    configs.put(\"serializers.registry.string.class\", StringSerdeFactory.class.getName());\r\n    configs.put(\"serializers.registry.json.class\", PageViewJsonSerdeFactory.class.getName());\r\n    List<PageView> received = new ArrayList<>();\r\n    class TestStreamApp implements StreamApplication {\r\n\r\n        @Override\r\n        public void describe(StreamApplicationDescriptor appDescriptor) {\r\n            DelegatingSystemDescriptor sd = new DelegatingSystemDescriptor(\"test\");\r\n            GenericInputDescriptor<KV<String, PageView>> isd = sd.getInputDescriptor(\"PageView\", KVSerde.of(new NoOpSerde<>(), new NoOpSerde<>()));\r\n            appDescriptor.getInputStream(isd).map(KV::getValue).partitionBy(pv -> pv.getMemberId(), pv -> pv, KVSerde.of(new NoOpSerde<>(), new NoOpSerde<>()), \"p1\").sink((m, collector, coordinator) -> {\r\n                received.add(m.getValue());\r\n            });\r\n        }\r\n    }\r\n    Config config = new MapConfig(configs);\r\n    final ApplicationRunner runner = ApplicationRunners.getApplicationRunner(new TestStreamApp(), config);\r\n    executeRun(runner, config);\r\n    // processors are only available when the app is running\r\n    Map<String, StreamOperatorTask> tasks = getTaskOperationGraphs((MockLocalApplicationRunner) runner);\r\n    runner.waitForFinish();\r\n    // wait for the completion to ensure that all tasks are actually initialized and the OperatorImplGraph is initialized\r\n    StreamOperatorTask task0 = tasks.get(\"Partition 0\");\r\n    OperatorImplGraph graph = TestStreamOperatorTask.getOperatorImplGraph(task0);\r\n    OperatorImpl pb = getOperator(graph, OperatorSpec.OpCode.PARTITION_BY);\r\n    assertEquals(TestOperatorImpl.getInputWatermark(pb), 4);\r\n    assertEquals(TestOperatorImpl.getOutputWatermark(pb), 4);\r\n    OperatorImpl sink = getOperator(graph, OperatorSpec.OpCode.SINK);\r\n    assertEquals(TestOperatorImpl.getInputWatermark(sink), 3);\r\n    assertEquals(TestOperatorImpl.getOutputWatermark(sink), 3);\r\n    StreamOperatorTask task1 = tasks.get(\"Partition 1\");\r\n    graph = TestStreamOperatorTask.getOperatorImplGraph(task1);\r\n    pb = getOperator(graph, OperatorSpec.OpCode.PARTITION_BY);\r\n    assertEquals(TestOperatorImpl.getInputWatermark(pb), 3);\r\n    assertEquals(TestOperatorImpl.getOutputWatermark(pb), 3);\r\n    sink = getOperator(graph, OperatorSpec.OpCode.SINK);\r\n    assertEquals(TestOperatorImpl.getInputWatermark(sink), 3);\r\n    assertEquals(TestOperatorImpl.getOutputWatermark(sink), 3);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainHighLevelApiIntegrationTest.java",
  "methodName" : "testDrain",
  "sourceCode" : "/**\r\n * This test will test drain and consumption of some messages from the in-memory topic.\r\n * In order to simulate the real-world behaviour of drain, the test adds messages to the in-memory topic buffer periodically\r\n * in a delayed fashion instead of all at once. The test then writes the drain notification message to the in-memory\r\n * metadata store to drain and stop the pipeline. This write is done shortly after the pipeline starts and before all\r\n * the messages are written to the topic's buffer. As a result, the total count of the processed messages will be less\r\n * than the expected count of messages.\r\n */\r\n@Test\r\npublic void testDrain() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    long drainTriggerDelay = 10_000L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME1);\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(STREAM_ID1, new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new PageViewEventCountHighLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // write drain message after a delay\r\n    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();\r\n    executorService.schedule(new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            UUID uuid = DrainUtils.writeDrainNotification(metadataStore);\r\n            return uuid.toString();\r\n        }\r\n    }, drainTriggerDelay, TimeUnit.MILLISECONDS);\r\n    testRunner.run(Duration.ofSeconds(40));\r\n    assertTrue(RECEIVED.size() < numPageViews && RECEIVED.size() > 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainHighLevelApiIntegrationTest.java",
  "methodName" : "testDrainWithoutReshuffleStages",
  "sourceCode" : "@Test\r\npublic void testDrainWithoutReshuffleStages() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    long drainTriggerDelay = 10_000L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME2);\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(STREAM_ID2, new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new SimpleHighLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // write drain message after a delay\r\n    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();\r\n    executorService.schedule(new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            UUID uuid = DrainUtils.writeDrainNotification(metadataStore);\r\n            return uuid.toString();\r\n        }\r\n    }, drainTriggerDelay, TimeUnit.MILLISECONDS);\r\n    testRunner.run(Duration.ofSeconds(40));\r\n    assertTrue(RECEIVED.size() < numPageViews && RECEIVED.size() > 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainHighLevelApiIntegrationTest.java",
  "methodName" : "testDrainOnContainerStart",
  "sourceCode" : "/**\r\n * This test will test drain and that no messages are processed as drain notification is written to the metadata store\r\n * before start.\r\n */\r\n@Test\r\npublic void testDrainOnContainerStart() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME1);\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(STREAM_ID1, new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new PageViewEventCountHighLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // write on the test thread to ensure that drain notification is available on container start\r\n    DrainUtils.writeDrainNotification(metadataStore);\r\n    testRunner.run(Duration.ofSeconds(20));\r\n    assertEquals(RECEIVED.size(), 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainHighLevelApiIntegrationTest.java",
  "methodName" : "testDrainOnContainerStartWithoutReshuffleStages",
  "sourceCode" : "@Test\r\npublic void testDrainOnContainerStartWithoutReshuffleStages() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME2);\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(STREAM_ID2, new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new SimpleHighLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // write on the test thread to ensure that drain notification is available on container start\r\n    DrainUtils.writeDrainNotification(metadataStore);\r\n    testRunner.run(Duration.ofSeconds(20));\r\n    assertEquals(RECEIVED.size(), 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainLowLevelApiIntegrationTest.java",
  "methodName" : "testDrain",
  "sourceCode" : "/**\r\n * This test will test drain and consumption of some messages from the in-memory topic.\r\n * In order to simulate the real-world behaviour of drain, the test adds messages to the in-memory topic buffer in\r\n * a delayed fashion instead of all at once. The test then writes the drain notification message to the in-memory\r\n * metadata store to drain and stop the pipeline. This write is done shortly after the pipeline starts and before all\r\n * the messages are written to the topic's buffer. As a result, the total count of the processed messages will be less\r\n * than the expected count of messages.\r\n */\r\n@Test\r\npublic void testDrain() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    long drainTriggerDelay = 5000L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new PageViewEventCountLowLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // Trigger drain after a delay\r\n    ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();\r\n    executorService.schedule(new Callable<String>() {\r\n\r\n        @Override\r\n        public String call() throws Exception {\r\n            UUID uuid = DrainUtils.writeDrainNotification(metadataStore);\r\n            return uuid.toString();\r\n        }\r\n    }, drainTriggerDelay, TimeUnit.MILLISECONDS);\r\n    testRunner.run(Duration.ofSeconds(40));\r\n    assertTrue(RECEIVED.size() < numPageViews && RECEIVED.size() > 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\drain\\DrainLowLevelApiIntegrationTest.java",
  "methodName" : "testDrainOnContainerStart",
  "sourceCode" : "@Test\r\npublic void testDrainOnContainerStart() {\r\n    int numPageViews = 200;\r\n    int numPartitions = 4;\r\n    long delayBetweenMessagesInMillis = 500L;\r\n    String runId = \"DrainTestId\";\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    InMemoryMetadataStoreFactory metadataStoreFactory = new InMemoryMetadataStoreFactory();\r\n    Map<String, String> customConfig = ImmutableMap.of(ApplicationConfig.APP_RUN_ID, runId, JobConfig.DRAIN_MONITOR_POLL_INTERVAL_MILLIS, \"100\", JobConfig.DRAIN_MONITOR_ENABLED, \"true\");\r\n    // Create a TestRunner\r\n    // Set a InMemoryMetadataFactory.This factory is shared between TestRunner and DrainUtils's write drain method\r\n    TestRunner testRunner = TestRunner.of(new PageViewEventCountLowLevelApplication()).setInMemoryMetadataFactory(metadataStoreFactory).addConfig(customConfig).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, numPartitions), delayBetweenMessagesInMillis);\r\n    Config configFromRunner = testRunner.getConfig();\r\n    MetadataStore metadataStore = metadataStoreFactory.getMetadataStore(\"NoOp\", configFromRunner, new MetricsRegistryMap());\r\n    // Write configs to the coordinator stream here as neither the passthrough JC nor the StreamProcessor is writing\r\n    // configs to coordinator stream. RemoteApplicationRunner typically write the configs to the metadata store\r\n    // before starting the JC.\r\n    // We are doing this so that DrainUtils.writeDrainNotification can read app.run.id from the config\r\n    CoordinatorStreamUtil.writeConfigToCoordinatorStream(metadataStore, configFromRunner);\r\n    // write on the test thread to ensure that drain notification is available on container start\r\n    DrainUtils.writeDrainNotification(metadataStore);\r\n    testRunner.run(Duration.ofSeconds(40));\r\n    assertEquals(RECEIVED.size(), 0);\r\n    RECEIVED.clear();\r\n    clearMetadataStore(metadataStore);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\AsyncStreamTaskIntegrationTest.java",
  "methodName" : "testAsyncTaskWithSinglePartition",
  "sourceCode" : "@Test\r\npublic void testAsyncTaskWithSinglePartition() throws Exception {\r\n    List<Integer> inputList = Arrays.asList(1, 2, 3, 4, 5);\r\n    List<Integer> outputList = Arrays.asList(10, 20, 30, 40, 50);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"async-test\");\r\n    InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"ints\", new NoOpSerde<Integer>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"ints-out\", new NoOpSerde<>());\r\n    TestRunner.of(MyAsyncStreamTask.class).addInputStream(imid, inputList).addOutputStream(imod, 1).run(Duration.ofSeconds(2));\r\n    Assert.assertThat(TestRunner.consumeStream(imod, Duration.ofMillis(1000)).get(0), IsIterableContainingInOrder.contains(outputList.toArray()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\AsyncStreamTaskIntegrationTest.java",
  "methodName" : "testAsyncTaskWithSinglePartitionUsingStreamAssert",
  "sourceCode" : "@Test\r\npublic void testAsyncTaskWithSinglePartitionUsingStreamAssert() throws Exception {\r\n    List<Integer> inputList = Arrays.asList(1, 2, 3, 4, 5);\r\n    List<Integer> outputList = Arrays.asList(50, 10, 20, 30, 40);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"async-test\");\r\n    InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"ints\", new NoOpSerde<Integer>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"ints-out\", new NoOpSerde<>());\r\n    TestRunner.of(MyAsyncStreamTask.class).addInputStream(imid, inputList).addOutputStream(imod, 1).run(Duration.ofSeconds(2));\r\n    StreamAssert.containsInAnyOrder(outputList, imod, Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\AsyncStreamTaskIntegrationTest.java",
  "methodName" : "testAsyncTaskWithMultiplePartition",
  "sourceCode" : "@Test\r\npublic void testAsyncTaskWithMultiplePartition() throws Exception {\r\n    Map<Integer, List<KV>> inputPartitionData = new HashMap<>();\r\n    Map<Integer, List<Integer>> expectedOutputPartitionData = new HashMap<>();\r\n    genData(inputPartitionData, expectedOutputPartitionData);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"async-test\");\r\n    InMemoryInputDescriptor<KV> imid = isd.getInputDescriptor(\"ints\", new NoOpSerde<KV>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"ints-out\", new NoOpSerde<>());\r\n    TestRunner.of(MyAsyncStreamTask.class).addInputStream(imid, inputPartitionData).addOutputStream(imod, 5).run(Duration.ofSeconds(2));\r\n    StreamAssert.containsInOrder(expectedOutputPartitionData, imod, Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\AsyncStreamTaskIntegrationTest.java",
  "methodName" : "testAsyncTaskWithMultiplePartitionMultithreaded",
  "sourceCode" : "@Test\r\npublic void testAsyncTaskWithMultiplePartitionMultithreaded() throws Exception {\r\n    Map<Integer, List<KV>> inputPartitionData = new HashMap<>();\r\n    Map<Integer, List<Integer>> expectedOutputPartitionData = new HashMap<>();\r\n    genData(inputPartitionData, expectedOutputPartitionData);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"async-test\");\r\n    InMemoryInputDescriptor<KV> imid = isd.getInputDescriptor(\"ints\", new NoOpSerde<>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"ints-out\", new NoOpSerde<>());\r\n    TestRunner.of(MyAsyncStreamTask.class).addInputStream(imid, inputPartitionData).addOutputStream(imod, 5).addConfig(\"task.max.concurrency\", \"4\").run(Duration.ofSeconds(2));\r\n    StreamAssert.containsInAnyOrder(expectedOutputPartitionData, imod, Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\AsyncStreamTaskIntegrationTest.java",
  "methodName" : "testSamzaJobTimeoutFailureForAsyncTask",
  "sourceCode" : "/**\r\n * Job should fail because it times out too soon\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testSamzaJobTimeoutFailureForAsyncTask() {\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"async-test\");\r\n    InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"ints\", new NoOpSerde<>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"ints-out\", new NoOpSerde<>());\r\n    TestRunner.of(MyAsyncStreamTask.class).addInputStream(imid, Arrays.asList(1, 2, 3, 4)).addOutputStream(imod, 1).run(Duration.ofMillis(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\FaultInjectionTest.java",
  "methodName" : "testRaceCondition",
  "sourceCode" : "@Test\r\npublic void testRaceCondition() throws InterruptedException {\r\n    int taskShutdownInMs = (int) (Math.random() * 10000);\r\n    CountDownLatch containerShutdownLatch = new CountDownLatch(1);\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.apache.samza.zk.ZkJobCoordinatorFactory\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    configs.put(TaskConfig.GROUPER_FACTORY, \"org.apache.samza.container.grouper.task.GroupByContainerIdsFactory\");\r\n    configs.put(FaultInjectionStreamApp.INPUT_TOPIC_NAME_PROP, \"page-views\");\r\n    configs.put(TaskConfig.INPUT_STREAMS, \"kafka.page-views\");\r\n    configs.put(ZkConfig.ZK_CONNECT, zkConnect());\r\n    configs.put(JobConfig.JOB_DEBOUNCE_TIME_MS, \"5000\");\r\n    // we purposefully randomize the task.shutdown.ms to make sure we can consistently verify if status is unsuccessfulFinish\r\n    // even though the reason for failure can either be container exception or container shutdown timing out.\r\n    configs.put(\"task.shutdown.ms\", Integer.toString(taskShutdownInMs));\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    createTopic(PAGE_VIEWS, 2);\r\n    // create events for the following user activity.\r\n    // userId: (viewId, pageId, (adIds))\r\n    // u1: (v1, p1, (a1)), (v2, p2, (a3))\r\n    // u2: (v3, p1, (a1)), (v4, p3, (a5))\r\n    produceMessage(PAGE_VIEWS, 0, \"p1\", \"{\\\"viewId\\\":\\\"v1\\\",\\\"pageId\\\":\\\"p1\\\",\\\"userId\\\":\\\"u1\\\"}\");\r\n    produceMessage(PAGE_VIEWS, 1, \"p2\", \"{\\\"viewId\\\":\\\"v2\\\",\\\"pageId\\\":\\\"p2\\\",\\\"userId\\\":\\\"u1\\\"}\");\r\n    FaultInjectionStreamApp app = new FaultInjectionStreamApp();\r\n    FaultInjectionStreamApp.containerShutdownLatch = containerShutdownLatch;\r\n    RunApplicationContext context = runApplication(app, \"fault-injection-app\", configs);\r\n    containerShutdownLatch.await();\r\n    context.getRunner().kill();\r\n    context.getRunner().waitForFinish();\r\n    assertEquals(context.getRunner().status(), ApplicationStatus.UnsuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\SchedulingTest.java",
  "methodName" : "testJob",
  "sourceCode" : "@Test\r\npublic void testJob() throws InterruptedException {\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configs.put(\"job.systemstreampartition.grouper.factory\", \"org.apache.samza.container.grouper.stream.AllSspToSingleTaskGrouperFactory\");\r\n    configs.put(\"task.name.grouper.factory\", \"org.apache.samza.container.grouper.task.SingleContainerGrouperFactory\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    runApplication(new TestSchedulingApp(), \"SchedulingTest\", configs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamApplicationIntegrationTest.java",
  "methodName" : "testStatefulJoinWithLocalTable",
  "sourceCode" : "@Test\r\npublic void testStatefulJoinWithLocalTable() {\r\n    Random random = new Random();\r\n    List<KV<String, TestTableData.PageView>> pageViews = Arrays.asList(TestTableData.generatePageViews(10)).stream().map(x -> KV.of(PAGEKEYS[random.nextInt(PAGEKEYS.length)], x)).collect(Collectors.toList());\r\n    List<KV<String, TestTableData.Profile>> profiles = Arrays.asList(TestTableData.generateProfiles(10)).stream().map(x -> KV.of(PAGEKEYS[random.nextInt(PAGEKEYS.length)], x)).collect(Collectors.toList());\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<KV<String, TestTableData.PageView>> pageViewStreamDesc = isd.getInputDescriptor(\"PageView\", new NoOpSerde<KV<String, TestTableData.PageView>>());\r\n    InMemoryInputDescriptor<KV<String, TestTableData.Profile>> profileStreamDesc = isd.getInputDescriptor(\"Profile\", new NoOpSerde<KV<String, TestTableData.Profile>>()).shouldBootstrap();\r\n    InMemoryOutputDescriptor<TestTableData.EnrichedPageView> outputStreamDesc = isd.getOutputDescriptor(\"EnrichedPageView\", new NoOpSerde<>());\r\n    InMemoryOutputDescriptor<String> joinKeysDescriptor = isd.getOutputDescriptor(\"JoinPageKeys\", new NoOpSerde<>());\r\n    TestRunner.of(new PageViewProfileViewJoinApplication()).addInputStream(pageViewStreamDesc, pageViews).addInputStream(profileStreamDesc, profiles).addOutputStream(outputStreamDesc, 1).addOutputStream(joinKeysDescriptor, 1).run(Duration.ofSeconds(2));\r\n    Assert.assertEquals(10, TestRunner.consumeStream(outputStreamDesc, Duration.ofSeconds(1)).get(0).size());\r\n    Assert.assertEquals(10, TestRunner.consumeStream(joinKeysDescriptor, Duration.ofSeconds(1)).get(0).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamApplicationIntegrationTest.java",
  "methodName" : "testHighLevelApi",
  "sourceCode" : "@Test\r\npublic void testHighLevelApi() throws Exception {\r\n    Random random = new Random();\r\n    int count = 10;\r\n    List<PageView> pageViews = new ArrayList<>();\r\n    for (int memberId = 0; memberId < count; memberId++) {\r\n        String pagekey = PAGEKEYS[random.nextInt(PAGEKEYS.length - 1)];\r\n        PageView pv = new PageView(pagekey, memberId);\r\n        pageViews.add(pv);\r\n    }\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<PageView> imid = isd.getInputDescriptor(\"PageView\", new NoOpSerde<PageView>());\r\n    InMemoryOutputDescriptor<PageView> imod = isd.getOutputDescriptor(\"Output\", new NoOpSerde<PageView>());\r\n    TestRunner.of(new PageViewRepartitionApplication()).addInputStream(imid, pageViews).addOutputStream(imod, 10).run(Duration.ofMillis(1500));\r\n    Assert.assertEquals(TestRunner.consumeStream(imod, Duration.ofMillis(1000)).get(random.nextInt(count)).size(), 1);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamApplicationIntegrationTest.java",
  "methodName" : "testSamzaJobFailureForStreamApplication",
  "sourceCode" : "/**\r\n * Null page key is passed in input data which should fail filter logic\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testSamzaJobFailureForStreamApplication() {\r\n    int count = 10;\r\n    List<TestData.PageView> pageviews = new ArrayList<>();\r\n    for (int memberId = 0; memberId < count; memberId++) {\r\n        pageviews.add(new TestData.PageView(null, memberId));\r\n    }\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<PageView> imid = isd.getInputDescriptor(\"PageView\", new NoOpSerde<PageView>());\r\n    InMemoryOutputDescriptor<PageView> imod = isd.getOutputDescriptor(\"Output\", new NoOpSerde<PageView>());\r\n    TestRunner.of(new PageViewFilterApplication()).addInputStream(imid, pageviews).addOutputStream(imod, 10).run(Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testStatefulTaskWithLocalTable",
  "sourceCode" : "@Test\r\npublic void testStatefulTaskWithLocalTable() {\r\n    List<PageView> pageViews = Arrays.asList(TestTableData.generatePageViews(10));\r\n    List<Profile> profiles = Arrays.asList(TestTableData.generateProfiles(10));\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> pageViewStreamDesc = isd.getInputDescriptor(\"PageView\", new NoOpSerde<TestTableData.PageView>());\r\n    InMemoryInputDescriptor<TestTableData.Profile> profileStreamDesc = isd.getInputDescriptor(\"Profile\", new NoOpSerde<TestTableData.Profile>()).shouldBootstrap();\r\n    InMemoryOutputDescriptor<TestTableData.EnrichedPageView> outputStreamDesc = isd.getOutputDescriptor(\"EnrichedPageView\", new NoOpSerde<>());\r\n    TestRunner.of(new JoinTaskApplication()).addInputStream(pageViewStreamDesc, pageViews).addInputStream(profileStreamDesc, profiles).addOutputStream(outputStreamDesc, 1).run(Duration.ofSeconds(2));\r\n    Assert.assertEquals(10, TestRunner.consumeStream(outputStreamDesc, Duration.ofSeconds(1)).get(0).size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSyncTaskWithSinglePartition",
  "sourceCode" : "@Test\r\npublic void testSyncTaskWithSinglePartition() throws Exception {\r\n    List<Integer> inputList = Arrays.asList(1, 2, 3, 4, 5);\r\n    List<Integer> outputList = Arrays.asList(10, 20, 30, 40, 50);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"input\", new NoOpSerde<Integer>());\r\n    InMemoryOutputDescriptor<Integer> imod = isd.getOutputDescriptor(\"output\", new NoOpSerde<Integer>());\r\n    TestRunner.of(MyStreamTestTask.class).addInputStream(imid, inputList).addOutputStream(imod, 1).addExternalContext(new TestContext(10)).run(Duration.ofSeconds(1));\r\n    Assert.assertThat(TestRunner.consumeStream(imod, Duration.ofMillis(1000)).get(0), IsIterableContainingInOrder.contains(outputList.toArray()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSamzaJobFailureForSyncTask",
  "sourceCode" : "/**\r\n * Samza job logic expects integers, but doubles are passed here which results in failure\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testSamzaJobFailureForSyncTask() {\r\n    List<Double> inputList = Arrays.asList(1.2, 2.3, 3.33, 4.5);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<Double> imid = isd.getInputDescriptor(\"doubles\", new NoOpSerde<Double>());\r\n    InMemoryOutputDescriptor imod = isd.getOutputDescriptor(\"output\", new NoOpSerde<>());\r\n    TestRunner.of(MyStreamTestTask.class).addInputStream(imid, inputList).addOutputStream(imod, 1).addExternalContext(new TestContext(10)).run(Duration.ofSeconds(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSyncTaskWithSinglePartitionMultithreaded",
  "sourceCode" : "@Test\r\npublic void testSyncTaskWithSinglePartitionMultithreaded() throws Exception {\r\n    List<Integer> inputList = Arrays.asList(1, 2, 3, 4, 5);\r\n    List<Integer> outputList = Arrays.asList(10, 20, 30, 40, 50);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"input\", new NoOpSerde<Integer>());\r\n    InMemoryOutputDescriptor<Integer> imod = isd.getOutputDescriptor(\"output\", new NoOpSerde<Integer>());\r\n    TestRunner.of(MyStreamTestTask.class).addInputStream(imid, inputList).addOutputStream(imod, 1).addConfig(\"job.container.thread.pool.size\", \"4\").addExternalContext(new TestContext(10)).run(Duration.ofSeconds(1));\r\n    StreamAssert.containsInOrder(outputList, imod, Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSyncTaskWithMultiplePartition",
  "sourceCode" : "@Test\r\npublic void testSyncTaskWithMultiplePartition() throws Exception {\r\n    Map<Integer, List<KV>> inputPartitionData = new HashMap<>();\r\n    Map<Integer, List<Integer>> expectedOutputPartitionData = new HashMap<>();\r\n    genData(inputPartitionData, expectedOutputPartitionData);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<KV> imid = isd.getInputDescriptor(\"input\", new NoOpSerde<KV>());\r\n    InMemoryOutputDescriptor<Integer> imod = isd.getOutputDescriptor(\"output\", new NoOpSerde<Integer>());\r\n    TestRunner.of(MyStreamTestTask.class).addInputStream(imid, inputPartitionData).addOutputStream(imod, 5).addExternalContext(new TestContext(10)).run(Duration.ofSeconds(2));\r\n    StreamAssert.containsInOrder(expectedOutputPartitionData, imod, Duration.ofMillis(1000));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSyncTaskWithMultiplePartitionMultithreaded",
  "sourceCode" : "@Test\r\npublic void testSyncTaskWithMultiplePartitionMultithreaded() throws Exception {\r\n    Map<Integer, List<KV>> inputPartitionData = new HashMap<>();\r\n    Map<Integer, List<Integer>> expectedOutputPartitionData = new HashMap<>();\r\n    genData(inputPartitionData, expectedOutputPartitionData);\r\n    syncTaskWithMultiplePartitionMultithreadedHelper(inputPartitionData, expectedOutputPartitionData);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\StreamTaskIntegrationTest.java",
  "methodName" : "testSyncTaskWithMultiplePartitionMultithreadedWithCustomIME",
  "sourceCode" : "@Test\r\npublic void testSyncTaskWithMultiplePartitionMultithreadedWithCustomIME() throws Exception {\r\n    Map<Integer, List<KV>> inputPartitionData = new HashMap<>();\r\n    Map<Integer, List<KV>> inputPartitionIME = new HashMap<>();\r\n    Map<Integer, List<Integer>> expectedOutputPartitionData = new HashMap<>();\r\n    genData(inputPartitionData, expectedOutputPartitionData);\r\n    for (Map.Entry<Integer, List<KV>> entry : inputPartitionData.entrySet()) {\r\n        Integer partitionId = entry.getKey();\r\n        List<KV> messages = entry.getValue();\r\n        SystemStreamPartition ssp = new SystemStreamPartition(\"test\", \"input\", new Partition(partitionId));\r\n        inputPartitionIME.put(partitionId, new ArrayList<>());\r\n        int offset = 0;\r\n        for (KV message : messages) {\r\n            IncomingMessageEnvelope ime = new IncomingMessageEnvelope(ssp, String.valueOf(offset++), message.key, message.getValue());\r\n            inputPartitionIME.get(partitionId).add(KV.of(message.key, ime));\r\n        }\r\n    }\r\n    syncTaskWithMultiplePartitionMultithreadedHelper(inputPartitionIME, expectedOutputPartitionData);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\framework\\TestStreamApplicationIntegrationTestHarness.java",
  "methodName" : "testTheTestHarness",
  "sourceCode" : "@Test\r\npublic void testTheTestHarness() {\r\n    List<String> inputMessages = Arrays.asList(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\");\r\n    // create input topic and produce the first batch of input messages\r\n    boolean topicCreated = createTopic(INPUT_TOPIC, 1);\r\n    if (!topicCreated) {\r\n        fail(\"Could not create input topic.\");\r\n    }\r\n    inputMessages.forEach(m -> produceMessage(INPUT_TOPIC, 0, m, m));\r\n    // verify that the input messages were produced successfully\r\n    if (inputMessages.size() > 0) {\r\n        List<ConsumerRecord<String, String>> inputRecords = consumeMessages(INPUT_TOPIC, inputMessages.size());\r\n        List<String> readInputMessages = inputRecords.stream().map(ConsumerRecord::value).collect(Collectors.toList());\r\n        Assert.assertEquals(inputMessages, readInputMessages);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\functions\\TestSchedulerFunction.java",
  "methodName" : "testImmediateTimer",
  "sourceCode" : "@Test\r\npublic void testImmediateTimer() {\r\n    final InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    final InMemoryInputDescriptor<Integer> imid = isd.getInputDescriptor(\"test-input\", new IntegerSerde());\r\n    StreamApplication app = new StreamApplication() {\r\n\r\n        @Override\r\n        public void describe(StreamApplicationDescriptor appDescriptor) {\r\n            appDescriptor.getInputStream(imid).map(new TestFunction());\r\n        }\r\n    };\r\n    TestRunner.of(app).addInputStream(imid, Arrays.asList(1, 2, 3, 4, 5)).run(Duration.ofSeconds(1));\r\n    assertTrue(timerFired.get());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\kafka\\KafkaCheckpointManagerIntegrationTest.java",
  "methodName" : "testCheckpoint",
  "sourceCode" : "@Test\r\npublic void testCheckpoint() {\r\n    createTopic(INPUT_STREAM, 2);\r\n    produceMessages(0);\r\n    produceMessages(1);\r\n    // run application once and verify processed messages before shutdown\r\n    runApplication(new CheckpointApplication(true), \"CheckpointApplication\", CONFIGS).getRunner().waitForFinish();\r\n    verifyProcessedMessagesFirstRun();\r\n    // run application a second time and verify that certain messages had to be re-processed\r\n    runApplication(new CheckpointApplication(false), \"CheckpointApplication\", CONFIGS).getRunner().waitForFinish();\r\n    verifyProcessedMessagesSecondRun();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestAsyncFlatMap.java",
  "methodName" : "testProcessingFutureCompletesSuccessfully",
  "sourceCode" : "@Test\r\npublic void testProcessingFutureCompletesSuccessfully() {\r\n    List<PageView> expectedPageViews = PAGE_VIEWS.stream().filter(pageView -> !pageView.getPageId().equals(LOGIN_PAGE) && Long.parseLong(pageView.getUserId()) > 0).collect(Collectors.toList());\r\n    List<PageView> actualPageViews = runTest(new HashMap<>());\r\n    assertEquals(\"Mismatch between expected vs actual page views\", expectedPageViews, actualPageViews);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestAsyncFlatMap.java",
  "methodName" : "testProcessingFutureCompletesAfterTaskTimeout",
  "sourceCode" : "@Test\r\npublic void testProcessingFutureCompletesAfterTaskTimeout() {\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(TaskConfig.CALLBACK_TIMEOUT_MS, \"100\");\r\n    configs.put(PROCESS_JITTER, \"2000\");\r\n    try {\r\n        runTest(configs);\r\n        fail(\"App execution should have failed due to a task callback timeout\");\r\n    } catch (SamzaException e) {\r\n        /*\r\n       * TestRunner throws SamzaException on failures in general, so check the actual cause. The timeout message is\r\n       * nested within a bunch of other exceptions.\r\n       */\r\n        Throwable rootCause = findRootCause(e);\r\n        assertTrue(String.format(\"Got exception %s with message %s\", rootCause.getClass(), rootCause.getMessage()), rootCause instanceof SamzaException);\r\n        // the \"{}\" is intentional, since the exception message actually includes it (probably a logging bug)\r\n        assertEquals(\"Callback for task {} Partition 0 timed out after 100 ms.\", rootCause.getMessage());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestAsyncFlatMap.java",
  "methodName" : "testProcessingExceptionIsBubbledUp",
  "sourceCode" : "@Test\r\npublic void testProcessingExceptionIsBubbledUp() {\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(FAIL_PROCESS, \"true\");\r\n    try {\r\n        runTest(configs);\r\n        fail(\"App execution should have failed due to a ProcessFailureException\");\r\n    } catch (SamzaException e) {\r\n        /*\r\n       * TestRunner throws SamzaException on failures in general, so check the actual cause. The actual exception is\r\n       * nested within a bunch of other exceptions.\r\n       */\r\n        Throwable rootCause = findRootCause(e);\r\n        assertTrue(String.format(\"Got exception %s with message %s\", rootCause.getClass(), rootCause.getMessage()), rootCause instanceof ProcessFailureException);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestAsyncFlatMap.java",
  "methodName" : "testDownstreamOperatorExceptionIsBubbledUp",
  "sourceCode" : "@Test\r\npublic void testDownstreamOperatorExceptionIsBubbledUp() {\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(FAIL_DOWNSTREAM_OPERATOR, \"true\");\r\n    try {\r\n        runTest(configs);\r\n        fail(\"App execution should have failed due to a FilterFailureException\");\r\n    } catch (SamzaException e) {\r\n        /*\r\n       * TestRunner throws SamzaException on failures in general, so check the actual cause. The actual exception is\r\n       * nested within a bunch of other exceptions.\r\n       */\r\n        Throwable rootCause = findRootCause(e);\r\n        assertTrue(String.format(\"Got exception %s with message %s\", rootCause.getClass(), rootCause.getMessage()), rootCause instanceof FilterFailureException);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestRepartitionJoinWindowApp.java",
  "methodName" : "testRepartitionJoinWindowAppWithoutDeletionOnCommit",
  "sourceCode" : "@Test\r\npublic void testRepartitionJoinWindowAppWithoutDeletionOnCommit() throws Exception {\r\n    String inputTopicName1 = \"page-views\";\r\n    String inputTopicName2 = \"ad-clicks\";\r\n    String outputTopicName = \"user-ad-click-counts\";\r\n    KafkaSystemAdmin.deleteMessageCalled = false;\r\n    initializeTopics(inputTopicName1, inputTopicName2, outputTopicName);\r\n    // run the application\r\n    RepartitionJoinWindowApp app = new RepartitionJoinWindowApp();\r\n    String appName = \"UserPageAdClickCounter\";\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    configs.put(TaskConfig.GROUPER_FACTORY, \"org.apache.samza.container.grouper.task.GroupByContainerIdsFactory\");\r\n    configs.put(\"systems.kafka.samza.delete.committed.messages\", \"false\");\r\n    configs.put(RepartitionJoinWindowApp.INPUT_TOPIC_1_CONFIG_KEY, inputTopicName1);\r\n    configs.put(RepartitionJoinWindowApp.INPUT_TOPIC_2_CONFIG_KEY, inputTopicName2);\r\n    configs.put(RepartitionJoinWindowApp.OUTPUT_TOPIC_CONFIG_KEY, outputTopicName);\r\n    runApplication(app, appName, configs);\r\n    // consume and validate result\r\n    List<ConsumerRecord<String, String>> messages = consumeMessages(outputTopicName, 2);\r\n    assertEquals(2, messages.size());\r\n    Assert.assertFalse(KafkaSystemAdmin.deleteMessageCalled);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestRepartitionJoinWindowApp.java",
  "methodName" : "ittestRepartitionJoinWindowAppAndDeleteMessagesOnCommit",
  "sourceCode" : "@Test\r\npublic void ittestRepartitionJoinWindowAppAndDeleteMessagesOnCommit() throws Exception {\r\n    String inputTopicName1 = \"page-views2\";\r\n    String inputTopicName2 = \"ad-clicks2\";\r\n    String outputTopicName = \"user-ad-click-counts2\";\r\n    initializeTopics(inputTopicName1, inputTopicName2, outputTopicName);\r\n    // run the application\r\n    RepartitionJoinWindowApp app = new RepartitionJoinWindowApp();\r\n    final String appName = \"UserPageAdClickCounter2\";\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    configs.put(TaskConfig.GROUPER_FACTORY, \"org.apache.samza.container.grouper.task.GroupByContainerIdsFactory\");\r\n    configs.put(\"systems.kafka.samza.delete.committed.messages\", \"true\");\r\n    configs.put(RepartitionJoinWindowApp.INPUT_TOPIC_1_CONFIG_KEY, inputTopicName1);\r\n    configs.put(RepartitionJoinWindowApp.INPUT_TOPIC_2_CONFIG_KEY, inputTopicName2);\r\n    configs.put(RepartitionJoinWindowApp.OUTPUT_TOPIC_CONFIG_KEY, outputTopicName);\r\n    runApplication(app, appName, configs);\r\n    // consume and validate result\r\n    List<ConsumerRecord<String, String>> messages = consumeMessages(outputTopicName, 2);\r\n    assertEquals(2, messages.size());\r\n    for (ConsumerRecord<String, String> message : messages) {\r\n        String key = message.key();\r\n        String value = message.value();\r\n        Assert.assertTrue(key.equals(\"u1\") || key.equals(\"u2\"));\r\n        assertEquals(\"2\", value);\r\n    }\r\n    // Verify that messages in the intermediate stream will be deleted in 10 seconds\r\n    long startTimeMs = System.currentTimeMillis();\r\n    for (String streamId : app.getIntermediateStreamIds()) {\r\n        long remainingMessageNum = -1;\r\n        while (remainingMessageNum != 0 && System.currentTimeMillis() - startTimeMs < 10000) {\r\n            remainingMessageNum = 0;\r\n            SystemStreamMetadata metadatas = (SystemStreamMetadata) systemAdmin.getSystemStreamMetadata(new HashSet<>(Arrays.asList(streamId)), new ExponentialSleepStrategy.Mock(3)).get(streamId);\r\n            for (Map.Entry<Partition, SystemStreamPartitionMetadata> entry : metadatas.getSystemStreamPartitionMetadata().entrySet()) {\r\n                SystemStreamPartitionMetadata metadata = entry.getValue();\r\n                remainingMessageNum += Long.parseLong(metadata.getUpcomingOffset()) - Long.parseLong(metadata.getOldestOffset());\r\n            }\r\n        }\r\n        assertEquals(0, remainingMessageNum);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestRepartitionJoinWindowApp.java",
  "methodName" : "testBroadcastApp",
  "sourceCode" : "@Test\r\npublic void testBroadcastApp() {\r\n    String inputTopicName1 = \"page-views\";\r\n    String inputTopicName2 = \"ad-clicks\";\r\n    String outputTopicName = \"user-ad-click-counts\";\r\n    Map<String, String> configs = new HashMap<>();\r\n    configs.put(JobCoordinatorConfig.JOB_COORDINATOR_FACTORY, \"org.apache.samza.standalone.PassthroughJobCoordinatorFactory\");\r\n    configs.put(JobConfig.PROCESSOR_ID, \"0\");\r\n    configs.put(TaskConfig.GROUPER_FACTORY, \"org.apache.samza.container.grouper.task.GroupByContainerIdsFactory\");\r\n    configs.put(BroadcastAssertApp.INPUT_TOPIC_NAME_PROP, inputTopicName1);\r\n    initializeTopics(inputTopicName1, inputTopicName2, outputTopicName);\r\n    runApplication(new BroadcastAssertApp(), \"BroadcastTest\", configs);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\operator\\TestRepartitionWindowApp.java",
  "methodName" : "testRepartitionedSessionWindowCounter",
  "sourceCode" : "@Test\r\npublic void testRepartitionedSessionWindowCounter() throws Exception {\r\n    Map<Integer, List<KV<String, PageView>>> pageViews = new HashMap<>();\r\n    pageViews.put(0, ImmutableList.of(KV.of(\"userId1\", new PageView(\"india\", \"5.com\", \"userId1\")), KV.of(\"userId1\", new PageView(\"india\", \"2.com\", \"userId1\"))));\r\n    pageViews.put(1, ImmutableList.of(KV.of(\"userId2\", new PageView(\"china\", \"4.com\", \"userId2\")), KV.of(\"userId1\", new PageView(\"india\", \"3.com\", \"userId1\"))));\r\n    pageViews.put(2, ImmutableList.of(KV.of(\"userId1\", new PageView(\"india\", \"1.com\", \"userId1\"))));\r\n    InMemorySystemDescriptor sd = new InMemorySystemDescriptor(SYSTEM);\r\n    InMemoryInputDescriptor<KV<String, PageView>> inputDescriptor = sd.getInputDescriptor(INPUT_TOPIC, KVSerde.of(new NoOpSerde<>(), new NoOpSerde<>()));\r\n    /*\r\n     * Technically, this should have a message type of KV, because a KV is passed to sendTo, but\r\n     * StreamAssert.containsInAnyOrder requires the type to match the output type of the actual messages. In\r\n     * high-level, sendTo splits up the KV, so the actual messages are just the \"V\" part of the KV.\r\n     * TestRunner only uses NoOpSerde anyways, so it doesn't matter if the typing isn't KV.\r\n     */\r\n    InMemoryOutputDescriptor<String> outputDescriptor = sd.getOutputDescriptor(OUTPUT_TOPIC, new NoOpSerde<>());\r\n    TestRunner.of(new RepartitionWindowApp()).addInputStream(inputDescriptor, pageViews).addOutputStream(outputDescriptor, 1).addConfig(\"task.window.ms\", \"1000\").run(Duration.ofSeconds(10));\r\n    StreamAssert.containsInAnyOrder(Arrays.asList(\"userId1 4\", \"userId2 1\"), outputDescriptor, Duration.ofSeconds(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestStreamProcessor.java",
  "methodName" : "testStreamProcessor",
  "sourceCode" : "/**\r\n * Testing a basic identity stream task - reads data from a topic and writes it to another topic\r\n * (without any modifications)\r\n *\r\n * <p>\r\n * The standalone version in this test uses KafkaSystemFactory and it uses a SingleContainerGrouperFactory. Hence,\r\n * no matter how many tasks are present, it will always be run in a single processor instance. This simplifies testing\r\n */\r\n@Test\r\npublic void testStreamProcessor() {\r\n    final String testSystem = \"test-system\";\r\n    final String inputTopic = \"numbers\";\r\n    final String outputTopic = \"output\";\r\n    final int messageCount = 20;\r\n    final Config configs = new MapConfig(createConfigs(\"1\", testSystem, inputTopic, outputTopic, messageCount));\r\n    // Note: createTopics needs to be called before creating a StreamProcessor. Otherwise it fails with a\r\n    // TopicExistsException since StreamProcessor auto-creates them.\r\n    createTopics(inputTopic, outputTopic);\r\n    final TestStubs stubs = new TestStubs(configs, IdentityStreamTask::new, bootstrapServers());\r\n    produceMessages(stubs.producer, inputTopic, messageCount);\r\n    run(stubs.processor, stubs.shutdownLatch);\r\n    verifyNumMessages(stubs.consumer, outputTopic, messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestStreamProcessor.java",
  "methodName" : "testStreamProcessorWithStreamTaskFactory",
  "sourceCode" : "/**\r\n * Should be able to create task instances from the provided task factory.\r\n */\r\n@Test\r\npublic void testStreamProcessorWithStreamTaskFactory() {\r\n    final String testSystem = \"test-system\";\r\n    final String inputTopic = \"numbers2\";\r\n    final String outputTopic = \"output2\";\r\n    final int messageCount = 20;\r\n    final Config configs = new MapConfig(createConfigs(\"1\", testSystem, inputTopic, outputTopic, messageCount));\r\n    createTopics(inputTopic, outputTopic);\r\n    final TestStubs stubs = new TestStubs(configs, IdentityStreamTask::new, bootstrapServers());\r\n    produceMessages(stubs.producer, inputTopic, messageCount);\r\n    run(stubs.processor, stubs.shutdownLatch);\r\n    verifyNumMessages(stubs.consumer, outputTopic, messageCount);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestStreamProcessor.java",
  "methodName" : "testStreamProcessorWithAsyncStreamTaskFactory",
  "sourceCode" : "/**\r\n * Should be able to create task instances from the provided task factory.\r\n */\r\n@Test\r\npublic void testStreamProcessorWithAsyncStreamTaskFactory() {\r\n    final String testSystem = \"test-system\";\r\n    final String inputTopic = \"numbers3\";\r\n    final String outputTopic = \"output3\";\r\n    final int messageCount = 20;\r\n    final Config configs = new MapConfig(createConfigs(\"1\", testSystem, inputTopic, outputTopic, messageCount));\r\n    final ExecutorService executorService = Executors.newSingleThreadExecutor();\r\n    createTopics(inputTopic, outputTopic);\r\n    final AsyncStreamTaskFactory stf = () -> new AsyncStreamTaskAdapter(new IdentityStreamTask(), executorService);\r\n    final TestStubs stubs = new TestStubs(configs, stf, bootstrapServers());\r\n    produceMessages(stubs.producer, inputTopic, messageCount);\r\n    run(stubs.processor, stubs.shutdownLatch);\r\n    verifyNumMessages(stubs.consumer, outputTopic, messageCount);\r\n    executorService.shutdownNow();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestStreamProcessor.java",
  "methodName" : "testStreamProcessorWithNoTask",
  "sourceCode" : "/**\r\n * Should fail to create a SamzaContainer when neither task factory nor task.class are provided.\r\n */\r\n@Test(expected = SamzaException.class)\r\npublic void testStreamProcessorWithNoTask() {\r\n    final String testSystem = \"test-system\";\r\n    final String inputTopic = \"numbers4\";\r\n    final String outputTopic = \"output4\";\r\n    final int messageCount = 20;\r\n    final Map<String, String> configMap = createConfigs(PROCESSOR_ID, testSystem, inputTopic, outputTopic, messageCount);\r\n    configMap.remove(\"task.class\");\r\n    final Config configs = new MapConfig(configMap);\r\n    final TestStubs stubs = new TestStubs(configs, (StreamTaskFactory) null, bootstrapServers());\r\n    run(stubs.processor, stubs.shutdownLatch);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "shouldStopNewProcessorsJoiningGroupWhenNumContainersIsGreaterThanNumTasks",
  "sourceCode" : "/**\r\n * sspGrouper is set to GroupBySystemStreamPartitionFactory.\r\n * Run a stream application(appRunner1) consuming messages from input topic(effectively one container).\r\n *\r\n * In the callback triggered by appRunner1 after processing a message, bring up an another stream application(appRunner2).\r\n *\r\n * Assertions:\r\n *           A) JobModel generated before and after the addition of appRunner2 should be equal.\r\n *           B) Second stream application(appRunner2) should not join the group and process any message.\r\n */\r\n@Test\r\npublic void shouldStopNewProcessorsJoiningGroupWhenNumContainersIsGreaterThanNumTasks() throws InterruptedException {\r\n    // Set up kafka topics.\r\n    publishKafkaEvents(inputSinglePartitionKafkaTopic, 0, NUM_KAFKA_EVENTS * 2, PROCESSOR_IDS[0]);\r\n    // Configuration, verification variables\r\n    MapConfig testConfig = new MapConfig(ImmutableMap.of(JobConfig.SSP_GROUPER_FACTORY, \"org.apache.samza.container.grouper.stream.GroupBySystemStreamPartitionFactory\", JobConfig.JOB_DEBOUNCE_TIME_MS, \"10\", ClusterManagerConfig.JOB_HOST_AFFINITY_ENABLED, \"true\"));\r\n    // Declared as final array to update it from streamApplication callback(Variable should be declared final to access in lambda block).\r\n    final JobModel[] previousJobModel = new JobModel[1];\r\n    final String[] previousJobModelVersion = new String[1];\r\n    AtomicBoolean hasSecondProcessorJoined = new AtomicBoolean(false);\r\n    final CountDownLatch secondProcessorRegistered = new CountDownLatch(1);\r\n    zkUtils.subscribeToProcessorChange((parentPath, currentChilds) -> {\r\n        // When appRunner2 with id: PROCESSOR_IDS[1] is registered, run processing message in appRunner1.\r\n        if (currentChilds.contains(PROCESSOR_IDS[1])) {\r\n            secondProcessorRegistered.countDown();\r\n        }\r\n    });\r\n    // Set up stream app appRunner2.\r\n    CountDownLatch processedMessagesLatch = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    Config localTestConfig2 = new MapConfig(applicationConfig2, testConfig);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputSinglePartitionKafkaTopic), outputSinglePartitionKafkaTopic, processedMessagesLatch, null, null, localTestConfig2), localTestConfig2);\r\n    // Callback handler for appRunner1.\r\n    TestStreamApplication.StreamApplicationCallback callback = m -> {\r\n        if (hasSecondProcessorJoined.compareAndSet(false, true)) {\r\n            previousJobModelVersion[0] = zkUtils.getJobModelVersion();\r\n            previousJobModel[0] = JobModelUtil.readJobModel(previousJobModelVersion[0], zkMetadataStore);\r\n            executeRun(appRunner2, localTestConfig2);\r\n            try {\r\n                // Wait for appRunner2 to register with zookeeper.\r\n                secondProcessorRegistered.await();\r\n            } catch (InterruptedException e) {\r\n            }\r\n        }\r\n    };\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS * 2);\r\n    // Set up stream app appRunner1.\r\n    Config localTestConfig1 = new MapConfig(applicationConfig1, testConfig);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputSinglePartitionKafkaTopic), outputSinglePartitionKafkaTopic, null, callback, kafkaEventsConsumedLatch, localTestConfig1), localTestConfig1);\r\n    executeRun(appRunner1, localTestConfig1);\r\n    kafkaEventsConsumedLatch.await();\r\n    String currentJobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel updatedJobModel = JobModelUtil.readJobModel(currentJobModelVersion, zkMetadataStore);\r\n    // Job model before and after the addition of second stream processor should be the same.\r\n    assertEquals(previousJobModel[0], updatedJobModel);\r\n    assertEquals(new MapConfig(), updatedJobModel.getConfig());\r\n    assertEquals(NUM_KAFKA_EVENTS, processedMessagesLatch.getCount());\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    assertEquals(appRunner2.status(), ApplicationStatus.UnsuccessfulFinish);\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    assertEquals(appRunner1.status(), ApplicationStatus.SuccessfulFinish);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "shouldUpdateJobModelWhenNewProcessorJoiningGroupUsingAllSspToSingleTaskGrouperFactory",
  "sourceCode" : "/**\r\n * sspGrouper is set to AllSspToSingleTaskGrouperFactory (All ssps from input kafka topic are mapped to a single task per container).\r\n * AllSspToSingleTaskGrouperFactory should be used only with high-level consumers which do the partition management\r\n * by themselves. Using the factory with the consumers that do not do the partition management will result in\r\n * each processor/task consuming all the messages from all the partitions.\r\n * Run a stream application(streamApp1) consuming messages from input topic(effectively one container).\r\n *\r\n * In the callback triggered by streamApp1 after processing a message, bring up an another stream application(streamApp2).\r\n *\r\n * Assertions:\r\n *           A) JobModel generated before and after the addition of streamApp2 should not be equal.\r\n *           B) Second stream application(streamApp2) should join the group and process all the messages.\r\n */\r\n@Test\r\npublic void shouldUpdateJobModelWhenNewProcessorJoiningGroupUsingAllSspToSingleTaskGrouperFactory() throws InterruptedException {\r\n    // Set up kafka topics.\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS * 2, PROCESSOR_IDS[0]);\r\n    // Configuration, verification variables\r\n    MapConfig testConfig = new MapConfig(ImmutableMap.of(JobConfig.SSP_GROUPER_FACTORY, \"org.apache.samza.container.grouper.stream.AllSspToSingleTaskGrouperFactory\", JobConfig.JOB_DEBOUNCE_TIME_MS, \"10\", ClusterManagerConfig.HOST_AFFINITY_ENABLED, \"false\"));\r\n    // Declared as final array to update it from streamApplication callback(Variable should be declared final to access in lambda block).\r\n    final JobModel[] previousJobModel = new JobModel[1];\r\n    final String[] previousJobModelVersion = new String[1];\r\n    AtomicBoolean hasSecondProcessorJoined = new AtomicBoolean(false);\r\n    final CountDownLatch secondProcessorRegistered = new CountDownLatch(1);\r\n    zkUtils.subscribeToProcessorChange((parentPath, currentChilds) -> {\r\n        // When appRunner2 with id: PROCESSOR_IDS[1] is registered, start processing message in appRunner1.\r\n        if (currentChilds.contains(PROCESSOR_IDS[1])) {\r\n            secondProcessorRegistered.countDown();\r\n        }\r\n    });\r\n    // Set up appRunner2.\r\n    CountDownLatch processedMessagesLatch = new CountDownLatch(NUM_KAFKA_EVENTS * 2);\r\n    Config testAppConfig2 = new MapConfig(applicationConfig2, testConfig);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch, null, null, testAppConfig2), testAppConfig2);\r\n    // Callback handler for appRunner1.\r\n    TestStreamApplication.StreamApplicationCallback streamApplicationCallback = message -> {\r\n        if (hasSecondProcessorJoined.compareAndSet(false, true)) {\r\n            previousJobModelVersion[0] = zkUtils.getJobModelVersion();\r\n            previousJobModel[0] = JobModelUtil.readJobModel(previousJobModelVersion[0], zkMetadataStore);\r\n            executeRun(appRunner2, testAppConfig2);\r\n            try {\r\n                // Wait for appRunner2 to register with zookeeper.\r\n                secondProcessorRegistered.await();\r\n            } catch (InterruptedException e) {\r\n            }\r\n        }\r\n    };\r\n    // This is the latch for the messages received by appRunner1. Since appRunner1 is run first, it gets one event\r\n    // redelivered due to re-balancing done by Zk after the appRunner2 joins (See the callback above).\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS * 2 + 1);\r\n    // Set up stream app appRunner1.\r\n    Config testAppConfig1 = new MapConfig(applicationConfig1, testConfig);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, null, streamApplicationCallback, kafkaEventsConsumedLatch, testAppConfig1), testAppConfig1);\r\n    executeRun(appRunner1, testAppConfig1);\r\n    kafkaEventsConsumedLatch.await();\r\n    String currentJobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel updatedJobModel = JobModelUtil.readJobModel(currentJobModelVersion, zkMetadataStore);\r\n    // JobModelVersion check to verify that leader publishes new jobModel.\r\n    assertTrue(Integer.parseInt(previousJobModelVersion[0]) < Integer.parseInt(currentJobModelVersion));\r\n    // Job model before and after the addition of second stream processor should not be the same.\r\n    assertTrue(!previousJobModel[0].equals(updatedJobModel));\r\n    // Task names in the job model should be different but the set of partitions should be the same and each task name\r\n    // should be assigned to a different container.\r\n    assertEquals(new MapConfig(), previousJobModel[0].getConfig());\r\n    assertEquals(previousJobModel[0].getContainers().get(PROCESSOR_IDS[0]).getTasks().size(), 1);\r\n    assertEquals(new MapConfig(), updatedJobModel.getConfig());\r\n    assertEquals(updatedJobModel.getContainers().get(PROCESSOR_IDS[0]).getTasks().size(), 1);\r\n    assertEquals(updatedJobModel.getContainers().get(PROCESSOR_IDS[1]).getTasks().size(), 1);\r\n    Map<TaskName, TaskModel> updatedTaskModelMap1 = updatedJobModel.getContainers().get(PROCESSOR_IDS[0]).getTasks();\r\n    Map<TaskName, TaskModel> updatedTaskModelMap2 = updatedJobModel.getContainers().get(PROCESSOR_IDS[1]).getTasks();\r\n    assertEquals(updatedTaskModelMap1.size(), 1);\r\n    assertEquals(updatedTaskModelMap2.size(), 1);\r\n    TaskModel taskModel1 = updatedTaskModelMap1.values().stream().findFirst().get();\r\n    TaskModel taskModel2 = updatedTaskModelMap2.values().stream().findFirst().get();\r\n    assertEquals(taskModel1.getSystemStreamPartitions(), taskModel2.getSystemStreamPartitions());\r\n    assertFalse(taskModel1.getTaskName().getTaskName().equals(taskModel2.getTaskName().getTaskName()));\r\n    processedMessagesLatch.await();\r\n    assertEquals(ApplicationStatus.Running, appRunner2.status());\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner1.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "shouldReElectLeaderWhenLeaderDies",
  "sourceCode" : "@Test\r\npublic void shouldReElectLeaderWhenLeaderDies() throws InterruptedException {\r\n    // Set up kafka topics.\r\n    publishKafkaEvents(inputKafkaTopic, 0, 2 * NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create stream applications.\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(2 * NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch3 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, kafkaEventsConsumedLatch, applicationConfig2), applicationConfig2);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch3, null, kafkaEventsConsumedLatch, applicationConfig3), applicationConfig3);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    // Wait until all processors have processed a message.\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // Verifications before killing the leader.\r\n    String jobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    assertEquals(2, jobModel.getContainers().size());\r\n    assertEquals(Sets.newHashSet(\"0000000000\", \"0000000001\"), jobModel.getContainers().keySet());\r\n    assertEquals(\"1\", jobModelVersion);\r\n    List<String> processorIdsFromZK = zkUtils.getActiveProcessorsIDs(Arrays.asList(PROCESSOR_IDS));\r\n    assertEquals(2, processorIdsFromZK.size());\r\n    assertEquals(PROCESSOR_IDS[0], processorIdsFromZK.get(0));\r\n    // Kill the leader. Since appRunner1 is the first to join the cluster, it's the leader.\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner1.status());\r\n    kafkaEventsConsumedLatch.await();\r\n    publishKafkaEvents(inputKafkaTopic, 0, 2 * NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    executeRun(appRunner3, applicationConfig3);\r\n    processedMessagesLatch3.await();\r\n    // Verifications after killing the leader.\r\n    processorIdsFromZK = zkUtils.getActiveProcessorsIDs(ImmutableList.of(PROCESSOR_IDS[1], PROCESSOR_IDS[2]));\r\n    assertEquals(2, processorIdsFromZK.size());\r\n    assertEquals(PROCESSOR_IDS[1], processorIdsFromZK.get(0));\r\n    jobModelVersion = zkUtils.getJobModelVersion();\r\n    jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    assertEquals(Sets.newHashSet(\"0000000001\", \"0000000002\"), jobModel.getContainers().keySet());\r\n    assertEquals(2, jobModel.getContainers().size());\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner2.status());\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner3.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "shouldFailWhenNewProcessorJoinsWithSameIdAsExistingProcessor",
  "sourceCode" : "@Test\r\npublic void shouldFailWhenNewProcessorJoinsWithSameIdAsExistingProcessor() throws InterruptedException {\r\n    // Set up kafka topics.\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create StreamApplications.\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, kafkaEventsConsumedLatch, applicationConfig2), applicationConfig2);\r\n    // Run stream applications.\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    // Wait for message processing to run in both the processors.\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // Create a stream app with same processor id as SP2 and run it. It should fail.\r\n    publishKafkaEvents(inputKafkaTopic, NUM_KAFKA_EVENTS, 2 * NUM_KAFKA_EVENTS, PROCESSOR_IDS[2]);\r\n    kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, null, null, kafkaEventsConsumedLatch, applicationConfig2), applicationConfig2);\r\n    // Fail when the duplicate processor joins.\r\n    expectedException.expect(SamzaException.class);\r\n    try {\r\n        executeRun(appRunner3, applicationConfig2);\r\n    } finally {\r\n        appRunner1.kill();\r\n        appRunner2.kill();\r\n        appRunner1.waitForFinish();\r\n        appRunner2.waitForFinish();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testRollingUpgradeOfStreamApplicationsShouldGenerateSameJobModel",
  "sourceCode" : "@Test\r\npublic void testRollingUpgradeOfStreamApplicationsShouldGenerateSameJobModel() throws Exception {\r\n    // Set up kafka topics.\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, false, Optional.empty());\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    Config applicationConfig1 = new MapConfig(configMap);\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    Config applicationConfig2 = new MapConfig(configMap);\r\n    List<TestKafkaEvent> messagesProcessed = new ArrayList<>();\r\n    TestStreamApplication.StreamApplicationCallback streamApplicationCallback = messagesProcessed::add;\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, kafkaEventsConsumedLatch, applicationConfig2), applicationConfig2);\r\n    // Run stream application.\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // Read job model before rolling upgrade.\r\n    String jobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    int lastProcessedMessageId = -1;\r\n    for (TestKafkaEvent message : messagesProcessed) {\r\n        lastProcessedMessageId = Math.max(lastProcessedMessageId, Integer.parseInt(message.getEventData()));\r\n    }\r\n    messagesProcessed.clear();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner1.status());\r\n    processedMessagesLatch1 = new CountDownLatch(1);\r\n    publishKafkaEvents(inputKafkaTopic, NUM_KAFKA_EVENTS, 2 * NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch, applicationConfig1), applicationConfig1);\r\n    executeRun(appRunner3, applicationConfig1);\r\n    processedMessagesLatch1.await();\r\n    // Read new job model after rolling upgrade.\r\n    String newJobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel newJobModel = JobModelUtil.readJobModel(newJobModelVersion, zkMetadataStore);\r\n    assertEquals(Integer.parseInt(jobModelVersion) + 1, Integer.parseInt(newJobModelVersion));\r\n    assertEquals(jobModel.getContainers(), newJobModel.getContainers());\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner2.status());\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner3.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testShouldStopStreamApplicationWhenShutdownTimeOutIsLessThanContainerShutdownTime",
  "sourceCode" : "@Test\r\npublic void testShouldStopStreamApplicationWhenShutdownTimeOutIsLessThanContainerShutdownTime() throws Exception {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, false, Optional.empty());\r\n    configMap.put(TaskConfig.TASK_SHUTDOWN_MS, \"0\");\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    Config applicationConfig1 = new MapConfig(configMap);\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    Config applicationConfig2 = new MapConfig(configMap);\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch kafkaEventsConsumedLatch = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, kafkaEventsConsumedLatch, applicationConfig2), applicationConfig2);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    kafkaEventsConsumedLatch.await();\r\n    // At this stage, both the processors are running and have drained the kakfa source.\r\n    // Trigger re-balancing phase, by manually adding a new processor.\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[2]);\r\n    // Reset the task shutdown ms for 3rd application to give it ample time to shutdown cleanly\r\n    configMap.put(TaskConfig.TASK_SHUTDOWN_MS, TASK_SHUTDOWN_MS);\r\n    Config applicationConfig3 = new MapConfig(configMap);\r\n    CountDownLatch processedMessagesLatch3 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch3, null, kafkaEventsConsumedLatch, applicationConfig3), applicationConfig3);\r\n    executeRun(appRunner3, applicationConfig3);\r\n    publishKafkaEvents(inputKafkaTopic, NUM_KAFKA_EVENTS, 2 * NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    processedMessagesLatch3.await();\r\n    appRunner1.waitForFinish();\r\n    appRunner2.waitForFinish();\r\n    /**\r\n     * If the processing has started in the third stream processor, then other two stream processors should be stopped.\r\n     */\r\n    assertEquals(ApplicationStatus.UnsuccessfulFinish, appRunner1.status());\r\n    assertEquals(ApplicationStatus.UnsuccessfulFinish, appRunner2.status());\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n    assertEquals(ApplicationStatus.SuccessfulFinish, appRunner3.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testShouldGenerateJobModelOnPartitionCountChange",
  "sourceCode" : "/**\r\n * A. Create a kafka topic with partition count set to 5.\r\n * B. Create and launch a samza application which consumes events from the kafka topic.\r\n * C. Validate that the {@link JobModel} contains 5 {@link SystemStreamPartition}'s.\r\n * D. Increase the partition count of the input kafka topic to 100.\r\n * E. Validate that the new {@link JobModel} contains 100 {@link SystemStreamPartition}'s.\r\n */\r\n@Test\r\npublic void testShouldGenerateJobModelOnPartitionCountChange() throws Exception {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch kafkaEventsConsumedLatch1 = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, false, Optional.of(\"test-store-partition-count-change\"));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    ApplicationConfig applicationConfig = new ApplicationConfig(new MapConfig(configMap));\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch1, applicationConfig), applicationConfig);\r\n    executeRun(appRunner1, applicationConfig);\r\n    processedMessagesLatch1.await();\r\n    String jobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    List<SystemStreamPartition> ssps = getSystemStreamPartitions(jobModel);\r\n    // Validate that the input partition count is 5 in the JobModel.\r\n    Assert.assertEquals(5, ssps.size());\r\n    // Increase the partition count of input kafka topic to 100.\r\n    increasePartitionsTo(inputKafkaTopic, 100);\r\n    long jobModelWaitTimeInMillis = 10;\r\n    while (Objects.equals(zkUtils.getJobModelVersion(), jobModelVersion)) {\r\n        LOGGER.info(\"Waiting for new jobModel to be published\");\r\n        Thread.sleep(jobModelWaitTimeInMillis);\r\n        jobModelWaitTimeInMillis = jobModelWaitTimeInMillis * 2;\r\n    }\r\n    String newJobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel newJobModel = JobModelUtil.readJobModel(newJobModelVersion, zkMetadataStore);\r\n    ssps = getSystemStreamPartitions(newJobModel);\r\n    // Validate that the input partition count is 100 in the new JobModel.\r\n    Assert.assertEquals(100, ssps.size());\r\n    // Validate that configuration is stored in coordinator stream.\r\n    MapConfig config = getConfigFromCoordinatorStream(applicationConfig);\r\n    // Execution plan and serialized DAG of a samza job is stored in the config of coordinator stream. Thus, direct equals comparison between\r\n    // the application configuration and the coordinator config will fail. Iterating through the entire configuration bag and verify that expected\r\n    // configuration is present in the coordinator configuration.\r\n    for (Map.Entry<String, String> entry : applicationConfig.entrySet()) {\r\n        Assert.assertTrue(config.containsKey(entry.getKey()));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testStatefulSamzaApplicationShouldRedistributeInputPartitionsToCorrectTasksWhenAInputStreamIsExpanded",
  "sourceCode" : "/**\r\n * A. Create a input kafka topic with partition count set to 32.\r\n * B. Create and launch a stateful samza application which consumes events from the input kafka topic.\r\n * C. Validate that the {@link JobModel} contains 32 {@link SystemStreamPartition}'s.\r\n * D. Increase the partition count of the input kafka topic to 64.\r\n * E. Validate that the new {@link JobModel} contains 64 {@link SystemStreamPartition}'s and the input\r\n * SystemStreamPartitions are mapped to the correct task.\r\n */\r\n@Test\r\npublic void testStatefulSamzaApplicationShouldRedistributeInputPartitionsToCorrectTasksWhenAInputStreamIsExpanded() throws Exception {\r\n    // Setup input topics.\r\n    String statefulInputKafkaTopic = String.format(\"test-input-topic-%s\", UUID.randomUUID().toString());\r\n    createTopic(statefulInputKafkaTopic, 32, 1);\r\n    // Generate configuration for the test.\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, false, Optional.of(\"test-store-redistribute-partitions\"));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    Config applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    publishKafkaEvents(statefulInputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch kafkaEventsConsumedLatch1 = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(statefulInputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch1, applicationConfig1), applicationConfig1);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    processedMessagesLatch1.await();\r\n    // Generate the correct task assignments before the input stream expansion.\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedTaskAssignments = new HashMap<>();\r\n    for (int partition = 0; partition < 32; ++partition) {\r\n        TaskName taskName = new TaskName(String.format(\"Partition %d\", partition));\r\n        SystemStreamPartition systemStreamPartition = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic, new Partition(partition));\r\n        expectedTaskAssignments.put(taskName, ImmutableSet.of(systemStreamPartition));\r\n    }\r\n    // Read the latest JobModel for validation.\r\n    String jobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    List<SystemStreamPartition> ssps = getSystemStreamPartitions(jobModel);\r\n    // Validate that the input partition count is 32 in the JobModel.\r\n    Assert.assertEquals(32, ssps.size());\r\n    // Validate that the new JobModel has the expected task assignments before the input stream expansion.\r\n    Map<TaskName, Set<SystemStreamPartition>> actualTaskAssignments = getTaskAssignments(jobModel);\r\n    Assert.assertEquals(expectedTaskAssignments, actualTaskAssignments);\r\n    // Increase the partition count of the input kafka topic to 64.\r\n    increasePartitionsTo(statefulInputKafkaTopic, 64);\r\n    // Wait for the JobModel version to change due to the increase in the input partition count.\r\n    long jobModelWaitTimeInMillis = 10;\r\n    while (true) {\r\n        LOGGER.info(\"Waiting for new jobModel to be published\");\r\n        jobModelVersion = zkUtils.getJobModelVersion();\r\n        jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n        ssps = getSystemStreamPartitions(jobModel);\r\n        if (ssps.size() == 64) {\r\n            break;\r\n        }\r\n        Thread.sleep(jobModelWaitTimeInMillis);\r\n    }\r\n    // Validate that the input partition count is 64 in the new JobModel.\r\n    Assert.assertEquals(64, ssps.size());\r\n    // Generate the correct task assignments after the input stream expansion.\r\n    expectedTaskAssignments = new HashMap<>();\r\n    for (int partition = 0; partition < 32; ++partition) {\r\n        TaskName taskName = new TaskName(String.format(\"Partition %d\", partition));\r\n        SystemStreamPartition ssp = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic, new Partition(partition));\r\n        SystemStreamPartition expandedSSP = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic, new Partition(partition + 32));\r\n        expectedTaskAssignments.put(taskName, ImmutableSet.of(ssp, expandedSSP));\r\n    }\r\n    // Validate that the new JobModel has the expected task assignments.\r\n    actualTaskAssignments = getTaskAssignments(jobModel);\r\n    for (Map.Entry<TaskName, Set<SystemStreamPartition>> entry : expectedTaskAssignments.entrySet()) {\r\n        TaskName taskName = entry.getKey();\r\n        Set<SystemStreamPartition> expectedSSPs = entry.getValue();\r\n        Assert.assertTrue(actualTaskAssignments.containsKey(taskName));\r\n        Set<SystemStreamPartition> actualSSPs = actualTaskAssignments.get(taskName);\r\n        Assert.assertEquals(expectedSSPs, actualSSPs);\r\n    }\r\n    Assert.assertEquals(expectedTaskAssignments, actualTaskAssignments);\r\n    Assert.assertEquals(32, jobModel.getMaxChangeLogStreamPartitions());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testStatefulSamzaApplicationShouldRedistributeInputPartitionsToCorrectTasksWhenMultipleInputStreamsAreExpanded",
  "sourceCode" : "/**\r\n * A. Create a input kafka topic: T1 with partition count set to 32.\r\n * B. Create a input kafka topic: T2 with partition count set to 32.\r\n * C. Create and launch a stateful samza application which consumes events from the kafka topics: T1 and T2.\r\n * D. Validate that the {@link JobModel} contains 32 {@link SystemStreamPartition}'s of T1 and 32 {@link SystemStreamPartition}'s of T2.\r\n * E. Increase the partition count of the input kafka topic: T1 to 64.\r\n * F. Increase the partition count of the input kafka topic: T2 to 64.\r\n * G. Validate that the new {@link JobModel} contains 64 {@link SystemStreamPartition}'s of T1, T2 and the input\r\n * SystemStreamPartitions are mapped to the correct task.\r\n */\r\n@Test\r\npublic void testStatefulSamzaApplicationShouldRedistributeInputPartitionsToCorrectTasksWhenMultipleInputStreamsAreExpanded() throws Exception {\r\n    // Setup the two input kafka topics.\r\n    String statefulInputKafkaTopic1 = String.format(\"test-input-topic-%s\", UUID.randomUUID().toString());\r\n    String statefulInputKafkaTopic2 = String.format(\"test-input-topic-%s\", UUID.randomUUID().toString());\r\n    createTopic(statefulInputKafkaTopic1, 32, 1);\r\n    createTopic(statefulInputKafkaTopic2, 32, 1);\r\n    // Generate configuration for the test.\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, false, Optional.of(\"test-store-redistribute-partitions\"));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    Config applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    // Publish events into the input kafka topics.\r\n    publishKafkaEvents(statefulInputKafkaTopic1, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    publishKafkaEvents(statefulInputKafkaTopic2, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create and launch the StreamApplication from configuration.\r\n    CountDownLatch kafkaEventsConsumedLatch1 = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(statefulInputKafkaTopic1, statefulInputKafkaTopic2), outputKafkaTopic, processedMessagesLatch1, null, kafkaEventsConsumedLatch1, applicationConfig1), applicationConfig1);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    processedMessagesLatch1.await();\r\n    kafkaEventsConsumedLatch1.await();\r\n    // Generate the correct task assignments before the input stream expansion.\r\n    Map<TaskName, Set<SystemStreamPartition>> expectedTaskAssignments = new HashMap<>();\r\n    for (int partition = 0; partition < 32; ++partition) {\r\n        TaskName taskName = new TaskName(String.format(\"Partition %d\", partition));\r\n        SystemStreamPartition systemStreamPartition1 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic1, new Partition(partition));\r\n        SystemStreamPartition systemStreamPartition2 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic2, new Partition(partition));\r\n        expectedTaskAssignments.put(taskName, ImmutableSet.of(systemStreamPartition1, systemStreamPartition2));\r\n    }\r\n    // Read the latest JobModel for validation.\r\n    String jobModelVersion = zkUtils.getJobModelVersion();\r\n    JobModel jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n    List<SystemStreamPartition> ssps = getSystemStreamPartitions(jobModel);\r\n    // Validate that the input 64 partitions are present in JobModel.\r\n    Assert.assertEquals(64, ssps.size());\r\n    // Validate that the new JobModel has the expected task assignments before the input stream expansion.\r\n    Map<TaskName, Set<SystemStreamPartition>> actualTaskAssignments = getTaskAssignments(jobModel);\r\n    Assert.assertEquals(expectedTaskAssignments, actualTaskAssignments);\r\n    // Increase the partition count of the input kafka topic1 to 64.\r\n    increasePartitionsTo(statefulInputKafkaTopic1, 64);\r\n    // Increase the partition count of the input kafka topic2 to 64.\r\n    increasePartitionsTo(statefulInputKafkaTopic2, 64);\r\n    // Wait for the JobModel version to change due to the increase in the input partition count.\r\n    long jobModelWaitTimeInMillis = 10;\r\n    while (true) {\r\n        LOGGER.info(\"Waiting for new jobModel to be published\");\r\n        jobModelVersion = zkUtils.getJobModelVersion();\r\n        jobModel = JobModelUtil.readJobModel(jobModelVersion, zkMetadataStore);\r\n        ssps = getSystemStreamPartitions(jobModel);\r\n        if (ssps.size() == 128) {\r\n            break;\r\n        }\r\n        Thread.sleep(jobModelWaitTimeInMillis);\r\n    }\r\n    // Validate that the input partition count is 128 in the new JobModel.\r\n    Assert.assertEquals(128, ssps.size());\r\n    // Generate the correct task assignments after the input stream expansion.\r\n    expectedTaskAssignments = new HashMap<>();\r\n    for (int partition = 0; partition < 32; ++partition) {\r\n        TaskName taskName = new TaskName(String.format(\"Partition %d\", partition));\r\n        SystemStreamPartition ssp1 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic1, new Partition(partition));\r\n        SystemStreamPartition expandedSSP1 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic1, new Partition(partition + 32));\r\n        SystemStreamPartition ssp2 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic2, new Partition(partition));\r\n        SystemStreamPartition expandedSSP2 = new SystemStreamPartition(TEST_SYSTEM, statefulInputKafkaTopic2, new Partition(partition + 32));\r\n        expectedTaskAssignments.put(taskName, ImmutableSet.of(ssp1, expandedSSP1, ssp2, expandedSSP2));\r\n    }\r\n    // Validate that the new JobModel has the expected task assignments.\r\n    actualTaskAssignments = getTaskAssignments(jobModel);\r\n    Assert.assertEquals(expectedTaskAssignments, actualTaskAssignments);\r\n    Assert.assertEquals(32, jobModel.getMaxChangeLogStreamPartitions());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testApplicationShutdownShouldBeIndependentOfPerMessageProcessingTime",
  "sourceCode" : "@Test\r\npublic void testApplicationShutdownShouldBeIndependentOfPerMessageProcessingTime() throws Exception {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    // Create a TaskApplication with only one task per container.\r\n    // The task does not invokes taskCallback.complete for any of the dispatched message.\r\n    CountDownLatch shutdownLatch = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    TaskApplication taskApplication = new TestTaskApplication(TEST_SYSTEM, inputKafkaTopic, outputKafkaTopic, processedMessagesLatch1, shutdownLatch);\r\n    MapConfig taskApplicationConfig = new MapConfig(ImmutableList.of(applicationConfig1, ImmutableMap.of(TaskConfig.MAX_CONCURRENCY, \"1\", JobConfig.SSP_GROUPER_FACTORY, \"org.apache.samza.container.grouper.stream.AllSspToSingleTaskGrouperFactory\")));\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(taskApplication, taskApplicationConfig);\r\n    // Run the application.\r\n    executeRun(appRunner, applicationConfig1);\r\n    // Wait for the task to receive at least one dispatched message.\r\n    processedMessagesLatch1.await();\r\n    // Kill the application when none of the dispatched messages is acknowledged as completed by the task.\r\n    appRunner.kill();\r\n    appRunner.waitForFinish();\r\n    // Expect the shutdown latch to be triggered.\r\n    shutdownLatch.await();\r\n    // Assert that the shutdown was successful.\r\n    Assert.assertEquals(ApplicationStatus.SuccessfulFinish, appRunner.status());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testAgreeingOnSameRunIdForBatch",
  "sourceCode" : "/**\r\n * Test if two processors coming up at the same time agree on a single runid\r\n * 1. bring up two processors\r\n * 2. wait till they start consuimg messages\r\n * 3. check if first processor run.id matches that of second processor\r\n */\r\n@Test\r\npublic void testAgreeingOnSameRunIdForBatch() throws InterruptedException {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, true, Optional.empty());\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    applicationConfig2 = new ApplicationConfig(new MapConfig(configMap));\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, null, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, null, applicationConfig2), applicationConfig2);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // At this stage, both the processors are running.\r\n    // check if their runId matches\r\n    LocalApplicationRunner localApplicationRunner1 = (LocalApplicationRunner) appRunner1;\r\n    LocalApplicationRunner localApplicationRunner2 = (LocalApplicationRunner) appRunner2;\r\n    assertEquals(\"RunId of the two processors does not match\", localApplicationRunner2.getRunId(), localApplicationRunner1.getRunId());\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testNewProcessorGetsSameRunIdForBatch",
  "sourceCode" : "/**\r\n * Test if a new processors joining an existing qurorum get the same runid\r\n * 1. bring up two processors\r\n * 2. wait till they start consuming messages\r\n * 3. bring up a third processor\r\n * 4. wait till third processor starts consuming messsages\r\n * 5. check if third processor run.id matches that of first twp\r\n */\r\n@Test\r\npublic void testNewProcessorGetsSameRunIdForBatch() throws InterruptedException {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, true, Optional.empty());\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    applicationConfig2 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[2]);\r\n    applicationConfig3 = new ApplicationConfig(new MapConfig(configMap));\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, null, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, null, applicationConfig2), applicationConfig2);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // At this stage, both the processors are running.\r\n    // check if their runId matches\r\n    LocalApplicationRunner localApplicationRunner1 = (LocalApplicationRunner) appRunner1;\r\n    LocalApplicationRunner localApplicationRunner2 = (LocalApplicationRunner) appRunner2;\r\n    assertEquals(\"RunId of the two processors does not match\", localApplicationRunner2.getRunId(), localApplicationRunner1.getRunId());\r\n    //Bring up a new processsor\r\n    CountDownLatch processedMessagesLatch3 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch3, null, null, applicationConfig3), applicationConfig3);\r\n    executeRun(appRunner3, applicationConfig3);\r\n    processedMessagesLatch3.await();\r\n    // At this stage, the new processor is running.\r\n    // check if new processor's runId matches that of the older processors\r\n    LocalApplicationRunner localApplicationRunner3 = (LocalApplicationRunner) appRunner3;\r\n    assertEquals(\"RunId of the new processor does not match that of old processor\", localApplicationRunner3.getRunId(), localApplicationRunner1.getRunId());\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testAllProcesssorDieNewProcessorGetsNewRunIdForBatch",
  "sourceCode" : "/**\r\n * Test one group of processors dying and a new processor coming up generates new run.id\r\n * 1. bring up two processors\r\n * 2. wait till they start consuimg messages\r\n * 3. kill and shutdown neatly both the processors\r\n * 4. bring up a new processor\r\n * 5. wait till new processor starts consuming messages\r\n * 6. check if new processor has new runid different from shutdown processors\r\n */\r\n@Test\r\npublic void testAllProcesssorDieNewProcessorGetsNewRunIdForBatch() throws InterruptedException {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, true, Optional.empty());\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    applicationConfig2 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[2]);\r\n    applicationConfig3 = new ApplicationConfig(new MapConfig(configMap));\r\n    // Create StreamApplication from configuration.\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, null, applicationConfig1), applicationConfig1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, null, applicationConfig2), applicationConfig2);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    processedMessagesLatch1.await();\r\n    processedMessagesLatch2.await();\r\n    // At this stage, both the processors are running.\r\n    // check if their runId matches\r\n    LocalApplicationRunner localApplicationRunner1 = (LocalApplicationRunner) appRunner1;\r\n    LocalApplicationRunner localApplicationRunner2 = (LocalApplicationRunner) appRunner2;\r\n    assertEquals(\"RunId of the two processors does not match\", localApplicationRunner2.getRunId(), localApplicationRunner1.getRunId());\r\n    String oldRunId = localApplicationRunner1.getRunId().get();\r\n    // shut down both the processors\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    //Bring up a new processsor\r\n    CountDownLatch processedMessagesLatch3 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch3, null, null, applicationConfig3), applicationConfig3);\r\n    executeRun(appRunner3, applicationConfig3);\r\n    processedMessagesLatch3.await();\r\n    // At this stage, the new processor is running.\r\n    // check if new processor's runId matches that of the older processors\r\n    LocalApplicationRunner localApplicationRunner3 = (LocalApplicationRunner) appRunner3;\r\n    assertNotEquals(\"RunId of the new processor same as that of old stopped processors\", oldRunId, localApplicationRunner3.getRunId());\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\processor\\TestZkLocalApplicationRunner.java",
  "methodName" : "testFirstProcessorDiesButSameRunIdForBatch",
  "sourceCode" : "/**\r\n * Test if first processor dying changes the runid for new processors joining\r\n * 1. bring up two processors\r\n * 2. wait till they start consuimg messages\r\n * 3. kill and shutdown first processor\r\n * 4. bring up a new processor\r\n * 5. wait till new processor starts consuming messages\r\n * 6. check if new processor gets same run.id\r\n */\r\n@Test\r\npublic void testFirstProcessorDiesButSameRunIdForBatch() throws InterruptedException {\r\n    publishKafkaEvents(inputKafkaTopic, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0]);\r\n    Map<String, String> configMap = buildStreamApplicationConfigMap(testStreamAppName, testStreamAppId, true, Optional.empty());\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[0]);\r\n    applicationConfig1 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[1]);\r\n    applicationConfig2 = new ApplicationConfig(new MapConfig(configMap));\r\n    configMap.put(JobConfig.PROCESSOR_ID, PROCESSOR_IDS[2]);\r\n    applicationConfig3 = new ApplicationConfig(new MapConfig(configMap));\r\n    CountDownLatch processedMessagesLatch1 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner1 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch1, null, null, applicationConfig1), applicationConfig1);\r\n    executeRun(appRunner1, applicationConfig1);\r\n    // firt processor is up and running\r\n    processedMessagesLatch1.await();\r\n    LocalApplicationRunner localApplicationRunner1 = (LocalApplicationRunner) appRunner1;\r\n    // bring up second processor\r\n    CountDownLatch processedMessagesLatch2 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner2 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch2, null, null, applicationConfig2), applicationConfig2);\r\n    executeRun(appRunner2, applicationConfig2);\r\n    // second processor is up and running\r\n    processedMessagesLatch2.await();\r\n    LocalApplicationRunner localApplicationRunner2 = (LocalApplicationRunner) appRunner2;\r\n    assertEquals(\"RunId of the two processors does not match\", localApplicationRunner2.getRunId(), localApplicationRunner1.getRunId());\r\n    // shut down first processor\r\n    appRunner1.kill();\r\n    appRunner1.waitForFinish();\r\n    //Bring up a new processsor\r\n    CountDownLatch processedMessagesLatch3 = new CountDownLatch(1);\r\n    ApplicationRunner appRunner3 = ApplicationRunners.getApplicationRunner(TestStreamApplication.getInstance(TEST_SYSTEM, ImmutableList.of(inputKafkaTopic), outputKafkaTopic, processedMessagesLatch3, null, null, applicationConfig3), applicationConfig3);\r\n    executeRun(appRunner3, applicationConfig3);\r\n    processedMessagesLatch3.await();\r\n    // At this stage, the new processor is running.\r\n    // check if new processor runid matches the old ones\r\n    LocalApplicationRunner localApplicationRunner3 = (LocalApplicationRunner) appRunner3;\r\n    assertEquals(\"RunId of the new processor is not the same as that of earlier processors\", localApplicationRunner2.getRunId(), localApplicationRunner3.getRunId());\r\n    appRunner2.kill();\r\n    appRunner2.waitForFinish();\r\n    appRunner3.kill();\r\n    appRunner3.waitForFinish();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEnd",
  "sourceCode" : "@Test\r\npublic void testEndToEnd() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithSystemMessages",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithSystemMessages() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String avroSamzaToRelMsgConverterDomain = String.format(SamzaSqlApplicationConfig.CFG_FMT_SAMZA_REL_CONVERTER_DOMAIN, \"avro\");\r\n    staticConfigs.put(avroSamzaToRelMsgConverterDomain + SamzaSqlApplicationConfig.CFG_FACTORY, SampleRelConverterFactory.class.getName());\r\n    String sql = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndDisableSystemMessages",
  "sourceCode" : "@Ignore\r\n@Test\r\npublic void testEndToEndDisableSystemMessages() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String avroSamzaToRelMsgConverterDomain = String.format(SamzaSqlApplicationConfig.CFG_FMT_SAMZA_REL_CONVERTER_DOMAIN, \"avro\");\r\n    staticConfigs.put(avroSamzaToRelMsgConverterDomain + SamzaSqlApplicationConfig.CFG_FACTORY, SampleRelConverterFactory.class.getName());\r\n    String sql = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_PROCESS_SYSTEM_EVENTS, \"false\");\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals((numMessages + 1) / 2, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithNullRecords",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithNullRecords() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(Collections.emptyMap(), numMessages, false, true);\r\n    String sql = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> x.getMessage() == null || ((GenericRecord) x.getMessage()).get(\"id\") == null ? null : Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).filter(Objects::nonNull).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages - ((numMessages - 1) / TestAvroSystemFactory.NULL_RECORD_FREQUENCY + 1), outMessages.size());\r\n    Assert.assertEquals(IntStream.range(0, numMessages).boxed().filter(x -> x % TestAvroSystemFactory.NULL_RECORD_FREQUENCY != 0).collect(Collectors.toList()), outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithDifferentSystemSameStream",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithDifferentSystemSameStream() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro2.SIMPLE1 select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndMultiSqlStmts",
  "sourceCode" : "@Test\r\npublic void testEndToEndMultiSqlStmts() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    String sql2 = \"Insert into testavro.SIMPLE3 select * from testavro.SIMPLE2\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages * 2, outMessages.size());\r\n    Set<Integer> outMessagesSet = new HashSet<>(outMessages);\r\n    Assert.assertEquals(numMessages, outMessagesSet.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(new ArrayList<>(outMessagesSet)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndMultiSqlStmtsWithSameSystemStreamAsInputAndOutput",
  "sourceCode" : "@Test\r\npublic void testEndToEndMultiSqlStmtsWithSameSystemStreamAsInputAndOutput() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.SIMPLE1 select * from testavro.SIMPLE2\";\r\n    String sql2 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages * 2, outMessages.size());\r\n    Set<Integer> outMessagesSet = new HashSet<>(outMessages);\r\n    Assert.assertEquals(numMessages, outMessagesSet.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(new ArrayList<>(outMessagesSet)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndFanIn",
  "sourceCode" : "@Test\r\npublic void testEndToEndFanIn() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE2\";\r\n    String sql2 = \"Insert into testavro.simpleOutputTopic select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages * 2, outMessages.size());\r\n    Set<Integer> outMessagesSet = new HashSet<>(outMessages);\r\n    Assert.assertEquals(numMessages, outMessagesSet.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(new ArrayList<>(outMessagesSet)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndFanOut",
  "sourceCode" : "@Ignore\r\n@Test\r\npublic void testEndToEndFanOut() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.SIMPLE2 select * from testavro.SIMPLE1\";\r\n    String sql2 = \"Insert into testavro.SIMPLE3 select * from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1, sql2);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages * 2, outMessages.size());\r\n    Set<Integer> outMessagesSet = new HashSet<>(outMessages);\r\n    Assert.assertEquals(numMessages, outMessagesSet.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(new ArrayList<>(outMessagesSet)));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithProjection",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithProjection() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value, long_value) \" + \" select id, NOT(id = 5) as bool_value, TIMESTAMPDIFF(HOUR, CURRENT_TIMESTAMP(), LOCALTIMESTAMP()) + MONTH(CURRENT_DATE()) as long_value from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertEquals(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()), outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithBooleanCheck",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithBooleanCheck() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select * from testavro.COMPLEX1 where bool_value IS TRUE\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    Assert.assertEquals(numMessages / 2, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndCompoundBooleanCheck",
  "sourceCode" : "@Test\r\npublic void testEndToEndCompoundBooleanCheck() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select * from testavro.COMPLEX1 where id >= 0 and bool_value IS TRUE\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    Assert.assertEquals(numMessages / 2, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndCompoundBooleanCheckWorkaround",
  "sourceCode" : "@Test\r\npublic void testEndToEndCompoundBooleanCheckWorkaround() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    // BUG Compound boolean checks dont work in calcite, So workaround by casting it to String\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select * from testavro.COMPLEX1 where id >= 0 and CAST(bool_value AS VARCHAR) =  'TRUE'\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    Assert.assertEquals(10, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithProjectionWithCase",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithProjectionWithCase() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, long_value) \" + \" select id, NOT(id = 5) as bool_value, CASE WHEN id IN (5, 6, 7) THEN CAST('foo' AS VARCHAR) WHEN id < 5 THEN CAST('bars' AS VARCHAR) ELSE NULL END as string_value from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithLike",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithLike() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value, string_value) \" + \" select id, NOT(id = 5) as bool_value, name as string_value from testavro.SIMPLE1 where name like 'Name%'\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertTrue(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndFlatten",
  "sourceCode" : "@Test\r\npublic void testEndToEndFlatten() {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(string_value, id, bool_value, bytes_value, fixed_value, float_value0, array_values) \" + \" select Flatten(array_values) as string_value, id, NOT(id = 5) as bool_value, bytes_value, fixed_value, float_value0, array_values\" + \" from testavro.COMPLEX1\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    // Test invariant for each input Row with rank i will contain a column array_values with i elements $\\sum_1^n{i}$.\r\n    int expectedMessages = (numMessages * (numMessages - 1)) / 2;\r\n    //Assert.assertEquals(outMessages.size(), actualList.size());\r\n    Assert.assertEquals(expectedMessages, outMessages.size());\r\n    // check that values are actually not null and within the expected range\r\n    Optional<GenericRecord> nullValueRecord = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> x.get(\"string_value\") == null).findFirst();\r\n    // The String value column is result of dot product thus must be present in the Array column\r\n    Optional<GenericRecord> missingValue = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> {\r\n        String value = (String) x.get(\"string_value\");\r\n        List<Object> arrayValues = (List<Object>) x.get(\"array_values\");\r\n        if (arrayValues == null) {\r\n            return true;\r\n        }\r\n        Optional<Object> notThere = arrayValues.stream().filter(v -> v.toString().equalsIgnoreCase(value)).findAny();\r\n        return !notThere.isPresent();\r\n    }).findFirst();\r\n    Assert.assertFalse(\"Null value \" + nullValueRecord.orElse(null), nullValueRecord.isPresent());\r\n    Assert.assertFalse(\"Absent Value \" + missingValue.orElse(null), missingValue.isPresent());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndComplexRecord",
  "sourceCode" : "@Test\r\npublic void testEndToEndComplexRecord() throws SamzaSqlValidatorException {\r\n    int numMessages = 10;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select bool_value, map_values['key0'] as string_value, union_value, array_values, map_values, id, bytes_value,\" + \" fixed_value, float_value0 from testavro.COMPLEX1\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndWithFloatToStringConversion",
  "sourceCode" : "@Test\r\npublic void testEndToEndWithFloatToStringConversion() {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select 'urn:li:member:' || cast(cast(float_value0 as int) as varchar) as string_value, id, float_value0, \" + \" double_value, true as bool_value from testavro.COMPLEX1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"string_value\").toString().split(\":\")[3])).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    Assert.assertEquals(IntStream.range(0, numMessages).boxed().collect(Collectors.toList()), outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndNestedRecord",
  "sourceCode" : "@Ignore\r\n@Test\r\npublic void testEndToEndNestedRecord() throws SamzaSqlValidatorException {\r\n    int numMessages = 10;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic\" + \" select `phoneNumbers`[0].`kind`\" + \" from testavro.PROFILE as p\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndFlattenWithUdf",
  "sourceCode" : "@Test\r\npublic void testEndToEndFlattenWithUdf() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value) select Flatten(MyTestArray(id)) as id, NOT(id = 5) as bool_value\" + \" from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    // Test invariant for each input Row with rank i will contain a column array_values with i elements $\\sum_1^n{i}$.\r\n    int expectedMessages = (numMessages * (numMessages - 1)) / 2;\r\n    // Flatten de-normalizes the data. So there is separate record for each entry in the array.\r\n    Assert.assertEquals(expectedMessages, outMessages.size());\r\n    // check that values are actually not null and within the expected range\r\n    Optional<GenericRecord> nullValueRecord = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> x.get(\"id\") == null).findFirst();\r\n    Assert.assertFalse(\"Null value \" + nullValueRecord.orElse(null), nullValueRecord.isPresent());\r\n    //TODO this is failing for now and that is because of udf weak type system, fixing it will be beyond this work.\r\n    /* // The String value column is result of dot product thus must be present in the Array column\r\n    Optional<GenericRecord> missingValue = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> {\r\n      String value = (String) x.get(\"string_value\");\r\n      List<Object> arrayValues = (List<Object>) x.get(\"array_values\");\r\n      if (arrayValues == null) {\r\n        return true;\r\n      }\r\n      Optional<Object> notThere = arrayValues.stream().filter(v -> v.toString().equalsIgnoreCase(value)).findAny();\r\n      return !notThere.isPresent();\r\n    }).findFirst();\r\n    Assert.assertFalse(\"Absent Value \" + missingValue.orElse(null), missingValue.isPresent());\r\n    */\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndSubQuery",
  "sourceCode" : "@Test\r\npublic void testEndToEndSubQuery() throws InterruptedException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value) select Flatten(a) as id, true as bool_value\" + \" from (select MyTestArray(id) a from testavro.SIMPLE1)\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<OutgoingMessageEnvelope> outMessages = new ArrayList<>(TestAvroSystemFactory.messages);\r\n    // Test invariant for each input Row with rank i will contain a column array_values with i elements $\\sum_1^n{i}$.\r\n    int expectedMessages = (numMessages * (numMessages - 1)) / 2;\r\n    // Flatten de-normalizes the data. So there is separate record for each entry in the array.\r\n    Assert.assertEquals(expectedMessages, outMessages.size());\r\n    // check that values are actually not null and within the expected range\r\n    Optional<GenericRecord> nullValueRecord = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> x.get(\"id\") == null).findFirst();\r\n    Assert.assertFalse(\"Null value \" + nullValueRecord.orElse(null), nullValueRecord.isPresent());\r\n    //TODO this is failing for now and that is because of udf weak type system, fixing it will be beyond this work.\r\n    /* // The String value column is result of dot product thus must be present in the Array column\r\n    Optional<GenericRecord> missingValue = outMessages.stream().map(x -> (GenericRecord) x.getMessage()).filter(x -> {\r\n      String value = (String) x.get(\"string_value\");\r\n      List<Object> arrayValues = (List<Object>) x.get(\"array_values\");\r\n      if (arrayValues == null) {\r\n        return true;\r\n      }\r\n      Optional<Object> notThere = arrayValues.stream().filter(v -> v.toString().equalsIgnoreCase(value)).findAny();\r\n      return !notThere.isPresent();\r\n    }).findFirst();\r\n    Assert.assertFalse(\"Absent Value \" + missingValue.orElse(null), missingValue.isPresent());\r\n    */\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testUdfUnTypedArgumentToTypedUdf",
  "sourceCode" : "@Test\r\npublic void testUdfUnTypedArgumentToTypedUdf() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value, long_value) \" + \"select id, NOT(id = 5) as bool_value, MyTest(MyTestObj(id)) as long_value from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    LOG.info(\"output Messages \" + TestAvroSystemFactory.messages);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"long_value\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(outMessages.size(), numMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testMismatchedUdfArgumentTypeShouldFailWithException",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testMismatchedUdfArgumentTypeShouldFailWithException() {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(long_value) \" + \"select MyTestObj(pageKey) as long_value from testavro.PAGEVIEW\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndUdf",
  "sourceCode" : "@Test\r\npublic void testEndToEndUdf() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value, long_value) \" + \"select id, NOT(id = 5) as bool_value, MYTest(id) as long_value from testavro.SIMPLE1;;\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    LOG.info(\"output Messages \" + TestAvroSystemFactory.messages);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"long_value\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(outMessages.size(), numMessages);\r\n    MyTestUdf udf = new MyTestUdf();\r\n    Assert.assertTrue(IntStream.range(0, numMessages).map(udf::execute).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndUdfWithDisabledArgCheck",
  "sourceCode" : "@Test\r\npublic void testEndToEndUdfWithDisabledArgCheck() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.PROFILE1(id, address) \" + \"select id, BuildOutputRecord('key', GetNestedField(address, 'zip')) as address from testavro.PROFILE\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    runApplication(new MapConfig(staticConfigs));\r\n    LOG.info(\"output Messages \" + TestAvroSystemFactory.messages);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"id\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(outMessages.size(), numMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndUdfPolymorphism",
  "sourceCode" : "@Test\r\npublic void testEndToEndUdfPolymorphism() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value, long_value) \" + \"select MyTestPoly(id) as long_value, NOT(id = 5) as bool_value, MyTestPoly(name) as id from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    LOG.info(\"output Messages \" + TestAvroSystemFactory.messages);\r\n    List<Integer> outMessages = TestAvroSystemFactory.messages.stream().map(x -> Integer.valueOf(((GenericRecord) x.getMessage()).get(\"long_value\").toString())).sorted().collect(Collectors.toList());\r\n    Assert.assertEquals(outMessages.size(), numMessages);\r\n    MyTestUdf udf = new MyTestUdf();\r\n    Assert.assertTrue(IntStream.range(0, numMessages).map(udf::execute).boxed().collect(Collectors.toList()).equals(outMessages));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testRegexMatchUdfInWhereClause",
  "sourceCode" : "@Test\r\npublic void testRegexMatchUdfInWhereClause() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql1 = \"Insert into testavro.outputTopic(id, bool_value) \" + \"select id, NOT(id = 5) as bool_value \" + \"from testavro.SIMPLE1 \" + \"where RegexMatch('.*4', name)\";\r\n    List<String> sqlStmts = Collections.singletonList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    LOG.info(\"output Messages \" + TestAvroSystemFactory.messages);\r\n    // There should be two messages that contain \"4\"\r\n    Assert.assertEquals(TestAvroSystemFactory.messages.size(), 2);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoin",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoin() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoinWithPrimaryKey",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoinWithPrimaryKey() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableJoinWithSubQuery",
  "sourceCode" : "@Ignore\r\n@Test\r\npublic void testEndToEndStreamTableJoinWithSubQuery() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic\" + \" select p.name as profileName, pv.pageKey as pageKey, p.address as profileAddress, coalesce(null, 'N/A') as companyName\" + \" from (SELECT * FROM (SELECT * from testavro.PAGEVIEW pv1 where pv1.profileId=0) as pv2) as pv\" + \" join testavro.PROFILE.`$table` as p\" + \" on p.id = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(1, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoin(1);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoinWithUdf",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoinWithUdf() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on MyTest(p.id) = MyTest(pv.profileId)\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoinWithNestedRecord",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoinWithNestedRecord() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> {\r\n        GenericRecord profileAddr = (GenericRecord) ((GenericRecord) x.getMessage()).get(\"profileAddress\");\r\n        GenericRecord streetNum = (GenericRecord) (profileAddr.get(\"streetnum\"));\r\n        return ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString()) + \",\" + profileAddr.get(\"zip\") + \",\" + streetNum.get(\"number\");\r\n    }).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameAddressJoin(numMessages);\r\n    Assert.assertEquals(outMessages, expectedOutMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoinWithFilter",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoinWithFilter() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId \" + \"where p.name = 'Mike'\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(4, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoin(numMessages).stream().filter(msg -> msg.endsWith(\"Mike\")).collect(Collectors.toList());\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableInnerJoinWithNullForeignKeys",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableInnerJoinWithNullForeignKeys() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(Collections.emptyMap(), numMessages, true);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PAGEVIEW as pv \" + \"join testavro.PROFILE.`$table` as p \" + \" on pv.profileId = p.id\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    // Half the foreign keys are null.\r\n    Assert.assertEquals(numMessages / 2, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoinWithNullForeignKeys(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableLeftJoin",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableLeftJoin() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(Collections.emptyMap(), numMessages, true);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PAGEVIEW as pv \" + \"left join testavro.PROFILE.`$table` as p \" + \" on pv.profileId = p.id\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameOuterJoinWithNullForeignKeys(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableRightJoin",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableRightJoin() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(Collections.emptyMap(), numMessages, true);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, p.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PROFILE.`$table` as p \" + \"right join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameOuterJoinWithNullForeignKeys(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableTableJoin",
  "sourceCode" : "@Ignore\r\n@Test\r\npublic void testEndToEndStreamTableTableJoin() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, c.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PAGEVIEW as pv \" + \"join testavro.PROFILE.`$table` as p \" + \" on MyTest(p.id) = MyTest(pv.profileId) \" + \" join testavro.COMPANY.`$table` as c \" + \" on MyTest(p.companyId) = MyTest(c.id)\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"profileName\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"companyName\").toString()).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileCompanyNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableNestedJoinWithPrimaryKeys",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableNestedJoinWithPrimaryKeys() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, c.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PAGEVIEW as pv \" + \"join testavro.PROFILE.`$table` as p \" + \" on MyTest(p.__key__) = MyTest(pv.profileId) \" + \" join testavro.COMPANY.`$table` as c \" + \" on MyTest(p.companyId) = MyTest(c.__key__)\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"profileName\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"companyName\").toString()).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileCompanyNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableNestedJoinWithSubQuery",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableNestedJoinWithSubQuery() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select t.pageKey as __key__, t.pageKey as pageKey, c.name as companyName, t.profileName as profileName,\" + \"       address as profileAddress \" + \"from (select p.companyId as companyId, p.name as profileName, p.address as address, pv.pageKey as pageKey\" + \"      from testavro.PAGEVIEW as pv \" + \"      join testavro.PROFILE.`$table` as p \" + \"      on MyTest(p.__key__) = MyTest(pv.profileId)) as t \" + \"join testavro.COMPANY.`$table` as c \" + \"on MyTest(t.companyId) = MyTest(c.__key__)\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"profileName\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"companyName\").toString()).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileCompanyNameJoin(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndStreamTableNestedJoinWithCompositeKey",
  "sourceCode" : "@Test\r\npublic void testEndToEndStreamTableNestedJoinWithCompositeKey() throws Exception {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, c.name as companyName, p.name as profileName,\" + \"       p.address as profileAddress \" + \"from testavro.PAGEVIEW as pv \" + \"join testavro.PROFILE.`$table` as p \" + \" on p.id = pv.profileId \" + \" join testavro.COMPANY.`$table` as c \" + \" on p.companyId = c.id AND c.id = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"profileName\").toString() + \",\" + ((GenericRecord) x.getMessage()).get(\"companyName\").toString()).collect(Collectors.toList());\r\n    Assert.assertEquals(TestAvroSystemFactory.COMPANIES.length, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileCompanyNameJoin(TestAvroSystemFactory.COMPANIES.length);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlEndToEnd.java",
  "methodName" : "testEndToEndGroupBy",
  "sourceCode" : "// Disabling the test until SAMZA-1652 and SAMZA-1661 are fixed.\r\n@Ignore\r\n@Test\r\npublic void testEndToEndGroupBy() throws Exception {\r\n    int numMessages = 200;\r\n    long windowDurationMs = 200;\r\n    TestAvroSystemFactory.messages.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(Collections.emptyMap(), numMessages, false, false, windowDurationMs);\r\n    String sql = \"Insert into testavro.pageViewCountTopic\" + \" select 'SampleJob' as jobName, pv.pageKey, count(*) as `count`\" + \" from testavro.PAGEVIEW as pv\" + \" where pv.pageKey = 'job' or pv.pageKey = 'inbox'\" + \" group by (pv.pageKey)\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    // Let's capture the list of windows/counts per key.\r\n    HashMap<String, List<String>> pageKeyCountListMap = new HashMap<>();\r\n    TestAvroSystemFactory.messages.stream().map(x -> {\r\n        String pageKey = ((GenericRecord) x.getMessage()).get(\"pageKey\").toString();\r\n        String count = ((GenericRecord) x.getMessage()).get(\"count\").toString();\r\n        pageKeyCountListMap.computeIfAbsent(pageKey, k -> new ArrayList<>()).add(count);\r\n        return pageKeyCountListMap;\r\n    });\r\n    HashMap<String, Integer> pageKeyCountMap = new HashMap<>();\r\n    pageKeyCountListMap.forEach((key, list) -> {\r\n        // Check that the number of windows per key is non-zero but less than the number of input messages per key.\r\n        Assert.assertTrue(list.size() > 1 && list.size() < numMessages / TestAvroSystemFactory.PAGE_KEYS.length);\r\n        // Collapse the count of messages per key\r\n        pageKeyCountMap.put(key, list.stream().mapToInt(Integer::parseInt).sum());\r\n    });\r\n    Set<String> pageKeys = new HashSet<>(Arrays.asList(\"job\", \"inbox\"));\r\n    HashMap<String, Integer> expectedPageKeyCountMap = TestAvroSystemFactory.getPageKeyGroupByResult(numMessages, pageKeys);\r\n    Assert.assertEquals(expectedPageKeyCountMap, pageKeyCountMap);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSinkEndToEndWithKey",
  "sourceCode" : "@Test\r\npublic void testSinkEndToEndWithKey() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testRemoteStore.testTable.`$table` select __key__, id, name from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    Assert.assertEquals(numMessages, RemoteStoreIOResolverTestFactory.records.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSinkEndToEndWithKeyWithNullRecords",
  "sourceCode" : "@Test\r\n@Ignore(\"Disabled due to flakiness related to data generation; Refer Pull Request #905 for details\")\r\npublic void testSinkEndToEndWithKeyWithNullRecords() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> props = new HashMap<>();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(props, numMessages, false, true);\r\n    String sql1 = \"Insert into testRemoteStore.testTable.`$table` select __key__, id, name from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql1);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    Assert.assertEquals(numMessages, RemoteStoreIOResolverTestFactory.records.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSinkEndToEndWithoutKey",
  "sourceCode" : "@Test(expected = AssertionError.class)\r\npublic void testSinkEndToEndWithoutKey() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(numMessages);\r\n    String sql = \"Insert into testRemoteStore.testTable.`$table`(id,name) select id, name from testavro.SIMPLE1\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    Assert.assertEquals(numMessages, RemoteStoreIOResolverTestFactory.records.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEnd",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEnd() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithUdf",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithUdf() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithUdfHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithOptimizer",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithOptimizer() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithUdfAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithUdfAndOptimizer() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithUdfHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithFilter",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithFilter() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithFilterHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithUdfAndFilter",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithUdfAndFilter() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithUdfAndFilterHelper(false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithFilterHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinEndToEndWithUdfAndFilterAndOptimizer",
  "sourceCode" : "@Test\r\npublic void testJoinEndToEndWithUdfAndFilterAndOptimizer() throws SamzaSqlValidatorException {\r\n    testJoinEndToEndWithUdfAndFilterHelper(true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSourceEndToEndWithKeyWithNullForeignKeys",
  "sourceCode" : "@Test\r\npublic void testSourceEndToEndWithKeyWithNullForeignKeys() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    populateProfileTable(staticConfigs, numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages / 2, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameJoinWithNullForeignKeys(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSourceEndToEndWithKeyWithNullForeignKeysRightOuterJoin",
  "sourceCode" : "@Test\r\npublic void testSourceEndToEndWithKeyWithNullForeignKeysRightOuterJoin() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    TestAvroSystemFactory.messages.clear();\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    populateProfileTable(staticConfigs, numMessages);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"right join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    List<String> outMessages = TestAvroSystemFactory.messages.stream().map(x -> ((GenericRecord) x.getMessage()).get(\"pageKey\").toString() + \",\" + (((GenericRecord) x.getMessage()).get(\"profileName\") == null ? \"null\" : ((GenericRecord) x.getMessage()).get(\"profileName\").toString())).collect(Collectors.toList());\r\n    Assert.assertEquals(numMessages, outMessages.size());\r\n    List<String> expectedOutMessages = TestAvroSystemFactory.getPageKeyProfileNameOuterJoinWithNullForeignKeys(numMessages);\r\n    Assert.assertEquals(expectedOutMessages, outMessages);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinConditionWithMoreThanOneConjunction",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testJoinConditionWithMoreThanOneConjunction() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"right join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId and p.__key__ = pv.pageKey where p.name is null or  p.name <> '0'\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testJoinConditionMissing__key__",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testJoinConditionMissing__key__() throws SamzaSqlValidatorException {\r\n    int numMessages = 20;\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    String sql = \"Insert into testavro.enrichedPageViewTopic \" + \"select pv.pageKey as __key__, pv.pageKey as pageKey, coalesce(null, 'N/A') as companyName,\" + \"       p.name as profileName, p.address as profileAddress \" + \"from testRemoteStore.Profile.`$table` as p \" + \"right join testavro.PAGEVIEW as pv \" + \" on p.id = pv.profileId where p.name is null or  p.name <> '0'\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testSameJoinTargetSinkEndToEndRightOuterJoin",
  "sourceCode" : "@Test\r\npublic void testSameJoinTargetSinkEndToEndRightOuterJoin() throws SamzaSqlValidatorException {\r\n    int numMessages = 21;\r\n    TestAvroSystemFactory.messages.clear();\r\n    RemoteStoreIOResolverTestFactory.records.clear();\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    populateProfileTable(staticConfigs, numMessages);\r\n    // The below query reads messages from a stream and deletes the corresponding records from the table.\r\n    // Since the stream has alternate messages with null foreign key, only half of the messages will have\r\n    // successful joins and hence only half of the records in the table will be deleted. Although join is\r\n    // redundant here, keeping it just for testing purpose.\r\n    String sql = \"Insert into testRemoteStore.Profile.`$table` \" + \"select p.__key__ as __key__, 'DELETE' as __op__ \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId \";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n    runApplication(config);\r\n    Assert.assertEquals((numMessages + 1) / 2, RemoteStoreIOResolverTestFactory.records.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testDeleteOpValidation",
  "sourceCode" : "@Test\r\npublic void testDeleteOpValidation() throws SamzaSqlValidatorException {\r\n    int numMessages = 1;\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    String sql = \"Insert into testRemoteStore.Profile.`$table` \" + \"select p.__key__ as __key__, 'DELETE' as __op__ \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId \";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testUnsupportedOpValidation",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testUnsupportedOpValidation() throws SamzaSqlValidatorException {\r\n    int numMessages = 1;\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    String sql = \"Insert into testRemoteStore.Profile.`$table` \" + \"select p.__key__ as __key__, 'UPDATE' as __op__ \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId\";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\samzasql\\TestSamzaSqlRemoteTable.java",
  "methodName" : "testNonKeyWithDeleteOpValidation",
  "sourceCode" : "@Test(expected = SamzaSqlValidatorException.class)\r\npublic void testNonKeyWithDeleteOpValidation() throws SamzaSqlValidatorException {\r\n    int numMessages = 1;\r\n    Map<String, String> staticConfigs = SamzaSqlTestConfig.fetchStaticConfigsWithFactories(new HashMap<>(), numMessages, true);\r\n    String sql = \"Insert into testRemoteStore.Profile.`$table` \" + \"select p.__key__ as pageKey, 'UPDATE' as __op__ \" + \"from testRemoteStore.Profile.`$table` as p \" + \"join testavro.PAGEVIEW as pv \" + \" on p.__key__ = pv.profileId \";\r\n    List<String> sqlStmts = Arrays.asList(sql);\r\n    staticConfigs.put(SamzaSqlApplicationConfig.CFG_SQL_STMTS_JSON, JsonUtil.toJson(sqlStmts));\r\n    Config config = new MapConfig(staticConfigs);\r\n    new SamzaSqlValidator(config).validate(sqlStmts);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointSpecific",
  "sourceCode" : "@Test\r\npublic void testStartpointSpecific() throws InterruptedException {\r\n    Map<Integer, RecordMetadata> sentEvents1 = publishKafkaEventsWithDelayPerEvent(inputKafkaTopic1, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[0], Duration.ofMillis(2));\r\n    ConcurrentHashMap<String, IncomingMessageEnvelope> recvEventsInputStartpointSpecific = new ConcurrentHashMap<>();\r\n    CoordinatorStreamStore coordinatorStreamStore = createCoordinatorStreamStore(applicationConfig1);\r\n    coordinatorStreamStore.init();\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    startpointManager.start();\r\n    StartpointSpecific startpointSpecific = new StartpointSpecific(String.valueOf(sentEvents1.get(100).offset()));\r\n    writeStartpoints(startpointManager, inputKafkaTopic1, ZK_TEST_PARTITION_COUNT, startpointSpecific);\r\n    startpointManager.stop();\r\n    coordinatorStreamStore.close();\r\n    TestTaskApplication.TaskApplicationProcessCallback processedCallback = (IncomingMessageEnvelope ime, TaskCallback callback) -> {\r\n        try {\r\n            String streamName = ime.getSystemStreamPartition().getStream();\r\n            TestKafkaEvent testKafkaEvent = TestKafkaEvent.fromString((String) ime.getMessage());\r\n            String eventIndex = testKafkaEvent.getEventData();\r\n            if (inputKafkaTopic1.equals(streamName)) {\r\n                recvEventsInputStartpointSpecific.put(eventIndex, ime);\r\n            } else {\r\n                throw new RuntimeException(\"Unexpected input stream: \" + streamName);\r\n            }\r\n            callback.complete();\r\n        } catch (Exception ex) {\r\n            callback.failure(ex);\r\n        }\r\n    };\r\n    // Just fetch a few messages\r\n    CountDownLatch processedMessagesLatchStartpointSpecific = new CountDownLatch(100);\r\n    CountDownLatch shutdownLatchStartpointSpecific = new CountDownLatch(1);\r\n    TestTaskApplication testTaskApplicationStartpointSpecific = new TestTaskApplication(TEST_SYSTEM, inputKafkaTopic1, outputKafkaTopic, processedMessagesLatchStartpointSpecific, shutdownLatchStartpointSpecific, Optional.of(processedCallback));\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(testTaskApplicationStartpointSpecific, applicationConfig1);\r\n    executeRun(appRunner, applicationConfig1);\r\n    assertTrue(processedMessagesLatchStartpointSpecific.await(1, TimeUnit.MINUTES));\r\n    appRunner.kill();\r\n    appRunner.waitForFinish();\r\n    assertTrue(shutdownLatchStartpointSpecific.await(1, TimeUnit.MINUTES));\r\n    Integer startpointSpecificOffset = Integer.valueOf(startpointSpecific.getSpecificOffset());\r\n    for (IncomingMessageEnvelope ime : recvEventsInputStartpointSpecific.values()) {\r\n        Integer eventOffset = Integer.valueOf(ime.getOffset());\r\n        String assertMsg = String.format(\"Expecting message offset: %d >= Startpoint specific offset: %d\", eventOffset, startpointSpecificOffset);\r\n        assertTrue(assertMsg, eventOffset >= startpointSpecificOffset);\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointTimestamp",
  "sourceCode" : "@Test\r\npublic void testStartpointTimestamp() throws InterruptedException {\r\n    Map<Integer, RecordMetadata> sentEvents2 = publishKafkaEventsWithDelayPerEvent(inputKafkaTopic2, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[1], Duration.ofMillis(2));\r\n    ConcurrentHashMap<String, IncomingMessageEnvelope> recvEventsInputStartpointTimestamp = new ConcurrentHashMap<>();\r\n    CoordinatorStreamStore coordinatorStreamStore = createCoordinatorStreamStore(applicationConfig1);\r\n    coordinatorStreamStore.init();\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    startpointManager.start();\r\n    StartpointTimestamp startpointTimestamp = new StartpointTimestamp(sentEvents2.get(150).timestamp());\r\n    writeStartpoints(startpointManager, inputKafkaTopic2, ZK_TEST_PARTITION_COUNT, startpointTimestamp);\r\n    startpointManager.stop();\r\n    coordinatorStreamStore.close();\r\n    TestTaskApplication.TaskApplicationProcessCallback processedCallback = (IncomingMessageEnvelope ime, TaskCallback callback) -> {\r\n        try {\r\n            String streamName = ime.getSystemStreamPartition().getStream();\r\n            TestKafkaEvent testKafkaEvent = TestKafkaEvent.fromString((String) ime.getMessage());\r\n            String eventIndex = testKafkaEvent.getEventData();\r\n            if (inputKafkaTopic2.equals(streamName)) {\r\n                recvEventsInputStartpointTimestamp.put(eventIndex, ime);\r\n            } else {\r\n                throw new RuntimeException(\"Unexpected input stream: \" + streamName);\r\n            }\r\n            callback.complete();\r\n        } catch (Exception ex) {\r\n            callback.failure(ex);\r\n        }\r\n    };\r\n    // Just fetch a few messages\r\n    CountDownLatch processedMessagesLatchStartpointTimestamp = new CountDownLatch(100);\r\n    CountDownLatch shutdownLatchStartpointTimestamp = new CountDownLatch(1);\r\n    TestTaskApplication testTaskApplicationStartpointTimestamp = new TestTaskApplication(TEST_SYSTEM, inputKafkaTopic2, outputKafkaTopic, processedMessagesLatchStartpointTimestamp, shutdownLatchStartpointTimestamp, Optional.of(processedCallback));\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(testTaskApplicationStartpointTimestamp, applicationConfig2);\r\n    executeRun(appRunner, applicationConfig2);\r\n    assertTrue(processedMessagesLatchStartpointTimestamp.await(1, TimeUnit.MINUTES));\r\n    appRunner.kill();\r\n    appRunner.waitForFinish();\r\n    assertTrue(shutdownLatchStartpointTimestamp.await(1, TimeUnit.MINUTES));\r\n    for (IncomingMessageEnvelope ime : recvEventsInputStartpointTimestamp.values()) {\r\n        Integer eventOffset = Integer.valueOf(ime.getOffset());\r\n        // sanity check\r\n        assertNotEquals(0, ime.getEventTime());\r\n        String assertMsg = String.format(\"Expecting message timestamp: %d >= Startpoint timestamp: %d\", ime.getEventTime(), startpointTimestamp.getTimestampOffset());\r\n        assertTrue(assertMsg, ime.getEventTime() >= startpointTimestamp.getTimestampOffset());\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointOldest",
  "sourceCode" : "@Test\r\npublic void testStartpointOldest() throws InterruptedException {\r\n    publishKafkaEventsWithDelayPerEvent(inputKafkaTopic3, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[2], Duration.ofMillis(2));\r\n    ConcurrentHashMap<String, IncomingMessageEnvelope> recvEventsInputStartpointOldest = new ConcurrentHashMap<>();\r\n    CoordinatorStreamStore coordinatorStreamStore = createCoordinatorStreamStore(applicationConfig1);\r\n    coordinatorStreamStore.init();\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    startpointManager.start();\r\n    StartpointOldest startpointOldest = new StartpointOldest();\r\n    writeStartpoints(startpointManager, inputKafkaTopic3, ZK_TEST_PARTITION_COUNT, startpointOldest);\r\n    startpointManager.stop();\r\n    coordinatorStreamStore.close();\r\n    TestTaskApplication.TaskApplicationProcessCallback processedCallback = (IncomingMessageEnvelope ime, TaskCallback callback) -> {\r\n        try {\r\n            String streamName = ime.getSystemStreamPartition().getStream();\r\n            TestKafkaEvent testKafkaEvent = TestKafkaEvent.fromString((String) ime.getMessage());\r\n            String eventIndex = testKafkaEvent.getEventData();\r\n            if (inputKafkaTopic3.equals(streamName)) {\r\n                recvEventsInputStartpointOldest.put(eventIndex, ime);\r\n            } else {\r\n                throw new RuntimeException(\"Unexpected input stream: \" + streamName);\r\n            }\r\n            callback.complete();\r\n        } catch (Exception ex) {\r\n            callback.failure(ex);\r\n        }\r\n    };\r\n    // Fetch all since consuming from oldest\r\n    CountDownLatch processedMessagesLatchStartpointOldest = new CountDownLatch(NUM_KAFKA_EVENTS);\r\n    CountDownLatch shutdownLatchStartpointOldest = new CountDownLatch(1);\r\n    TestTaskApplication testTaskApplicationStartpointOldest = new TestTaskApplication(TEST_SYSTEM, inputKafkaTopic3, outputKafkaTopic, processedMessagesLatchStartpointOldest, shutdownLatchStartpointOldest, Optional.of(processedCallback));\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(testTaskApplicationStartpointOldest, applicationConfig3);\r\n    executeRun(appRunner, applicationConfig3);\r\n    assertTrue(processedMessagesLatchStartpointOldest.await(1, TimeUnit.MINUTES));\r\n    appRunner.kill();\r\n    appRunner.waitForFinish();\r\n    assertTrue(shutdownLatchStartpointOldest.await(1, TimeUnit.MINUTES));\r\n    assertEquals(\"Expecting to have processed all the events\", NUM_KAFKA_EVENTS, recvEventsInputStartpointOldest.size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\startpoint\\TestStartpoint.java",
  "methodName" : "testStartpointUpcoming",
  "sourceCode" : "@Test\r\npublic void testStartpointUpcoming() throws InterruptedException {\r\n    publishKafkaEventsWithDelayPerEvent(inputKafkaTopic4, 0, NUM_KAFKA_EVENTS, PROCESSOR_IDS[3], Duration.ofMillis(2));\r\n    ConcurrentHashMap<String, IncomingMessageEnvelope> recvEventsInputStartpointUpcoming = new ConcurrentHashMap<>();\r\n    CoordinatorStreamStore coordinatorStreamStore = createCoordinatorStreamStore(applicationConfig1);\r\n    coordinatorStreamStore.init();\r\n    StartpointManager startpointManager = new StartpointManager(coordinatorStreamStore);\r\n    startpointManager.start();\r\n    StartpointUpcoming startpointUpcoming = new StartpointUpcoming();\r\n    writeStartpoints(startpointManager, inputKafkaTopic4, ZK_TEST_PARTITION_COUNT, startpointUpcoming);\r\n    startpointManager.stop();\r\n    coordinatorStreamStore.close();\r\n    TestTaskApplication.TaskApplicationProcessCallback processedCallback = (IncomingMessageEnvelope ime, TaskCallback callback) -> {\r\n        try {\r\n            String streamName = ime.getSystemStreamPartition().getStream();\r\n            TestKafkaEvent testKafkaEvent = TestKafkaEvent.fromString((String) ime.getMessage());\r\n            String eventIndex = testKafkaEvent.getEventData();\r\n            if (inputKafkaTopic4.equals(streamName)) {\r\n                recvEventsInputStartpointUpcoming.put(eventIndex, ime);\r\n            } else {\r\n                throw new RuntimeException(\"Unexpected input stream: \" + streamName);\r\n            }\r\n            callback.complete();\r\n        } catch (Exception ex) {\r\n            callback.failure(ex);\r\n        }\r\n    };\r\n    // Expecting none, so just attempt a small number of fetches.\r\n    CountDownLatch processedMessagesLatchStartpointUpcoming = new CountDownLatch(5);\r\n    CountDownLatch shutdownLatchStartpointUpcoming = new CountDownLatch(1);\r\n    TestTaskApplication testTaskApplicationStartpointUpcoming = new TestTaskApplication(TEST_SYSTEM, inputKafkaTopic4, outputKafkaTopic, processedMessagesLatchStartpointUpcoming, shutdownLatchStartpointUpcoming, Optional.of(processedCallback));\r\n    // Startpoint upcoming\r\n    ApplicationRunner appRunner = ApplicationRunners.getApplicationRunner(testTaskApplicationStartpointUpcoming, applicationConfig4);\r\n    executeRun(appRunner, applicationConfig4);\r\n    assertFalse(\"Expecting to timeout and not process any old messages.\", processedMessagesLatchStartpointUpcoming.await(15, TimeUnit.SECONDS));\r\n    assertEquals(\"Expecting not to process any old messages.\", 0, recvEventsInputStartpointUpcoming.size());\r\n    appRunner.kill();\r\n    appRunner.waitForFinish();\r\n    assertTrue(shutdownLatchStartpointUpcoming.await(1, TimeUnit.MINUTES));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestCouchbaseRemoteTableEndToEnd.java",
  "methodName" : "testEndToEnd",
  "sourceCode" : "@Test\r\npublic void testEndToEnd() {\r\n    Bucket inputBucket = cluster.openBucket(inputBucketName);\r\n    inputBucket.upsert(ByteArrayDocument.create(\"Alice\", \"20\".getBytes()));\r\n    inputBucket.upsert(ByteArrayDocument.create(\"Bob\", \"30\".getBytes()));\r\n    inputBucket.upsert(ByteArrayDocument.create(\"Chris\", \"40\".getBytes()));\r\n    inputBucket.upsert(ByteArrayDocument.create(\"David\", \"50\".getBytes()));\r\n    inputBucket.close();\r\n    List<String> users = Arrays.asList(\"Alice\", \"Bob\", \"Chris\", \"David\");\r\n    final StreamApplication app = appDesc -> {\r\n        DelegatingSystemDescriptor inputSystemDescriptor = new DelegatingSystemDescriptor(\"test\");\r\n        GenericInputDescriptor<String> inputDescriptor = inputSystemDescriptor.getInputDescriptor(\"User\", new NoOpSerde<>());\r\n        CouchbaseTableReadFunction<String> readFunction = new CouchbaseTableReadFunction<>(inputBucketName, String.class, \"couchbase://127.0.0.1\").withBootstrapCarrierDirectPort(couchbaseMock.getCarrierPort(inputBucketName)).withBootstrapHttpDirectPort(couchbaseMock.getHttpPort()).withSerde(new StringSerde());\r\n        CouchbaseTableWriteFunction<JsonObject> writeFunction = new CouchbaseTableWriteFunction<>(outputBucketName, JsonObject.class, \"couchbase://127.0.0.1\").withBootstrapCarrierDirectPort(couchbaseMock.getCarrierPort(outputBucketName)).withBootstrapHttpDirectPort(couchbaseMock.getHttpPort());\r\n        RemoteTableDescriptor inputTableDesc = new RemoteTableDescriptor<String, String, Void>(\"input-table\").withReadFunction(readFunction).withRateLimiterDisabled();\r\n        Table<KV<String, String>> inputTable = appDesc.getTable(inputTableDesc);\r\n        RemoteTableDescriptor outputTableDesc = new RemoteTableDescriptor<String, JsonObject, Object>(\"output-table\").withReadFunction(new NoOpTableReadFunction<>()).withWriteFunction(writeFunction).withRateLimiterDisabled();\r\n        Table<KV<String, JsonObject>> outputTable = appDesc.getTable(outputTableDesc);\r\n        appDesc.getInputStream(inputDescriptor).map(k -> KV.of(k, k)).join(inputTable, new JoinFunction()).sendTo(outputTable);\r\n    };\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"User\", new NoOpSerde<>());\r\n    TestRunner.of(app).addInputStream(inputDescriptor, users).run(Duration.ofSeconds(10));\r\n    Bucket outputBucket = cluster.openBucket(outputBucketName);\r\n    Assert.assertEquals(\"{\\\"name\\\":\\\"Alice\\\",\\\"age\\\":\\\"20\\\"}\", outputBucket.get(\"Alice\").content().toString());\r\n    Assert.assertEquals(\"{\\\"name\\\":\\\"Bob\\\",\\\"age\\\":\\\"30\\\"}\", outputBucket.get(\"Bob\").content().toString());\r\n    Assert.assertEquals(\"{\\\"name\\\":\\\"Chris\\\",\\\"age\\\":\\\"40\\\"}\", outputBucket.get(\"Chris\").content().toString());\r\n    Assert.assertEquals(\"{\\\"name\\\":\\\"David\\\",\\\"age\\\":\\\"50\\\"}\", outputBucket.get(\"David\").content().toString());\r\n    outputBucket.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableEndToEnd.java",
  "methodName" : "testSendTo",
  "sourceCode" : "@Test\r\npublic void testSendTo() {\r\n    MyMapFunction mapFn = new MyMapFunction();\r\n    StreamApplication app = appDesc -> {\r\n        Table<KV<Integer, Profile>> table = appDesc.getTable(new InMemoryTableDescriptor<>(\"t1\", KVSerde.of(new IntegerSerde(), new ProfileJsonSerde())));\r\n        DelegatingSystemDescriptor ksd = new DelegatingSystemDescriptor(SYSTEM_NAME);\r\n        GenericInputDescriptor<Profile> isd = ksd.getInputDescriptor(PROFILE_STREAM, new NoOpSerde<>());\r\n        appDesc.getInputStream(isd).map(mapFn).sendTo(table);\r\n    };\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME);\r\n    InMemoryInputDescriptor<Profile> profileStreamDesc = isd.getInputDescriptor(PROFILE_STREAM, new NoOpSerde<>());\r\n    int numProfilesPerPartition = 10;\r\n    int numInputPartitions = 4;\r\n    Map<Integer, List<Profile>> inputProfiles = TestTableData.generatePartitionedProfiles(numProfilesPerPartition * numInputPartitions, numInputPartitions);\r\n    TestRunner.of(app).addInputStream(profileStreamDesc, inputProfiles).run(Duration.ofSeconds(10));\r\n    for (int i = 0; i < numInputPartitions; i++) {\r\n        MyMapFunction mapFnCopy = MyMapFunction.getMapFunctionByTask(String.format(\"Partition %d\", i));\r\n        assertEquals(numProfilesPerPartition, mapFnCopy.received.size());\r\n        mapFnCopy.received.forEach(p -> assertNotNull(mapFnCopy.table.get(p.getMemberId())));\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableEndToEnd.java",
  "methodName" : "testStreamTableJoin",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoin() {\r\n    int totalPageViews = 40;\r\n    int partitionCount = 4;\r\n    Map<Integer, List<PageView>> inputPageViews = TestTableData.generatePartitionedPageViews(totalPageViews, partitionCount);\r\n    // 10 is the max member id for page views\r\n    Map<Integer, List<Profile>> inputProfiles = TestTableData.generatePartitionedProfiles(10, partitionCount);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME);\r\n    InMemoryInputDescriptor<PageView> pageViewStreamDesc = isd.getInputDescriptor(PAGEVIEW_STREAM, new NoOpSerde<>());\r\n    InMemoryInputDescriptor<Profile> profileStreamDesc = isd.getInputDescriptor(PROFILE_STREAM, new NoOpSerde<>());\r\n    TestRunner.of(new StreamTableJoinApp()).addInputStream(pageViewStreamDesc, inputPageViews).addInputStream(profileStreamDesc, inputProfiles).run(Duration.ofSeconds(10));\r\n    assertEquals(totalPageViews, StreamTableJoinApp.received.size());\r\n    assertEquals(totalPageViews, StreamTableJoinApp.joined.size());\r\n    assertNotNull(StreamTableJoinApp.joined.get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableEndToEnd.java",
  "methodName" : "testDualStreamTableJoin",
  "sourceCode" : "@Test\r\npublic void testDualStreamTableJoin() {\r\n    int totalPageViews = 40;\r\n    int partitionCount = 4;\r\n    Map<Integer, List<PageView>> inputPageViews1 = TestTableData.generatePartitionedPageViews(totalPageViews, partitionCount);\r\n    Map<Integer, List<PageView>> inputPageViews2 = TestTableData.generatePartitionedPageViews(totalPageViews, partitionCount);\r\n    // 10 is the max member id for page views\r\n    int numProfiles = 10;\r\n    Map<Integer, List<Profile>> inputProfiles1 = TestTableData.generatePartitionedProfiles(numProfiles, partitionCount);\r\n    Map<Integer, List<Profile>> inputProfiles2 = TestTableData.generatePartitionedProfiles(numProfiles, partitionCount);\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(SYSTEM_NAME);\r\n    InMemoryInputDescriptor<PageView> pageViewStreamDesc1 = isd.getInputDescriptor(PAGEVIEW_STREAM + \"1\", new NoOpSerde<>());\r\n    InMemoryInputDescriptor<PageView> pageViewStreamDesc2 = isd.getInputDescriptor(PAGEVIEW_STREAM + \"2\", new NoOpSerde<>());\r\n    InMemoryInputDescriptor<Profile> profileStreamDesc1 = isd.getInputDescriptor(PROFILE_STREAM + \"1\", new NoOpSerde<>());\r\n    InMemoryInputDescriptor<Profile> profileStreamDesc2 = isd.getInputDescriptor(PROFILE_STREAM + \"2\", new NoOpSerde<>());\r\n    TestRunner.of(new DualStreamTableJoinApp()).addInputStream(pageViewStreamDesc1, inputPageViews1).addInputStream(pageViewStreamDesc2, inputPageViews2).addInputStream(profileStreamDesc1, inputProfiles1).addInputStream(profileStreamDesc2, inputProfiles2).run(Duration.ofSeconds(10));\r\n    assertEquals(numProfiles, DualStreamTableJoinApp.sentToProfileTable1.size());\r\n    assertEquals(numProfiles, DualStreamTableJoinApp.sentToProfileTable2.size());\r\n    assertEquals(totalPageViews, DualStreamTableJoinApp.joinedPageViews1.size());\r\n    assertEquals(totalPageViews, DualStreamTableJoinApp.joinedPageViews2.size());\r\n    assertNotNull(DualStreamTableJoinApp.joinedPageViews1.get(0));\r\n    assertNotNull(DualStreamTableJoinApp.joinedPageViews2.get(0));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableWithConfigRewriterEndToEnd.java",
  "methodName" : "testWithConfigRewriter",
  "sourceCode" : "/**\r\n * MyTaskApplication does not include table descriptor, so if the rewriter does not add the table configs properly,\r\n * then the application will fail to execute.\r\n */\r\n@Test\r\npublic void testWithConfigRewriter() {\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    TestRunner.of(new MyTaskApplication()).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(40, 4)).addConfig(\"job.config.rewriters\", \"my-rewriter\").addConfig(\"job.config.rewriter.my-rewriter.class\", MyConfigRewriter.class.getName()).run(Duration.ofSeconds(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableWithLowLevelApiEndToEnd.java",
  "methodName" : "testTableWithLowLevelApi",
  "sourceCode" : "@Test\r\npublic void testTableWithLowLevelApi() {\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<TestTableData.PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    TestRunner.of(new TestLocalTableWithConfigRewriterEndToEnd.MyTaskApplication()).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(40, 4)).addConfig(\"job.config.rewriters\", \"my-rewriter\").addConfig(\"job.config.rewriter.my-rewriter.class\", TestLocalTableWithConfigRewriterEndToEnd.MyConfigRewriter.class.getName()).run(Duration.ofSeconds(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableWithSideInputsEndToEnd.java",
  "methodName" : "testLowLevelJoinWithSideInputsTable",
  "sourceCode" : "@Test\r\npublic void testLowLevelJoinWithSideInputsTable() throws InterruptedException {\r\n    int partitionCount = 4;\r\n    IntegerSerde integerSerde = new IntegerSerde();\r\n    // for low-level, need to pre-partition the input in the same way that the profiles are partitioned\r\n    Map<Integer, List<PageView>> pageViewsPartitionedByMemberId = TestTableData.generatePartitionedPageViews(20, partitionCount).values().stream().flatMap(List::stream).collect(Collectors.groupingBy(pageView -> Math.abs(Arrays.hashCode(integerSerde.toBytes(pageView.getMemberId()))) % partitionCount));\r\n    runTest(new LowLevelPageViewProfileJoin(), pageViewsPartitionedByMemberId, TestTableData.generatePartitionedProfiles(10, partitionCount));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableWithSideInputsEndToEnd.java",
  "methodName" : "testJoinWithSideInputsTable",
  "sourceCode" : "@Test\r\npublic void testJoinWithSideInputsTable() throws InterruptedException {\r\n    runTest(new PageViewProfileJoin(), TestTableData.generatePartitionedPageViews(20, 4), TestTableData.generatePartitionedProfiles(10, 4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestLocalTableWithSideInputsEndToEnd.java",
  "methodName" : "testJoinWithDurableSideInputTable",
  "sourceCode" : "@Test\r\npublic void testJoinWithDurableSideInputTable() throws InterruptedException {\r\n    runTest(new DurablePageViewProfileJoin(), TestTableData.generatePartitionedPageViews(20, 4), TestTableData.generatePartitionedProfiles(10, 4));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithDefaults",
  "sourceCode" : "@Test()\r\npublic void testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithDefaults() throws Exception {\r\n    // Test will attempt to apply updates. If there is no pre-existing records, it will PUT a default and\r\n    // then attempt an update\r\n    doTestStreamTableJoinRemoteTableWithFirstTimeUpdates(\"testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithDefaults\", true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithoutDefaults",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithoutDefaults() throws Exception {\r\n    // Test will attempt to apply updates. It will fail as there is no pre-existing record and no default will be\r\n    // inserted\r\n    // RecordNotFoundException is wrapped in multiple levels of exceptions, hence we only check the top level\r\n    // exception i.e SamzaException\r\n    doTestStreamTableJoinRemoteTableWithFirstTimeUpdates(\"testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithoutDefaults\", false, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithMiscException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithMiscException() throws Exception {\r\n    // Updates should fail as updates always fail in the WriteFunction\r\n    doTestStreamTableJoinRemoteTableWithFirstTimeUpdates(\"testStreamTableJoinRemoteTableWithFirstTimeUpdatesWithMiscException\", false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testSendToWithDefaultsAndUpdateOnly",
  "sourceCode" : "// Test fails with the following exception:\r\n// org.apache.samza.SamzaException: Put default failed for update as the UpdateOptions was set to UPDATE_ONLY.\r\n// Please use UpdateOptions.UPDATE_WITH_DEFAULTS instead.\r\n@Test(expected = SamzaException.class)\r\npublic void testSendToWithDefaultsAndUpdateOnly() throws Exception {\r\n    String testName = \"testSendToWithDefaultsAndUpdateOnly\";\r\n    final String profiles = Base64Serializer.serialize(generateProfiles(30));\r\n    final RateLimiter readRateLimiter = mock(RateLimiter.class, withSettings().serializable());\r\n    final TableRateLimiter.CreditFunction creditFunction = (k, v, args) -> 1;\r\n    final StreamApplication app = appDesc -> {\r\n        final RemoteTableDescriptor joinTableDesc = new RemoteTableDescriptor<Integer, TestTableData.Profile, Void>(\"profile-table-1\").withReadFunction(InMemoryProfileReadFunction.getInMemoryReadFunction(profiles)).withRateLimiter(readRateLimiter, creditFunction, null);\r\n        final RemoteTableDescriptor outputTableDesc = new RemoteTableDescriptor<Integer, EnrichedPageView, EnrichedPageView>(\"enriched-page-view-table-1\").withReadFunction(new NoOpTableReadFunction<>()).withReadRateLimiterDisabled().withWriteFunction(new InMemoryEnrichedPageViewWriteFunction2(testName, false)).withWriteRateLimit(1000);\r\n        // counters to count puts and updates\r\n        COUNTERS.put(testName + \"-put\", new AtomicInteger());\r\n        COUNTERS.put(testName + \"-update\", new AtomicInteger());\r\n        final Table<KV<Integer, Profile>> outputTable = appDesc.getTable(outputTableDesc);\r\n        final Table<KV<Integer, Profile>> joinTable = appDesc.getTable(joinTableDesc);\r\n        final DelegatingSystemDescriptor ksd = new DelegatingSystemDescriptor(\"test\");\r\n        final GenericInputDescriptor<PageView> isd = ksd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n        appDesc.getInputStream(isd).map(pv -> new KV<>(pv.getMemberId(), pv)).join(joinTable, new PageViewToProfileJoinFunction()).map(m -> new KV(m.getMemberId(), UpdateMessage.of(m, m))).sendTo(outputTable, UpdateOptions.UPDATE_ONLY);\r\n    };\r\n    int numPageViews = 15;\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    Map<Integer, List<PageView>> integerListMap = TestTableData.generatePartitionedPageViews(numPageViews, 1);\r\n    TestRunner.of(app).addInputStream(inputDescriptor, integerListMap).run(Duration.ofSeconds(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testSendToUpdatesFailureAfterPutDefault",
  "sourceCode" : "// Test fails with the following exception:\r\n// org.apache.samza.SamzaException: Update after Put default failed with exception.\r\n@Test(expected = SamzaException.class)\r\npublic void testSendToUpdatesFailureAfterPutDefault() throws Exception {\r\n    // the test checks for failure when update after put default fails\r\n    String testName = \"testSendToUpdatesFailureAfterPutDefault\";\r\n    final String profiles = Base64Serializer.serialize(generateProfiles(30));\r\n    final RateLimiter readRateLimiter = mock(RateLimiter.class, withSettings().serializable());\r\n    final TableRateLimiter.CreditFunction creditFunction = (k, v, args) -> 1;\r\n    final StreamApplication app = appDesc -> {\r\n        final RemoteTableDescriptor joinTableDesc = new RemoteTableDescriptor<Integer, TestTableData.Profile, Void>(\"profile-table-1\").withReadFunction(InMemoryProfileReadFunction.getInMemoryReadFunction(profiles)).withRateLimiter(readRateLimiter, creditFunction, null);\r\n        final RemoteTableDescriptor outputTableDesc = new RemoteTableDescriptor<Integer, EnrichedPageView, EnrichedPageView>(\"enriched-page-view-table-1\").withReadFunction(new NoOpTableReadFunction<>()).withReadRateLimiterDisabled().withWriteFunction(new InMemoryEnrichedPageViewWriteFunction2(testName, false, true)).withWriteRateLimit(1000);\r\n        final Table<KV<Integer, Profile>> outputTable = appDesc.getTable(outputTableDesc);\r\n        final Table<KV<Integer, Profile>> joinTable = appDesc.getTable(joinTableDesc);\r\n        final DelegatingSystemDescriptor ksd = new DelegatingSystemDescriptor(\"test\");\r\n        final GenericInputDescriptor<PageView> isd = ksd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n        appDesc.getInputStream(isd).map(pv -> new KV<>(pv.getMemberId(), pv)).join(joinTable, new PageViewToProfileJoinFunction()).map(m -> new KV(m.getMemberId(), UpdateMessage.of(m, m))).sendTo(outputTable, UpdateOptions.UPDATE_WITH_DEFAULTS);\r\n    };\r\n    int numPageViews = 15;\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    Map<Integer, List<PageView>> integerListMap = TestTableData.generatePartitionedPageViews(numPageViews, 1);\r\n    TestRunner.of(app).addInputStream(inputDescriptor, integerListMap).run(Duration.ofSeconds(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTable",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTable() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(false, false, false, \"testStreamTableJoinRemoteTable\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithCache",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableWithCache() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(true, false, false, \"testStreamTableJoinRemoteTableWithCache\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithDefaultCache",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableWithDefaultCache() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(true, true, false, \"testStreamTableJoinRemoteTableWithDefaultCache\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithUpdates",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableWithUpdates() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(false, false, true, \"testStreamTableJoinRemoteTableWithUpdates\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithUpdatesWithCache",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testStreamTableJoinRemoteTableWithUpdatesWithCache() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(true, false, true, \"testStreamTableJoinRemoteTableWithUpdatesWithCache\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableWithUpdatesWithDefaultCache",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testStreamTableJoinRemoteTableWithUpdatesWithDefaultCache() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(true, true, true, \"testStreamTableJoinRemoteTableWithUpdatesWithDefaultCache\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testSendToUpdatesWithoutUpdateOptions",
  "sourceCode" : "// Test will fail as we use sendTo with KV<K, UpdateMessage> stream without UpdateOptions\r\n@Test(expected = SamzaException.class)\r\npublic void testSendToUpdatesWithoutUpdateOptions() throws Exception {\r\n    // max member id for page views is 10\r\n    final String profiles = Base64Serializer.serialize(generateProfiles(10));\r\n    final RateLimiter readRateLimiter = mock(RateLimiter.class, withSettings().serializable());\r\n    final TableRateLimiter.CreditFunction creditFunction = (k, v, args) -> 1;\r\n    final StreamApplication app = appDesc -> {\r\n        final RemoteTableDescriptor joinTableDesc = new RemoteTableDescriptor<Integer, TestTableData.Profile, Void>(\"profile-table-1\").withReadFunction(InMemoryProfileReadFunction.getInMemoryReadFunction(profiles)).withRateLimiter(readRateLimiter, creditFunction, null);\r\n        final RemoteTableDescriptor outputTableDesc = new RemoteTableDescriptor<Integer, EnrichedPageView, EnrichedPageView>(\"enriched-page-view-table-1\").withReadFunction(new NoOpTableReadFunction<>()).withReadRateLimiterDisabled().withWriteFunction(new InMemoryEnrichedPageViewWriteFunction2(\"testUpdateWithoutUpdateOptions\", false)).withWriteRateLimit(1000);\r\n        final Table<KV<Integer, Profile>> outputTable = appDesc.getTable(outputTableDesc);\r\n        final Table<KV<Integer, Profile>> joinTable = appDesc.getTable(joinTableDesc);\r\n        final DelegatingSystemDescriptor ksd = new DelegatingSystemDescriptor(\"test\");\r\n        final GenericInputDescriptor<PageView> isd = ksd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n        appDesc.getInputStream(isd).map(pv -> new KV<>(pv.getMemberId(), pv)).join(joinTable, new PageViewToProfileJoinFunction()).map(m -> new KV(m.getMemberId(), UpdateMessage.of(m, m))).sendTo(outputTable);\r\n    };\r\n    int numPageViews = 40;\r\n    InMemorySystemDescriptor isd = new InMemorySystemDescriptor(\"test\");\r\n    InMemoryInputDescriptor<PageView> inputDescriptor = isd.getInputDescriptor(\"PageView\", new NoOpSerde<>());\r\n    TestRunner.of(app).addInputStream(inputDescriptor, TestTableData.generatePartitionedPageViews(numPageViews, 4)).run(Duration.ofSeconds(10));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testCatchReaderException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testCatchReaderException() {\r\n    TableReadFunction<String, ?> reader = mock(TableReadFunction.class);\r\n    CompletableFuture<String> future = new CompletableFuture<>();\r\n    future.completeExceptionally(new RuntimeException(\"Expected test exception\"));\r\n    doReturn(future).when(reader).getAsync(anyString());\r\n    TableRateLimiter rateLimitHelper = mock(TableRateLimiter.class);\r\n    RemoteTable<String, String, Void> table = new RemoteTable<>(\"table1\", reader, null, rateLimitHelper, null, null, Executors.newSingleThreadExecutor(), null, null, null, null, null, null);\r\n    table.init(createMockContext());\r\n    table.get(\"abc\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testCatchWriterException",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testCatchWriterException() {\r\n    TableReadFunction<String, String> reader = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, Void> writer = mock(TableWriteFunction.class);\r\n    CompletableFuture<String> future = new CompletableFuture<>();\r\n    future.completeExceptionally(new RuntimeException(\"Expected test exception\"));\r\n    doReturn(future).when(writer).putAsync(anyString(), any());\r\n    TableRateLimiter rateLimitHelper = mock(TableRateLimiter.class);\r\n    RemoteTable<String, String, Void> table = new RemoteTable<>(\"table1\", reader, writer, rateLimitHelper, rateLimitHelper, rateLimitHelper, Executors.newSingleThreadExecutor(), null, null, null, null, null, null);\r\n    table.init(createMockContext());\r\n    table.put(\"abc\", \"efg\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testCatchWriterExceptionWithUpdates",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testCatchWriterExceptionWithUpdates() {\r\n    TableReadFunction<String, String> reader = mock(TableReadFunction.class);\r\n    TableWriteFunction<String, String, String> writer = mock(TableWriteFunction.class);\r\n    CompletableFuture<String> future = new CompletableFuture<>();\r\n    future.completeExceptionally(new RuntimeException(\"Expected test exception\"));\r\n    doReturn(future).when(writer).updateAsync(anyString(), anyString());\r\n    TableRateLimiter rateLimitHelper = mock(TableRateLimiter.class);\r\n    RemoteTable<String, String, String> table = new RemoteTable<>(\"table1\", reader, writer, rateLimitHelper, rateLimitHelper, rateLimitHelper, Executors.newSingleThreadExecutor(), null, null, null, null, null, null);\r\n    table.init(createMockContext());\r\n    table.update(\"abc\", \"xyz\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableEndToEnd.java",
  "methodName" : "testUninitializedWriter",
  "sourceCode" : "@Test\r\npublic void testUninitializedWriter() {\r\n    TableReadFunction<String, String> reader = mock(TableReadFunction.class);\r\n    TableRateLimiter rateLimitHelper = mock(TableRateLimiter.class);\r\n    RemoteTable<String, String, Void> table = new RemoteTable<>(\"table1\", reader, null, rateLimitHelper, null, null, Executors.newSingleThreadExecutor(), null, null, null, null, null, null);\r\n    table.init(createMockContext());\r\n    try {\r\n        table.put(\"abc\", \"efg\");\r\n        Assert.fail();\r\n    } catch (SamzaException ex) {\r\n        // Ignore\r\n    }\r\n    try {\r\n        table.delete(\"abc\");\r\n        Assert.fail();\r\n    } catch (SamzaException ex) {\r\n        // Ignore\r\n    }\r\n    table.flush();\r\n    table.close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableWithBatchEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableBatchingReadWrite",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableBatchingReadWrite() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(\"testStreamTableJoinRemoteTableBatchingReadWrite\", true, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableWithBatchEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableBatchingRead",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableBatchingRead() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(\"testStreamTableJoinRemoteTableBatchingRead\", true, false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableWithBatchEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompleteBatch",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompleteBatch() throws Exception {\r\n    doTestStreamTableJoinRemoteTablePartialUpdates(\"testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompleteBatch\", false);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableWithBatchEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompactBatch",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompactBatch() throws Exception {\r\n    // test should fail with SamzaException as Batching is not supported with Compact Batches for partial updates\r\n    doTestStreamTableJoinRemoteTablePartialUpdates(\"testStreamTableJoinRemoteTableBatchReadsWithUpdatesCompactBatch\", true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-test\\src\\test\\java\\org\\apache\\samza\\test\\table\\TestRemoteTableWithBatchEndToEnd.java",
  "methodName" : "testStreamTableJoinRemoteTableBatchingWrite",
  "sourceCode" : "@Test\r\npublic void testStreamTableJoinRemoteTableBatchingWrite() throws Exception {\r\n    doTestStreamTableJoinRemoteTable(\"testStreamTableJoinRemoteTableBatchingWrite\", false, true);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\main\\java\\org\\apache\\samza\\job\\yarn\\YarnClusterResourceManager.java",
  "methodName" : "getAllocatedResources",
  "sourceCode" : "@VisibleForTesting\r\nConcurrentHashMap<SamzaResource, Container> getAllocatedResources() {\r\n    return allocatedResources;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\main\\java\\org\\apache\\samza\\job\\yarn\\YarnFaultDomainManager.java",
  "methodName" : "computeHostToFaultDomainMap",
  "sourceCode" : "/**\r\n * This method computes the host to rack map from Yarn.\r\n * Only the hosts that are running in the cluster will be a part of this map.\r\n * @return map of the host and the rack it resides on\r\n */\r\n@VisibleForTesting\r\nMultimap<String, FaultDomain> computeHostToFaultDomainMap() {\r\n    Multimap<String, FaultDomain> hostToRackMap = HashMultimap.create();\r\n    try {\r\n        List<NodeReport> nodeReport = yarnClient.getNodeReports(NodeState.RUNNING);\r\n        nodeReport.forEach(report -> {\r\n            FaultDomain rack = new FaultDomain(FaultDomainType.RACK, report.getRackName());\r\n            hostToRackMap.put(report.getNodeId().getHost(), rack);\r\n        });\r\n        log.info(\"Computed the host to rack map successfully from Yarn.\");\r\n    } catch (YarnException | IOException e) {\r\n        throw new SamzaException(\"Yarn threw an exception while getting NodeReports.\", e);\r\n    }\r\n    return hostToRackMap;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\config\\TestYarnConfig.java",
  "methodName" : "testGetPackagePath",
  "sourceCode" : "@Test\r\npublic void testGetPackagePath() {\r\n    String packagePath = \"http://some.package.path\";\r\n    Config config = new MapConfig(Collections.singletonMap(YarnConfig.PACKAGE_PATH, packagePath));\r\n    assertEquals(packagePath, new YarnConfig(config).getPackagePath());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\config\\TestYarnConfig.java",
  "methodName" : "testGetPackagePathMissingConfig",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetPackagePathMissingConfig() {\r\n    new YarnConfig(new MapConfig()).getPackagePath();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testFileSystemImplConfigSuccess",
  "sourceCode" : "@Test\r\npublic void testFileSystemImplConfigSuccess() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\");\r\n    configMap.put(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    Config conf = new MapConfig(configMap);\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(conf);\r\n    assertEquals(2, manager.getSchemes().size());\r\n    assertEquals(\"http\", manager.getSchemes().get(0));\r\n    assertEquals(\"myscheme\", manager.getSchemes().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testNullConfig",
  "sourceCode" : "@Test\r\npublic void testNullConfig() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"config cannot be null\");\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testSchemeWithSubkeys",
  "sourceCode" : "@Test\r\npublic void testSchemeWithSubkeys() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\");\r\n    configMap.put(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    configMap.put(\"fs.http.impl.key1\", \"val1\");\r\n    configMap.put(\"fs.http.impl.key2\", \"val2\");\r\n    Config conf = new MapConfig(configMap);\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(conf);\r\n    Map<String, String> expectedFsHttpImplConfs = //Scheme with additional subkeys\r\n    ImmutableMap.//Scheme with additional subkeys\r\n    of(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\", \"fs.http.impl.key1\", \"val1\", \"fs.http.impl.key2\", \"val2\");\r\n    Map<String, String> expectedFsMyschemeImplConfs = // Scheme without subkeys\r\n    ImmutableMap.// Scheme without subkeys\r\n    of(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    assertEquals(Arrays.asList(\"http\", \"myscheme\"), manager.getSchemes());\r\n    assertEquals(expectedFsHttpImplConfs, manager.getSchemeConfig(\"http\"));\r\n    assertEquals(expectedFsMyschemeImplConfs, manager.getSchemeConfig(\"myscheme\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testResourceConfigIncluded",
  "sourceCode" : "@Test\r\npublic void testResourceConfigIncluded() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    assertEquals(1, manager.getResourceNames().size());\r\n    assertEquals(\"myResource1\", manager.getResourceNames().get(0));\r\n    assertEquals(\"readme\", manager.getResourceLocalName(\"myResource1\"));\r\n    assertEquals(LocalResourceType.FILE, manager.getResourceLocalType(\"myResource1\"));\r\n    assertEquals(LocalResourceVisibility.PUBLIC, manager.getResourceLocalVisibility(\"myResource1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testResourcrConfigNotIncluded",
  "sourceCode" : "@Test\r\npublic void testResourcrConfigNotIncluded() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"otherconfig\", \"https://host2.com/not_included\");\r\n    configMap.put(\"yarn.resources.myResource2.local.name\", \"notExisting\");\r\n    configMap.put(\"yarn.resources.myResource2.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource2.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    assertEquals(0, manager.getResourceNames().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testNullConfig",
  "sourceCode" : "@Test\r\npublic void testNullConfig() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"config cannot be null\");\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidVisibility",
  "sourceCode" : "@Test\r\npublic void testInvalidVisibility() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceVisibility.INVALIDVISIBILITY\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"invalidVisibility\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourceLocalVisibility(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidType",
  "sourceCode" : "@Test\r\npublic void testInvalidType() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceType.INVALIDTYPE\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourceLocalType(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidPath",
  "sourceCode" : "@Test\r\npublic void testInvalidPath() {\r\n    thrown.expect(LocalizerResourceException.class);\r\n    thrown.expectMessage(\"resource path is required but not defined in config for resource myResource1\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourcePath(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapSuccess",
  "sourceCode" : "@Test\r\npublic void testResourceMapSuccess() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    configMap.put(\"yarn.resources.myResource2.path\", \"https://host2.com/package\");\r\n    configMap.put(\"yarn.resources.myResource2.local.name\", \"__package\");\r\n    configMap.put(\"yarn.resources.myResource2.local.type\", \"archive\");\r\n    configMap.put(\"yarn.resources.myResource2.local.visibility\", \"private\");\r\n    configMap.put(\"yarn.resources.myResource3.path\", \"https://host3.com/csr\");\r\n    configMap.put(\"yarn.resources.myResource3.local.name\", \"csr\");\r\n    configMap.put(\"yarn.resources.myResource3.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource3.local.visibility\", \"application\");\r\n    configMap.put(\"otherconfig\", \"https://host4.com/not_included\");\r\n    configMap.put(\"yarn.resources.myResource4.local.name\", \"notExisting\");\r\n    configMap.put(\"yarn.resources.myResource4.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource4.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n    Map<String, LocalResource> resourceMap = mapper.getResourceMap();\r\n    assertEquals(\"resourceMap has 3 resources\", 3, resourceMap.size());\r\n    // resource1\r\n    assertEquals(\"host1.com\", resourceMap.get(\"readme\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"readme\").getType());\r\n    assertEquals(LocalResourceVisibility.PUBLIC, resourceMap.get(\"readme\").getVisibility());\r\n    // resource 2\r\n    assertEquals(\"host2.com\", resourceMap.get(\"__package\").getResource().getHost());\r\n    assertEquals(LocalResourceType.ARCHIVE, resourceMap.get(\"__package\").getType());\r\n    assertEquals(LocalResourceVisibility.PRIVATE, resourceMap.get(\"__package\").getVisibility());\r\n    // resource 3\r\n    assertEquals(\"host3.com\", resourceMap.get(\"csr\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"csr\").getType());\r\n    assertEquals(LocalResourceVisibility.APPLICATION, resourceMap.get(\"csr\").getVisibility());\r\n    // resource 4 should not exist\r\n    assertNull(\"Resource does not exist with the name myResource4\", resourceMap.get(\"myResource4\"));\r\n    assertNull(\"Resource does not exist with the defined config name notExisting for myResource4 either\", resourceMap.get(\"notExisting\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithDefaultValues",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithDefaultValues() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n    Map<String, LocalResource> resourceMap = mapper.getResourceMap();\r\n    assertNull(\"Resource does not exist with a name readme\", resourceMap.get(\"readme\"));\r\n    assertNotNull(\"Resource exists with a name myResource1\", resourceMap.get(\"myResource1\"));\r\n    assertEquals(\"host1.com\", resourceMap.get(\"myResource1\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"myResource1\").getType());\r\n    assertEquals(LocalResourceVisibility.APPLICATION, resourceMap.get(\"myResource1\").getVisibility());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithFileStatusFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithFileStatusFailure() {\r\n    thrown.expect(LocalizerResourceException.class);\r\n    thrown.expectMessage(\"IO Exception when accessing the resource file status from the filesystem\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"unknown://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithInvalidVisibilityFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithInvalidVisibilityFailure() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceVisibility.INVALIDVISIBILITY\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"invalidVisibility\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithInvalidTypeFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithInvalidTypeFailure() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceType.INVALIDTYPE\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testErrorInStartContainerShouldUpdateState",
  "sourceCode" : "@Test\r\npublic void testErrorInStartContainerShouldUpdateState() {\r\n    // create mocks\r\n    final int samzaContainerId = 1;\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnAppState.pendingProcessors.put(String.valueOf(samzaContainerId), new YarnContainer(Container.newInstance(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(10000L, 1), 1), 1), NodeId.newInstance(\"host1\", 8088), \"http://host1\", Resource.newInstance(1024, 1), Priority.newInstance(1), Token.newInstance(\"id\".getBytes(), \"read\", \"password\".getBytes(), \"service\"))));\r\n    yarnClusterResourceManager.start();\r\n    assertEquals(1, yarnAppState.pendingProcessors.size());\r\n    yarnClusterResourceManager.onStartContainerError(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(10000L, 1), 1), 1), new Exception());\r\n    assertEquals(0, yarnAppState.pendingProcessors.size());\r\n    verify(callback, times(1)).onStreamProcessorLaunchFailure(anyObject(), any(Exception.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAllocatedResourceExpiryForYarn",
  "sourceCode" : "@Test\r\npublic void testAllocatedResourceExpiryForYarn() {\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource allocatedResource = mock(SamzaResource.class);\r\n    when(allocatedResource.getTimestamp()).thenReturn(System.currentTimeMillis() - Duration.ofMinutes(10).toMillis());\r\n    Assert.assertTrue(yarnClusterResourceManager.isResourceExpired(allocatedResource));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMShutdownOnRMCallback",
  "sourceCode" : "@Test\r\npublic void testAMShutdownOnRMCallback() throws IOException, YarnException {\r\n    // create mocks\r\n    SamzaYarnAppMasterLifecycle lifecycle = Mockito.spy(new SamzaYarnAppMasterLifecycle(512, 2, mock(SamzaApplicationState.class), yarnAppState, asyncClient, false));\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.onShutdownRequest();\r\n    verify(lifecycle, times(1)).onShutdown(SamzaApplicationState.SamzaAppStatus.FAILED);\r\n    verify(asyncClient, times(1)).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    verify(asyncClient, times(1)).stop();\r\n    verify(asyncNMClient, times(1)).stop();\r\n    verify(service, times(1)).onShutdown();\r\n    verify(metrics, times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMShutdownThrowingExceptionOnRMCallback",
  "sourceCode" : "@Test\r\npublic void testAMShutdownThrowingExceptionOnRMCallback() throws IOException, YarnException {\r\n    // create mocks\r\n    SamzaYarnAppMasterLifecycle lifecycle = Mockito.spy(new SamzaYarnAppMasterLifecycle(512, 2, mock(SamzaApplicationState.class), yarnAppState, asyncClient, false));\r\n    doThrow(InvalidApplicationMasterRequestException.class).when(asyncClient).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.onShutdownRequest();\r\n    verify(lifecycle, times(1)).onShutdown(SamzaApplicationState.SamzaAppStatus.FAILED);\r\n    verify(asyncClient, times(1)).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    verify(asyncClient, times(1)).stop();\r\n    verify(asyncNMClient, times(1)).stop();\r\n    verify(service, times(1)).onShutdown();\r\n    verify(metrics, times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMHACallbackInvokedForPreviousAttemptContainers",
  "sourceCode" : "@Test\r\npublic void testAMHACallbackInvokedForPreviousAttemptContainers() {\r\n    String previousAttemptContainerId = \"0\";\r\n    String previousAttemptYarnContainerId = \"container_1607304997422_0008_02_000002\";\r\n    // create mocks\r\n    YarnAppState yarnAppState = Mockito.spy(new YarnAppState(0, mock(ContainerId.class), \"host\", 8080, 8081));\r\n    ContainerId containerId = mock(ContainerId.class);\r\n    when(containerId.toString()).thenReturn(previousAttemptYarnContainerId);\r\n    YarnContainer yarnContainer = mock(YarnContainer.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(resource.getMemory()).thenReturn(1024);\r\n    Mockito.when(resource.getVirtualCores()).thenReturn(1);\r\n    Mockito.when(yarnContainer.resource()).thenReturn(resource);\r\n    Mockito.when(yarnContainer.id()).thenReturn(containerId);\r\n    NodeId nodeId = mock(NodeId.class);\r\n    when(nodeId.getHost()).thenReturn(\"host\");\r\n    when(yarnContainer.nodeId()).thenReturn(nodeId);\r\n    yarnAppState.pendingProcessors.put(previousAttemptContainerId, yarnContainer);\r\n    Set<ContainerId> previousAttemptContainers = new HashSet<>();\r\n    previousAttemptContainers.add(containerId);\r\n    when(lifecycle.onInit()).thenReturn(previousAttemptContainers);\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(JobConfig.YARN_AM_HIGH_AVAILABILITY_ENABLED, \"true\");\r\n    Config config = new MapConfig(configMap);\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.start();\r\n    verify(lifecycle).onInit();\r\n    ArgumentCaptor<SamzaResource> samzaResourceArgumentCaptor = ArgumentCaptor.forClass(SamzaResource.class);\r\n    verify(callback).onStreamProcessorLaunchSuccess(samzaResourceArgumentCaptor.capture());\r\n    ArgumentCaptor<Integer> containerFromPreviousAttemptCaptor = ArgumentCaptor.forClass(Integer.class);\r\n    verify(metrics).setContainersFromPreviousAttempts(containerFromPreviousAttemptCaptor.capture());\r\n    SamzaResource samzaResource = samzaResourceArgumentCaptor.getValue();\r\n    assertEquals(previousAttemptYarnContainerId, samzaResource.getContainerId());\r\n    assertEquals(1, containerFromPreviousAttemptCaptor.getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testStopStreamProcessorForContainerFromPreviousAttempt",
  "sourceCode" : "@Test\r\npublic void testStopStreamProcessorForContainerFromPreviousAttempt() {\r\n    String containerId = \"Yarn_Container_id_0\";\r\n    String processorId = \"Container_id_0\";\r\n    YarnContainer runningYarnContainer = mock(YarnContainer.class);\r\n    ContainerId previousRunningContainerId = mock(ContainerId.class);\r\n    YarnAppState yarnAppState = Mockito.spy(new YarnAppState(0, mock(ContainerId.class), \"host\", 8080, 8081));\r\n    yarnAppState.runningProcessors.put(processorId, runningYarnContainer);\r\n    when(runningYarnContainer.id()).thenReturn(previousRunningContainerId);\r\n    when(previousRunningContainerId.toString()).thenReturn(containerId);\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource containerResourceFromPreviousRun = mock(SamzaResource.class);\r\n    when(containerResourceFromPreviousRun.getContainerId()).thenReturn(containerId);\r\n    yarnClusterResourceManager.stopStreamProcessor(containerResourceFromPreviousRun);\r\n    verify(asyncClient, times(1)).releaseAssignedContainer(previousRunningContainerId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testStopStreamProcessorForContainerStartedInCurrentLifecycle",
  "sourceCode" : "@Test\r\npublic void testStopStreamProcessorForContainerStartedInCurrentLifecycle() {\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource allocatedContainerResource = mock(SamzaResource.class);\r\n    Container runningContainer = mock(Container.class);\r\n    ContainerId runningContainerId = mock(ContainerId.class);\r\n    NodeId runningNodeId = mock(NodeId.class);\r\n    when(runningContainer.getId()).thenReturn(runningContainerId);\r\n    when(runningContainer.getNodeId()).thenReturn(runningNodeId);\r\n    yarnClusterResourceManager.getAllocatedResources().put(allocatedContainerResource, runningContainer);\r\n    yarnClusterResourceManager.stopStreamProcessor(allocatedContainerResource);\r\n    verify(asyncNMClient, times(1)).stopContainerAsync(runningContainerId, runningNodeId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testIncrementAllocatedContainersInBuffer",
  "sourceCode" : "@Test\r\npublic void testIncrementAllocatedContainersInBuffer() {\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.start();\r\n    Container allocatedContainer = mock(Container.class);\r\n    ContainerId allocatedContainerId = mock(ContainerId.class);\r\n    NodeId allocatedNodeId = mock(NodeId.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(allocatedNodeId.getHost()).thenReturn(\"fake_host\");\r\n    when(resource.getVirtualCores()).thenReturn(1);\r\n    when(resource.getMemory()).thenReturn(1024);\r\n    when(allocatedContainer.getId()).thenReturn(allocatedContainerId);\r\n    when(allocatedContainer.getNodeId()).thenReturn(allocatedNodeId);\r\n    when(allocatedContainer.getResource()).thenReturn(resource);\r\n    yarnClusterResourceManager.onContainersAllocated(ImmutableList.of(allocatedContainer));\r\n    verify(metrics).incrementAllocatedContainersInBuffer();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testDecrementAllocatedContainersInBuffer",
  "sourceCode" : "@Test\r\npublic void testDecrementAllocatedContainersInBuffer() {\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.start();\r\n    SamzaResource allocatedContainerResource = mock(SamzaResource.class);\r\n    Container runningContainer = mock(Container.class);\r\n    ContainerId runningContainerId = mock(ContainerId.class);\r\n    NodeId runningNodeId = mock(NodeId.class);\r\n    when(runningContainer.getId()).thenReturn(runningContainerId);\r\n    when(runningContainer.getNodeId()).thenReturn(runningNodeId);\r\n    yarnClusterResourceManager.getAllocatedResources().put(allocatedContainerResource, runningContainer);\r\n    yarnClusterResourceManager.releaseResources(allocatedContainerResource);\r\n    verify(metrics).decrementAllocatedContainersInBuffer();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testGetFaultDomainOfHostWhichExistsInCache",
  "sourceCode" : "@Test\r\npublic void testGetFaultDomainOfHostWhichExistsInCache() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    Set<FaultDomain> expectedFaultDomainSet = new HashSet<>();\r\n    expectedFaultDomainSet.add(new FaultDomain(FaultDomainType.RACK, rackName1));\r\n    Set<FaultDomain> actualFaultDomainSet = yarnFaultDomainManager.getFaultDomainsForHost(hostName3);\r\n    assertNotNull(actualFaultDomainSet);\r\n    assertEquals(expectedFaultDomainSet.iterator().next(), actualFaultDomainSet.iterator().next());\r\n    verify(mockCounter, times(0)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testGetFaultDomainOfHostWhichDoesNotExistInCache",
  "sourceCode" : "@Test\r\npublic void testGetFaultDomainOfHostWhichDoesNotExistInCache() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    Set<FaultDomain> expectedFaultDomainSet = new HashSet<>();\r\n    expectedFaultDomainSet.add(new FaultDomain(FaultDomainType.RACK, rackName1));\r\n    List<NodeReport> updatedNodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5, nodeReport6);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(updatedNodeReport);\r\n    Set<FaultDomain> actualFaultDomainSet = yarnFaultDomainManager.getFaultDomainsForHost(hostName6);\r\n    assertNotNull(actualFaultDomainSet);\r\n    assertEquals(expectedFaultDomainSet.iterator().next(), actualFaultDomainSet.iterator().next());\r\n    verify(mockCounter, times(1)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenTrue",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenTrue() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName3);\r\n    assertTrue(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenFalse",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenFalse() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName2);\r\n    assertFalse(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenHostDoesNotExistInCache",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenHostDoesNotExistInCache() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    List<NodeReport> updatedNodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5, nodeReport6);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(updatedNodeReport);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName6);\r\n    assertTrue(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testComputeHostToFaultDomainMap",
  "sourceCode" : "@Test\r\npublic void testComputeHostToFaultDomainMap() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, null);\r\n    List<NodeReport> nodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(nodeReport);\r\n    Multimap<String, FaultDomain> hostToRackMap = yarnFaultDomainManager.computeHostToFaultDomainMap();\r\n    assertEquals(this.hostToRackMap.size(), hostToRackMap.size());\r\n    assertEquals(this.hostToRackMap.keySet(), hostToRackMap.keySet());\r\n    Iterator<FaultDomain> expectedValues = this.hostToRackMap.values().iterator();\r\n    Iterator<FaultDomain> computedValues = hostToRackMap.values().iterator();\r\n    expectedValues.forEachRemaining(expectedRack -> assertFaultDomainEquals(expectedRack, computedValues.next()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildEnvironment",
  "sourceCode" : "@Test\r\npublic void testBuildEnvironment() throws IOException {\r\n    String amJvmOptions = \"-Xmx1g -Dconfig.key='config value'\";\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.JOB_COORDINATOR_SYSTEM, \"jobCoordinatorSystem\").put(YarnConfig.AM_JVM_OPTIONS, // needs escaping\r\n    amJvmOptions).build());\r\n    String expectedCoordinatorStreamConfigStringValue = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(CoordinatorStreamUtil.buildCoordinatorStreamConfig(config)));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_COORDINATOR_SYSTEM_CONFIG, expectedCoordinatorStreamConfigStringValue, ShellCommandConfig.ENV_JAVA_OPTS, Util.envVarEscape(amJvmOptions), ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildEnvironmentWithAMJavaHome",
  "sourceCode" : "@Test\r\npublic void testBuildEnvironmentWithAMJavaHome() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.JOB_COORDINATOR_SYSTEM, \"jobCoordinatorSystem\").put(YarnConfig.AM_JVM_OPTIONS, \"\").put(YarnConfig.AM_JAVA_HOME, \"/some/path/to/java/home\").build());\r\n    String expectedCoordinatorStreamConfigStringValue = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(CoordinatorStreamUtil.buildCoordinatorStreamConfig(config)));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_COORDINATOR_SYSTEM_CONFIG, expectedCoordinatorStreamConfigStringValue, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_JAVA_HOME, \"/some/path/to/java/home\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildJobSubmissionEnvironment",
  "sourceCode" : "@Test\r\npublic void testBuildJobSubmissionEnvironment() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\").put(YarnConfig.AM_JVM_OPTIONS, \"\").build());\r\n    String expectedSubmissionConfig = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(config));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_SUBMISSION_CONFIG, expectedSubmissionConfig, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildJobWithAdditionalClassPath",
  "sourceCode" : "@Test\r\npublic void testBuildJobWithAdditionalClassPath() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\").put(YarnConfig.AM_JVM_OPTIONS, \"\").put(ShellCommandConfig.ADDITIONAL_CLASSPATH_DIR, \"./sqlapp/lib/*\").build());\r\n    String expectedSubmissionConfig = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(config));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_SUBMISSION_CONFIG, expectedSubmissionConfig, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"./sqlapp/lib/*\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithDefaultFsImpl",
  "sourceCode" : "@Test\r\npublic void testGetJobWithDefaultFsImpl() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig());\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(HttpFileSystem.class.getName(), hConfig.get(\"fs.http.impl\"));\r\n    assertEquals(HttpFileSystem.class.getName(), hConfig.get(\"fs.https.impl\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithFsImplOverride",
  "sourceCode" : "@Test\r\npublic void testGetJobWithFsImplOverride() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig(ImmutableMap.of(\"fs.http.impl\", \"org.apache.myHttp\", \"fs.myscheme.impl\", \"org.apache.myScheme\")));\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(\"org.apache.myHttp\", hConfig.get(\"fs.http.impl\"));\r\n    assertEquals(\"org.apache.myScheme\", hConfig.get(\"fs.myscheme.impl\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithFsImplSubkeys",
  "sourceCode" : "@Test\r\npublic void testGetJobWithFsImplSubkeys() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig(ImmutableMap.of(\"fs.myscheme.impl\", \"org.apache.myScheme\", \"fs.myscheme.impl.client\", \"org.apache.mySchemeClient\")));\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(\"org.apache.myScheme\", hConfig.get(\"fs.myscheme.impl\"));\r\n    assertEquals(\"org.apache.mySchemeClient\", hConfig.get(\"fs.myscheme.impl.client\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\util\\hadoop\\TestHttpFileSystem.java",
  "methodName" : "testHttpFileSystemReadTimeouts",
  "sourceCode" : "@Test\r\npublic void testHttpFileSystemReadTimeouts() throws Exception {\r\n    HttpServer server = new HttpServer(\"/\", 0, null, new ServletHolder(DefaultServlet.class));\r\n    try {\r\n        server.addServlet(\"/download\", new PartialFileFetchServlet());\r\n        server.start();\r\n        String serverUrl = server.getUrl().toString() + \"download\";\r\n        FileSystemClientThread fileSystemClientThread = new FileSystemClientThread(new URI(serverUrl));\r\n        fileSystemClientThread.start();\r\n        fileSystemClientThread.join();\r\n        Assert.assertEquals(fileSystemClientThread.getTotalBytesRead(), THRESHOLD_BYTES);\r\n        Assert.assertNull(clientException);\r\n        Assert.assertNull(serverException);\r\n    } finally {\r\n        server.stop();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateAppId",
  "sourceCode" : "@Test\r\npublic void testValidateAppId() throws Exception {\r\n    ApplicationReport appReport = mock(ApplicationReport.class);\r\n    when(appReport.getName()).thenReturn(jobName + \"_\" + jobId);\r\n    when(appReport.getApplicationId()).thenReturn(appId);\r\n    when(client.getApplications()).thenReturn(Collections.singletonList(appReport));\r\n    assertTrue(tool.validateAppId().equals(appId));\r\n    when(appReport.getName()).thenReturn(\"dummy\");\r\n    exception.expect(SamzaException.class);\r\n    tool.validateAppId();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateRunningAttemptId",
  "sourceCode" : "@Test\r\npublic void testValidateRunningAttemptId() throws Exception {\r\n    ApplicationReport appReport = mock(ApplicationReport.class);\r\n    when(client.getApplicationReport(appId)).thenReturn(appReport);\r\n    when(appReport.getCurrentApplicationAttemptId()).thenReturn(attemptId);\r\n    ApplicationAttemptReport attemptReport = mock(ApplicationAttemptReport.class);\r\n    when(attemptReport.getYarnApplicationAttemptState()).thenReturn(YarnApplicationAttemptState.RUNNING);\r\n    when(attemptReport.getApplicationAttemptId()).thenReturn(attemptId);\r\n    when(client.getApplicationAttemptReport(attemptId)).thenReturn(attemptReport);\r\n    assertTrue(tool.validateRunningAttemptId(appId).equals(attemptId));\r\n    when(attemptReport.getYarnApplicationAttemptState()).thenReturn(YarnApplicationAttemptState.FAILED);\r\n    exception.expect(SamzaException.class);\r\n    tool.validateRunningAttemptId(appId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateContainerCount",
  "sourceCode" : "@Test\r\npublic void testValidateContainerCount() throws Exception {\r\n    List<ContainerReport> containerReports = new ArrayList<>();\r\n    for (int i = 0; i <= containerCount; i++) {\r\n        ContainerReport report = mock(ContainerReport.class);\r\n        when(report.getContainerState()).thenReturn(ContainerState.RUNNING);\r\n        containerReports.add(report);\r\n    }\r\n    when(client.getContainers(attemptId)).thenReturn(containerReports);\r\n    assertTrue(tool.validateContainerCount(attemptId) == (containerCount + 1));\r\n    containerReports.remove(0);\r\n    exception.expect(SamzaException.class);\r\n    tool.validateContainerCount(attemptId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateJmxMetrics",
  "sourceCode" : "@Test\r\npublic void testValidateJmxMetrics() throws MetricsValidationFailureException {\r\n    JmxMetricsAccessor jmxMetricsAccessor = mock(JmxMetricsAccessor.class);\r\n    Map<String, Long> values = new HashMap<>();\r\n    values.put(\"samza-container-0\", 100L);\r\n    when(jmxMetricsAccessor.getCounterValues(SamzaContainerMetrics.class.getName(), \"commit-calls\")).thenReturn(values);\r\n    validator.validate(jmxMetricsAccessor);\r\n    values.put(\"samza-container-0\", -1L);\r\n    // the mock validator will fail if the commit-calls are less than or equal to 0\r\n    exception.expect(MetricsValidationFailureException.class);\r\n    validator.validate(jmxMetricsAccessor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetMetricsSuccess",
  "sourceCode" : "@Test\r\npublic void testGetMetricsSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    MetricsRegistryMap registry = new MetricsRegistryMap();\r\n    assignMetricValues(samzaAppState, registry);\r\n    String response = ApplicationMasterRestServlet.getMetrics(jsonMapper, registry);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Map<String, Object>> metricsResult = client.getMetrics();\r\n    String group = SamzaAppMasterMetrics.class.getCanonicalName();\r\n    assertEquals(1, metricsResult.size());\r\n    assertTrue(metricsResult.containsKey(group));\r\n    Map<String, Object> amMetricsGroup = metricsResult.get(group);\r\n    assertEquals(9, amMetricsGroup.size());\r\n    assertEquals(samzaAppState.runningProcessors.size(), amMetricsGroup.get(\"running-containers\"));\r\n    assertEquals(samzaAppState.neededProcessors.get(), amMetricsGroup.get(\"needed-containers\"));\r\n    assertEquals(samzaAppState.completedProcessors.get(), amMetricsGroup.get(\"completed-containers\"));\r\n    assertEquals(samzaAppState.failedContainers.get(), amMetricsGroup.get(\"failed-containers\"));\r\n    assertEquals(samzaAppState.releasedContainers.get(), amMetricsGroup.get(\"released-containers\"));\r\n    assertEquals(samzaAppState.processorCount.get(), amMetricsGroup.get(\"container-count\"));\r\n    assertEquals(samzaAppState.jobHealthy.get() ? 1 : 0, amMetricsGroup.get(\"job-healthy\"));\r\n    assertEquals(0, amMetricsGroup.get(\"container-from-previous-attempt\"));\r\n    assertEquals(0, amMetricsGroup.get(\"allocated-containers-in-buffer\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetMetricsError",
  "sourceCode" : "@Test\r\npublic void testGetMetricsError() throws IOException {\r\n    setupErrorTest(\"metrics\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetTaskContextSuccess",
  "sourceCode" : "@Test\r\npublic void testGetTaskContextSuccess() throws IOException {\r\n    ContainerId containerId = ConverterUtils.toContainerId(YARN_CONTAINER_ID_1);\r\n    YarnAppState yarnAppState = createYarnAppState(containerId);\r\n    String response = ApplicationMasterRestServlet.getTaskContext(jsonMapper, yarnAppState);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> taskContextResult = client.getTaskContext();\r\n    assertEquals(2, taskContextResult.size());\r\n    assertEquals(2, taskContextResult.get(\"task-id\"));\r\n    assertEquals(containerId.toString(), taskContextResult.get(\"name\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testTaskContextError",
  "sourceCode" : "@Test\r\npublic void testTaskContextError() throws IOException {\r\n    setupErrorTest(\"task context\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getTaskContext();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetAmStateSuccess",
  "sourceCode" : "@Test\r\npublic void testGetAmStateSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    ApplicationAttemptId attemptId = ConverterUtils.toApplicationAttemptId(APP_ATTEMPT_ID);\r\n    ContainerId containerId = ConverterUtils.toContainerId(YARN_CONTAINER_ID_1);\r\n    YarnAppState yarnAppState = createYarnAppState(containerId);\r\n    String response = ApplicationMasterRestServlet.getAmState(jsonMapper, samzaAppState, yarnAppState);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> amStateResult = client.getAmState();\r\n    assertEquals(4, amStateResult.size());\r\n    assertEquals(String.format(\"%s:%s\", yarnAppState.nodeHost, yarnAppState.rpcUrl.getPort()), amStateResult.get(\"host\"));\r\n    assertEquals(containerId.toString(), amStateResult.get(\"container-id\"));\r\n    // Can only validate the keys because up-time changes everytime it's requested\r\n    assertEquals(buildExpectedContainerResponse(yarnAppState.runningProcessors, samzaAppState).keySet(), ((Map<String, Object>) amStateResult.get(\"containers\")).keySet());\r\n    assertEquals(attemptId.toString(), amStateResult.get(\"app-attempt-id\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetAmStateError",
  "sourceCode" : "@Test\r\npublic void testGetAmStateError() throws IOException {\r\n    setupErrorTest(\"AM state\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getAmState();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetConfigSuccess",
  "sourceCode" : "@Test\r\npublic void testGetConfigSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    Map<String, String> configMap = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\r\n    Config config = new MapConfig(configMap);\r\n    String response = ApplicationMasterRestServlet.getConfig(jsonMapper, config);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> configResult = client.getConfig();\r\n    assertEquals(configMap, configResult);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetConfigError",
  "sourceCode" : "@Test\r\npublic void testGetConfigError() throws IOException {\r\n    setupErrorTest(\"config\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getConfig();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testCloseMethodClosesHttpClient",
  "sourceCode" : "@Test\r\npublic void testCloseMethodClosesHttpClient() throws IOException {\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.close();\r\n    verify(mockClient).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadContainerLocality",
  "sourceCode" : "@Test\r\npublic void testReadContainerLocality() throws Exception {\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality\");\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    LocalityModel locality = mapper.readValue(response, LocalityModel.class);\r\n    assertEquals(\"Expected locality for two containers\", 2, locality.getProcessorLocalities().size());\r\n    assertEquals(\"Mismatch in locality for processor \" + PROCESSOR_ID1, locality.getProcessorLocality(PROCESSOR_ID1), PROCESSOR_1_LOCALITY);\r\n    assertEquals(\"Mismatch in locality for processor \" + PROCESSOR_ID2, locality.getProcessorLocality(PROCESSOR_ID2), PROCESSOR_2_LOCALITY);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadContainerLocalityWithNoLocality",
  "sourceCode" : "@Test\r\npublic void testReadContainerLocalityWithNoLocality() throws Exception {\r\n    final LocalityModel expectedLocality = new LocalityModel(Collections.emptyMap());\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality\");\r\n    when(localityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of()));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    LocalityModel locality = mapper.readValue(response, LocalityModel.class);\r\n    assertEquals(\"Expected empty response but got \" + locality, locality, expectedLocality);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadProcessorLocality",
  "sourceCode" : "@Test\r\npublic void testReadProcessorLocality() throws Exception {\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality?processorId=\" + PROCESSOR_ID1);\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    assertEquals(\"Mismatch in the locality for processor \" + PROCESSOR_ID1, mapper.readValue(response, ProcessorLocality.class), PROCESSOR_1_LOCALITY);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadProcessorLocalityWithNoLocality",
  "sourceCode" : "@Test\r\npublic void testReadProcessorLocalityWithNoLocality() throws Exception {\r\n    final ProcessorLocality expectedProcessorLocality = new ProcessorLocality(PROCESSOR_ID2, \"\");\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality?processorId=\" + PROCESSOR_ID2);\r\n    when(localityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of()));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    ProcessorLocality processorLocality = mapper.readValue(response, ProcessorLocality.class);\r\n    assertEquals(\"Expected empty response for processor locality \" + PROCESSOR_ID2 + \" but got \" + processorLocality, processorLocality, expectedProcessorLocality);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestYarnContainerHeartbeatServlet.java",
  "methodName" : "testContainerHeartbeatWhenValid",
  "sourceCode" : "@Test\r\npublic void testContainerHeartbeatWhenValid() throws IOException {\r\n    String validContainerId = \"container_1350670447861_0003_01_000002\";\r\n    when(container.id()).thenReturn(ConverterUtils.toContainerId(validContainerId));\r\n    yarnAppState.runningProcessors.put(validContainerId, container);\r\n    URL url = new URL(String.format(CoordinationConstants.YARN_CONTAINER_HEARTBEAT_ENDPOINT_FORMAT, webApp.getUrl().toString(), validContainerId));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    heartbeat = mapper.readValue(response, ContainerHeartbeatResponse.class);\r\n    Assert.assertTrue(heartbeat.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn\\src\\test\\java\\org\\apache\\samza\\webapp\\TestYarnContainerHeartbeatServlet.java",
  "methodName" : "testContainerHeartbeatWhenInvalid",
  "sourceCode" : "@Test\r\npublic void testContainerHeartbeatWhenInvalid() throws IOException {\r\n    String validContainerId = \"container_1350670447861_0003_01_000003\";\r\n    String invalidContainerId = \"container_1350670447861_0003_01_000002\";\r\n    when(container.id()).thenReturn(ConverterUtils.toContainerId(validContainerId));\r\n    yarnAppState.runningProcessors.put(validContainerId, container);\r\n    URL url = new URL(String.format(CoordinationConstants.YARN_CONTAINER_HEARTBEAT_ENDPOINT_FORMAT, webApp.getUrl().toString(), invalidContainerId));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    heartbeat = mapper.readValue(response, ContainerHeartbeatResponse.class);\r\n    Assert.assertFalse(heartbeat.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\main\\java\\org\\apache\\samza\\job\\yarn\\YarnClusterResourceManager.java",
  "methodName" : "getAllocatedResources",
  "sourceCode" : "@VisibleForTesting\r\nConcurrentHashMap<SamzaResource, Container> getAllocatedResources() {\r\n    return allocatedResources;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\main\\java\\org\\apache\\samza\\job\\yarn\\YarnFaultDomainManager.java",
  "methodName" : "computeHostToFaultDomainMap",
  "sourceCode" : "/**\r\n * This method computes the host to rack map from Yarn.\r\n * Only the hosts that are running in the cluster will be a part of this map.\r\n * @return map of the host and the rack it resides on\r\n */\r\n@VisibleForTesting\r\nMultimap<String, FaultDomain> computeHostToFaultDomainMap() {\r\n    Multimap<String, FaultDomain> hostToRackMap = HashMultimap.create();\r\n    try {\r\n        List<NodeReport> nodeReport = yarnClient.getNodeReports(NodeState.RUNNING);\r\n        nodeReport.forEach(report -> {\r\n            FaultDomain rack = new FaultDomain(FaultDomainType.RACK, report.getRackName());\r\n            hostToRackMap.put(report.getNodeId().getHost(), rack);\r\n        });\r\n        log.info(\"Computed the host to rack map successfully from Yarn.\");\r\n    } catch (YarnException | IOException e) {\r\n        throw new SamzaException(\"Yarn threw an exception while getting NodeReports.\", e);\r\n    }\r\n    return hostToRackMap;\r\n}",
  "annotations" : [ "VisibleForTesting" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\config\\TestYarnConfig.java",
  "methodName" : "testGetPackagePath",
  "sourceCode" : "@Test\r\npublic void testGetPackagePath() {\r\n    String packagePath = \"http://some.package.path\";\r\n    Config config = new MapConfig(Collections.singletonMap(YarnConfig.PACKAGE_PATH, packagePath));\r\n    assertEquals(packagePath, new YarnConfig(config).getPackagePath());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\config\\TestYarnConfig.java",
  "methodName" : "testGetPackagePathMissingConfig",
  "sourceCode" : "@Test(expected = SamzaException.class)\r\npublic void testGetPackagePathMissingConfig() {\r\n    new YarnConfig(new MapConfig()).getPackagePath();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testFileSystemImplConfigSuccess",
  "sourceCode" : "@Test\r\npublic void testFileSystemImplConfigSuccess() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\");\r\n    configMap.put(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    Config conf = new MapConfig(configMap);\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(conf);\r\n    assertEquals(2, manager.getSchemes().size());\r\n    assertEquals(\"http\", manager.getSchemes().get(0));\r\n    assertEquals(\"myscheme\", manager.getSchemes().get(1));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testNullConfig",
  "sourceCode" : "@Test\r\npublic void testNullConfig() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"config cannot be null\");\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestFileSystemImplConfig.java",
  "methodName" : "testSchemeWithSubkeys",
  "sourceCode" : "@Test\r\npublic void testSchemeWithSubkeys() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\");\r\n    configMap.put(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    configMap.put(\"fs.http.impl.key1\", \"val1\");\r\n    configMap.put(\"fs.http.impl.key2\", \"val2\");\r\n    Config conf = new MapConfig(configMap);\r\n    FileSystemImplConfig manager = new FileSystemImplConfig(conf);\r\n    Map<String, String> expectedFsHttpImplConfs = //Scheme with additional subkeys\r\n    ImmutableMap.//Scheme with additional subkeys\r\n    of(\"fs.http.impl\", \"org.apache.samza.HttpFileSystem\", \"fs.http.impl.key1\", \"val1\", \"fs.http.impl.key2\", \"val2\");\r\n    Map<String, String> expectedFsMyschemeImplConfs = // Scheme without subkeys\r\n    ImmutableMap.// Scheme without subkeys\r\n    of(\"fs.myscheme.impl\", \"org.apache.samza.MySchemeFileSystem\");\r\n    assertEquals(Arrays.asList(\"http\", \"myscheme\"), manager.getSchemes());\r\n    assertEquals(expectedFsHttpImplConfs, manager.getSchemeConfig(\"http\"));\r\n    assertEquals(expectedFsMyschemeImplConfs, manager.getSchemeConfig(\"myscheme\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testResourceConfigIncluded",
  "sourceCode" : "@Test\r\npublic void testResourceConfigIncluded() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    assertEquals(1, manager.getResourceNames().size());\r\n    assertEquals(\"myResource1\", manager.getResourceNames().get(0));\r\n    assertEquals(\"readme\", manager.getResourceLocalName(\"myResource1\"));\r\n    assertEquals(LocalResourceType.FILE, manager.getResourceLocalType(\"myResource1\"));\r\n    assertEquals(LocalResourceVisibility.PUBLIC, manager.getResourceLocalVisibility(\"myResource1\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testResourcrConfigNotIncluded",
  "sourceCode" : "@Test\r\npublic void testResourcrConfigNotIncluded() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"otherconfig\", \"https://host2.com/not_included\");\r\n    configMap.put(\"yarn.resources.myResource2.local.name\", \"notExisting\");\r\n    configMap.put(\"yarn.resources.myResource2.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource2.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    assertEquals(0, manager.getResourceNames().size());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testNullConfig",
  "sourceCode" : "@Test\r\npublic void testNullConfig() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"config cannot be null\");\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(null);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidVisibility",
  "sourceCode" : "@Test\r\npublic void testInvalidVisibility() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceVisibility.INVALIDVISIBILITY\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"invalidVisibility\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourceLocalVisibility(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidType",
  "sourceCode" : "@Test\r\npublic void testInvalidType() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceType.INVALIDTYPE\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourceLocalType(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceConfig.java",
  "methodName" : "testInvalidPath",
  "sourceCode" : "@Test\r\npublic void testInvalidPath() {\r\n    thrown.expect(LocalizerResourceException.class);\r\n    thrown.expectMessage(\"resource path is required but not defined in config for resource myResource1\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    LocalizerResourceConfig manager = new LocalizerResourceConfig(conf);\r\n    manager.getResourcePath(\"myResource1\");\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapSuccess",
  "sourceCode" : "@Test\r\npublic void testResourceMapSuccess() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    configMap.put(\"yarn.resources.myResource2.path\", \"https://host2.com/package\");\r\n    configMap.put(\"yarn.resources.myResource2.local.name\", \"__package\");\r\n    configMap.put(\"yarn.resources.myResource2.local.type\", \"archive\");\r\n    configMap.put(\"yarn.resources.myResource2.local.visibility\", \"private\");\r\n    configMap.put(\"yarn.resources.myResource3.path\", \"https://host3.com/csr\");\r\n    configMap.put(\"yarn.resources.myResource3.local.name\", \"csr\");\r\n    configMap.put(\"yarn.resources.myResource3.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource3.local.visibility\", \"application\");\r\n    configMap.put(\"otherconfig\", \"https://host4.com/not_included\");\r\n    configMap.put(\"yarn.resources.myResource4.local.name\", \"notExisting\");\r\n    configMap.put(\"yarn.resources.myResource4.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource4.local.visibility\", \"application\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n    Map<String, LocalResource> resourceMap = mapper.getResourceMap();\r\n    assertEquals(\"resourceMap has 3 resources\", 3, resourceMap.size());\r\n    // resource1\r\n    assertEquals(\"host1.com\", resourceMap.get(\"readme\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"readme\").getType());\r\n    assertEquals(LocalResourceVisibility.PUBLIC, resourceMap.get(\"readme\").getVisibility());\r\n    // resource 2\r\n    assertEquals(\"host2.com\", resourceMap.get(\"__package\").getResource().getHost());\r\n    assertEquals(LocalResourceType.ARCHIVE, resourceMap.get(\"__package\").getType());\r\n    assertEquals(LocalResourceVisibility.PRIVATE, resourceMap.get(\"__package\").getVisibility());\r\n    // resource 3\r\n    assertEquals(\"host3.com\", resourceMap.get(\"csr\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"csr\").getType());\r\n    assertEquals(LocalResourceVisibility.APPLICATION, resourceMap.get(\"csr\").getVisibility());\r\n    // resource 4 should not exist\r\n    assertNull(\"Resource does not exist with the name myResource4\", resourceMap.get(\"myResource4\"));\r\n    assertNull(\"Resource does not exist with the defined config name notExisting for myResource4 either\", resourceMap.get(\"notExisting\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithDefaultValues",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithDefaultValues() {\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n    Map<String, LocalResource> resourceMap = mapper.getResourceMap();\r\n    assertNull(\"Resource does not exist with a name readme\", resourceMap.get(\"readme\"));\r\n    assertNotNull(\"Resource exists with a name myResource1\", resourceMap.get(\"myResource1\"));\r\n    assertEquals(\"host1.com\", resourceMap.get(\"myResource1\").getResource().getHost());\r\n    assertEquals(LocalResourceType.FILE, resourceMap.get(\"myResource1\").getType());\r\n    assertEquals(LocalResourceVisibility.APPLICATION, resourceMap.get(\"myResource1\").getVisibility());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithFileStatusFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithFileStatusFailure() {\r\n    thrown.expect(LocalizerResourceException.class);\r\n    thrown.expectMessage(\"IO Exception when accessing the resource file status from the filesystem\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"unknown://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithInvalidVisibilityFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithInvalidVisibilityFailure() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceVisibility.INVALIDVISIBILITY\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"file\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"invalidVisibility\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestLocalizerResourceMapper.java",
  "methodName" : "testResourceMapWithInvalidTypeFailure",
  "sourceCode" : "@Test\r\npublic void testResourceMapWithInvalidTypeFailure() {\r\n    thrown.expect(IllegalArgumentException.class);\r\n    thrown.expectMessage(\"No enum constant org.apache.hadoop.yarn.api.records.LocalResourceType.INVALIDTYPE\");\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(\"yarn.resources.myResource1.path\", \"http://host1.com/readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.name\", \"readme\");\r\n    configMap.put(\"yarn.resources.myResource1.local.type\", \"invalidType\");\r\n    configMap.put(\"yarn.resources.myResource1.local.visibility\", \"public\");\r\n    Config conf = new MapConfig(configMap);\r\n    YarnConfiguration yarnConfiguration = new YarnConfiguration();\r\n    yarnConfiguration.set(\"fs.http.impl\", HttpFileSystem.class.getName());\r\n    yarnConfiguration.set(\"fs.https.impl\", HttpFileSystem.class.getName());\r\n    LocalizerResourceMapper mapper = new LocalizerResourceMapper(new LocalizerResourceConfig(conf), yarnConfiguration);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testErrorInStartContainerShouldUpdateState",
  "sourceCode" : "@Test\r\npublic void testErrorInStartContainerShouldUpdateState() {\r\n    // create mocks\r\n    final int samzaContainerId = 1;\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnAppState.pendingProcessors.put(String.valueOf(samzaContainerId), new YarnContainer(Container.newInstance(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(10000L, 1), 1), 1), NodeId.newInstance(\"host1\", 8088), \"http://host1\", Resource.newInstance(1024, 1), Priority.newInstance(1), Token.newInstance(\"id\".getBytes(), \"read\", \"password\".getBytes(), \"service\"))));\r\n    yarnClusterResourceManager.start();\r\n    assertEquals(1, yarnAppState.pendingProcessors.size());\r\n    yarnClusterResourceManager.onStartContainerError(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(10000L, 1), 1), 1), new Exception());\r\n    assertEquals(0, yarnAppState.pendingProcessors.size());\r\n    verify(callback, times(1)).onStreamProcessorLaunchFailure(anyObject(), any(Exception.class));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAllocatedResourceExpiryForYarn",
  "sourceCode" : "@Test\r\npublic void testAllocatedResourceExpiryForYarn() {\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource allocatedResource = mock(SamzaResource.class);\r\n    when(allocatedResource.getTimestamp()).thenReturn(System.currentTimeMillis() - Duration.ofMinutes(10).toMillis());\r\n    Assert.assertTrue(yarnClusterResourceManager.isResourceExpired(allocatedResource));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMShutdownOnRMCallback",
  "sourceCode" : "@Test\r\npublic void testAMShutdownOnRMCallback() throws IOException, YarnException {\r\n    // create mocks\r\n    SamzaYarnAppMasterLifecycle lifecycle = Mockito.spy(new SamzaYarnAppMasterLifecycle(512, 2, mock(SamzaApplicationState.class), yarnAppState, asyncClient, false));\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.onShutdownRequest();\r\n    verify(lifecycle, times(1)).onShutdown(SamzaApplicationState.SamzaAppStatus.FAILED);\r\n    verify(asyncClient, times(1)).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    verify(asyncClient, times(1)).stop();\r\n    verify(asyncNMClient, times(1)).stop();\r\n    verify(service, times(1)).onShutdown();\r\n    verify(metrics, times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMShutdownThrowingExceptionOnRMCallback",
  "sourceCode" : "@Test\r\npublic void testAMShutdownThrowingExceptionOnRMCallback() throws IOException, YarnException {\r\n    // create mocks\r\n    SamzaYarnAppMasterLifecycle lifecycle = Mockito.spy(new SamzaYarnAppMasterLifecycle(512, 2, mock(SamzaApplicationState.class), yarnAppState, asyncClient, false));\r\n    doThrow(InvalidApplicationMasterRequestException.class).when(asyncClient).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.onShutdownRequest();\r\n    verify(lifecycle, times(1)).onShutdown(SamzaApplicationState.SamzaAppStatus.FAILED);\r\n    verify(asyncClient, times(1)).unregisterApplicationMaster(FinalApplicationStatus.FAILED, null, null);\r\n    verify(asyncClient, times(1)).stop();\r\n    verify(asyncNMClient, times(1)).stop();\r\n    verify(service, times(1)).onShutdown();\r\n    verify(metrics, times(1)).stop();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testAMHACallbackInvokedForPreviousAttemptContainers",
  "sourceCode" : "@Test\r\npublic void testAMHACallbackInvokedForPreviousAttemptContainers() {\r\n    String previousAttemptContainerId = \"0\";\r\n    String previousAttemptYarnContainerId = \"container_1607304997422_0008_02_000002\";\r\n    // create mocks\r\n    YarnAppState yarnAppState = Mockito.spy(new YarnAppState(0, mock(ContainerId.class), \"host\", 8080, 8081));\r\n    ContainerId containerId = mock(ContainerId.class);\r\n    when(containerId.toString()).thenReturn(previousAttemptYarnContainerId);\r\n    YarnContainer yarnContainer = mock(YarnContainer.class);\r\n    Resource resource = mock(Resource.class);\r\n    when(resource.getMemory()).thenReturn(1024);\r\n    Mockito.when(resource.getVirtualCores()).thenReturn(1);\r\n    Mockito.when(yarnContainer.resource()).thenReturn(resource);\r\n    Mockito.when(yarnContainer.id()).thenReturn(containerId);\r\n    NodeId nodeId = mock(NodeId.class);\r\n    when(nodeId.getHost()).thenReturn(\"host\");\r\n    when(yarnContainer.nodeId()).thenReturn(nodeId);\r\n    yarnAppState.pendingProcessors.put(previousAttemptContainerId, yarnContainer);\r\n    Set<ContainerId> previousAttemptContainers = new HashSet<>();\r\n    previousAttemptContainers.add(containerId);\r\n    when(lifecycle.onInit()).thenReturn(previousAttemptContainers);\r\n    Map<String, String> configMap = new HashMap<>();\r\n    configMap.put(JobConfig.YARN_AM_HIGH_AVAILABILITY_ENABLED, \"true\");\r\n    Config config = new MapConfig(configMap);\r\n    // start the cluster manager\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    yarnClusterResourceManager.start();\r\n    verify(lifecycle).onInit();\r\n    ArgumentCaptor<SamzaResource> samzaResourceArgumentCaptor = ArgumentCaptor.forClass(SamzaResource.class);\r\n    verify(callback).onStreamProcessorLaunchSuccess(samzaResourceArgumentCaptor.capture());\r\n    ArgumentCaptor<Integer> containerFromPreviousAttemptCaptor = ArgumentCaptor.forClass(Integer.class);\r\n    verify(metrics).setContainersFromPreviousAttempts(containerFromPreviousAttemptCaptor.capture());\r\n    SamzaResource samzaResource = samzaResourceArgumentCaptor.getValue();\r\n    assertEquals(previousAttemptYarnContainerId, samzaResource.getContainerId());\r\n    assertEquals(1, containerFromPreviousAttemptCaptor.getValue().intValue());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testStopStreamProcessorForContainerFromPreviousAttempt",
  "sourceCode" : "@Test\r\npublic void testStopStreamProcessorForContainerFromPreviousAttempt() {\r\n    String containerId = \"Yarn_Container_id_0\";\r\n    String processorId = \"Container_id_0\";\r\n    YarnContainer runningYarnContainer = mock(YarnContainer.class);\r\n    ContainerId previousRunningContainerId = mock(ContainerId.class);\r\n    YarnAppState yarnAppState = Mockito.spy(new YarnAppState(0, mock(ContainerId.class), \"host\", 8080, 8081));\r\n    yarnAppState.runningProcessors.put(processorId, runningYarnContainer);\r\n    when(runningYarnContainer.id()).thenReturn(previousRunningContainerId);\r\n    when(previousRunningContainerId.toString()).thenReturn(containerId);\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource containerResourceFromPreviousRun = mock(SamzaResource.class);\r\n    when(containerResourceFromPreviousRun.getContainerId()).thenReturn(containerId);\r\n    yarnClusterResourceManager.stopStreamProcessor(containerResourceFromPreviousRun);\r\n    verify(asyncClient, times(1)).releaseAssignedContainer(previousRunningContainerId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnClusterResourceManager.java",
  "methodName" : "testStopStreamProcessorForContainerStartedInCurrentLifecycle",
  "sourceCode" : "@Test\r\npublic void testStopStreamProcessorForContainerStartedInCurrentLifecycle() {\r\n    YarnClusterResourceManager yarnClusterResourceManager = new YarnClusterResourceManager(asyncClient, asyncNMClient, callback, yarnAppState, lifecycle, service, metrics, yarnConfiguration, config);\r\n    SamzaResource allocatedContainerResource = mock(SamzaResource.class);\r\n    Container runningContainer = mock(Container.class);\r\n    ContainerId runningContainerId = mock(ContainerId.class);\r\n    NodeId runningNodeId = mock(NodeId.class);\r\n    when(runningContainer.getId()).thenReturn(runningContainerId);\r\n    when(runningContainer.getNodeId()).thenReturn(runningNodeId);\r\n    yarnClusterResourceManager.getAllocatedResources().put(allocatedContainerResource, runningContainer);\r\n    yarnClusterResourceManager.stopStreamProcessor(allocatedContainerResource);\r\n    verify(asyncNMClient, times(1)).stopContainerAsync(runningContainerId, runningNodeId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testGetFaultDomainOfHostWhichExistsInCache",
  "sourceCode" : "@Test\r\npublic void testGetFaultDomainOfHostWhichExistsInCache() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    Set<FaultDomain> expectedFaultDomainSet = new HashSet<>();\r\n    expectedFaultDomainSet.add(new FaultDomain(FaultDomainType.RACK, rackName1));\r\n    Set<FaultDomain> actualFaultDomainSet = yarnFaultDomainManager.getFaultDomainsForHost(hostName3);\r\n    assertNotNull(actualFaultDomainSet);\r\n    assertEquals(expectedFaultDomainSet.iterator().next(), actualFaultDomainSet.iterator().next());\r\n    verify(mockCounter, times(0)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testGetFaultDomainOfHostWhichDoesNotExistInCache",
  "sourceCode" : "@Test\r\npublic void testGetFaultDomainOfHostWhichDoesNotExistInCache() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    Set<FaultDomain> expectedFaultDomainSet = new HashSet<>();\r\n    expectedFaultDomainSet.add(new FaultDomain(FaultDomainType.RACK, rackName1));\r\n    List<NodeReport> updatedNodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5, nodeReport6);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(updatedNodeReport);\r\n    Set<FaultDomain> actualFaultDomainSet = yarnFaultDomainManager.getFaultDomainsForHost(hostName6);\r\n    assertNotNull(actualFaultDomainSet);\r\n    assertEquals(expectedFaultDomainSet.iterator().next(), actualFaultDomainSet.iterator().next());\r\n    verify(mockCounter, times(1)).inc();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenTrue",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenTrue() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName3);\r\n    assertTrue(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenFalse",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenFalse() {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName2);\r\n    assertFalse(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testHasSameFaultDomainsWhenHostDoesNotExistInCache",
  "sourceCode" : "@Test\r\npublic void testHasSameFaultDomainsWhenHostDoesNotExistInCache() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, hostToRackMap);\r\n    List<NodeReport> updatedNodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5, nodeReport6);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(updatedNodeReport);\r\n    boolean result = yarnFaultDomainManager.hasSameFaultDomains(hostName1, hostName6);\r\n    assertTrue(result);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnFaultDomainManager.java",
  "methodName" : "testComputeHostToFaultDomainMap",
  "sourceCode" : "@Test\r\npublic void testComputeHostToFaultDomainMap() throws IOException, YarnException {\r\n    YarnFaultDomainManager yarnFaultDomainManager = new YarnFaultDomainManager(mockMetricsRegistry, yarnClient, null);\r\n    List<NodeReport> nodeReport = ImmutableList.of(nodeReport1, nodeReport2, nodeReport3, nodeReport4, nodeReport5);\r\n    when(yarnClient.getNodeReports(NodeState.RUNNING)).thenReturn(nodeReport);\r\n    Multimap<String, FaultDomain> hostToRackMap = yarnFaultDomainManager.computeHostToFaultDomainMap();\r\n    assertEquals(this.hostToRackMap.size(), hostToRackMap.size());\r\n    assertEquals(this.hostToRackMap.keySet(), hostToRackMap.keySet());\r\n    Iterator<FaultDomain> expectedValues = this.hostToRackMap.values().iterator();\r\n    Iterator<FaultDomain> computedValues = hostToRackMap.values().iterator();\r\n    expectedValues.forEachRemaining(expectedRack -> assertFaultDomainEquals(expectedRack, computedValues.next()));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildEnvironment",
  "sourceCode" : "@Test\r\npublic void testBuildEnvironment() throws IOException {\r\n    String amJvmOptions = \"-Xmx1g -Dconfig.key='config value'\";\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.JOB_COORDINATOR_SYSTEM, \"jobCoordinatorSystem\").put(YarnConfig.AM_JVM_OPTIONS, // needs escaping\r\n    amJvmOptions).build());\r\n    String expectedCoordinatorStreamConfigStringValue = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(CoordinatorStreamUtil.buildCoordinatorStreamConfig(config)));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_COORDINATOR_SYSTEM_CONFIG, expectedCoordinatorStreamConfigStringValue, ShellCommandConfig.ENV_JAVA_OPTS, Util.envVarEscape(amJvmOptions), ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildEnvironmentWithAMJavaHome",
  "sourceCode" : "@Test\r\npublic void testBuildEnvironmentWithAMJavaHome() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.JOB_COORDINATOR_SYSTEM, \"jobCoordinatorSystem\").put(YarnConfig.AM_JVM_OPTIONS, \"\").put(YarnConfig.AM_JAVA_HOME, \"/some/path/to/java/home\").build());\r\n    String expectedCoordinatorStreamConfigStringValue = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(CoordinatorStreamUtil.buildCoordinatorStreamConfig(config)));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_COORDINATOR_SYSTEM_CONFIG, expectedCoordinatorStreamConfigStringValue, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_JAVA_HOME, \"/some/path/to/java/home\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildJobSubmissionEnvironment",
  "sourceCode" : "@Test\r\npublic void testBuildJobSubmissionEnvironment() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\").put(YarnConfig.AM_JVM_OPTIONS, \"\").build());\r\n    String expectedSubmissionConfig = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(config));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_SUBMISSION_CONFIG, expectedSubmissionConfig, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJob.java",
  "methodName" : "testBuildJobWithAdditionalClassPath",
  "sourceCode" : "@Test\r\npublic void testBuildJobWithAdditionalClassPath() throws IOException {\r\n    Config config = new MapConfig(new ImmutableMap.Builder<String, String>().put(JobConfig.JOB_NAME, \"jobName\").put(JobConfig.JOB_ID, \"jobId\").put(JobConfig.CONFIG_LOADER_FACTORY, \"org.apache.samza.config.loaders.PropertiesConfigLoaderFactory\").put(YarnConfig.AM_JVM_OPTIONS, \"\").put(ShellCommandConfig.ADDITIONAL_CLASSPATH_DIR, \"./sqlapp/lib/*\").build());\r\n    String expectedSubmissionConfig = Util.envVarEscape(SamzaObjectMapper.getObjectMapper().writeValueAsString(config));\r\n    Map<String, String> expected = ImmutableMap.of(ShellCommandConfig.ENV_SUBMISSION_CONFIG, expectedSubmissionConfig, ShellCommandConfig.ENV_JAVA_OPTS, \"\", ShellCommandConfig.ENV_ADDITIONAL_CLASSPATH_DIR, \"./sqlapp/lib/*\");\r\n    assertEquals(expected, JavaConverters.mapAsJavaMapConverter(YarnJob$.MODULE$.buildEnvironment(config, new YarnConfig(config), new JobConfig(config))).asJava());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithDefaultFsImpl",
  "sourceCode" : "@Test\r\npublic void testGetJobWithDefaultFsImpl() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig());\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(HttpFileSystem.class.getName(), hConfig.get(\"fs.http.impl\"));\r\n    assertEquals(HttpFileSystem.class.getName(), hConfig.get(\"fs.https.impl\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithFsImplOverride",
  "sourceCode" : "@Test\r\npublic void testGetJobWithFsImplOverride() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig(ImmutableMap.of(\"fs.http.impl\", \"org.apache.myHttp\", \"fs.myscheme.impl\", \"org.apache.myScheme\")));\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(\"org.apache.myHttp\", hConfig.get(\"fs.http.impl\"));\r\n    assertEquals(\"org.apache.myScheme\", hConfig.get(\"fs.myscheme.impl\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\TestYarnJobFactory.java",
  "methodName" : "testGetJobWithFsImplSubkeys",
  "sourceCode" : "@Test\r\npublic void testGetJobWithFsImplSubkeys() {\r\n    YarnJobFactory jobFactory = new YarnJobFactory();\r\n    YarnJob yarnJob = jobFactory.getJob(new MapConfig(ImmutableMap.of(\"fs.myscheme.impl\", \"org.apache.myScheme\", \"fs.myscheme.impl.client\", \"org.apache.mySchemeClient\")));\r\n    Configuration hConfig = yarnJob.client().yarnClient().getConfig();\r\n    assertEquals(\"org.apache.myScheme\", hConfig.get(\"fs.myscheme.impl\"));\r\n    assertEquals(\"org.apache.mySchemeClient\", hConfig.get(\"fs.myscheme.impl.client\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\job\\yarn\\util\\hadoop\\TestHttpFileSystem.java",
  "methodName" : "testHttpFileSystemReadTimeouts",
  "sourceCode" : "@Test\r\npublic void testHttpFileSystemReadTimeouts() throws Exception {\r\n    HttpServer server = new HttpServer(\"/\", 0, null, new ServletHolder(DefaultServlet.class));\r\n    try {\r\n        server.addServlet(\"/download\", new PartialFileFetchServlet());\r\n        server.start();\r\n        String serverUrl = server.getUrl().toString() + \"download\";\r\n        FileSystemClientThread fileSystemClientThread = new FileSystemClientThread(new URI(serverUrl));\r\n        fileSystemClientThread.start();\r\n        fileSystemClientThread.join();\r\n        Assert.assertEquals(fileSystemClientThread.getTotalBytesRead(), THRESHOLD_BYTES);\r\n        Assert.assertNull(clientException);\r\n        Assert.assertNull(serverException);\r\n    } finally {\r\n        server.stop();\r\n    }\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateAppId",
  "sourceCode" : "@Test\r\npublic void testValidateAppId() throws Exception {\r\n    ApplicationReport appReport = mock(ApplicationReport.class);\r\n    when(appReport.getName()).thenReturn(jobName + \"_\" + jobId);\r\n    when(appReport.getApplicationId()).thenReturn(appId);\r\n    when(client.getApplications()).thenReturn(Collections.singletonList(appReport));\r\n    assertTrue(tool.validateAppId().equals(appId));\r\n    when(appReport.getName()).thenReturn(\"dummy\");\r\n    exception.expect(SamzaException.class);\r\n    tool.validateAppId();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateRunningAttemptId",
  "sourceCode" : "@Test\r\npublic void testValidateRunningAttemptId() throws Exception {\r\n    ApplicationReport appReport = mock(ApplicationReport.class);\r\n    when(client.getApplicationReport(appId)).thenReturn(appReport);\r\n    when(appReport.getCurrentApplicationAttemptId()).thenReturn(attemptId);\r\n    ApplicationAttemptReport attemptReport = mock(ApplicationAttemptReport.class);\r\n    when(attemptReport.getYarnApplicationAttemptState()).thenReturn(YarnApplicationAttemptState.RUNNING);\r\n    when(attemptReport.getApplicationAttemptId()).thenReturn(attemptId);\r\n    when(client.getApplicationAttemptReport(attemptId)).thenReturn(attemptReport);\r\n    assertTrue(tool.validateRunningAttemptId(appId).equals(attemptId));\r\n    when(attemptReport.getYarnApplicationAttemptState()).thenReturn(YarnApplicationAttemptState.FAILED);\r\n    exception.expect(SamzaException.class);\r\n    tool.validateRunningAttemptId(appId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateContainerCount",
  "sourceCode" : "@Test\r\npublic void testValidateContainerCount() throws Exception {\r\n    List<ContainerReport> containerReports = new ArrayList<>();\r\n    for (int i = 0; i <= containerCount; i++) {\r\n        ContainerReport report = mock(ContainerReport.class);\r\n        when(report.getContainerState()).thenReturn(ContainerState.RUNNING);\r\n        containerReports.add(report);\r\n    }\r\n    when(client.getContainers(attemptId)).thenReturn(containerReports);\r\n    assertTrue(tool.validateContainerCount(attemptId) == (containerCount + 1));\r\n    containerReports.remove(0);\r\n    exception.expect(SamzaException.class);\r\n    tool.validateContainerCount(attemptId);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\validation\\TestYarnJobValidationTool.java",
  "methodName" : "testValidateJmxMetrics",
  "sourceCode" : "@Test\r\npublic void testValidateJmxMetrics() throws MetricsValidationFailureException {\r\n    JmxMetricsAccessor jmxMetricsAccessor = mock(JmxMetricsAccessor.class);\r\n    Map<String, Long> values = new HashMap<>();\r\n    values.put(\"samza-container-0\", 100L);\r\n    when(jmxMetricsAccessor.getCounterValues(SamzaContainerMetrics.class.getName(), \"commit-calls\")).thenReturn(values);\r\n    validator.validate(jmxMetricsAccessor);\r\n    values.put(\"samza-container-0\", -1L);\r\n    // the mock validator will fail if the commit-calls are less than or equal to 0\r\n    exception.expect(MetricsValidationFailureException.class);\r\n    validator.validate(jmxMetricsAccessor);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetMetricsSuccess",
  "sourceCode" : "@Test\r\npublic void testGetMetricsSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    MetricsRegistryMap registry = new MetricsRegistryMap();\r\n    assignMetricValues(samzaAppState, registry);\r\n    String response = ApplicationMasterRestServlet.getMetrics(jsonMapper, registry);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Map<String, Object>> metricsResult = client.getMetrics();\r\n    String group = SamzaAppMasterMetrics.class.getCanonicalName();\r\n    assertEquals(1, metricsResult.size());\r\n    assertTrue(metricsResult.containsKey(group));\r\n    Map<String, Object> amMetricsGroup = metricsResult.get(group);\r\n    assertEquals(8, amMetricsGroup.size());\r\n    assertEquals(samzaAppState.runningProcessors.size(), amMetricsGroup.get(\"running-containers\"));\r\n    assertEquals(samzaAppState.neededProcessors.get(), amMetricsGroup.get(\"needed-containers\"));\r\n    assertEquals(samzaAppState.completedProcessors.get(), amMetricsGroup.get(\"completed-containers\"));\r\n    assertEquals(samzaAppState.failedContainers.get(), amMetricsGroup.get(\"failed-containers\"));\r\n    assertEquals(samzaAppState.releasedContainers.get(), amMetricsGroup.get(\"released-containers\"));\r\n    assertEquals(samzaAppState.processorCount.get(), amMetricsGroup.get(\"container-count\"));\r\n    assertEquals(samzaAppState.jobHealthy.get() ? 1 : 0, amMetricsGroup.get(\"job-healthy\"));\r\n    assertEquals(0, amMetricsGroup.get(\"container-from-previous-attempt\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetMetricsError",
  "sourceCode" : "@Test\r\npublic void testGetMetricsError() throws IOException {\r\n    setupErrorTest(\"metrics\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getMetrics();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetTaskContextSuccess",
  "sourceCode" : "@Test\r\npublic void testGetTaskContextSuccess() throws IOException {\r\n    ContainerId containerId = ConverterUtils.toContainerId(YARN_CONTAINER_ID_1);\r\n    YarnAppState yarnAppState = createYarnAppState(containerId);\r\n    String response = ApplicationMasterRestServlet.getTaskContext(jsonMapper, yarnAppState);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> taskContextResult = client.getTaskContext();\r\n    assertEquals(2, taskContextResult.size());\r\n    assertEquals(2, taskContextResult.get(\"task-id\"));\r\n    assertEquals(containerId.toString(), taskContextResult.get(\"name\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testTaskContextError",
  "sourceCode" : "@Test\r\npublic void testTaskContextError() throws IOException {\r\n    setupErrorTest(\"task context\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getTaskContext();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetAmStateSuccess",
  "sourceCode" : "@Test\r\npublic void testGetAmStateSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    ApplicationAttemptId attemptId = ConverterUtils.toApplicationAttemptId(APP_ATTEMPT_ID);\r\n    ContainerId containerId = ConverterUtils.toContainerId(YARN_CONTAINER_ID_1);\r\n    YarnAppState yarnAppState = createYarnAppState(containerId);\r\n    String response = ApplicationMasterRestServlet.getAmState(jsonMapper, samzaAppState, yarnAppState);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> amStateResult = client.getAmState();\r\n    assertEquals(4, amStateResult.size());\r\n    assertEquals(String.format(\"%s:%s\", yarnAppState.nodeHost, yarnAppState.rpcUrl.getPort()), amStateResult.get(\"host\"));\r\n    assertEquals(containerId.toString(), amStateResult.get(\"container-id\"));\r\n    // Can only validate the keys because up-time changes everytime it's requested\r\n    assertEquals(buildExpectedContainerResponse(yarnAppState.runningProcessors, samzaAppState).keySet(), ((Map<String, Object>) amStateResult.get(\"containers\")).keySet());\r\n    assertEquals(attemptId.toString(), amStateResult.get(\"app-attempt-id\"));\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetAmStateError",
  "sourceCode" : "@Test\r\npublic void testGetAmStateError() throws IOException {\r\n    setupErrorTest(\"AM state\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getAmState();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetConfigSuccess",
  "sourceCode" : "@Test\r\npublic void testGetConfigSuccess() throws IOException {\r\n    SamzaApplicationState samzaAppState = createSamzaApplicationState();\r\n    Map<String, String> configMap = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\r\n    Config config = new MapConfig(configMap);\r\n    String response = ApplicationMasterRestServlet.getConfig(jsonMapper, config);\r\n    setupMockClientResponse(HttpStatus.SC_OK, \"Success\", response);\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    Map<String, Object> configResult = client.getConfig();\r\n    assertEquals(configMap, configResult);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testGetConfigError",
  "sourceCode" : "@Test\r\npublic void testGetConfigError() throws IOException {\r\n    setupErrorTest(\"config\");\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.getConfig();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestApplicationMasterRestClient.java",
  "methodName" : "testCloseMethodClosesHttpClient",
  "sourceCode" : "@Test\r\npublic void testCloseMethodClosesHttpClient() throws IOException {\r\n    ApplicationMasterRestClient client = new ApplicationMasterRestClient(mockClient, AM_HOST_NAME, AM_RPC_PORT);\r\n    client.close();\r\n    verify(mockClient).close();\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadContainerLocality",
  "sourceCode" : "@Test\r\npublic void testReadContainerLocality() throws Exception {\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality\");\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    LocalityModel locality = mapper.readValue(response, LocalityModel.class);\r\n    assertEquals(\"Expected locality for two containers\", 2, locality.getProcessorLocalities().size());\r\n    assertEquals(\"Mismatch in locality for processor \" + PROCESSOR_ID1, locality.getProcessorLocality(PROCESSOR_ID1), PROCESSOR_1_LOCALITY);\r\n    assertEquals(\"Mismatch in locality for processor \" + PROCESSOR_ID2, locality.getProcessorLocality(PROCESSOR_ID2), PROCESSOR_2_LOCALITY);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadContainerLocalityWithNoLocality",
  "sourceCode" : "@Test\r\npublic void testReadContainerLocalityWithNoLocality() throws Exception {\r\n    final LocalityModel expectedLocality = new LocalityModel(Collections.emptyMap());\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality\");\r\n    when(localityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of()));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    LocalityModel locality = mapper.readValue(response, LocalityModel.class);\r\n    assertEquals(\"Expected empty response but got \" + locality, locality, expectedLocality);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadProcessorLocality",
  "sourceCode" : "@Test\r\npublic void testReadProcessorLocality() throws Exception {\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality?processorId=\" + PROCESSOR_ID1);\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    assertEquals(\"Mismatch in the locality for processor \" + PROCESSOR_ID1, mapper.readValue(response, ProcessorLocality.class), PROCESSOR_1_LOCALITY);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestLocalityServlet.java",
  "methodName" : "testReadProcessorLocalityWithNoLocality",
  "sourceCode" : "@Test\r\npublic void testReadProcessorLocalityWithNoLocality() throws Exception {\r\n    final ProcessorLocality expectedProcessorLocality = new ProcessorLocality(PROCESSOR_ID2, \"\");\r\n    URL url = new URL(webApp.getUrl().toString() + \"locality?processorId=\" + PROCESSOR_ID2);\r\n    when(localityManager.readLocality()).thenReturn(new LocalityModel(ImmutableMap.of()));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    ProcessorLocality processorLocality = mapper.readValue(response, ProcessorLocality.class);\r\n    assertEquals(\"Expected empty response for processor locality \" + PROCESSOR_ID2 + \" but got \" + processorLocality, processorLocality, expectedProcessorLocality);\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestYarnContainerHeartbeatServlet.java",
  "methodName" : "testContainerHeartbeatWhenValid",
  "sourceCode" : "@Test\r\npublic void testContainerHeartbeatWhenValid() throws IOException {\r\n    String validContainerId = \"container_1350670447861_0003_01_000002\";\r\n    when(container.id()).thenReturn(ConverterUtils.toContainerId(validContainerId));\r\n    yarnAppState.runningProcessors.put(validContainerId, container);\r\n    URL url = new URL(String.format(CoordinationConstants.YARN_CONTAINER_HEARTBEAT_ENDPOINT_FORMAT, webApp.getUrl().toString(), validContainerId));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    heartbeat = mapper.readValue(response, ContainerHeartbeatResponse.class);\r\n    Assert.assertTrue(heartbeat.isAlive());\r\n}",
  "annotations" : [ "Test" ]
}, {
  "filePath" : "d:\\chenhao\\test project\\samza\\samza-yarn3\\src\\test\\java\\org\\apache\\samza\\webapp\\TestYarnContainerHeartbeatServlet.java",
  "methodName" : "testContainerHeartbeatWhenInvalid",
  "sourceCode" : "@Test\r\npublic void testContainerHeartbeatWhenInvalid() throws IOException {\r\n    String validContainerId = \"container_1350670447861_0003_01_000003\";\r\n    String invalidContainerId = \"container_1350670447861_0003_01_000002\";\r\n    when(container.id()).thenReturn(ConverterUtils.toContainerId(validContainerId));\r\n    yarnAppState.runningProcessors.put(validContainerId, container);\r\n    URL url = new URL(String.format(CoordinationConstants.YARN_CONTAINER_HEARTBEAT_ENDPOINT_FORMAT, webApp.getUrl().toString(), invalidContainerId));\r\n    String response = HttpUtil.read(url, 1000, new ExponentialSleepStrategy());\r\n    heartbeat = mapper.readValue(response, ContainerHeartbeatResponse.class);\r\n    Assert.assertFalse(heartbeat.isAlive());\r\n}",
  "annotations" : [ "Test" ]
} ]